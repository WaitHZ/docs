{
    "pass": false,
    "messages": [
        {
            "role": "user",
            "content": "I've been learning about large-scale language models recently, and I've decided to train a decoder-only language model on my own. The first step is to prepare pre-training data, so I need your help organizing the pre-training data for llama and gpt-neo into the `ptdata` sheet in `LLM Pre-training Data` spreadsheet. The pre-training data should be sorted in descending order by data size, with columns named `name`, `use in llm` (the value should only include llama and gpt-neo; if used by both models, it should be either gpt-neo or llama), `size` (the numeric value in GB, but do not include \"GB\" in cell), and `link` in order. The link to huggingface is preferred; if huggingface doesn't have the dataset, we'll try to provide other links."
        },
        {
            "role": "assistant",
            "content": "I'll help you organize the pre-training data for LLaMA and GPT-Neo models. Let me start by searching for information about the pre-training datasets used by these models.",
            "tool_calls": [
                {
                    "id": "toolu_01WJF5jmg1fHcsdkvsKQxutp",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"LLaMA pre-training datasets data sources size GB\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: How Big Is the Llama Model in GB? Size Explained - BytePlus\nLink: https://www.byteplus.com/en/topic/464557\nSnippet: From the relatively lightweight 7-billion-parameter version to the colossal 405-billion-parameter giant, the storage requirements can range from a few gigabytes ...\nSitelinks: No sitelinks\n\nTitle: Model requirements for training and validation datasets\nLink: https://docs.aws.amazon.com/bedrock/latest/userguide/model-training-validation-requirements.html\nSnippet: Maximum training dataset size, 10 GB. Maximum validation dataset size, 1 GB ... Agents automate tasks by orchestrating interactions between models, data sources ...\nSitelinks: No sitelinks\n\nTitle: What limits the size of the large model, the size of the data set or the ...\nLink: https://www.reddit.com/r/LocalLLaMA/comments/14n8ycb/what_limits_the_size_of_the_large_model_the_size/\nSnippet: The current maximum size of open source models is 65B (maybe 130B?). Or is it not worth increasing the size of the model? Already reached the ceiling?\nSitelinks: No sitelinks\n\nTitle: Pre-training logs, entry 1: Training a smol Llama from scratch - Medium\nLink: https://medium.com/@gupta.aman/pre-training-logs-entry-1-training-a-smol-llama-from-scratch-04e4b5d4c5f7\nSnippet: Essentially, dataset size. Typically, post-training datasets are in the millions or 10s of millions of tokens in size. Compare that to 600B ...\nSitelinks: No sitelinks\n\nTitle: LLM training datasets - Glenn K. Lockwood\nLink: https://www.glennklockwood.com/garden/LLM-training-datasets\nSnippet: We can estimate the size (in GB) of various LLM training datasets: Dataset, Training tokens, Training Bytes (est). Llama-3, > 15 trillion, 60 TB ...\nSitelinks: No sitelinks\n\nTitle: Official Llama 3 META page : r/LocalLLaMA - Reddit\nLink: https://www.reddit.com/r/LocalLLaMA/comments/1c76n8p/official_llama_3_meta_page/\nSnippet: The implementation i've been working on ends up like 7B models are 7 GB in size. ... We've set the pre-training context window to 8K tokens.\nSitelinks: No sitelinks\n\nTitle: Llama (language model) - Wikipedia\nLink: https://en.wikipedia.org/wiki/Llama_(language_model)\nSnippet: Llama models come in different sizes, ranging from 1 billion to 2 trillion parameters. Initially only a foundation model, starting with Llama 2, Meta AI ...\nSitelinks: No sitelinks\n\nTitle: Llama 3.1 - 405B, 70B & 8B with multilinguality and long context\nLink: https://huggingface.co/blog/llama31\nSnippet: The Llama 3.1 release introduces six new open LLM models based on the Llama 3 architecture. They come in three sizes: 8B, 70B, and 405B parameters.\nSitelinks: No sitelinks\n\nTitle: Introducing Meta Llama 3: The most capable openly available LLM ...\nLink: https://ai.meta.com/blog/meta-llama-3/\nSnippet: Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources. Our training dataset is seven times larger ...\nSitelinks: No sitelinks\n\nTitle: Dataset Size after Preprocessing in Pretrainning Tiny Llama #1023\nLink: https://github.com/Lightning-AI/litgpt/issues/1023\nSnippet: You will require 1.1 TB of disk space for Starcoder and 2.5 TB of space for the SlimPajama dataset. is just us rounding up to the next integer ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01WJF5jmg1fHcsdkvsKQxutp"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01TjWBPU5QnoBoJHCFXQiHUs",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"GPT-Neo pre-training datasets EleutherAI data sources size GB\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: EleutherAI/gpt-neo-2.7B - Hugging Face\nLink: https://huggingface.co/EleutherAI/gpt-neo-2.7B\nSnippet: GPT-Neo 2.7B was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model. Training ...\nSitelinks: No sitelinks\n\nTitle: [P] EleutherAI releases 1.3B and 2.7B GPT-3-style models trained ...\nLink: https://www.reddit.com/r/MachineLearning/comments/ma9kaw/p_eleutherai_releases_13b_and_27b_gpt3style/\nSnippet: The GPT-Neo project by EleutherAI has released 1.3B and 2.7B parameter GPT-3-style models. The models are trained on the Pile, a 800 GB curated dataset ...\nSitelinks: No sitelinks\n\nTitle: Eleuther AI NLP Models - Originality.AI\nLink: https://originality.ai/blog/eleutherai-nlp-models\nSnippet: The Pile\u2013 An 825 GB diverse dataset for training large-scale language models. The Models. Here are some of the popular Eleuther Models: GPT-Neo. GPT-Neo was ...\nSitelinks: No sitelinks\n\nTitle: EleutherAI/gpt-neo-1.3B \u00b7 [AUTOMATED] Model Memory ...\nLink: https://huggingface.co/EleutherAI/gpt-neo-1.3B/discussions/9\nSnippet: You will need about {'dtype': 'float16/bfloat16', 'Largest Layer or Residual Group': '196.32 MB', 'Total Size': '2.46 GB', 'Training using Adam': '9.83\nSitelinks: No sitelinks\n\nTitle: EleutherAI Open-Sources Six Billion Parameter GPT-3 Clone GPT-J\nLink: https://www.infoq.com/news/2021/07/eleutherai-gpt-j/\nSnippet: Compared to the 2.7GB GPT-Neo model, GPT-J shows a 125% improvement in training efficiency. In response to concerns about the misuse of its ...\nSitelinks: No sitelinks\n\nTitle: A Catalog of EleutherAI's Large Language Models - Comparison\nLink: https://orkg.org/comparisons/R609226\nSnippet: training corpus. Pile - 840 GB open source text dataset that combines 22 pre existing datasets. Pile corpus, a large-scale curated dataset created by EleutherAI.\nSitelinks: No sitelinks\n\nTitle: GPT-Neo - EleutherAI\nLink: https://researcher2.eleuther.ai/\nSnippet: The Pile is an 825 GiB diverse, open source language modelling dataset consisting of data from 22 high quality sources. It is useful for both training and ...\nSitelinks: No sitelinks\n\nTitle: [N] EleutherAI announces a 20 billion parameter model, GPT-NeoX ...\nLink: https://www.reddit.com/r/MachineLearning/comments/sit4ro/n_eleutherai_announces_a_20_billion_parameter/\nSnippet: A 20 billion parameter model trained using EleutherAI's GPT-NeoX, was announced today. They will publicly release the weights on February 9th, which is a week ...\nSitelinks: No sitelinks\n\nTitle: EleutherAI/gpt-neo: An implementation of model parallel ... - GitHub\nLink: https://github.com/EleutherAI/gpt-neo\nSnippet: An implementation of model & data parallel GPT3-like models using the mesh-tensorflow library. If you're just here to play with our pre-trained models,\nSitelinks: No sitelinks\n\nTitle: A Catalog of EleutherAI's Large Language Models - Comparison\nLink: https://orkg.org/comparisons/R604185\nSnippet: training corpus. Pile corpus, a large-scale curated dataset created by EleutherAI. Pile \u2014 840 GB open source text dataset that combines 22 preexisting datasets.\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01TjWBPU5QnoBoJHCFXQiHUs"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01WAEc5pAmBf8MB6XeGa2DiM",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"\\\"The Pile\\\" dataset size GB huggingface EleutherAI\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: EleutherAI/pile \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/EleutherAI/pile\nSnippet: The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together. Supported Tasks and ...\nSitelinks: No sitelinks\n\nTitle: The Pile (dataset)\nLink: https://en.wikipedia.org/wiki/The_Pile_(dataset)\nSnippet: The Pile is an 886 GB diverse, open-source dataset of English text created as a training dataset for large language models (LLMs).\nSitelinks: No sitelinks\n\nTitle: An 800GB Dataset of Diverse Text for Language Modeling\nLink: https://pile.eleuther.ai/paper.pdf\nSnippet: by L Gao \u00b7 Cited by 1025 \u2014 As we control for dataset size, we emphasize that our evaluation is generous to CC-100 (en), which is about 1/3 the size of the Pile in reality.\nSitelinks: No sitelinks\n\nTitle: The Pile: An 800GB Dataset of Diverse Text for Language ...\nLink: https://ar5iv.labs.arxiv.org/html/2101.00027\nSnippet: An 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets.\nSitelinks: No sitelinks\n\nTitle: EleutherAI/the-pile\nLink: https://github.com/EleutherAI/the-pile\nSnippet: The Pile is a large, diverse, open source language modelling data set that consists of many smaller datasets combined together.\nSitelinks: No sitelinks\n\nTitle: ArmelR/the-pile-splitted \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/ArmelR/the-pile-splitted\nSnippet: The pile is an 800GB dataset of english text designed by EleutherAI to train large-scale language models. The original version of the dataset can be found here.\nSitelinks: No sitelinks\n\nTitle: Researchers build massive AI training dataset using only ...\nLink: https://the-decoder.com/researchers-build-massive-ai-training-dataset-using-only-openly-licensed-sources/\nSnippet: Comma also edged out most benchmarks against The Pile, an 800 GB meta-dataset compiled by EleutherAI in 2020. The Pile is widely used in AI ...\nSitelinks: No sitelinks\n\nTitle: The Pile is a 825 GiB diverse, open-source language ...\nLink: https://news.ycombinator.com/item?id=39631516\nSnippet: The Pile contains code that turns 1 into 2 and code that turns 2 into 3. When you download Maroon 5 from a website, you are creating a dataset ...\nSitelinks: No sitelinks\n\nTitle: The Pile: An 800GB Dataset of Diverse Text for Language ...\nLink: https://arxiv.org/abs/2101.00027\nSnippet: by L Gao \u00b7 2020 \u00b7 Cited by 1025 \u2014 An 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets.\nSitelinks: No sitelinks\n\nTitle: What is the Pile Dataset\nLink: https://www.pickl.ai/blog/pile-dataset/\nSnippet: Summary: The Pile dataset is a massive 800GB open-source text resource created by EleutherAI for training advanced language models.\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01WAEc5pAmBf8MB6XeGa2DiM"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01F892Bx37WuRWNF3cZGNtSi",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"LLaMA pre-training datasets CommonCrawl C4 Wikipedia Books size GB\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: Open-Sourced Training Datasets for Large Language Models (LLMs)\nLink: https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models\nSnippet: The default English Wikipedia dataset contains 19.88 GB of vast examples of complete articles that help with language modeling tasks. It was used to train ...\nSitelinks: No sitelinks\n\nTitle: Llama (language model) - Wikipedia\nLink: https://en.wikipedia.org/wiki/Llama_(language_model)\nSnippet: Llama is a family of large language models (LLMs) released by Meta AI starting in February 2023. The latest version is Llama 4, released in April 2025.\nSitelinks: [{'title': 'Llama.cpp', 'link': 'https://en.wikipedia.org/wiki/Llama.cpp'}, {'title': 'Foundation model', 'link': 'https://en.wikipedia.org/wiki/Foundation_model'}, {'title': 'IBM Granite', 'link': 'https://en.wikipedia.org/wiki/IBM_Granite'}]\n\nTitle: A Glance of Datasets Behind the Popular Generative AIs - Medium\nLink: https://medium.com/@binfengmemphis/how-do-generative-ais-actually-work-part-1-datasets-behind-the-scenes-19bc64bd2b32\nSnippet: C4 dataset is a colossal, cleaned version of Common Crawl's web crawl corpus. It has about 750GB of English language text based on the Common ...\nSitelinks: No sitelinks\n\nTitle: The training data - all 1.2 trillion tokens - can be downloaded by ...\nLink: https://news.ycombinator.com/item?id=35604459\nSnippet: Sizes here are in GB: common_crawl 1341.6166818914935 c4 806.7667234372348 github 212.1786002581939 wikipedia 111.89125544670969 book ...\nSitelinks: No sitelinks\n\nTitle: Common Crawl - Wikipedia\nLink: https://en.wikipedia.org/wiki/Common_Crawl\nSnippet: Common Crawl is a nonprofit 501(c)(3) organization that crawls the web and freely provides its archives and datasets to the public.\nSitelinks: No sitelinks\n\nTitle: What Are The Real Data Sources For Training LLMs? - simplyAI\nLink: https://www.mdfarook.com/what-are-the-real-data-sources-for-training-llms/\nSnippet: The size of all the compressed articles was approximately 22.14 GB without media. ... - LLaMa : 4% from all Books, most likely BookCorpus. ArXiv\nSitelinks: No sitelinks\n\nTitle: 20 Open-Source Datasets for Generative and Agentic AI\nLink: https://www.analyticsvidhya.com/blog/2025/02/open-source-datasets-for-generative-and-agentic-ai/\nSnippet: RedPajama\u20111T is an open-source reproduction of LLaMA's pretraining dataset, consisting of 1.2 trillion tokens from CommonCrawl, Wikipedia, Books ...\nSitelinks: No sitelinks\n\nTitle: Papers Explained 55: LLaMA - Medium\nLink: https://medium.com/dair-ai/papers-explained-55-llama-c4f302809d6b\nSnippet: LLaMA is a collection of foundation language models ranging from 7B to 65B parameters, trained on trillions of tokens using publicly available datasets ...\nSitelinks: No sitelinks\n\nTitle: Large Language Models: Modern Gen4 LLM Overview (LLaMA ...\nLink: https://admantium.com/blog/llm05_gen4_overview/\nSnippet: The result are the LLamA language model family. To train the models, the following sources are used: Common Crawl, C4, Github, wikipedia, books, ...\nSitelinks: No sitelinks\n\nTitle: What's in the RedPajama-Data-1T LLM training set\nLink: https://simonw.substack.com/p/whats-in-the-redpajama-data-1t-llm\nSnippet: They just announced their first release: RedPajama-Data-1T, a 1.2 trillion token dataset modelled on the training data described in the original LLaMA paper.\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01F892Bx37WuRWNF3cZGNtSi"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01PGywDU74ce47HspmLCTdCY",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"RedPajama dataset size GB components CommonCrawl C4 Wikipedia Books ArXiv StackExchange\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: RedPajama: an Open Dataset for Training Large Language Models\nLink: https://arxiv.org/html/2411.12372v1\nSnippet: The pretraining data of the LLaMA training corpus are drawn from seven datasets: English CommonCrawl, C4, GitHub, Wikipedia, Books (Project Gutenberg and Books3) ...\nSitelinks: No sitelinks\n\nTitle: GitHub - togethercomputer/RedPajama-Data\nLink: https://github.com/togethercomputer/RedPajama-Data\nSnippet: RedPajama-V2 is an open dataset for training large language models. The dataset includes over 100B text documents coming from 84 CommonCrawl snapshots.\nSitelinks: No sitelinks\n\nTitle: Papers Explained 299: Red Pajama. RedPajama-V1 ... - Ritvik Rastogi\nLink: https://ritvik19.medium.com/papers-explained-299-red-pajama-4aced4a3ff72\nSnippet: The pretraining data of the LLaMA training corpus are drawn from seven datasets: English CommonCrawl, C4, GitHub, Wikipedia, Books (Project ...\nSitelinks: No sitelinks\n\nTitle: RedPajama: an Open Dataset for Training Large Language Models\nLink: https://arxiv.org/abs/2411.12372\nSnippet: We release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata.\nSitelinks: No sitelinks\n\nTitle: togethercomputer/RedPajama-Data-1T \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T\nSnippet: ... dataset. Dataset, Token Count. Commoncrawl, 878 Billion. C4, 175 Billion. GitHub, 59 Billion. ArXiv, 28 Billion. Wikipedia, 24 Billion. StackExchange, 20 ...\nSitelinks: No sitelinks\n\nTitle: RedPajama: Reproduction of LLaMA with friendly license\nLink: https://news.ycombinator.com/item?id=35600860\nSnippet: Participants in building the RedPajama dataset including [\u2026] LAION. Meta AI \u2014 [\u2026]. EleutherAI \u2014 This project is built on the backs of the great ...\nSitelinks: No sitelinks\n\nTitle: 20 Open-Source Datasets for Generative and Agentic AI\nLink: https://www.analyticsvidhya.com/blog/2025/02/open-source-datasets-for-generative-and-agentic-ai/\nSnippet: RedPajama\u20111T is an open-source reproduction of LLaMA's pretraining dataset, consisting of 1.2 trillion tokens from CommonCrawl, Wikipedia, Books ...\nSitelinks: No sitelinks\n\nTitle: RedPajama, a project to create leading open-source models, starts ...\nLink: https://www.together.ai/blog/redpajama\nSnippet: The RedPajama base dataset ; C4, 175 billion, 190 billion ; Github, 59 billion, 100 billion ; Books, 26 billion, 25 billion ; ArXiv, 28 billion, 33 ...\nSitelinks: No sitelinks\n\nTitle: README.md \u00b7 togethercomputer/RedPajama-Data-1T at main\nLink: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T/blob/main/README.md\nSnippet: RedPajama is a clean-room, fully open-source implementation of the LLaMa dataset. Dataset, Token Count. Commoncrawl, 878 Billion. C4, 175 ...\nSitelinks: No sitelinks\n\nTitle: The Backbone of Large Language Models: Essential Datasets\nLink: https://medium.com/calance-data/the-backbone-of-large-language-models-essential-datasets-20da20784c8f\nSnippet: The total dataset was estimated to contain 570 GB of text data. \u00b7 Llama was also trained on a similar dataset along with C4, GitHub, Arxiv and ...\nSitelinks: [{'title': 'Pre-Training Corpora', 'link': 'https://medium.com/calance-data/the-backbone-of-large-language-models-essential-datasets-20da20784c8f#:~:text=Pre%2Dtraining%20corpora,-The%20pre%2Dtraining%20corpora%20are%20large'}, {'title': \"Get Calance Data's Stories...\", 'link': 'https://medium.com/calance-data/the-backbone-of-large-language-models-essential-datasets-20da20784c8f#:~:text=Get%20Calance%20Data%27s%20stories%20in%20your%20inbox'}, {'title': 'Text Summarization Datasets', 'link': 'https://medium.com/calance-data/the-backbone-of-large-language-models-essential-datasets-20da20784c8f#:~:text=Text%20Summarization%20datasets,-The%20task%20of%20text%20summarization'}]\n",
            "tool_call_id": "toolu_01PGywDU74ce47HspmLCTdCY"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01X7fg2WbNq8zdwFbjJQquvS",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"\\\"Common Crawl\\\" dataset size GB terabytes\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: Common Crawl\nLink: https://en.wikipedia.org/wiki/Common_Crawl\nSnippet: Common Crawl is a nonprofit 501(c)(3) organization that crawls the web and freely provides its archives and datasets to the public.See more\nSitelinks: No sitelinks\n\nTitle: The Increase of Common Crawl Citations in Academic ...\nLink: https://commoncrawl.org/blog/the-increase-of-common-crawl-citations-in-academic-research\nSnippet: The Sloan Digital Sky Survey occupies 5 TB of storage, the Common Crawl Web corpus is 81 TB in size, and the 1000 Genomes Project requires 200 ...See more\nSitelinks: No sitelinks\n\nTitle: You may not grasp just how large the Common Crawl ...\nLink: https://news.ycombinator.com/item?id=26596310\nSnippet: You may not grasp just how large the Common Crawl dataset is. It's been growing steadily at 200-300 TB per month for the last few years.See more\nSitelinks: No sitelinks\n\nTitle: Get Started\nLink: https://commoncrawl.org/get-started\nSnippet: Common Crawl currently stores the crawl data using the Web ARChive (WARC) Format. Previously (prior to Summer 2013) the data was stored in the ARC Format.See more\nSitelinks: No sitelinks\n\nTitle: Exploring Common Crawl Database\nLink: https://medium.com/@alok.g.v/exploring-common-crawl-database-262ab0bfe79a\nSnippet: Each crawl consists of about 3 billion web pages with a compressed size of 100 TB. In Addition to WARC file , Common crawl provides index files ...See more\nSitelinks: No sitelinks\n\nTitle: Common Crawl - Blog\nLink: https://commoncrawl.org/blog\nSnippet: Common Crawl builds and maintains an open repository of web crawl data that can be accessed and analyzed by anyone.See more\nSitelinks: No sitelinks\n\nTitle: Data | CS324\nLink: https://stanford-cs324.github.io/winter2022/lectures/data/\nSnippet: The April 2021 snapshot of Common Crawl has 320 terabytes of data, which is a few orders of magnitude smaller than the Google index. Representation. Despite ...See more\nSitelinks: No sitelinks\n\nTitle: Common Crawl - Open Repository of Web Crawl Data\nLink: https://commoncrawl.org/\nSnippet: Common Crawl maintains a free, open repository of web crawl data that can be used by anyone. ; Free and open corpus since 2007. ; 10,000 research papers. ; 3\u20135 ...See more\nSitelinks: No sitelinks\n\nTitle: Extracting Data from Common Crawl Dataset\nLink: https://blog.qburst.com/2020/07/extracting-data-from-common-crawl-dataset/\nSnippet: The Common Crawl archive for CC-MAIN-2020-05 has 0.017% of Malayalam records, which means around 50 GB of Malayalam HTML raw data could be found ...See more\nSitelinks: No sitelinks\n\nTitle: ChatGPT \u2014 Show me the Data Sources | by Dennis Layton\nLink: https://medium.com/@dlaytonj2/chatgpt-show-me-the-data-sources-11e9433d57e8\nSnippet: The Common Crawl is an open, and free-to-use dataset that contains petabytes of data collected from the web since 2008.See more\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01X7fg2WbNq8zdwFbjJQquvS"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01R8bFWoAJvfXiXjkn4Nu7AT",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"\\\"C4 dataset\\\" size GB \\\"Colossal Clean Crawled Corpus\\\"\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: Data | CS324\nLink: https://stanford-cs324.github.io/winter2022/lectures/data/\nSnippet: The Colossal Clean Crawled Corpus (C4) is a larger was created to train the ... 2021 performed a thorough analysis of the C4 dataset. Documentation ...\nSitelinks: No sitelinks\n\nTitle: brando/small-c4-dataset \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/brando/small-c4-dataset\nSnippet: The Small C4 Dataset is a reduced version of the original C4 dataset (Colossal Clean Crawled Corpus), designed to facilitate lightweight experimentation and ...\nSitelinks: No sitelinks\n\nTitle: The data that those large language models were built on\nLink: https://dataman-ai.medium.com/large-language-model-datasets-95df319a110\nSnippet: Colossal Clean Crawled Corpus. C4 is a dataset consisting of ... This large effort produced the cleaned C4 dataset. The above ...\nSitelinks: No sitelinks\n\nTitle: Exploring the Inclusion of eDiscovery-Centric Resources in ...\nLink: https://complexdiscovery.com/exploring-the-inclusion-of-ediscovery-centric-resources-in-the-google-c4-dataset-a-highly-selective-search/\nSnippet: Exploring the Inclusion of eDiscovery-Centric Resources in the Google C4 Dataset: A Highly Selective Search ... Google's C4 (Colossal Clean Crawled Corpus) ...\nSitelinks: No sitelinks\n\nTitle: Preprocess and fine-tune LLMs quickly and cost-effectively ...\nLink: https://aws.amazon.com/blogs/big-data/preprocess-and-fine-tune-llms-quickly-and-cost-effectively-using-amazon-emr-serverless-and-amazon-sagemaker/\nSnippet: Another important dataset worth mentioning is the C4 dataset. C4, short for Colossal Clean Crawled Corpus, is a dataset derived from ...\nSitelinks: No sitelinks\n\nTitle: Exploring Google's T5 Text-To-Text Transformer Model\nLink: https://wandb.ai/mukilan/T5_transformer/reports/Exploring-Google-s-T5-Text-To-Text-Transformer-Model--VmlldzoyNjkzOTE2\nSnippet: Training dataset: C4 dataset. Colossal Clean Crawled Corpus, C4 is a 750 GB clean English text scraped from the Internet. This dataset was ...\nSitelinks: No sitelinks\n\nTitle: Documenting the English Colossal Clean Crawled Corpus\nLink: https://www.researchgate.net/publication/350991473_Documenting_the_English_Colossal_Clean_Crawled_Corpus\nSnippet: Documenting the English Colossal Clean Crawled Corpus. April 2021. DOI ... (C4) dataset (comprises 156 billion tokens) (Dodge et al., 2021) ...\nSitelinks: No sitelinks\n\nTitle: Exploring Linguistic Proximity in C4 Multilingual Data through ...\nLink: https://juser.fz-juelich.de/record/1041549/files/Bachelorarbeit_Rahmdel_424069.pdf\nSnippet: The dataset used in this thesis is an updated and advanced version of the original Colossal Clean Crawled Corpus (C4) dataset, which is a large- ...\nSitelinks: No sitelinks\n\nTitle: WuDaoCorpora: A super large-scale Chinese corpora for ...\nLink: https://www.sciencedirect.com/science/article/pii/S2666651021000152\nSnippet: by S Yuan \u00b7 2021 \u00b7 Cited by 73 \u2014 ... colossal clean crawled corpus (C4) to train the 1.6-trillion-parameter ... Raffel proposed the C4 dataset (Raffel et al., 2020), a 750 GB English ...\nSitelinks: No sitelinks\n\nTitle: What's in the RedPajama-Data-1T LLM training set\nLink: https://simonw.substack.com/p/whats-in-the-redpajama-data-1t-llm\nSnippet: ... C4 dataset - Colossal Clean Crawled Corpus, a filtered version of Common Crawl that's often used for training large language models. They include a neat ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01R8bFWoAJvfXiXjkn4Nu7AT"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01Cn3ATAeFf8Lqc8UkrADyC2",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"Wikipedia dataset size GB English \\\"text dataset\\\"\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: README.md \u00b7 Team-PIXEL/rendered-wikipedia-english at ...\nLink: https://huggingface.co/datasets/Team-PIXEL/rendered-wikipedia-english/blob/504638a427b89c21bd99c1d1307e726f746e8231/README.md\nSnippet: This dataset contains the full English Wikipedia from February 1, 2018, rendered into images of 16x8464 resolution. The original text dataset ...\nSitelinks: No sitelinks\n\nTitle: jhu-clsp/megawika-2 \u00b7 Datasets at ...\nLink: https://huggingface.co/datasets/jhu-clsp/megawika-2\nSnippet: MegaWika 2 is an improved multilingual text dataset containing a structured view of Wikipedia articles, the web sources they cite, source text quality ...\nSitelinks: No sitelinks\n\nTitle: Wikipedia:Wikipedia Signpost/2025-06-24/Recent research\nLink: https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2025-06-24/Recent_research\nSnippet: (At the time of writing, Hugging Face listed it as the 13th most downloaded text dataset.) ... GB out of a total size of 1838.3 GB, i.e. about 3.1%. (On a ...\nSitelinks: No sitelinks\n\nTitle: WIT: Wikipedia-based Image Text Dataset for Multimodal ...\nLink: https://arxiv.org/pdf/2103.01913\nSnippet: by K Srinivasan \u00b7 2021 \u00b7 Cited by 402 \u2014 3 WIT: WIKIPEDIA IMAGE TEXT DATASET. The existing multimodal datasets can be classified into two cate- gories based on the type of creation ...\nSitelinks: No sitelinks\n\nTitle: 785 Million Language Translation Database for AI\nLink: https://www.kaggle.com/datasets/ramakrishnan1984/785-million-language-translation-database-ai-ml\nSnippet: Size of the dataset \u2013 41GB(Uncompressed) and Compressed \u2013 20GB. Key Features ... Tamil (Tamizh) Wikipedia Text Dataset for NLP. more_vert. Younus_Mohamed ...\nSitelinks: No sitelinks\n\nTitle: WIT: Wikipedia-based Image Text Dataset for Multimodal ...\nLink: https://marc.najork.org/papers/sigir2021.pdf\nSnippet: by K Srinivasan \u00b7 2021 \u00b7 Cited by 405 \u2014 3 WIT: WIKIPEDIA IMAGE TEXT DATASET. We would like to marry the benefits of curated datasets like Flickr30K and MS-COCO (consistent, high quality image text ...\nSitelinks: No sitelinks\n\nTitle: Wikipedia:Wikipedia Signpost/2023-11-20/Recent research\nLink: https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-11-20/Recent_research\nSnippet: \"WikiTableT: A Large-Scale Data-to-Text Dataset for Generating Wikipedia Article Sections\". From the \"Conclusion\" section: \"We created WIKITABLET, a ...\nSitelinks: No sitelinks\n\nTitle: 33 Top NLP Datasets to Boost Your Machine Learning ...\nLink: https://www.shaip.com/blog/15-nlp-dataset-for-nlp/\nSnippet: Text Dataset. The Wiki QA Corpus (Link). Created to help the open ... The 2000 HUB5 English dataset has 40 telephone conversation transcripts in ...\nSitelinks: No sitelinks\n\nTitle: The Pile: An 800GB Dataset of Diverse Text for Language ...\nLink: https://ar5iv.labs.arxiv.org/html/2101.00027\nSnippet: To address this need, we introduce the Pile: a 825.18 825.18 825.18 GiB English text dataset designed for training large scale language models. ... 2.10 Wikipedia ...\nSitelinks: No sitelinks\n\nTitle: List of datasets in computer vision and image processing\nLink: https://en.wikipedia.org/wiki/List_of_datasets_in_computer_vision_and_image_processing\nSnippet: Wikipedia-based Image Text Dataset, 37.5 million image-text examples with 11.5 million unique images across 108 Wikipedia languages. ; Visual Genome, Images and ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01Cn3ATAeFf8Lqc8UkrADyC2"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01Q6YiKwApu9xCZDxjFwjq63",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"\\\"Books3\\\" dataset size GB \\\"Project Gutenberg\\\" books\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: The value of books in the age of generative AI training data\nLink: https://journals.sagepub.com/doi/10.1177/13548565251358020?int.sj-full-text.similar-articles.6\nSnippet: Books3 was a compilation of around 190 thousand plain text files extracted from EPUB files (a popular ebook format), pre-packaged for use as ...\nSitelinks: No sitelinks\n\nTitle: Inside the Great AI Data Grab \u2014 Comprehensive Analysis ...\nLink: https://medium.com/@adnanmasood/inside-the-great-ai-data-grab-comprehensive-analysis-of-public-and-proprietary-corpora-utilised-49b4770abc47\nSnippet: Some from public domain (Project Gutenberg) and others from \u201cshadow libraries\u201d (e.g. Books3 from Bibliotik). Provenance in official disclosures ...\nSitelinks: No sitelinks\n\nTitle: (final) Authors Guild Comments - AI Study (10-30-23)\nLink: https://authorsguild.org/app/uploads/2023/10/Authors-Guild-Comments-AI-and-Copyright-October-30-2023.pdf\nSnippet: The Pile, an 800GB publicly available dataset, contains. 10.88GB of books from Project Gutenberg (and 100.96GB of pirated books in the Books3 ...\nSitelinks: No sitelinks\n\nTitle: An 800GB Dataset of Diverse Text for Language Modeling\nLink: https://pile.eleuther.ai/paper.pdf\nSnippet: by L Gao \u00b7 Cited by 1025 \u2014 Books3 is a dataset of books derived from a copy of the contents ... with Project Gutenberg and Books3, which consist of published books.\nSitelinks: No sitelinks\n\nTitle: A Standardized Project Gutenberg Corpus for Statistical ...\nLink: https://www.researchgate.net/publication/338722669_A_Standardized_Project_Gutenberg_Corpus_for_Statistical_Analysis_of_Natural_Language_and_Quantitative_Linguistics\nSnippet: In fact, new books are added to Project Gutenberg daily. An ... Controversies around AI companies' use of pirated book collections, including Books3 ...\nSitelinks: No sitelinks\n\nTitle: Navigating the Open-Source AI Landscape: Data, Funding ...\nLink: https://www.lesswrong.com/posts/69CRFgqbQyFBoYcg5/navigating-the-open-source-ai-landscape-data-funding-and\nSnippet: Prose: Books3 (Bibliotik, e-books), Project Gutenberg (PG-19, classic books), and BookCorpus2 (books written by \u201cas of yet unpublished authors\u201d) ...\nSitelinks: No sitelinks\n\nTitle: Books in .txt format for AI training purposes\nLink: https://news.ycombinator.com/item?id=24884789\nSnippet: I suppose there may be pros and cons to each, with Project Gutenberg being generally older books. ... I made the books3 dataset. I am delighted to ...\nSitelinks: No sitelinks\n\nTitle: davanstrien/dataset-tldr\nLink: https://huggingface.co/datasets/davanstrien/dataset-tldr/viewer/default/train\nSnippet: ... Books3 datasets are downloaded from Huggingface. After downloading, we use ... Project Gutenberg | books | 20.4 | 0.056 | 4.0 | 6.0 | | Wikipedia ...\nSitelinks: No sitelinks\n\nTitle: The Ethical AI Bombshell That Just Blew Up Big Tech's ...\nLink: https://www.linkedin.com/pulse/ethical-ai-bombshell-just-blew-up-big-techs-biggest-lie-dion-wiggins-qkgic\nSnippet: Books3 and LibGen\u2014massive pirate libraries of over 7 million copyrighted books. GitHub scraping\u2014used without regard for GPL, MIT, or author ...\nSitelinks: No sitelinks\n\nTitle: Trace - lecture_13 - CS336\nLink: https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_13.json\nSnippet: PG-19: books from Project Gutenberg before 2019. [article]. [article]. 279. 280. 281def books3():. 282. Books3 [Presser, 2020] ... Goal: define a standard dataset ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01Q6YiKwApu9xCZDxjFwjq63"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_0143Hej8bk9bgdWaErUo4yWV",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"\\\"ArXiv dataset\\\" size GB academic papers\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: arXiv Dataset\nLink: https://www.kaggle.com/datasets/Cornell-University/arxiv\nSnippet: To help make the arXiv more accessible, we present a free, open pipeline on Kaggle to the machine-readable arXiv dataset: a repository of 1.7 million articles, ...\nSitelinks: No sitelinks\n\nTitle: 1.7 million arXiv research papers and counting\nLink: https://www.rdworldonline.com/1-7m-arxiv-papers-and-counting-open-research-offers-a-data-flood/\nSnippet: Network graph showing the co-occurrence of the top 20 research categories in the arXiv dataset. Nodes represent categories, while edges ...\nSitelinks: No sitelinks\n\nTitle: on the use of arxiv as a dataset\nLink: https://arxiv.org/pdf/1905.00075\nSnippet: by CB Clement \u00b7 2019 \u00b7 Cited by 150 \u2014 The extracted raw-text dataset is \u223c 64 GB in size, totaling \u223c 11 billion words. ... comparison on the arxiv dataset. CoRR, abs/1309.0326, 2013.\nSitelinks: No sitelinks\n\nTitle: Forecasting high-impact research topics via machine ...\nLink: https://arxiv.org/html/2402.08640v2\nSnippet: The arXiv dataset can be directly downloaded from Kaggle, while ... size of 68 GB, comprising around 92 million scientific papers.\nSitelinks: No sitelinks\n\nTitle: Understanding a Dataset: arXiv.org\nLink: https://www.authorea.com/users/5713/articles/51708-understanding-a-dataset-arxiv-org\nSnippet: The purpose of this blog post is to correct an early error of not digging deeper into the properties of the arXiv dataset, and to record some of the relevant ...\nSitelinks: No sitelinks\n\nTitle: Arxiv Papers Metadata Dataset\nLink: https://www.kaggle.com/datasets/tayorm/arxiv-papers-metadata\nSnippet: Usability 5.9 \u00b7 1 GB \u00b7 1,547 downloads. 1 File (CSV). arrow_drop_up 0. ArXiv Dataset. more_vert. Jimmy Yang \u00b7 Updated 3 months ago. Usability 5.9 \u00b7 707 MB \u00b7 61 ...\nSitelinks: No sitelinks\n\nTitle: ArXiv Scientific Papers Vector Similarity Search\nLink: https://medium.com/data-science/arxiv-scientific-papers-vector-similarity-search-15059970a73a\nSnippet: For this post, we'll use the arXiv Dataset from Kaggle, which the ... The data we downloaded from Kaggle is a 3.3GB JSON file containing around 2 ...\nSitelinks: No sitelinks\n\nTitle: On the Use of ArXiv as a Dataset\nLink: https://www.researchgate.net/publication/332799542_On_the_Use_of_ArXiv_as_a_Dataset\nSnippet: ... scienti\ufb01c publications using wikipedia and. natural language processing tools. comparison on the arxiv dataset. CoRR, abs/1309.0326, 2013. URL http://arxiv ...\nSitelinks: No sitelinks\n\nTitle: Building a Local Arxiv Paper Search Engine - datascience.fm\nLink: https://datascience.fm/building-a-local-arxiv-paper-search-engine/\nSnippet: Size: The metadata file is substantial, weighing in at about 4.31 GB ... arXiv dataset and metadata of 1.7M+ scholarly papers across STEM.\nSitelinks: No sitelinks\n\nTitle: Forecasting high-impact research topics via machine ...\nLink: https://arxiv.org/html/2402.08640v4\nSnippet: The arXiv dataset can be directly downloaded from Kaggle, while ... size of 68 GB, comprising around 92 million scientific papers. From ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_0143Hej8bk9bgdWaErUo4yWV"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_012aA27dxYCb66RF3aYuWfDP",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"\\\"StackExchange\\\" dataset size GB programming Q&A\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: Why are the sizes of programs so large?\nLink: https://softwareengineering.stackexchange.com/questions/298117/why-are-the-sizes-of-programs-so-large\nSnippet: The current sizes of programs so large compared to 10 or 15 years ago? The programs are not doing significantly more functions and do not look very different.\nSitelinks: [{'title': '15 answers', 'link': 'https://softwareengineering.stackexchange.com/a/298118'}]\n\nTitle: What is a realistic, real-world, maximum size for a SQLite ...\nLink: https://softwareengineering.stackexchange.com/questions/332069/what-is-a-realistic-real-world-maximum-size-for-a-sqlite-database\nSnippet: What is a realistic, real-world, maximum size for a SQLite database? ... 5. If stackExchange had a \"remind me\" feature I'd tag this question ...\nSitelinks: [{'title': '1 answer', 'link': 'https://softwareengineering.stackexchange.com/a/332071'}]\n\nTitle: How To Find Simple And Interesting Multi-Gigabytes Data Set\nLink: https://fullstackml.com/how-to-find-simple-and-interesting-multi-gigabytes-data-set-f7d9b42f525\nSnippet: The dataset contains all stackexchange data including stackoverflow and the overall size of the archive is 27 gigabytes. The size of the ...\nSitelinks: No sitelinks\n\nTitle: Is there an offline Stack Overflow?\nLink: https://meta.stackoverflow.com/questions/398590/is-there-an-offline-stack-overflow\nSnippet: Use the Stack Exchange quarterly datadump as found on https://archive.org/download/stackexchange (has files for all the sites around the SE ...\nSitelinks: [{'title': '6 answers', 'link': 'https://meta.stackoverflow.com/a/398592'}]\n\nTitle: sentence-transformers/stackexchange-duplicates\nLink: https://huggingface.co/datasets/sentence-transformers/stackexchange-duplicates\nSnippet: I was searching through the internet for an equation that defines the path followed by a racing line, when I came across the following formula on StackExchange ...\nSitelinks: No sitelinks\n\nTitle: What is the size of the Stack Overflow database?\nLink: https://www.quora.com/What-is-the-size-of-the-Stack-Overflow-database\nSnippet: So in the 307k-566k range currently. This isn't considering the other stackexchange.com websites that are part of the family.\nSitelinks: [{'title': '1 answer', 'link': 'https://www.quora.com/What-is-the-size-of-the-Stack-Overflow-database?top_ans=29598819'}]\n\nTitle: Performance - Stack Exchange : r/programming\nLink: https://www.reddit.com/r/programming/comments/9e4cw9/performance_stack_exchange/\nSnippet: Stackexchange is 300 REQ/s per server, multiply that by nine.\nSitelinks: No sitelinks\n\nTitle: StackOverflow Search Engine and Question Recommendation\nLink: https://sm27598.medium.com/stackoverflow-search-engine-and-question-recommendation-705a5241b84c\nSnippet: android.stackexchange.com.7z arduino.meta.stackexchange.com.7z ... After uncompressing, it is of size 3.92 GB. All individual topics ...\nSitelinks: No sitelinks\n\nTitle: Answer features extraction from StackOverflow\nLink: https://sol.sbc.org.br/index.php/errc/article/download/26011/25834/\nSnippet: by JB Gon\u00e7alves \u00b7 2023 \u2014 Data Processing. We downloaded the StackExchange programming data from StackOverflow. Firstly we extract the Questions that have answers from the Posts file.\nSitelinks: No sitelinks\n\nTitle: ArmelR/stack-exchange-instruction \u00b7 Datasets at ...\nLink: https://huggingface.co/datasets/ArmelR/stack-exchange-instruction\nSnippet: Grid is 8 x 8. 2022/04/17. [ \"https://puzzling.stackexchange.com/questions ... Size of downloaded dataset files: 14.6 GB. Size of the auto-converted Parquet ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_012aA27dxYCb66RF3aYuWfDP"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01BYfJWtrCD1tUdYymrcodvB",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"\\\"GitHub dataset\\\" size GB code repository programming\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: codeparrot/github-code \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/codeparrot/github-code\nSnippet: Size (GB). 0, Java, 19548190 ... Files of with the extensions given in the list above were retrieved from the GitHub dataset on BigQuery (full query here).\nSitelinks: No sitelinks\n\nTitle: Github Dataset\nLink: https://www.kaggle.com/datasets/johntukey/github-dataset\nSnippet: Github Dataset. A Representative User-centric Dataset of 10 Million GitHub ... If you download and extract it, its size will be 50 GB! To make it easier ...\nSitelinks: No sitelinks\n\nTitle: Aditya78b/codeparrot-java-all \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/Aditya78b/codeparrot-java-all\nSnippet: ... programming languages with 60 extensions totaling in 1TB of data. The dataset was created from the public GitHub dataset on Google BiqQuery. How to use it.\nSitelinks: No sitelinks\n\nTitle: public GitHub dataset for software engineering challenges\nLink: https://www.researchgate.net/publication/345781745_Summarising_big_data_public_GitHub_dataset_for_software_engineering_challenges\nSnippet: Summarising big data: public GitHub dataset for software engineering challenges ... One Git-based service, GitHub, is the most common code hosting and repository ...\nSitelinks: No sitelinks\n\nTitle: Use BigQuery to query GitHub data\nLink: https://codelabs.developers.google.com/codelabs/bigquery-github\nSnippet: Open the GitHub dataset in the BigQuery web UI. Get a quick preview of how the data looks. Query GitHub data; More public data ...\nSitelinks: No sitelinks\n\nTitle: The Stack: 3 TB of permissively licensed source code\nLink: https://arxiv.org/pdf/2211.15533\nSnippet: by D Kocetkov \u00b7 2022 \u00b7 Cited by 393 \u2014 CodeGen collected a private GitHub dataset of 217.3 GB of permissively licensed python files. They fine-tuned models on this dataset after ...\nSitelinks: No sitelinks\n\nTitle: A Dataset for GitHub Repository Deduplication\nLink: https://arxiv.org/pdf/2002.02314\nSnippet: by D Spinellis \u00b7 2020 \u00b7 Cited by 7 \u2014 Keywords: Deduplication, fork, project clone, GitHub, dataset. This ... Program Properties from \u201dBig Code\u201d. In Proceedings of the 42nd ...\nSitelinks: No sitelinks\n\nTitle: November 13, 2024 15:24 output A Survey on Code ...\nLink: https://dmalab.science.uoit.ca/hdavoudi/papers/Code_Representations_Survey.pdf\nSnippet: by PD Nagy \u00b7 Cited by 1 \u2014 The model is inspired by the NL model Doc2vec8. Code2vec was trained on a. GitHub dataset containing 14M Java methods. 3.2.2. Code2seq. Code2seq ...\nSitelinks: No sitelinks\n\nTitle: Learning to analyze huge BigQuery datasets using Python ...\nLink: https://medium.com/google-cloud/learning-to-analyze-huge-bigquery-datasets-using-python-on-kaggle-2c6c6153f542\nSnippet: Below is the code I used to create a BigQueryHelper object for the GitHub dataset which I named bq_assistant . Behind the scenes this ...\nSitelinks: No sitelinks\n\nTitle: (PDF) Summarising Big Data: Common GitHub Dataset for ...\nLink: https://www.researchgate.net/publication/342093973_Summarising_Big_Data_Common_GitHub_Dataset_for_Software_Engineering_Challenges\nSnippet: Summarising big data: public GitHub dataset for software engineering challenges \u00b7 New developer metrics: Are comments as crucial as code ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01BYfJWtrCD1tUdYymrcodvB"
        },
        {
            "role": "assistant",
            "content": "Now let me search for the specific Google Spreadsheet to work with:",
            "tool_calls": [
                {
                    "id": "toolu_01949yuspjiChBL5YiZ2wP2C",
                    "type": "function",
                    "function": {
                        "name": "google_sheet-list_spreadsheets",
                        "arguments": ""
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"{\\n  \\\"id\\\": \\\"1pkvieQSe2snpTOS94pnlHoGDwCXO2Fsx0Ct-_iGnQhM\\\",\\n  \\\"title\\\": \\\"LLM Pre-training Data\\\"\\n}\",\"annotations\":null}",
            "tool_call_id": "toolu_01949yuspjiChBL5YiZ2wP2C"
        },
        {
            "role": "assistant",
            "content": "Great! I found the \"LLM Pre-training Data\" spreadsheet. Now let me check what sheets exist in it:",
            "tool_calls": [
                {
                    "id": "toolu_01QPsxx8aXDn3rnr2ZTB6XbA",
                    "type": "function",
                    "function": {
                        "name": "google_sheet-list_sheets",
                        "arguments": "{\"spreadsheet_id\": \"1pkvieQSe2snpTOS94pnlHoGDwCXO2Fsx0Ct-_iGnQhM\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"ptdata\",\"annotations\":null}",
            "tool_call_id": "toolu_01QPsxx8aXDn3rnr2ZTB6XbA"
        },
        {
            "role": "assistant",
            "content": "Perfect! The \"ptdata\" sheet already exists. Let me check what's currently in it:",
            "tool_calls": [
                {
                    "id": "toolu_01VdpqcYhM57fB5NQQ82yTMW",
                    "type": "function",
                    "function": {
                        "name": "google_sheet-get_sheet_data",
                        "arguments": "{\"spreadsheet_id\": \"1pkvieQSe2snpTOS94pnlHoGDwCXO2Fsx0Ct-_iGnQhM\", \"sheet\": \"ptdata\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"{\\n  \\\"spreadsheetId\\\": \\\"1pkvieQSe2snpTOS94pnlHoGDwCXO2Fsx0Ct-_iGnQhM\\\",\\n  \\\"properties\\\": {\\n    \\\"title\\\": \\\"LLM Pre-training Data\\\",\\n    \\\"locale\\\": \\\"zh_CN\\\",\\n    \\\"autoRecalc\\\": \\\"ON_CHANGE\\\",\\n    \\\"timeZone\\\": \\\"Asia/Shanghai\\\",\\n    \\\"defaultFormat\\\": {\\n      \\\"backgroundColor\\\": {\\n        \\\"red\\\": 1,\\n        \\\"green\\\": 1,\\n        \\\"blue\\\": 1\\n      },\\n      \\\"padding\\\": {\\n        \\\"top\\\": 2,\\n        \\\"right\\\": 3,\\n        \\\"bottom\\\": 2,\\n        \\\"left\\\": 3\\n      },\\n      \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n      \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n      \\\"textFormat\\\": {\\n        \\\"foregroundColor\\\": {},\\n        \\\"fontFamily\\\": \\\"arial,sans,sans-serif\\\",\\n        \\\"fontSize\\\": 10,\\n        \\\"bold\\\": false,\\n        \\\"italic\\\": false,\\n        \\\"strikethrough\\\": false,\\n        \\\"underline\\\": false,\\n        \\\"foregroundColorStyle\\\": {\\n          \\\"rgbColor\\\": {}\\n        }\\n      },\\n      \\\"backgroundColorStyle\\\": {\\n        \\\"rgbColor\\\": {\\n          \\\"red\\\": 1,\\n          \\\"green\\\": 1,\\n          \\\"blue\\\": 1\\n        }\\n      }\\n    },\\n    \\\"spreadsheetTheme\\\": {\\n      \\\"primaryFontFamily\\\": \\\"Arial\\\",\\n      \\\"themeColors\\\": [\\n        {\\n          \\\"colorType\\\": \\\"TEXT\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {}\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"BACKGROUND\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 1,\\n              \\\"green\\\": 1,\\n              \\\"blue\\\": 1\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"ACCENT1\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 0.25882354,\\n              \\\"green\\\": 0.52156866,\\n              \\\"blue\\\": 0.95686275\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"ACCENT2\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 0.91764706,\\n              \\\"green\\\": 0.2627451,\\n              \\\"blue\\\": 0.20784314\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"ACCENT3\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 0.9843137,\\n              \\\"green\\\": 0.7372549,\\n              \\\"blue\\\": 0.015686275\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"ACCENT4\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 0.20392157,\\n              \\\"green\\\": 0.65882355,\\n              \\\"blue\\\": 0.3254902\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"ACCENT5\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 1,\\n              \\\"green\\\": 0.42745098,\\n              \\\"blue\\\": 0.003921569\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"ACCENT6\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 0.27450982,\\n              \\\"green\\\": 0.7411765,\\n              \\\"blue\\\": 0.7764706\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"LINK\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 0.06666667,\\n              \\\"green\\\": 0.33333334,\\n              \\\"blue\\\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  },\\n  \\\"sheets\\\": [\\n    {\\n      \\\"properties\\\": {\\n        \\\"sheetId\\\": 0,\\n        \\\"title\\\": \\\"ptdata\\\",\\n        \\\"index\\\": 0,\\n        \\\"sheetType\\\": \\\"GRID\\\",\\n        \\\"gridProperties\\\": {\\n          \\\"rowCount\\\": 1000,\\n          \\\"columnCount\\\": 26\\n        }\\n      },\\n      \\\"data\\\": [\\n        {\\n          \\\"rowMetadata\\\": [\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            },\\n            {\\n              \\\"pixelSize\\\": 21\\n            }\\n          ],\\n          \\\"columnMetadata\\\": [\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            },\\n            {\\n              \\\"pixelSize\\\": 100\\n            }\\n          ]\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"spreadsheetUrl\\\": \\\"https://docs.google.com/spreadsheets/d/1pkvieQSe2snpTOS94pnlHoGDwCXO2Fsx0Ct-_iGnQhM/edit\\\"\\n}\",\"annotations\":null}",
            "tool_call_id": "toolu_01VdpqcYhM57fB5NQQ82yTMW"
        },
        {
            "role": "assistant",
            "content": "I can see the sheet is empty. Let me search for more specific information about dataset sizes and HuggingFace links:",
            "tool_calls": [
                {
                    "id": "toolu_01Ka7Y89sjgMLn9CMGeREZyP",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"site:huggingface.co \\\"Common Crawl\\\" dataset\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: commoncrawl/statistics \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/commoncrawl/statistics\nSnippet: Common Crawl Statistics. Number of pages, distribution of top-level domains, crawl overlaps, etc. - basic metrics about Common Crawl Monthly ...\nSitelinks: No sitelinks\n\nTitle: commoncrawl (Common Crawl Foundation)\nLink: https://huggingface.co/commoncrawl\nSnippet: Welcome to the Common Crawl Foundation's Hugging Face page! We aim to provide metadata and experimental versions of our latest data products here.\nSitelinks: No sitelinks\n\nTitle: commoncrawl/citations \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/commoncrawl/citations/viewer\nSnippet: We use all available parallel training data, namely Europarl v7, Common Crawl and News Commentary v10 and apply the standard Moses tokenization to obtain 3.9M ...\nSitelinks: No sitelinks\n\nTitle: commoncrawl (Common Crawl Foundation)\nLink: https://huggingface.co/commoncrawl/datasets\nSnippet: Common Crawl Foundation \u00b7 Enterprise. non-profit. Verified. https://commoncrawl ... laurievb updated a dataset 13 days ago. commoncrawl/statistics \u00b7 tvaughan ...\nSitelinks: No sitelinks\n\nTitle: agentlans/common-crawl-sample \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/agentlans/common-crawl-sample\nSnippet: Common Crawl sample. A small unofficial random subset of the famous Common Crawl dataset. 60 random segment WET files were downloaded from Common Crawl on ...\nSitelinks: No sitelinks\n\nTitle: commoncrawl (Common Crawl Foundation)\nLink: https://huggingface.co/commoncrawl/models\nSnippet: Common Crawl Foundation \u00b7 Enterprise. non-profit. Verified. https://commoncrawl ... laurievb updated a dataset 3 days ago. commoncrawl/statistics \u00b7 tvaughan ...\nSitelinks: No sitelinks\n\nTitle: oscar-corpus/OSCAR-2201 \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/oscar-corpus/OSCAR-2201\nSnippet: Common Crawl is a non-profit foundation which produces and maintains an open repository of web crawled data that is both accessible and analysable. Common ...\nSitelinks: No sitelinks\n\nTitle: Introducing the Common Crawl Creative Commons Corpus ...\nLink: https://huggingface.co/posts/BramVanroy/471518743184430\nSnippet: Only 7 out of 100+ Common Crawl crawls have been processed so far. That's encouraging because it means there is a lot more Creative Commons data ...\nSitelinks: No sitelinks\n\nTitle: malaysia-ai/common-crawl \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/malaysia-ai/common-crawl\nSnippet: common-crawl. like 0. Follow. Malaysia AI 64. Modalities: Text. Formats: parquet ... No dataset card yet. Downloads last month. 131. Use this dataset. Size of ...\nSitelinks: No sitelinks\n\nTitle: commoncrawl/citations \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/commoncrawl/citations/embed/viewer/2016/train?p=1\nSnippet: We use a pretrained model of GloVe for word embeddings. This model is trained on Common Crawl with 840B tokens, 2.2M vocabulary. We represent each word by a ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01Ka7Y89sjgMLn9CMGeREZyP"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_0137EdqzyHFf5TcdXraJwGFo",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"site:huggingface.co \\\"C4\\\" dataset \\\"Colossal Clean Crawled Corpus\\\"\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: brando/small-c4-dataset \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/brando/small-c4-dataset\nSnippet: The Small C4 Dataset is a reduced version of the original C4 dataset (Colossal Clean Crawled Corpus), designed to facilitate lightweight experimentation and ...\nSitelinks: No sitelinks\n\nTitle: A Case Study on the Colossal Clean Crawled Corpus\nLink: https://huggingface.co/papers/2104.08758\nSnippet: The research provides documentation and analysis of the Colossal Clean Crawled Corpus (C4), revealing unexpected sources and machine-generated ...\nSitelinks: No sitelinks\n\nTitle: README.md \u00b7 brando/small-c4-dataset at ...\nLink: https://huggingface.co/datasets/brando/small-c4-dataset/blob/a1e57e00812378ce2e8cce052a63d841d596c0dc/README.md\nSnippet: The Small C4 Dataset is a reduced version of the original C4 dataset (Colossal Clean Crawled Corpus), designed to facilitate lightweight experimentation and ...\nSitelinks: No sitelinks\n\nTitle: raw\nLink: https://huggingface.co/datasets/brando/small-c4-dataset/raw/main/README.md\nSnippet: ... c4) (Colossal Clean Crawled Corpus), designed to facilitate lightweight experimentation and model training without the need to process the full C4 dataset.\nSitelinks: No sitelinks\n\nTitle: google-t5/t5-base\nLink: https://huggingface.co/google-t5/t5-base\nSnippet: The model is pre-trained on the Colossal Clean Crawled Corpus (C4), which was developed and released in the context of the same research paper ...\nSitelinks: No sitelinks\n\nTitle: agentlans/grammar-classification \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/agentlans/grammar-classification\nSnippet: This dataset, derived from the C4 (Colossal Clean Crawled Corpus), contains 600 000 examples for binary classification of grammatical ...\nSitelinks: No sitelinks\n\nTitle: google/t5-v1_1-small\nLink: https://huggingface.co/google/t5-v1_1-small\nSnippet: By combining the insights from our exploration with scale and our new \u201cColossal Clean Crawled Corpus\u201d, we achieve state-of-the-art results on many benchmarks ...\nSitelinks: No sitelinks\n\nTitle: google/switch-base-256\nLink: https://huggingface.co/google/switch-base-256\nSnippet: The model was trained on a Masked Language Modeling task, on Colossal Clean Crawled Corpus (C4) dataset, following the same procedure as T5 . Training ...\nSitelinks: No sitelinks\n\nTitle: google/t5-xxl-lm-adapt\nLink: https://huggingface.co/google/t5-xxl-lm-adapt\nSnippet: By combining the insights from our exploration with scale and our new \u201cColossal Clean Crawled Corpus\u201d, we achieve state-of-the-art results on ...\nSitelinks: No sitelinks\n\nTitle: README.md \u00b7 google/t5-v1_1-small at ...\nLink: https://huggingface.co/google/t5-v1_1-small/blame/869ada867c40f81d9e93c5ac26d5b297600fda9b/README.md\nSnippet: Pretraining Dataset: [C4](https://huggingface.co/datasets/c4) Other ... Colossal Clean Crawled Corpus\u201d, we achieve state-of-the-art results on many ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_0137EdqzyHFf5TcdXraJwGFo"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01NKjSV1ak6NGR2AG1sbq84u",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"site:huggingface.co \\\"The Pile\\\" EleutherAI dataset\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: EleutherAI/pile \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/EleutherAI/pile\nSnippet: The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together.\nSitelinks: No sitelinks\n\nTitle: ArmelR/the-pile-splitted \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/ArmelR/the-pile-splitted\nSnippet: The pile is an 800GB dataset of english text designed by EleutherAI to train large-scale language models. ... The dataset is divided into 22 smaller high-quality ...\nSitelinks: No sitelinks\n\nTitle: pile.py \u00b7 EleutherAI/pile at main\nLink: https://huggingface.co/datasets/EleutherAI/pile/blob/main/pile.py\nSnippet: class ThePile(datasets.GeneratorBasedBuilder):. \"\"\"The Pile dataset.\"\"\" VERSION = datasets.Version(\"1.1.0\"). BUILDER_CONFIG_CLASS ...\nSitelinks: No sitelinks\n\nTitle: The Common Pile v0.1\nLink: https://huggingface.co/blog/stellaathena/common-pile\nSnippet: Four and a half years ago, EleutherAI entered the AI scene by releasing the Pile: An 800GB Dataset of Diverse Text for Language Modeling.\nSitelinks: No sitelinks\n\nTitle: EleutherAI/pile-t5-base\nLink: https://huggingface.co/EleutherAI/pile-t5-base\nSnippet: The Pile is a 825GiB general-purpose dataset in English. It was created by EleutherAI specifically for training large language models. It ...\nSitelinks: No sitelinks\n\nTitle: EleutherAI/pile \u00b7 No longer downloadable (and solution!)\nLink: https://huggingface.co/datasets/EleutherAI/pile/discussions/15\nSnippet: The Pile has been removed from the servers at The Eye for reasons unknown, making it impossible to download. I've posted a backup of the Pile here.\nSitelinks: No sitelinks\n\nTitle: EleutherAI/pythia-6.9b-v0\nLink: https://huggingface.co/EleutherAI/pythia-6.9b-v0\nSnippet: The Pile is a 825GiB general-purpose dataset in English. It was created by EleutherAI specifically for training large language models. It ...\nSitelinks: No sitelinks\n\nTitle: README.md - Dataset Card for The Pile GitHub\nLink: https://huggingface.co/datasets/andstor/the_pile_github/blob/70c602f6101287e170b8e46afc342893666d5be6/README.md\nSnippet: The Pile dataset was released on January 1st, 2021. It is licensed under the MIT License. See the dataset for more details. Citation Information.\nSitelinks: No sitelinks\n\nTitle: EleutherAI/pythia-410m\nLink: https://huggingface.co/EleutherAI/pythia-410m\nSnippet: The Pile is a 825GiB general-purpose dataset in English. It was created by EleutherAI specifically for training large language models. It ...\nSitelinks: No sitelinks\n\nTitle: EleutherAI/pile \u00b7 Discussions\nLink: https://huggingface.co/datasets/EleutherAI/pile/discussions\nSnippet: Looking for Alternatives to The Pile Dataset with Timestamps \u00b7 Dataset Viewer issue: DatasetWithScriptNotSupportedError \u00b7 No longer downloadable (and solution!)\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01NKjSV1ak6NGR2AG1sbq84u"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01QuefbB7q2e99sPJ99meru9",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"site:huggingface.co \\\"RedPajama\\\" dataset\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: togethercomputer/RedPajama-Data-1T \u00b7 Datasets at ...\nLink: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T\nSnippet: RedPajama is a clean-room, fully open-source implementation of the LLaMa dataset. Languages Primarily English, though the Wikipedia slice contains multiple ...\nSitelinks: No sitelinks\n\nTitle: togethercomputer/RedPajama-Data-V2 \u00b7 Datasets at ...\nLink: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2\nSnippet: RedPajama-V2 is an open dataset for training large language models. The dataset includes over 100B text documents coming from 84 CommonCrawl snapshots.\nSitelinks: No sitelinks\n\nTitle: togethercomputer/RedPajama-Data-1T-Sample \u00b7 Datasets ...\nLink: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample\nSnippet: RedPajama is a clean-room, fully open-source implementation of the LLaMa dataset. This HuggingFace repo contains a 1B-token sample of the RedPajama dataset.\nSitelinks: No sitelinks\n\nTitle: togethercomputer/RedPajama-Data-Instruct \u00b7 Datasets at ...\nLink: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-Instruct\nSnippet: RedPajama-Instruct-Data is curated from a diverse collection of NLP tasks from both P3 (BigScience) and Natural Instruction (AI2), and conduct aggressive ...\nSitelinks: No sitelinks\n\nTitle: an Open Dataset for Training Large Language Models\nLink: https://huggingface.co/papers/2411.12372\nSnippet: The RedPajama datasets are introduced to address core challenges for open-source language models by providing transparent data curation, large ...\nSitelinks: No sitelinks\n\nTitle: vilm/RedPajama-v2-small \u00b7 Datasets at ...\nLink: https://huggingface.co/datasets/vilm/RedPajama-v2-small\nSnippet: Small RedPajama-v2. 500K Samples from RedPajama-v2. Downloads last month. 143. Use this dataset. Size of downloaded dataset files: 1.48 GB. Size ...\nSitelinks: No sitelinks\n\nTitle: README.md - togethercomputer/RedPajama-Data-1T\nLink: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T/blob/main/README.md\nSnippet: RedPajama is a clean-room, fully open-source implementation of the LLaMa dataset. Languages Primarily English, though the Wikipedia slice contains multiple ...\nSitelinks: No sitelinks\n\nTitle: README.md \u00b7 togethercomputer/RedPajama-Data-V2 at ...\nLink: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2/blame/923e66af6d6c7931920c8064258ff945b2096a04/README.md\nSnippet: ... Dataset --- ### Getting Started RedPajama-V2 is an open dataset for training large language models. The dataset includes over 100B text documents coming ...\nSitelinks: No sitelinks\n\nTitle: datajuicer/redpajama-arxiv-refined-by-data-juicer\nLink: https://huggingface.co/datasets/datajuicer/redpajama-arxiv-refined-by-data-juicer\nSnippet: RedPajama -- ArXiv (refined by Data-Juicer). A refined version of ArXiv dataset in RedPajama by Data-Juicer. Removing some \"bad\" samples from the original ...\nSitelinks: No sitelinks\n\nTitle: gair-prox/RedPajama-pro \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/gair-prox/RedPajama-pro\nSnippet: It contains about 30B high quality tokens, ready for general language model pre-training. License. RedPajama-pro is based on RedPajama-Data-V2, which is made ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01QuefbB7q2e99sPJ99meru9"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01EaVRmjDoHQq2VUPjL5t8ym",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"site:huggingface.co Wikipedia dataset\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: wikimedia/wikipedia \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/wikimedia/wikipedia\nSnippet: Dataset Card for Wikimedia Wikipedia. Dataset Summary. Wikipedia dataset containing cleaned articles of all languages. The dataset is built from the Wikipedia ...\nSitelinks: No sitelinks\n\nTitle: olm/wikipedia \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/olm/wikipedia\nSnippet: Wikipedia dataset containing cleaned articles of all languages. The datasets are built from the Wikipedia dump (https://dumps.wikimedia.org/) with one split ...\nSitelinks: No sitelinks\n\nTitle: openskyml/wikipedia \u00b7 Datasets at ...\nLink: https://huggingface.co/datasets/openskyml/wikipedia\nSnippet: Wikipedia dataset containing cleaned articles of all languages. The datasets are built from the Wikipedia dump (https://dumps.wikimedia.org/) with one split ...\nSitelinks: No sitelinks\n\nTitle: rcds/wikipedia-persons-masked \u00b7 Datasets at ...\nLink: https://huggingface.co/datasets/rcds/wikipedia-persons-masked\nSnippet: This dataset was created by using the wikipedia dataset from huggingface and processing it from there. People were queried via wikidata. The texts were split ...\nSitelinks: No sitelinks\n\nTitle: NeuML/wikipedia \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/NeuML/wikipedia\nSnippet: Dataset Card for Wikipedia. This repo is a fork of the olm/wikipedia repo which itself is a fork of the original Hugging Face Wikipedia repo ...\nSitelinks: No sitelinks\n\nTitle: Cohere/wikipedia-22-12-en-embeddings \u00b7 Datasets at ...\nLink: https://huggingface.co/datasets/Cohere/wikipedia-22-12-en-embeddings\nSnippet: You can find the Wikipedia datasets without embeddings at Cohere/wikipedia-22-12. Loading the dataset. You can either load the dataset like this: from ...\nSitelinks: No sitelinks\n\nTitle: Salesforce/wikitext \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/Salesforce/wikitext\nSnippet: The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.\nSitelinks: No sitelinks\n\nTitle: rahular/simple-wikipedia \u00b7 Datasets at ...\nLink: https://huggingface.co/datasets/rahular/simple-wikipedia\nSnippet: In geometry, capital A, B, C etc. are used to label line segments, lines, etc. Also, A is typically used as one of the letters to label an angle in a triangle.\nSitelinks: No sitelinks\n\nTitle: sentence-transformers/wikipedia-en-sentences \u00b7 Datasets ...\nLink: https://huggingface.co/datasets/sentence-transformers/wikipedia-en-sentences\nSnippet: Dataset Card for Wikipedia Sentences (English). This dataset contains 7.87 million English sentences and can be used in knowledge distillation of embedding ...\nSitelinks: No sitelinks\n\nTitle: Supabase/wikipedia-en-embeddings \u00b7 Datasets at ...\nLink: https://huggingface.co/datasets/Supabase/wikipedia-en-embeddings\nSnippet: These are the embeddings and corresponded simplified articles from the wikipedia \"simple english\" dump.\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01EaVRmjDoHQq2VUPjL5t8ym"
        },
        {
            "role": "assistant",
            "content": "Now let me search for more specific size information for some key datasets:",
            "tool_calls": [
                {
                    "id": "toolu_01JNPt1aCK7Q4xTo31KvLV3o",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"\\\"Books3\\\" dataset \\\"100 GB\\\" size\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: LLM Training Datasets - Data Behind the Models - LLMDB\nLink: https://llmdb.com/datasets\nSnippet: Books3. Created by EleutherAI2020. A dataset of approximately 196,640 books, primarily English fiction and non-fiction. Size. 100 GB. License. Research only.\nSitelinks: No sitelinks\n\nTitle: Inside the Great AI Data Grab \u2014 Comprehensive Analysis ...\nLink: https://medium.com/@adnanmasood/inside-the-great-ai-data-grab-comprehensive-analysis-of-public-and-proprietary-corpora-utilised-49b4770abc47\nSnippet: Meta's LLaMA, for instance, included the Project Gutenberg public domain books and the Books3 dataset (from the open Pile collection) ...\nSitelinks: No sitelinks\n\nTitle: WuDaoCorpora: A super large-scale Chinese corpora for ...\nLink: https://www.sciencedirect.com/science/article/pii/S2666651021000152\nSnippet: by S Yuan \u00b7 2021 \u00b7 Cited by 73 \u2014 For example, EleutherAI built a 1.2 TB English corpus including material like Books3, PubMed Central, and FreeLaw. ... 100 GB data, which suggests it is ...\nSitelinks: No sitelinks\n\nTitle: A Survey of LLM Datasets: From Autoregressive Model to ...\nLink: https://jcst.ict.ac.cn/fileup/1000-9000/PDF/JCST-2024-3-3-3767-542.pdf\nSnippet: by F Du \u00b7 2024 \u00b7 Cited by 16 \u2014 ... Books3[52], and CC-Net[57]. Common Crawl. Common Crawl is a big open- source ... It contains about 100 GB data whereas the other two only contain about ...\nSitelinks: No sitelinks\n\nTitle: A Review of Transformer Models\nLink: https://orkg.org/reviews/R609546\nSnippet: An English dataset comprising 232B tokens from Pile-CC, Books3 ... 100 GB/sec of interconnect bandwidth between servers using 8 InfiniBand ...\nSitelinks: No sitelinks\n\nTitle: Accelerating Large Language Model Training with Hybrid ...\nLink: https://arxiv.org/html/2409.02423v1\nSnippet: Mellanox IB EDR (100 Gb/s). TABLE IV: Lassen cluster ... We also documented test loss on Books3 for model performance validation.\nSitelinks: No sitelinks\n\nTitle: Updates to Consumer Terms and Privacy Policy\nLink: https://news.ycombinator.com/item?id=45062683\nSnippet: ... 100 GB of my DNA paid for by myself, my medical histories - take ... Books3, PiLiMi, scanned books, web scrapes and some other sources ...\nSitelinks: No sitelinks\n\nTitle: Daily Papers\nLink: https://huggingface.co/papers?q=search%20embedding\nSnippet: ... amount of memory due to the vast number of features in the dataset. ... dataset of phrases in context mined from the Books3 corpus. ... For example, indexing 100 GB ...\nSitelinks: No sitelinks\n\nTitle: U.S. court orders LibGen to pay $30M to publishers, issues ...\nLink: https://news.ycombinator.com/item?id=41661512\nSnippet: You just tell it to keep around 100 GB of files in a 10 TB torrent. ... At least meta admitted using a dataset called \"books3\" which ...\nSitelinks: No sitelinks\n\nTitle: s11390-024-3767-3 | PDF | Computing | Artificial Intelligence\nLink: https://www.scribd.com/document/889820243/s11390-024-3767-3\nSnippet: Summary of Pre-Train Datasets. Dataset Size Language Brief Introduction Link ... Books3 101 GB (No statistic) Collected based on ... It contains about 100 GB data ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_01JNPt1aCK7Q4xTo31KvLV3o"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01XPjFCmj5wzzZtWUm1o8otq",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"\\\"GitHub dataset\\\" \\\"100 GB\\\" \\\"200 GB\\\" programming code size\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "No search results found.",
            "tool_call_id": "toolu_01XPjFCmj5wzzZtWUm1o8otq"
        },
        {
            "role": "assistant",
            "content": "Let me search for more specific information about dataset sizes used in LLaMA and GPT-Neo:",
            "tool_calls": [
                {
                    "id": "toolu_015vHvPQzMaS5j328cq19mPN",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"LLaMA training data breakdown \\\"Common Crawl\\\" \\\"C4\\\" \\\"Wikipedia\\\" \\\"Books\\\" \\\"ArXiv\\\" \\\"StackExchange\\\" \\\"GitHub\\\" GB size\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: A Comprehensive Overview of Large Language Models\nLink: https://dl.acm.org/doi/10.1145/3744746\nSnippet: Common Crawl, WebText, Books Corpora, Wikipedia, \\(\\checkmark ... CommonCrawl, C4, GitHub, Wikipedia, Books, arXiv, StackExchange, \\(\\checkmark ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "toolu_015vHvPQzMaS5j328cq19mPN"
        },
        {
            "role": "assistant",
            "content": "Let me search for the specific RedPajama dataset breakdown which replicates LLaMA's training data:",
            "tool_calls": [
                {
                    "id": "toolu_01CrzQumLftYygHNuBKWWZvs",
                    "type": "function",
                    "function": {
                        "name": "fetch-fetch_txt",
                        "arguments": "{\"url\": \"https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Datasets: togethercomputer / RedPajama-Data-1T like 1.1k Follow Together 705 Tasks: Text Generation Modalities: Text Languages: English Size: 1M - 10M Libraries: Datasets Croissant Dataset card Data Studio Files Files and versions xet Community 32 Dataset Preview API Embed Data Studio Subset (7) arxiv arxivc4common_crawldefaultgithubstackexchangewikipedia (1.73M rows) Split (1) train train The full dataset viewer is not available (click to read why). Only showing a preview of the rows. Job manager crashed while running this job (missing heartbeats). Error code: JobManagerCrashedError Need help to make the dataset viewer work? Make sure to review how to configure the dataset viewer, and open a discussion for direct support. text string meta string red_pajama_subset string \\\\section{Introduction} Let $G$ be a simple undirected graph with the \\\\textit{vertex set} $V(G)$ and the \\\\textit{edge set} $E(G)$. A vertex with degree one is called a \\\\textit{pendant vertex}. The distance between the vertices $u$ and $v$ in graph $G$ is denoted by $d_G(u,v)$. A cycle $C$ is called \\\\textit{chordless} if $C$ has no \\\\textit{cycle chord} (that is an edge not in the edge set of $C$ whose endpoints lie on the vertices of $C$). The \\\\textit{Induced subgraph} on vertex set $S$ is denoted by $\\\\langle S\\\\rangle$. A path that starts in $v$ and ends in $u$ is denoted by $\\\\stackrel\\\\frown{v u}$. A \\\\textit{traceable} graph is a graph that possesses a Hamiltonian path. In a graph $G$, we say that a cycle $C$ is \\\\textit{formed by the path} $Q$ if $ | E(C) \\\\setminus E(Q) | = 1 $. So every vertex of $C$ belongs to $V(Q)$. In 2011 the following conjecture was proposed: \\\\begin{conjecture}(Hoffmann-Ostenhof \\\\cite{hoffman}) Let $G$ be a connected cubic graph. Then $G$ has a decomposition into a spanning tree, a matching and a family of cycles. \\\\end{conjecture} Conjecture \\\\theconjecture$\\\\,$ also appears in Problem 516 \\\\cite{cameron}. There are a few partial results known for Conjecture \\\\theconjecture. Kostochka \\\\cite{kostocha} noticed that the Petersen graph, the prisms over cycles, and many other graphs have a decomposition desired in Conjecture \\\\theconjecture. Ozeki and Ye \\\\cite{ozeki} proved that the conjecture holds for 3-connected cubic plane graphs. Furthermore, it was proved by Bachstein \\\\cite{bachstein} that Conjecture \\\\theconjecture$\\\\,$ is true for every 3-connected cubic graph embedded in torus or Klein-bottle. Akbari, Jensen and Siggers \\\\cite[Theorem 9]{akbari} showed that Conjecture \\\\theconjecture$\\\\,$ is true for Hamiltonian cubic graphs. In this paper, we show that Conjecture \\\\theconjecture$\\\\,$ holds for traceable cubic graphs. \\\\section{Results} Before proving the main result, we need the following lemma. \\\\begin{lemma} \\\\label{lemma:1} Let $G$ be a cubic graph. Suppose that $V(G)$ can be partitioned into a tree $T$ and finitely many cycles such that there is no edge between any pair of cycles (not necessarily distinct cycles), and every pendant vertex of $T$ is adjacent to at least one vertex of a cycle. Then, Conjecture \\\\theconjecture$\\\\,$ holds for $G$. \\\\end{lemma} \\\\begin{proof} By assumption, every vertex of each cycle in the partition is adjacent to exactly one vertex of $T$. Call the set of all edges with one endpoint in a cycle and another endpoint in $T$ by $Q$. Clearly, the induced subgraph on $E(T) \\\\cup Q$ is a spanning tree of $G$. We call it $T'$. Note that every edge between a pendant vertex of $T$ and the union of cycles in the partition is also contained in $T'$. Thus, every pendant vertex of $T'$ is contained in a cycle of the partition. Now, consider the graph $H = G \\\\setminus E(T')$. For every $v \\\\in V(T)$, $d_H(v) \\\\leq 1$. So Conjecture \\\\theconjecture$\\\\,$ holds for $G$. \\\\vspace{1em} \\\\end{proof} \\\\noindent\\\\textbf{Remark 1.} \\\\label{remark:1} Let $C$ be a cycle formed by the path $Q$. Then clearly there exists a chordless cycle formed by $Q$. Now, we are in a position to prove the main result. \\\\begin{theorem} Conjecture \\\\theconjecture$\\\\,$ holds for traceable cubic graphs. \\\\end{theorem} \\\\begin{proof} Let $G$ be a traceable cubic graph and $P : v_1, \\\\dots, v_n$ be a Hamiltonian path in $G$. By \\\\cite[Theorem 9]{akbari}, Conjecture A holds for $v_1 v_n \\\\in E(G)$. Thus we can assume that $v_1 v_n \\\\notin E(G)$. Let $v_1 v_j, v_1 v_{j'}, v_i v_n, v_{i'} v_n \\\\in E(G)\\\\setminus E(P)$ and $j' < j < n$, $1 < i < i'$. Two cases can occur: \\\\begin{enumerate}[leftmargin=0pt,label=] \\\\item \\\\textbf{Case 1.} Assume that $i < j$. Consider the following graph in Figure \\\\ref{fig:overlapping} in which the thick edges denote the path $P$. Call the three paths between $v_j$ and $v_i$, from the left to the right, by $P_1$, $P_2$ and $P_3$, respectively (note that $P_1$ contains the edge $e'$ and $P_3$ contains the edge $e$). \\\\begin{figure}[H] \\\\begin{center} \\\\includegraphics[width=40mm]{engImages/overlapping.pdf} \\\\caption{Paths $P_1$, $P_2$ and $P_3$} \\\\label{fig:overlapping} \\\\end{center} \\\\end{figure} If $P_2$ has order $2$, then $G$ is Hamiltonian and so by \\\\cite[Theorem 9]{akbari} Conjecture \\\\theconjecture$\\\\,$ holds. Thus we can assume that $P_1$, $P_2$ and $P_3$ have order at least $3$. Now, consider the following subcases:\\\\\\\\ \\\\begin{enumerate}[leftmargin=0pt,label=] \\\\label{case:1} \\\\item \\\\textbf{Subcase 1.} There is no edge between $V(P_r)$ and $V(P_s)$ for $1 \\\\leq r < s \\\\leq 3$. Since every vertex of $P_i$ has degree 3 for every $i$, by \\\\hyperref[remark:1]{Remark 1}$\\\\,$ there are two chordless cycles $C_1$ and $C_2$ formed by $P_1$ and $P_2$, respectively. Define a tree $T$ with the edge set $$ E\\\\Big(\\\\langle V(G) \\\\setminus \\\\big(V(C_1) \\\\cup V(C_2)\\\\big) \\\\rangle\\\\Big) \\\\bigcap \\\\big(\\\\bigcup_{i=1}^3 E(P_i)\\\\big).$$ Now, apply \\\\hyperref[lemma:1]{Lemma 1} $\\\\,$for the partition $\\\\{T, C_1, C_2\\\\}$.\\\\\\\\ \\\\item \\\\textbf{Subcase 2.} \\\\label{case:edge} There exists at least one edge between some $P_r$ and $P_s$, $r<s$. With no loss of generality, assume that $r=1$ and $s=2$. Suppose that $ab \\\\in E(G)$, where $a \\\\in V(P_1)$, $b \\\\in V(P_2)$ and $d_{P_1}(v_j, a) + d_{P_2}(v_j, b)$ is minimum. \\\\begin{figure}[H] \\\\begin{center} \\\\includegraphics[width=40mm]{engImages/ab.pdf} \\\\caption{The edge $ab$ between $P_1$ and $P_2$} \\\\label{fig:ab} \\\\end{center} \\\\end{figure} Three cases occur: \\\\\\\\ (a) There is no chordless cycle formed by either of the paths $\\\\stackrel\\\\frown{v_j a}$ or $\\\\stackrel\\\\frown{v_j b}$. Let $C$ be the chordless cycle $\\\\stackrel\\\\frown{v_j a}\\\\stackrel\\\\frown{ b v_j}$. Define $T$ with the edge set $$ E\\\\Big(\\\\langle V(G) \\\\setminus V(C)\\\\rangle\\\\Big) \\\\bigcap \\\\big(\\\\bigcup_{i=1}^3 E(P_i)\\\\big).$$ Now, apply \\\\hyperref[lemma:1]{Lemma 1} $\\\\,$for the partition $\\\\{T,C\\\\}$. \\\\\\\\ (b) There are two chordless cycles, say $C_1$ and $C_2$, respectively formed by the paths $\\\\stackrel\\\\frown{v_j a}$ and $\\\\stackrel\\\\frown{v_j b}$. Now, consider the partition $C_1$, $C_2$ and the tree induced on the following edges, $$E\\\\Big(\\\\langle V(G) \\\\setminus \\\\big(V(C_1) \\\\cup V(C_2)\\\\big) \\\\rangle\\\\Big) \\\\; \\\\bigcap \\\\; E\\\\Big(\\\\bigcup_{i=1}^3 P_i\\\\Big),$$ and apply \\\\hyperref[lemma:1]{Lemma 1}.\\\\\\\\ (c) With no loss of generality, there exists a chordless cycle formed by the path $\\\\stackrel\\\\frown{v_j a}$ and there is no chordless cycle formed by the path $\\\\stackrel\\\\frown{v_j b}$. First, suppose that for every chordless cycle $C_t$ on $\\\\stackrel\\\\frown{v_j a}$, at least one of the vertices of $C_t$ is adjacent to a vertex in $V(G) \\\\setminus V(P_1)$. We call one of the edges with one end in $C_t$ and other endpoint in $V(G) \\\\setminus V(P_1)$ by $e_t$. Let $v_j=w_0, w_1, \\\\dots, w_l=a$ be all vertices of the path $\\\\stackrel\\\\frown{v_j a}$ in $P_1$. Choose the shortest path $w_0 w_{i_1} w_{i_2} \\\\dots w_l$ such that $0 < i_1 < i_2 < \\\\dots < l$. Define a tree $T$ whose edge set is the thin edges in Figure \\\\ref{fig:deltaCycle}.\\\\\\\\ Call the cycle $w_0 w_{i_1} \\\\dots w_l \\\\stackrel\\\\frown{b w_0}$ by $C'$. Now, by removing $C'$, $q$ vertex disjoint paths $Q_1, \\\\dots, Q_q$ which are contained in $\\\\stackrel\\\\frown{v_j a}$ remain. Note that there exists a path of order $2$ in $C'$ which by adding this path to $Q_i$ we find a cycle $C_{t_i}$, for some $i$. Hence there exists an edge $e_{t_i}$ connecting $Q_i$ to $V(G) \\\\setminus V(P_1)$. Now, we define a tree $T$ whose the edge set is, $$\\\\quad\\\\quad\\\\quad \\\\bigg( E\\\\Big(\\\\langle V(G) \\\\setminus V(C') \\\\rangle \\\\Big)\\\\; \\\\bigcap \\\\; \\\\Big(\\\\bigcup_{i=1}^3 E(P_i)\\\\Big) \\\\bigg) \\\\bigcup \\\\Big(\\\\big\\\\{e_{t_i} \\\\mid 1 \\\\leq i \\\\leq q \\\\big\\\\} \\\\Big).$$ Apply \\\\hyperref[lemma:1]{Lemma 1} $\\\\,$for the partition $\\\\{T,C'\\\\}$.\\\\\\\\ \\\\begin{figure}[H] \\\\begin{center} \\\\includegraphics[width=40mm]{engImages/deltaCycle.pdf} \\\\caption{The cycle $C'$ and the tree $T$} \\\\label{fig:deltaCycle} \\\\end{center} \\\\end{figure} Next, assume that there exists a cycle $C_1$ formed by $\\\\stackrel\\\\frown{v_j a}$ such that none of the vertices of $C_1$ is adjacent to $V(G) \\\\setminus V(P_1)$. Choose the smallest cycle with this property. Obviously, this cycle is chordless. Now, three cases can be considered:\\\\\\\\ \\\\begin{enumerate}[leftmargin=5pt,label=(\\\\roman*)] \\\\item There exists a cycle $C_2$ formed by $P_2$ or $P_3$. Define the partition $C_1$, $C_2$ and a tree with the following edge set, $$E\\\\Big(\\\\langle V(G) \\\\setminus \\\\big(V(C_1) \\\\cup V(C_2)\\\\big)\\\\rangle \\\\Big) \\\\bigcap \\\\Big( \\\\bigcup_{i=1}^3 E(P_i) \\\\Big),$$ and apply \\\\hyperref[lemma:1]{Lemma 1}.\\\\\\\\ \\\\item There is no chordless cycle formed by $P_2$ and by $P_3$, and there is at least one edge between $V(P_2)$ and $V(P_3)$. Let $ab \\\\in E(G)$, $a \\\\in V(P_2)$ and $b \\\\in V(P_3)$ and moreover $d_{P_2}(v_j, a) + d_{P_3}(v_j,b)$ is minimum. Notice that the cycle $\\\\stackrel\\\\frown{v_j a} \\\\stackrel\\\\frown{b v_j}$ is chordless. Let us call this cycle by $C_2$. Now, define the partition $C_2$ and a tree with the following edge set, $$E\\\\Big(\\\\langle V(G) \\\\setminus V(C_2)\\\\rangle \\\\Big) \\\\bigcap \\\\Big( \\\\bigcup_{i=1}^3 E(P_i) \\\\Big),$$ and apply \\\\hyperref[lemma:1]{Lemma 1}.\\\\\\\\ \\\\item There is no chordless cycle formed by $P_2$ and by $P_3$, and there is no edge between $V(P_2)$ and $V(P_3)$. Let $C_2$ be the cycle consisting of two paths $P_2$ and $P_3$. Define the partition $C_2$ and a tree with the following edge set, $$E\\\\Big(\\\\langle V(G) \\\\setminus V(C_2)\\\\rangle \\\\Big) \\\\bigcap \\\\Big( \\\\bigcup_{i=1}^3 E(P_i) \\\\Big),$$ and apply \\\\hyperref[lemma:1]{Lemma 1}. \\\\end{enumerate} \\\\end{enumerate} \\\\vspace{5mm} \\\\item \\\\textbf{Case 2.} \\\\label{case:2} Assume that $j < i$ for all Hamiltonian paths. Among all Hamiltonian paths consider the path such that $i'-j'$ is maximum. Now, three cases can be considered:\\\\\\\\ \\\\begin{enumerate}[leftmargin=0pt,label=] \\\\item \\\\textbf{Subcase 1.} There is no $s < j'$ and $t > i'$ such that $v_s v_t \\\\in E(G)$. By \\\\hyperref[remark:1]{Remark 1} $\\\\,$ there are two chordless cycles $C_1$ and $C_2$, respectively formed by the paths $v_1 v_{j'}$ and $v_{i'} v_n$. By assumption there is no edge $xy$, where $x \\\\in V(C_1)$ and $y \\\\in V(C_2)$. Define a tree $T$ with the edge set: $$ E\\\\Big(\\\\langle V(G) \\\\setminus \\\\big(V(C_1) \\\\cup V(C_2)\\\\big) \\\\rangle \\\\Big) \\\\bigcap \\\\Big( E(P) \\\\cup \\\\{v_{i'}v_n, v_{j'}v_1\\\\} \\\\Big).$$ Now, apply \\\\hyperref[lemma:1]{Lemma 1} $\\\\,$for the partition $\\\\{T, C_1, C_2\\\\}$.\\\\\\\\ \\\\item \\\\textbf{Subcase 2.} \\\\label{subcase:22} There are at least four indices $s, s' < j$ and $t, t' > i$ such that $v_s v_t, v_{s'} v_{t'} \\\\in E(G)$. Choose four indices $g, h < j$ and $e, f > i$ such that $v_h v_e, v_g v_f \\\\in E(G)$ and $|g-h| + |e-f|$ is minimum. \\\\begin{figure}[H] \\\\begin{center} \\\\includegraphics[width=90mm]{engImages/case2-subcase2.pdf} \\\\caption{Two edges $v_h v_e$ and $v_g v_f$} \\\\label{fig:non-overlapping} \\\\end{center} \\\\end{figure} Three cases can be considered:\\\\\\\\ \\\\begin{enumerate}[leftmargin=0pt,label=(\\\\alph*)] \\\\item There is no chordless cycle formed by $\\\\stackrel\\\\frown{v_g v_h}$ and by $\\\\stackrel\\\\frown{v_e v_f}$. Consider the cycle $\\\\stackrel\\\\frown{v_g v_h} \\\\stackrel\\\\frown{v_e v_f}v_g$ and call it $C$. Now, define a tree $T$ with the edge set, $$\\\\,\\\\,\\\\,E\\\\Big(\\\\langle V(G) \\\\setminus V(C)\\\\rangle \\\\Big) \\\\bigcap \\\\Big( E(P) \\\\cup \\\\{v_1v_{j}, v_{i}v_n\\\\} \\\\Big),$$ apply \\\\hyperref[lemma:1]{Lemma 1} $\\\\,$for the partition $\\\\{T, C\\\\}$.\\\\\\\\ \\\\item With no loss of generality, there exists a chordless cycle formed by $\\\\stackrel\\\\frown{v_e v_f}$ and there is no chordless cycle formed by the path $\\\\stackrel\\\\frown{v_g v_h}$. First suppose that there is a chordless cycle $C_1$ formed by $\\\\stackrel\\\\frown{v_e v_f}$ such that there is no edge between $V(C_1)$ and $\\\\{v_1, \\\\dots, v_j\\\\}$. By \\\\hyperref[remark:1]{Remark 1} $,$ there exists a chordless cycle $C_2$ formed by $\\\\stackrel\\\\frown{v_1 v_j}$. By assumption there is no edge between $V(C_1)$ and $V(C_2)$. Now, define a tree $T$ with the edge set, $$\\\\quad\\\\quad\\\\quad\\\\quad E\\\\Big(\\\\langle V(G) \\\\setminus \\\\big(V(C_1) \\\\cup V(C_2)\\\\big)\\\\rangle \\\\Big) \\\\bigcap \\\\Big( E(P) \\\\cup \\\\{v_1v_{j}, v_{i}v_n\\\\} \\\\Big),$$ and apply \\\\hyperref[lemma:1]{Lemma 1} $\\\\,$for the partition $\\\\{T, C_1, C_2\\\\}$. $\\\\;$ Next assume that for every cycle $C_r$ formed by $\\\\stackrel\\\\frown{v_e v_f}$, there are two vertices $x_r \\\\in V(C_r)$ and $y_r \\\\in \\\\{v_1, \\\\dots, v_j\\\\}$ such that $x_r y_r \\\\in E(G)$. Let $v_e=w_0, w_1, \\\\dots, w_l=v_f$ be all vertices of the path $\\\\stackrel\\\\frown{v_e v_f}$ in $P$. Choose the shortest path $w_0 w_{i_1} w_{i_2} \\\\dots w_l$ such that $0 < i_1 < i_2 < \\\\dots < l$. Consider the cycle $w_0 w_{i_1} \\\\dots w_l \\\\stackrel\\\\frown{v_g v_h}$ and call it $C$. Now, by removing $C$, $q$ vertex disjoint paths $Q_1, \\\\dots, Q_q$ which are contained in $\\\\stackrel\\\\frown{v_e v_f}$ remain. Note that there exists a path of order $2$ in $C$ which by adding this path to $Q_i$ we find a cycle $C_{r_i}$, for some $i$. Hence there exists an edge $x_{r_i} y_{r_i}$ connecting $Q_i$ to $V(G) \\\\setminus V(\\\\stackrel\\\\frown{v_e v_f})$. We define a tree $T$ whose edge set is the edges, $$\\\\quad\\\\quad\\\\quad\\\\quad\\\\quad\\\\quad E\\\\Big(\\\\langle V(G) \\\\setminus V(C)\\\\rangle \\\\Big) \\\\bigcap \\\\Big( E(P) \\\\cup \\\\{v_1v_{j}, v_{i}v_n\\\\} \\\\cup \\\\big\\\\{x_{r_i} y_{r_i} \\\\mid 1 \\\\leq i \\\\leq q\\\\big\\\\} \\\\Big),$$ then apply \\\\hyperref[lemma:1]{Lemma 1} $\\\\,$ on the partition $\\\\{T, C\\\\}$.\\\\\\\\ \\\\begin{figure}[H] \\\\begin{center} \\\\includegraphics[width=90mm]{engImages/deltaNonOverlapping.pdf} \\\\caption{The tree $T$ and the shortest path $w_0 w_{i_1}\\\\dots w_l$} \\\\label{fig:delta-non-overlapping} \\\\end{center} \\\\end{figure} \\\\item There are at least two chordless cycles, say $C_1$ and $C_2$ formed by the paths $\\\\stackrel\\\\frown{v_g v_h}$ and $\\\\stackrel\\\\frown{v_e v_f}$, respectively. Since $|g-h| + |e-f|$ is minimum, there is no edge $xy \\\\in E(G)$ with $x \\\\in V(C_1)$ and $y \\\\in V(C_2)$. Now, define a tree $T$ with the edge set, $$\\\\quad\\\\quad\\\\quad\\\\quad E\\\\Big( \\\\langle V(G) \\\\setminus \\\\big(V(C_1) \\\\cup V(C_2)\\\\big) \\\\rangle \\\\Big) \\\\bigcap \\\\Big( E(P) \\\\cup \\\\{v_1 v_{j}, v_{i}v_n\\\\} \\\\Big),$$ and apply \\\\hyperref[lemma:1]{Lemma 1} $\\\\,$for the partition $\\\\{T, C_1, C_2\\\\}$.\\\\\\\\ \\\\end{enumerate} \\\\item \\\\textbf{Subcase 3.} There exist exactly two indices $s,t$, $s < j' < i' < t$ such that $v_s v_t \\\\in E(G)$ and there are no two other indices $s', t'$ such that $s' < j < i < t'$ and $v_{s'} v_{t'} \\\\in E(G)$. We can assume that there is no cycle formed by $\\\\stackrel\\\\frown{v_{s+1} v_j}$ or $\\\\stackrel\\\\frown{v_i v_{t-1}}$, to see this by symmetry consider a cycle $C$ formed by $\\\\stackrel\\\\frown{v_{s+1} v_j}$. By \\\\hyperref[remark:1]{Remark 1} $\\\\,$ there exist chordless cycles $C_1$ formed by $\\\\stackrel\\\\frown{v_{s+1} v_j}$ and $C_2$ formed by $\\\\stackrel\\\\frown{v_{i} v_n}$. By assumption $v_s v_t$ is the only edge such that $s < j$ and $t > i \\\\;$. Therefore, there is no edge between $V(C_1)$ and $V(C_2)$. Now, let $T$ be a tree defined by the edge set, $$ E\\\\Big(\\\\langle V(G) \\\\setminus \\\\big(V(C_1) \\\\cup V(C_2)\\\\big)\\\\rangle \\\\Big) \\\\bigcap \\\\Big( E(P) \\\\cup \\\\{v_1v_{j}, v_{i}v_n\\\\} \\\\Big),$$ and apply \\\\hyperref[lemma:1]{Lemma 1} $\\\\,$for the partition \\\\{$T$, $C_1$, $C_2$\\\\}.\\\\\\\\ $\\\\quad$Furthermore, we can also assume that either $s \\\\neq j'-1$ or $t \\\\neq i'+1$, otherwise we have the Hamiltonian cycle $\\\\stackrel\\\\frown{v_1 v_s} \\\\stackrel\\\\frown{v_t v_n} \\\\stackrel\\\\frown{v_{i'} v_{j'}} v_1$ and by \\\\cite[Theorem 9]{akbari} Conjecture \\\\theconjecture$\\\\,$ holds. $\\\\quad$By symmetry, suppose that $s \\\\neq j'-1$. Let $v_k$ be the vertex adjacent to $v_{j'-1}$, and $k \\\\notin \\\\{j'-2, j'\\\\}$. It can be shown that $k > j'-1$, since otherwise by considering the Hamiltonian path $P': \\\\; \\\\stackrel\\\\frown{ v_{k+1} v_{j'-1}}\\\\stackrel\\\\frown{v_k v_1} \\\\stackrel\\\\frown{v_{j'} v_n}$, the new $i'-j'$ is greater than the old one and this contradicts our assumption about $P$ in the \\\\hyperref[case:2]{Case 2}. $\\\\quad$We know that $j' < k < i$. Moreover, the fact that $\\\\stackrel\\\\frown{v_{s+1} v_j}$ does not form a cycle contradicts the case that $j' < k \\\\le j$. So $j < k < i$. Consider two cycles $C_1$ and $C_2$, respectively with the vertices $v_1 \\\\stackrel\\\\frown{v_{j'} v_{j}} v_1$ and $v_n \\\\stackrel\\\\frown{v_{i'} v_{i}} v_n$. The cycles $C_1$ and $C_2$ are chordless, otherwise there exist cycles formed by the paths $\\\\stackrel\\\\frown{v_{s+1} v_j}$ or $\\\\stackrel\\\\frown{v_i v_{t-1}}$. Now, define a tree $T$ with the edge set $$ E\\\\Big(\\\\langle V(G) \\\\setminus \\\\big(V(C_1) \\\\cup V(C_2)\\\\big)\\\\rangle \\\\Big) \\\\bigcap \\\\Big( E(P) \\\\cup \\\\{v_s v_t, v_k v_{j'-1}\\\\} \\\\Big),$$ and apply \\\\hyperref[lemma:1]{Lemma 1} $\\\\,$for the partition \\\\{$T$, $C_1$, $C_2$\\\\}. \\\\end{enumerate} \\\\end{enumerate} \\\\end{proof} \\\\noindent\\\\textbf{Remark 2.} \\\\label{remark:2} Indeed, in the proof of the previous theorem we showed a stronger result, that is, for every traceable cubic graph there is a decomposition with at most two cycles. {'timestamp': '2016-07-19T02:04:55', 'yymm': '1607', 'arxiv_id': '1607.04768', 'language': 'en', 'url': 'https://arxiv.org/abs/1607.04768'} arxiv \\\\section{Principle of nano strain-amplifier} \\\\begin{figure*}[t!] \\\\centering \\\\includegraphics[width=5.4in]{Fig1} \\\\vspace{-0.5em} \\\\caption{Schematic sketches of nanowire strain sensors. (a)(b) Conventional non-released and released NW structure; (c)(d) The proposed nano strain-amplifier and its simplified physical model.} \\\\label{fig:fig1} \\\\vspace{-1em} \\\\end{figure*} Figure \\\\ref{fig:fig1}(a) and 1(b) show the concept of the conventional structures of piezoresistive sensors. The piezoresistive elements are either released from, or kept on, the substrate. The sensitivity ($S$) of the sensors is defined based on the ratio of the relative resistance change ($\\\\Delta R/R$) of the sensing element and the strain applied to the substrate ($\\\\varepsilon_{sub}$): \\\\begin{equation} S = (\\\\Delta R/R)/\\\\varepsilon_{sub} \\\\label{eq:sensitivity} \\\\end{equation} In addition, the relative resistance change $\\\\Delta R/R$ can be calculated from the gauge factor ($GF$) of the material used to make the piezoresistive elements: $\\\\Delta R/R = GF \\\\varepsilon_{ind}$, where $\\\\varepsilon_{ind}$ is the strain induced into the piezoresistor. In most of the conventional strain gauges as shown in Fig. \\\\ref{fig:fig1} (a,b), the thickness of the sensing layer is typically below a few hundred nanometers, which is much smaller than that of the substrate. Therefore, the strain induced into the piezoresistive elements is approximately the same as that of the substrate ($\\\\varepsilon_{ind} \\\\approx \\\\varepsilon_{sub}$). Consequently, to improve the sensitivity of strain sensors (e.g. enlarging $\\\\Delta R/R$), electrical approaches which can enlarge the gauge factor ($GF$) are required. Nevertheless, as aforementioned, the existence of the large gauge factor in nanowires due to quantum confinement or surface state, is still considered as controversial. It is also evident from Eq. \\\\ref{eq:sensitivity} that the sensitivity of strain sensors can also be improved using a mechanical approach, which enlarges the strain induced into the piezoresistive element. Figure \\\\ref{fig:fig1}(c) shows our proposed nano strain-amplifier structure, in which the piezoresistive nanowires are locally fabricated at the centre of a released bridge. The key idea of this structure is that, under a certain strain applied to the substrate, a large strain will be concentrated at the locally fabricated SiC nanowires. The working principle of the nano strain-amplifier is similar to that of the well-known dogbone structure, which is widely used to characterize the tensile strength of materials \\\\cite{dogbone1,dogbone2}. That is, when a stress is applied to the dogbone-shape of a certain material, a crack, if generated, will occur at the middle part of the dogbone. The large strain concentrated at the narrow area located at the centre part with respect to the wider areas located at outer region, causes the crack. Qualitative and quantitative explanations of the nano strain-amplifier are presented as follows. For the sake of simplicity, the released micro frame and nanowire (single wire or array) of the nano strain-amplifier can be considered as solid springs, Fig. \\\\ref{fig:fig1}(d). The stiffness of these springs are proportional to their width ($w$) and inversely proportional to their length (l): $K \\\\propto w/l$. Consequently, the model of the released nanowire and micro frames can be simplified as a series of springs, where the springs with higher stiffness correspond to the micro frame, and the single spring with lower stiffness corresponds to the nanowire. It is well-known in classical physics that, for serially connected springs, a larger strain will be concentrated in the low--stiffness string, while a smaller strain will be induced in the high--stiffness string \\\\cite{Springbook}. The following analysis quantitatively explained the amplification of the strain. \\\\begin{figure}[b!] \\\\centering \\\\includegraphics[width=3in]{Fig2} \\\\vspace{-1em} \\\\caption{Finite element analysis of the strain induced in to the nanowire array utilizing nano strain-amplifier.} \\\\label{fig:fig2} \\\\end{figure} When a tensile mechanical strain ($\\\\varepsilon_{sub}$) is applied to the substrate, the released structure will also be elongated. Since the stiffness of the released frame is much smaller than that of the substrate, it is safe to assume that the released structure will follows the elongation of the substrate. The displacement of the released structure $\\\\Delta L$ is: \\\\begin{equation} \\\\Delta L = \\\\Delta L_m + \\\\Delta L_n = L_m \\\\varepsilon_m + L_n \\\\varepsilon_n \\\\label{eq:displacement} \\\\end{equation} where $L_m$, $L_n$ are the length; $\\\\Delta L_m$, $\\\\Delta L_n$ are the displacement; and $\\\\varepsilon_m$, $\\\\varepsilon_n$ are the strains induced into the micro spring and nano spring, respectively. The subscripts m and n stand for the micro frames and nanowires, respectively. Furthermore, due to the equilibrium of the stressing force ($F$) along the series of springs, the following relationship is established: $F= K_m\\\\Delta L_m = K_n \\\\Delta L_n$, where $K_m$, $K_n$ are the stiffness of the released micro frames and nanowires, respectively. Consequently the relationship between the displacement of the micro frame (higher stiffness) and nanowires (lower stiffness) is: \\\\begin{equation} \\\\frac{\\\\Delta L_m}{\\\\Delta L_n}=\\\\frac{K_n}{K_m}=\\\\frac{L_mw_n}{L_nw_m} \\\\label{eq:euili} \\\\end{equation} Substituting Eqn. \\\\ref{eq:euili} into Eqn. \\\\ref{eq:displacement}, the strain induced into the locally fabricated nanowires is: \\\\begin{equation} \\\\varepsilon_n = \\\\frac{\\\\Delta L_n}{L_n} = \\\\frac{1}{1-\\\\frac{w_m-w_n}{w_m}\\\\frac{L_m}{L}}\\\\varepsilon_{sub} \\\\label{eq:strainamp} \\\\end{equation} Equation \\\\ref{eq:strainamp} indicates that increasing the ratio of $w_m/w_n$ and $L_m/L_n$ significantly amplifies the strain induced into the nanowire from the strain applied to the substrate. This model is also applicable to the case of nanowire arrays, in which $w_n$ is the total width of all nanowires in the array. The theoretical model is then verified using the finite element analysis (FEA). In the FEA simulation, we compare the strain induced into (i) non released nanowires, (ii) the conventionally released nanowires, and (iii) our nano strain-amplifier structure, using COMSOL Multiphysics \\\\texttrademark. In our nano strain amplifying structure, the width of the released frame was set to be 8 $\\\\mu$m, while the width of each nanowire in the array (3 wires) was set to be 370 nm. The nanowires array structure was selected as it can enhance the electrical conductance of the SiC nanowires resistor which makes the subsequent experimental demonstration easier. The ratio between the length of nanowires and micro bridge was set to be 1: 20. With this geometrical dimensions, strain induced into nanowires array $\\\\varepsilon_n$ was numerically calculated to be approximately 6 times larger than $\\\\varepsilon_{sub}$, Eqn. \\\\ref{eq:strainamp}. The simulation results show that for all structure, the elongation of non-released and released nanowires follow that of the substrate. In addition, strain was almost completely transferred into conventional released and non-released structures. Furthermore, the ratio of the strain induced in to the locally fabricated nanowires was estimated to be 5.9 times larger than that of the substrate, Fig. \\\\ref{fig:fig2}. These results are in solid agreement with the theoretical analysis presented above. For a nanowire array with an average width of 470 nm, the amplified gain of strain was found to be 4.5. Based on the theoretical analysis, we conducted the following experiments to demonstrate the high sensitivity of SiC nanowire strain sensors using the nano strain-amplifier. A thin 3C-SiC film with its thickness of 300 nm was epitaxially grown on a 150 mm diameter Si wafer using low pressure chemical vapour deposition \\\\cite{SiC_growth}. The film was \\\\emph{in situ} doped using Al dopants. The carrier concentration of the p-type 3C-SiC was found to be $5 \\\\times 10^{18}$ cm$^{-3}$, using a hot probe technique \\\\cite{philip}. The details of the characteristics of the grown film can be found elsewhere \\\\cite{Phan_JMC}. Subsequently, I-shape p-type SiC resistors with aluminum electrodes deposited on the surface were patterned using inductive coupled plasma (ICP) etching. As the piezoresistance of p-type 3C-SiC depends on crystallographic orientation, all SiC resistors of the present work were aligned along [110] direction to maximize the piezoresistive effect. Next, the micro scale SiC resistors were then released from the Si substrate using dry etching (XeF$_2$). Finally, SiC nanowire arrays were formed at the centre of the released bridge using focused ion beam (FIB). Two types of nanowire array were fabricated with three nanowires for each array. The average width of each nanowire in each type were 380 nm and 470 nm, respectively. Figure \\\\ref{fig:fig3} shows the SEM images of the fabricated samples, including the conventional released structure, non-released nanowires, and the nano strain-amplifier. \\\\begin{figure}[t!] \\\\centering \\\\includegraphics[width=3in]{Fig3} \\\\caption{SEM image of SiC strain sensors. (a) Released SiC micro bridge used for the subsequent fabrication of the nano strain-amplifier; (b) SEM of a micro SiC resistor where the SiC nanowires array were formed using FIB; (c) SEM of non-released SiC nanowires; (d) SEM of locally fabricated SiC nanowires released from the Si substrate (nano strain-amplifier).} \\\\label{fig:fig3} \\\\vspace{-1em} \\\\end{figure} The current voltage (I-V) curves of all fabricated samples were characterized using a HP 4145 \\\\texttrademark ~parameter analyzer. The linear relationship between the applied voltage and measured current, indicated that Al made a good Ohmic contact with the highly doped SiC resistance, Fig. \\\\ref{fig:IV}. Additionally, the electrical conductivity of both nanowires and micro frame estimated from the I-V curve and the dimensions of the resistors shows almost the same value. This indicated that the FIB process did not cause a significant surface damage to the fabricated nanowires. \\\\begin{figure}[b!] \\\\centering \\\\includegraphics[width=3in]{Fig4} \\\\vspace{-1.5em} \\\\caption{Current voltage curves of the fabricated SiC resistors.} \\\\label{fig:IV} \\\\end{figure} The bending experiment was used to characterize the piezoresistive effect in micro size SiC resistors and locally fabricated SiC nanowire array. In this experiment one end of the Si cantilever (with a thickness of 625 $\\\\mu$m, and a width of 7 mm) was fixed while the other end was deflected by applying different forces. The distance from the fabricated nanowires to the free end of the Si cantilever was approximately 45 mm. The strain induced into the Si substrate is $\\\\varepsilon_\\\\text{sub} = Mt/2EI$, where $M$ is the applied bending moment; and $t$, $E$ and $I$ are the thickness, Young's modulus and the moment of inertia of the Si cantilever, respectively. The response of the SiC resistance to applied strain was then measured using a multimeter (Agilent \\\\texttrademark 34401 A). \\\\begin{figure}[h!] \\\\centering \\\\includegraphics[width=3in]{Fig5.eps} \\\\vspace{-1.5em} \\\\caption{Experimental results. (a) A comparision between the relative resistance change in the nano strain-amplifiers, non released nanowires and released micro frames; (b) The repeatability of the SiC nanowires strain sensors utilizing the proposed structure.} \\\\label{fig:DRR} \\\\vspace{-1em} \\\\end{figure} The relative resistance change ($\\\\Delta R/R$) of the micro and nano SiC resistors was plotted against the strain induced into the Si substrate $\\\\varepsilon_{sub}$, Fig. \\\\ref{fig:DRR}(a). For all fabricated samples, the relative resistance change shows a good linear relationship with the applied strain ($\\\\varepsilon_{sub}$). In addition, with the same applied strain to the Si substrate, the resistance change of the SiC nanowires using the nano strain-amplifier was much larger than that of the the SiC micro resistor and the conventional non-released SiC nanowires. In addition, reducing the width of the SiC nanowires also resulted in the increase of the sensitivity. The magnitude of the piezoresistive effect in the nano strain-amplifier as well as conventional structures were then quantitatively evaluated based on the effective gauge factor ($GF_{eff}$), which is defined as the ratio of the relative resistance change to the applied strain to the substrate: $GF_{eff} = (\\\\Delta R/R)/\\\\varepsilon_{sub}$. Accordingly, the effective gauge factor of the released micro SiC was found to be 28, while that of the non-released SiC nanowires was 35. From the data shown in Fig. \\\\ref{fig:DRR}, the effective gauge factor of the 380 nm and 470 nm SiC nanowires in the nano strain-amplifier were calculated as 150 and 124, respectively. Thus for nanowire arrays with average widths of 380 nm and 470 nm, the sensitivity of the nano strain-amplifier was 5.4 times and 4.6 times larger than the bulk SiC, respectively. These results were consistent with analytical and numerical models presented above. The relative resistance change of the nano strain-amplifier also showed excellent linearity with the applied strain, with a linear regression of above 99\\\\%. The resistance change of the nano strain-amplifier can also be converted into voltage signals using a Wheatstone bridge, Fig. \\\\ref{fig:DRR}(b). The output voltage of the nano strain-amplifier increases with increasing tensile strains from 0 ppm to 180 ppm, and returned to the initial value when the strain was completely removed, confirming a good repeatability after several strain induced cycles. The linearity of the relative resistance change, and the repeatability indicate that the proposed structure is promising for strain sensing applications. In conclusion, this work presents a novel mechanical approach to obtain highly sensitive piezoresistance in nanowires based on a nano strain-amplifier. The key factor of the nano strain-amplifier lies on nanowires locally fabricated on a released micro structure. Experimental studies were conducted on SiC nanowires, confirming that by utilizing our nano strain-amplifier, the sensitivity of SiC nanowires was 5.4 times larger than that of conventional structures. This result indicated that the nano strain-amplifier is an excellent platform for ultra sensitive strain sensing applications. {'timestamp': '2016-07-18T02:07:38', 'yymm': '1607', 'arxiv_id': '1607.04531', 'language': 'en', 'url': 'https://arxiv.org/abs/1607.04531'} arxiv \\\\section{Introduction}\\\\label{intro} Gas has a fundamental role in shaping the evolution of galaxies, through its accretion on to massive haloes, cooling and subsequent fuelling of star formation, to the triggering of extreme luminous activity around super massive black holes. Determining how the physical state of gas in galaxies changes as a function of redshift is therefore crucial to understanding how these processes evolve over cosmological time. The standard model of the gaseous interstellar medium (ISM) in galaxies comprises a thermally bistable medium (\\\\citealt*{Field:1969}) of dense ($n \\\\sim 100$\\\\,cm$^{-3}$) cold neutral medium (CNM) structures, with kinetic temperatures of $T_{\\\\rm k} \\\\sim 100$\\\\,K, embedded within a lower-density ($n \\\\sim 1$\\\\,cm$^{-3}$) warm neutral medium (WNM) with $T_{\\\\rm k} \\\\sim 10^{4}$\\\\,K. The WNM shields the cold gas and is in turn ionized by background cosmic rays and soft X-rays (e.g. \\\\citealt{Wolfire:1995, Wolfire:2003}). A further hot ($T_{\\\\rm k} \\\\sim 10^{6}$\\\\,K) ionized component was introduced into the model by \\\\cite{McKee:1977}, to account for heating by supernova-driven shocks within the inter-cloud medium. In the local Universe, this paradigm has successfully withstood decades of observational scrutiny, although there is some evidence (e.g. \\\\citealt{Heiles:2003b}; \\\\citealt*{Roy:2013b}; \\\\citealt{Murray:2015}) that a significant fraction of the WNM may exist at temperatures lower than expected for global conditions of stability, requiring additional dynamical processes to maintain local thermodynamic equilibrium. Since atomic hydrogen (\\\\mbox{H\\\\,{\\\\sc i}}) is one of the most abundant components of the neutral ISM and readily detectable through either the 21\\\\,cm or Lyman $\\\\alpha$ lines, it is often used as a tracer of the large-scale distribution and physical state of neutral gas in galaxies. The 21\\\\,cm line has successfully been employed in surveying the neutral ISM in the Milky Way (e.g. \\\\citealt{McClure-Griffiths:2009,Murray:2015}), the Local Group (e.g. \\\\citealt{Kim:2003,Bruns:2005,Braun:2009,Gratier:2010}) and low-redshift Universe (see \\\\citealt{Giovanelli:2016} for a review). However, beyond $z \\\\sim 0.4$ (\\\\citealt{Fernandez:2016}) \\\\mbox{H\\\\,{\\\\sc i}} emission from individual galaxies becomes too faint to be detectable by current 21\\\\,cm surveys and so we must rely on absorption against suitably bright background radio (21\\\\,cm) or UV (Lyman-$\\\\alpha$) continuum sources to probe the cosmological evolution of \\\\mbox{H\\\\,{\\\\sc i}}. The bulk of neutral gas is contained in high-column-density damped Lyman-$\\\\alpha$ absorbers (DLAs, $N_{\\\\rm HI} \\\\geq 2 \\\\times 10^{20}$\\\\,cm$^{-2}$; see \\\\citealt*{Wolfe:2005} for a review), which at $z \\\\gtrsim 1.7$ are detectable in the optical spectra of quasars. Studies of DLAs provide evidence that the atomic gas in the distant Universe appears to be consistent with a multi-phase neutral ISM similar to that seen in the Local Group (e.g. \\\\citealt*{Lane:2000}; \\\\citealt*{Kanekar:2001c}; \\\\citealt*{Wolfe:2003b}). However, there is some variation in the cold and warm fractions measured throughout the DLA population (e.g. \\\\citealt*{Howk:2005}; \\\\citealt{Srianand:2005, Lehner:2008}; \\\\citealt*{Jorgenson:2010}; \\\\citealt{Carswell:2011, Carswell:2012, Kanekar:2014a}; \\\\citealt*{Cooke:2015}; \\\\citealt*{Neeleman:2015}). The 21-cm spin temperature affords us an important line-of-enquiry in unraveling the physical state of high-redshift atomic gas. This quantity is sensitive to the processes that excite the ground-state of \\\\mbox{H\\\\,{\\\\sc i}} in the ISM (\\\\citealt{Purcell:1956,Field:1958,Field:1959b,Bahcall:1969}) and therefore dictates the detectability of the 21\\\\,cm line in absorption. In the CNM the spin temperature is governed by collisional excitation and so is driven to the kinetic temperature, while the lower densities in the WNM mean that the 21\\\\,cm transition is not thermalized by collisions between the hydrogen atoms, and so photo-excitation by the background Ly $\\\\alpha$ radiation field becomes important. Consequently the spin temperature in the WNM is lower than the kinetic temperature, in the range $\\\\sim$1000 -- 5000\\\\,K depending on the column density and number of multi-phase components (\\\\citealt{Liszt:2001}). Importantly, the spin temperature measured from a single detection of extragalactic absorption is equal to the harmonic mean of the spin temperature in individual gas components, weighted by their column densities, thereby providing a method of inferring the CNM fraction in high-redshift systems. Surveys for 21\\\\,cm absorption in known redshifted DLAs have been used to simultaneously measure the column density and spin temperature of \\\\mbox{H\\\\,{\\\\sc i}} (see \\\\citealt{Kanekar:2014a} and references therein). There is some evidence for an increase (at $4\\\\,\\\\sigma$ significance) in the spin temperature of DLAs at redshifts above $z = 2.4$, and a difference (at $6\\\\,\\\\sigma$ significance) between the distribution of spin temperatures in DLAs and the Milky Way (\\\\citealt{Kanekar:2014a}). The implication that at least 90\\\\,per\\\\,cent of high-redshift DLAs may have CNM fractions significantly less than that measured for the Milky Way has important consequences for the heating and cooling of neutral gas in the early Universe and star formation (e.g. \\\\citealt*{Wolfe:2003a}). However, these targeted observations rely on the limited availability of simultaneous 21\\\\,cm and optical/UV data for the DLAs and assumes commonality between the column density probed by the optical and radio sight-lines. The first issue can be overcome by improving the sample statistics through larger 21\\\\,cm line surveys of high-redshift DLAs, but the latter requires improvements to our methodology and understanding of the gas distribution in these systems. There are also concerns about the accuracy to which the fraction of the source structure subtended by the absorber can be measured in each system, which can only be resolved through spectroscopic very long baseline interferometry (VLBI). It has been suggested that the observed evolution in spin temperature could be biased by assumptions about the radio-source covering factor (\\\\citealt{Curran:2005}) and its behaviour as a function of redshift (\\\\citealt{Curran:2006b, Curran:2012b}). In this paper we consider an approach using the statistical constraint on the average spin temperature achievable with future large 21\\\\,cm surveys using precursor telescopes to the Square Kilometre Array (SKA). This will enable independent verification of the evolution in spin temperature at high redshift and provide a method of studying the global properties of neutral gas below $z \\\\approx 1.7$, where the Lyman\\\\,$\\\\alpha$ line is inaccessible using ground-based observatories. In an early attempt at a genuinely blind 21\\\\,cm absorption survey, \\\\cite{Darling:2011} used pilot data from the Arecibo Legacy Fast Arecibo L-band Feed Array (ALFALFA) survey to obtain upper limits on the column density frequency distribution from 21\\\\,cm absorption at low redshift ($z \\\\lesssim 0.06$). However, they also noted that the number of detections could be used to make inferences about the ratio of the spin temperature to covering factor. Building upon this work, \\\\cite{Wu:2015} found that their upper limits on the frequency distribution function measured from the 40\\\\,per\\\\,cent ALFALFA survey ({$\\\\alpha$}.40; \\\\citealt{Haynes:2011}) could only be reconciled with measurements from other low-redshift 21\\\\,cm surveys if the typical spin temperature to covering factor ratio was greater than 500\\\\,K. At higher redshifts, \\\\cite{Gupta:2009} found that the number density of 21\\\\,cm absorbers in known \\\\mbox{Mg\\\\,{\\\\sc ii}} absorbers appeared to decrease with redshift above $z \\\\sim 1$, consistent with a reduction in the CNM fraction. We pursue this idea further by investigating whether future wide-field 21\\\\,cm surveys can be used to measure the average spin temperature in distant galaxies that are rich in atomic gas. \\\\section{The expected number of intervening \\\\mbox{H\\\\,{\\\\sc i}} absorbers}\\\\label{section:expected_number} We estimate the expected number of intervening \\\\mbox{H\\\\,{\\\\sc i}} systems towards a sample of background radio sources by evaluating the following integral over all sight-lines \\\\begin{equation}\\\\label{equation:expected_number} \\\\mu = \\\\iint{f(N_{\\\\rm HI},X)\\\\,\\\\mathrm{d}X\\\\,\\\\mathrm{d}N_{\\\\rm HI}}, \\\\end{equation} where $f(N_{\\\\rm HI}, X)$ is the frequency distribution as a function of column density ($N_{\\\\rm HI}$) and comoving path length ($X$). We use the results of recent surveys for 21\\\\,cm emission in nearby galaxies (e.g. \\\\citealt{Zwaan:2005}) and high-redshift Lyman-$\\\\alpha$ absorption in the Sloan Digitial Sky Survey (SDSS; e.g. \\\\citealt*{Prochaska:2005}; \\\\citealt{Noterdaeme:2009}), which show that $f(N_{\\\\rm HI}, X)$ can be parametrized by a gamma function of the form \\\\begin{equation} f(N_{\\\\rm HI}, X) = \\\\left({f_{\\\\ast} \\\\over N_{\\\\ast}}\\\\right)\\\\left({N_{\\\\rm HI} \\\\over N_{\\\\ast}}\\\\right)^{-\\\\beta}\\\\exp{\\\\left(-{N_{\\\\rm HI} \\\\over N_{\\\\ast}}\\\\right)}\\\\,\\\\mathrm{cm}^{2}, \\\\end{equation} where $f_{\\\\ast} = 0.0193$, $\\\\log_{10}(N_{\\\\ast}) = 21.2$ and $\\\\beta = 1.24$ at $z = 0$ (\\\\citealt{Zwaan:2005}), and $f_{\\\\ast} = 0.0324$, $\\\\log_{10}(N_{\\\\ast}) = 21.26$ and $\\\\beta = 1.27$ at $z \\\\approx 3$ (\\\\citealt{Noterdaeme:2009}). While the observational data do not yet constrain models for evolution of the \\\\mbox{H\\\\,{\\\\sc i}} distribution at intermediate redshifts between $z \\\\sim 0.1$ and $3$\\\\footnote{Measurements of $f(N_{\\\\rm HI},X)$ at intermediate redshifts come from targeted ultra-violet surveys of DLAs using the \\\\emph{Hubble Space Telescope} (\\\\citealt*{Rao:2006}; \\\\citealt{Neeleman:2016}). However, due to the limited sample sizes these are currently an order-of-magnitude less sensitive than the nearby 21-cm and high-redshift optical Lyman-$\\\\alpha$ surveys.}, it is known to be much weaker than the significant decline seen in the global star-formation rate and molecular gas over the same epoch (e.g. \\\\citealt{Lagos:2014}). We therefore carry out a simple linear interpolation between the low and high redshift epochs to estimate $f(N_{\\\\rm HI},X)$ as a function of redshift. \\\\begin{figure} \\\\centering \\\\includegraphics[width=0.475\\\\textwidth]{width_dist.pdf} \\\\caption{The distribution of 21\\\\,cm line widths based on existing detections of intervening absorption at $z > 0.1$ (see the text for details of this sample). The sample size in each bin is denoted by the number above and errorbars denote the standard deviation. The solid red line is a log-normal fit to the data, from which we draw random samples for our analysis.}\\\\label{figure:width_dist} \\\\end{figure} The probability of detecting an absorbing system of given column density depends on the sensitivity of the survey, the flux density and structure of the background source and the fraction of \\\\mbox{H\\\\,{\\\\sc i}} in the lower spin state, given by the spin temperature. We express the column density ($N_{\\\\rm HI}$; in atoms\\\\,cm$^{-2}$) in terms of the optical depth ($\\\\tau$) and spin temperature ($T_{\\\\rm spin}$; in K) by \\\\begin{equation}\\\\label{equation:column_density} N_{\\\\rm HI} = 1.823\\\\times10^{18}\\\\,T_{\\\\rm spin} \\\\int{\\\\tau(v)\\\\mathrm{d}v}, \\\\end{equation} where the integral is performed across the spectral line in the system rest-frame velocity $v$ (in km\\\\,s$^{-1}$). We then express the optical depth in terms of the observables as \\\\begin{equation} \\\\tau = -\\\\ln\\\\left[1 + {\\\\Delta{S}\\\\over c_{\\\\rm f}S_{\\\\rm cont}}\\\\right], \\\\end{equation} where $\\\\Delta{S}$ is the observed change in flux density due to absorption, $S_{\\\\rm cont}$ is the background continuum flux density and $c_{\\\\rm f}$ is the (often unknown) fraction of background flux density subtended by the intervening gas. \\\\begin{figure} \\\\centering \\\\includegraphics[width=0.465\\\\textwidth]{covfact_dist.pdf} \\\\caption{The distribution of \\\\mbox{H\\\\,{\\\\sc i}} covering factors from the main sample of \\\\citet{Kanekar:2014a}, which were estimated using the fraction of total continuum flux density in the quasar core. The sample size in each bin is denoted by the number above and errorbars denote the standard deviation. The solid red line is the uniform distribution, from which we draw random samples for our analysis.}\\\\label{figure:covfact_dist} \\\\end{figure} We assume that a single intervening system can be described by a Gaussian velocity distribution of full width at half maximum (FWHM) dispersion ($\\\\Delta{v_{\\\\rm 50}}$) and peak optical depth ($\\\\tau_{\\\\rm peak}$), so that \\\\autoref{equation:column_density} can be re-written as \\\\begin{equation}\\\\label{equation:column_density_gaussian} N_{\\\\rm HI} = 1.941\\\\times10^{18}\\\\,T_{\\\\rm spin}\\\\,\\\\tau_{\\\\rm peak}\\\\,\\\\Delta{v_{\\\\rm 50}}. \\\\end{equation} If we further assume that the rms spectral noise is Gaussian, with a standard deviation $\\\\sigma_{\\\\rm chan}$ per independent channel $\\\\Delta{v_{\\\\rm chan}}$, then the 5$\\\\sigma$ column density detection limit is given by \\\\begin{equation} N_{5\\\\sigma} \\\\approx 1.941\\\\times10^{18}\\\\,T_{\\\\rm spin}\\\\,\\\\tau_{\\\\rm 5\\\\sigma}\\\\,\\\\Delta{v_{\\\\rm conv}}, \\\\end{equation} where \\\\begin{equation}\\\\label{equation:optical_depth_limit} \\\\tau_{5\\\\sigma} \\\\approx -\\\\ln\\\\left[1 - {5\\\\,\\\\sigma_{\\\\rm chan}\\\\over c_{\\\\rm f}\\\\,S_{\\\\rm cont}}\\\\sqrt{\\\\Delta{v}_{\\\\rm chan}\\\\over \\\\Delta{v_{\\\\rm conv}}}\\\\right], \\\\end{equation} and $\\\\Delta{v_{\\\\rm conv}} \\\\approx \\\\sqrt{\\\\Delta{v}_{\\\\rm chan}^{2} + \\\\Delta{v}_{50}^{2}}$, which is the observed width of the line, given by the convolution of the physical velocity distribution and the spectral resolution of the telescope. We now redefine $\\\\mu$ as the expected number of intervening \\\\mbox{H\\\\,{\\\\sc i}} detections in our survey as a function of the column density sensitivity along each sight-line where each comoving path element $\\\\delta{X}(z)$\\\\footnote{For the purposes of this work we adopt a flat $\\\\Lambda$ cold dark matter cosmology with $H_{0}$ = 70\\\\,km\\\\,s$^{-1}$, $\\\\Omega_\\\\mathrm{M}$ = 0.3 and $\\\\Omega_{\\\\Lambda}$ = 0.7. } in the integral defined by \\\\autoref{equation:expected_number} is given by \\\\begin{equation} \\\\delta{X}(z)= \\\\begin{cases} {\\\\delta{z}\\\\,(1+z)^{2}\\\\over \\\\sqrt{(1+z)^{2}(1+z\\\\Omega_{\\\\rm M})-z(z+2)\\\\Omega_{\\\\rm \\\\Lambda}}}, & \\\\text{if}\\\\ N_{\\\\rm HI} \\\\geq N_{5\\\\sigma}, \\\\\\\\ 0, & \\\\text{otherwise}. \\\\end{cases} \\\\end{equation} To calculate the column density sensitivity for each comoving element we draw random samples for $\\\\Delta{v}_{50}$ and $c_{\\\\rm f}$ from continuous prior distributions based on existing evidence. In the case of $\\\\Delta{v}_{50}$ we use a log-normal distribution obtained from a simple least-squares fit to the sample distribution from previous 21-cm absorption surveys reported in the literature (see \\\\autoref{figure:width_dist})\\\\footnote{References for the literature sample of line widths shown in \\\\autoref{figure:width_dist}: \\\\citet*{Briggs:2001}; \\\\citet*{Carilli:1993}; \\\\citet*{Chengalur:1999}; \\\\citet{Chengalur:2000, Curran:2007b, Davis:1978, Ellison:2012, Gupta:2009, Gupta:2012, Gupta:2013}; \\\\citet{Kanekar:2001b,Kanekar:2003b}; \\\\citet{Kanekar:2001c, Kanekar:2006, Kanekar:2009a, Kanekar:2013, Kanekar:2014a}; \\\\citet{Kanekar:2003a}, \\\\citet*{Kanekar:2007}; \\\\citet*{Kanekar:2014b}; \\\\citet{ Lane:2001, Lovell:1996, York:2007, Zwaan:2015}.}, assuming that this correctly describes the true distribution for the population of DLAs. However, direct measurement of the \\\\mbox{H\\\\,{\\\\sc i}} covering factor is significantly more difficult and so for the purposes of this work we draw random samples assuming a uniform distribution between 0 and 1. In \\\\autoref{figure:covfact_dist}, we show a comparison between this assumption and the sample distribution estimated by \\\\cite{Kanekar:2014a} from their main sample of 37 quasars. Kanekar et al. used VLBI synthesis imaging to measure the fraction of total quasar flux density contained within the core, which was then used as a proxy for the covering factor. By carrying out a two-tailed Kolmogorov-Smirnov (KS) test of the hypothesis that the Kanekar et al. data are consistent with our assumed uniform distribution, we find that this hypothesis is rejected at the 0.05 level, but not at the 0.01 level (this outcome is dominated by the paucity of quasars in the sample with $c_{\\\\rm f} \\\\lesssim 0.2$). It is therefore possible that the population distribution of \\\\mbox{H\\\\,{\\\\sc i}} covering factors may deviate somewhat from the uniform distribution assumed in this work. We discuss the implications of this further in \\\\autoref{section:covering_factor}. \\\\section{A 21\\\\,cm absorption survey with ASKAP}\\\\label{section:all_sky_survey} We use the Australian Square Kilometre Array Pathfinder (ASKAP; \\\\citealt{Johnston:2007}) as a case study to demonstrate the expected results from planned wide-field surveys for 21\\\\,cm absorption (e.g. the ASKAP First Large Absorption Survey in \\\\mbox{H\\\\,{\\\\sc i}} -- Sadler et al., the MeerKAT Absorption Line Survey -- Gupta et al., and the Search for HI absorption with AperTIF -- Morganti et al.). ASKAP is currently undergoing commissioning. Proof-of-concept observations with the Boolardy Engineering Test Array (\\\\citealt{Hotan:2014}) have already been used to successfully detect a new \\\\mbox{H\\\\,{\\\\sc i}} absorber associated with a probable young radio galaxy at $z = 0.44$ (\\\\citealt{Allison:2015a}). Here we predict the outcome of a future 2\\\\,h-per-pointing survey of the entire southern sky ($\\\\delta \\\\leq +10\\\\degr$) using the full 36-antenna ASKAP in a single 304\\\\,MHz band between 711.5 and 1015.5\\\\,MHz, equivalent to \\\\mbox{H\\\\,{\\\\sc i}} redshifts between $z = 0.4$ and 1.0. \\\\begin{figure} \\\\centering \\\\includegraphics[width=0.475\\\\textwidth]{nsources_flux.pdf} \\\\caption{The number of radio sources in our simulated southern sky survey ($\\\\delta \\\\leq +10\\\\degr$) estimated from existing catalogues at $L$-band frequencies (see the text for details). The grey region encloses the expected number across the 711.5 - 1015.5\\\\,MHz ASKAP frequency band, assuming a canonical spectral index of $\\\\alpha = -0.7$.}\\\\label{figure:nsources_flux} \\\\end{figure} Our expectations of the ASKAP performance are based on preliminary measurements by \\\\cite{Chippendale:2015} using the prototype Mark {\\\\sc II} phase array feed. We estimate the noise per spectral channel using the radiometer equation \\\\begin{equation} \\\\sigma_{\\\\rm chan} = {S_{\\\\rm system} \\\\over \\\\sqrt{n_{\\\\rm pol}\\\\,n_{\\\\rm ant}\\\\,(n_{\\\\rm ant} - 1)\\\\,\\\\Delta{t}_{\\\\rm in}\\\\,\\\\Delta{\\\\nu}_{\\\\rm chan}}}, \\\\end{equation} where $S_{\\\\rm system}$ is the system equivalent flux density, $n_{\\\\rm pol}$ is the number of polarizations, $n_{\\\\rm ant}$ is the number of antennas, $\\\\Delta{t}_{\\\\rm in}$ is the on-source integration time and $\\\\Delta{\\\\nu}_{\\\\rm chan}$ is the spectral resolution in frequency. The sensitivity of the telescope in the 711.5 - 1015.5\\\\,MHz band is expected to vary between $S_{\\\\rm system} \\\\approx 3200$ and $2000$\\\\,Jy, with the largest change in sensitivity between 700 and 800\\\\,MHz. ASKAP has dual linear polarization feeds, 36 antennas and a fine filter bank that produces 16\\\\,416 independent channels across the full 304\\\\,MHz bandwidth, so the expected noise per 18.5\\\\,kHz channel in a 2\\\\,h observation is approximately 5.5 - 3.5\\\\,mJy\\\\,beam$^{-1}$ across the band. In the case of an actual survey, the true sensitivity will of course be recorded in the spectral data as a function of redshift (see e.g. \\\\citealt{Allison:2015a}), but for the purposes of the simulated survey presented in this work we split the band into several frequency bins to capture the variation in sensitivity and velocity resolution (which is in the range 7.8\\\\,km\\\\,s$^{-1}$ at 711.5\\\\,MHz to 5.5\\\\,km\\\\,s$^{-1}$ at 1015.5\\\\,MHz). \\\\begin{figure} \\\\centering \\\\includegraphics[width=0.475\\\\textwidth]{zdist.pdf} \\\\caption{The distribution of CENSORS sources (\\\\citealt{Brookes:2008}) brighter than 10\\\\,mJy beyond a given redshift $z$. The red line denotes the cumulative distribution calculated from the parametric model of \\\\citet{deZotti:2010}.}\\\\label{figure:zdist} \\\\end{figure} In order to simulate a realistic survey of the southern sky we select all radio sources south of $\\\\delta = +10\\\\degr$ from catalogues of the National Radio Astronomy Observatory Very Large Array Sky Survey (NVSS, $\\\\nu = 1.4$\\\\,GHz, $S_{\\\\rm src} \\\\gtrsim 2.5$\\\\,mJy; \\\\citealt{Condon:1998}), the Sydney University Molonglo SkySurvey ($\\\\nu = 843$\\\\,MHz, $S_{\\\\rm src} \\\\gtrsim 10$\\\\,mJy; \\\\citealt{Mauch:2003}) and the second epoch Molonglo Galactic Plane Survey ($\\\\nu = 843$\\\\,MHz, $S_{\\\\rm src} \\\\gtrsim 10$\\\\,mJy; \\\\citealt{Murphy:2007}). The source flux densities, used to calculate the optical depth limit in \\\\autoref{equation:optical_depth_limit}, are estimated at the centre of each frequency bin by extrapolating from the catalogue values and assuming a canonical spectral index of $\\\\alpha = -0.7$. In \\\\autoref{figure:nsources_flux}, we show the resulting cumulative distribution of radio sources in our sample as a function of flux density across the band. \\\\begin{figure} \\\\centering \\\\includegraphics[width=0.475\\\\textwidth]{nsources_tau.pdf} \\\\caption{The number of sources in our simulated ASKAP survey with a 21\\\\,cm opacity sensitivity greater than or equal to $\\\\tau_{5\\\\sigma}$, as defined by \\\\autoref{equation:optical_depth_limit}. The grey region encloses opacity sensitivities for the 711.5 - 1011.5\\\\,MHz band. Random samples for the line FWHM and covering factor were drawn from the distributions shown in \\\\autoref{figure:width_dist} and \\\\autoref{figure:covfact_dist}.}\\\\label{figure:nsources_tau} \\\\end{figure} For any given sight-line, the redshift interval over which absorption may be detected is dependent upon the distance to the continuum source. The lack of accurate spectroscopic redshift measurements for most radio sources over the sky necessitates the use of a statistical approach based on a model for the source redshift distribution. We therefore apply a statistical weighting to each comoving path element $\\\\delta{X}(z)$ such that the expected number of absorber detections is now given by \\\\begin{equation}\\\\label{equation:weighted_sum_number} \\\\mu = \\\\iint{f(N_{\\\\rm HI},X)\\\\,\\\\mathcal{F}_{\\\\rm src}(z^{\\\\prime} \\\\geq z)\\\\,\\\\mathrm{d}X\\\\,\\\\mathrm{d}N_{\\\\rm HI}}, \\\\end{equation} where \\\\begin{equation} \\\\mathcal{F}_{\\\\rm src}(z^{\\\\prime} \\\\geq z) = {\\\\int_{z}^{\\\\infty} \\\\mathcal{N}_{\\\\rm src}(z^{\\\\prime})\\\\mathrm{d}z^{\\\\prime}\\\\over\\\\int_{0}^{\\\\infty}\\\\mathcal{N}_{\\\\rm src} (z^{\\\\prime})\\\\mathrm{d}z^{\\\\prime}}, \\\\end{equation} and $\\\\mathcal{N}_{\\\\rm src}(z)$ is the number of radio sources as a function of redshift. To estimate $\\\\mathcal{N}_{\\\\rm src}(z)$ we use the Combined EIS-NVSS Survey Of Radio Sources (CENSORS; \\\\citealt{Brookes:2008}), which forms a complete sample of radio sources brighter than 7.2\\\\,mJy at 1.4\\\\,GHz with spectroscopic redshifts out to cosmological distances. In \\\\autoref{figure:zdist} we show the distribution of CENSORS sources brighter than 10\\\\,mJy beyond a given redshift $z$, and the corresponding analytical function derived from the model fit of \\\\citet{deZotti:2010}, given by \\\\begin{equation} \\\\mathcal{N}_{\\\\rm src}(z) \\\\approx 1.29 + 32.37z - 32.89z^{2} + 11.13z^{3} - 1.25z^{4}, \\\\end{equation} which we use in our analysis. For the redshifts spanned by our simulated ASKAP survey, the fraction of background sources evolves from 87\\\\,per\\\\,cent at $z = 0.4$ to 53\\\\,per\\\\,cent at $z = 1.0$. We assume that this redshift distribution applies to any sight-line irrespective of the continuum flux density. However, this assumption is only true if the source population in the target sample evolves such that the effect of distance is nullified by an increase in luminosity. Given this criterion, and the sensitivity of our simulated survey, we limit our sample to sources with flux densities between 10 and 1000\\\\,mJy, which are dominated by the rapidly evolving population of high-excitation radio galaxies and quasars (e.g. \\\\citealt{Jackson:1999, Best:2012, Best:2014, Pracy:2016}) and for which the redshift distribution is known to be almost independent of flux density (e.g. \\\\citealt{Condon:1984, Condon:1998}). In \\\\autoref{figure:nsources_tau}, we show the number of sources from this sub-sample as a function of opacity sensitivity [as defined by \\\\autoref{equation:optical_depth_limit}], drawing random samples of the line FWHM and covering factor from the distributions shown in \\\\autoref{figure:width_dist} and \\\\autoref{figure:covfact_dist}. There are approximately 190\\\\,000 sightlines with sufficient sensitivity to detect absorption of optical depth greater than $\\\\tau_{5\\\\sigma} \\\\approx 1.0$ and 25\\\\,000 sensitive to optical depths greater than $\\\\tau_{5\\\\sigma} \\\\approx 0.1$. Since this distribution converges at optical depth sensitivities greater than $\\\\tau_{5\\\\sigma} \\\\approx 5$, the population of sources fainter than 10\\\\,mJy, which are excluded from our simulated ASKAP survey, would not significantly contribute to further detections of absorption. Similarly, while sources brighter than 1\\\\,Jy are good probes of low-column-density \\\\mbox{H\\\\,{\\\\sc i}} gas, they do not constitute a sufficiently large enough population to significantly affect the total number of absorber detections expected in the survey and can also be safely excluded. Based on these assumptions, we can estimate the number of absorbers we would expect to detect in our survey with ASKAP as a function of spin temperature. In \\\\autoref{figure:ndetections_nhi}, we show the expected detection yield as a cumulative function of column density. We show results for two scenarios where the spin temperature is fixed at a single value of either 100 or 1000\\\\,K, and the line FWHM and covering factors are drawn from the random distributions shown in \\\\autoref{figure:width_dist} and \\\\autoref{figure:covfact_dist}. We find that for both these cases the expected number of detections is not sensitive to column densities below the DLA definition of $N_{\\\\rm HI} = 2\\\\times 10^{20}$\\\\,cm$^{-2}$. We also show in \\\\autoref{figure:ndetections_total} the expected total detection yield (integrated over all \\\\mbox{H\\\\,{\\\\sc i}} column densities) as a function of a single spin temperature $T_\\\\mathrm{spin}$ and line FWHM $\\\\Delta{v}_\\\\mathrm{50}$. We find that for typical spin temperatures of a few hundred kelvin (consistent with the typical fraction of CNM observed in the local Universe) and a line FWHM of approximately $20$\\\\,km\\\\,s$^{-1}$, a wide-field 21\\\\,cm survey with ASKAP is expected to yield $\\\\sim 1000$ detections. However, even moderate evolution to a higher spin temperature in the DLA population should see significant reduction in the detection yield from this survey. \\\\section{Inferring the average spin temperature}\\\\label{section:spin_temp} \\\\begin{figure} \\\\centering \\\\includegraphics[width=0.475\\\\textwidth]{ndetections_nhi.pdf} \\\\caption{The expected number of absorber detections (as a cumulative function of column density) in our simulated ASKAP survey. We show two scenarios for a single spin temperature $T_{\\\\rm spin} = 100$ and $1000$\\\\,K, where we have drawn random samples for the line FWHM and covering factor from the distributions shown in \\\\autoref{figure:width_dist} and \\\\autoref{figure:covfact_dist}. In both cases we find that the expected number of detections is not sensitive to column densities below $N_{\\\\rm HI} = 2 \\\\times 10^{20}$\\\\,cm$^{-2}$, indicating that such a survey will only be sensitive to DLA systems.}\\\\label{figure:ndetections_nhi} \\\\end{figure} We cannot directly measure the spin temperatures of individual systems without additional data from either 21\\\\,cm emission or Lyman-$\\\\alpha$ absorption. However, from \\\\autoref{figure:ndetections_total} it is evident that the total number of absorbing systems expected to be detected with a reasonably large 21\\\\,cm survey is strongly dependent on the assumed value for the spin temperature. Therefore, by comparing the actual survey yield with that expected from the known \\\\mbox{H\\\\,{\\\\sc i}} distribution, we can infer the average spin temperature of the atomic gas within the DLA population for a given redshift interval. Assuming that the total number of detections follows a Poisson distribution, the probability of detecting $\\\\mathcal{N}$ intervening absorbing systems is given by \\\\begin{equation} p(\\\\mathcal{N}|\\\\overline{\\\\mu}) = {\\\\overline{\\\\mu}^{\\\\mathcal{N}} \\\\over \\\\mathcal{N}!} \\\\mathrm{e}^{-\\\\overline{\\\\mu}}, \\\\end{equation} where $\\\\overline{\\\\mu}$ is the expected total number of detections given by the integral \\\\begin{equation} \\\\overline{\\\\mu} = \\\\iiint{\\\\mu(T_{\\\\rm spin},\\\\Delta{v}_{50},c_{\\\\rm f})\\\\rho(T_{\\\\rm spin},\\\\Delta{v}_{50},c_{\\\\rm f})\\\\mathrm{d}T_{\\\\rm spin}\\\\mathrm{d}\\\\Delta{v}_{50}\\\\mathrm{d}c_{\\\\rm f}}, \\\\end{equation} and $\\\\rho$ is the distribution of systems as a function of spin temperature, line FWHM and covering factor. We assume that all three of these variables are independent\\\\footnote{In the case where thermal broadening contributes significantly to the velocity dispersion, and the spin temperature is dominated by collisional excitation, the assumption that these are independent may no longer hold. However, given that collisional excitation dominates in the CNM, where $T_{\\\\rm spin} \\\\sim 100\\\\,\\\\mathrm{K}$, the velocity dispersion would have to satisfy $\\\\Delta{v}_{50} \\\\ll 10$\\\\,km\\\\,s$^{-1}$ (c.f. the distribution shown in \\\\autoref{figure:width_dist}).} so that $\\\\rho$ factorizes into functions of each. We then marginalise over the covering factor and line width distributions shown in \\\\autoref{figure:width_dist} and \\\\autoref{figure:covfact_dist} so that the expression for $\\\\overline{\\\\mu}$ reduces to \\\\begin{equation}\\\\label{equation:harmonic_spin_temp} \\\\overline{\\\\mu} = \\\\int{\\\\mu(T_{\\\\rm spin})\\\\rho(T_{\\\\rm spin})\\\\mathrm{d}T_{\\\\rm spin}} = \\\\mu(\\\\overline{T}_{\\\\rm spin}), \\\\end{equation} where $\\\\overline{T}_{\\\\rm spin}$ is the harmonic mean of the unknown spin temperature distribution, weighted by column density. This is analogous to the spin temperature inferred from the detection of absorption in a single intervening galaxy averaged over several gaseous components at different temperatures (e.g. \\\\citealt{Carilli:1996}). \\\\begin{figure} \\\\centering \\\\includegraphics[width=0.475\\\\textwidth]{ndetections_total.pdf} \\\\caption{The expected total number of detections in our simulated ASKAP survey, as a function of a single spin temperature ($T_{\\\\rm spin}$) and line FWHM ($\\\\Delta{v}_{50}$). The vertical dotted lines enclose the velocity resolution across the observed frequency band. We draw random samples for the covering factor from a uniform distribution between 0 and 1 (as shown in \\\\autoref{figure:covfact_dist}). The contours are truncated at $\\\\mu < 1$ for clarity.}\\\\label{figure:ndetections_total} \\\\end{figure} In the event of the survey yielding $\\\\mathcal{N}$ detections, we can calculate the posterior probability density of $\\\\overline{T}_{\\\\rm spin}$ using the following relationship between conditional probabilities \\\\begin{equation} p(\\\\overline{T}_{\\\\rm spin}|\\\\mathcal{N}) = {p(\\\\mathcal{N}|\\\\overline{T}_{\\\\rm spin})p(\\\\overline{T}_{\\\\rm spin})\\\\over p(\\\\mathcal{N})}, \\\\end{equation} where $p(\\\\overline{T}_{\\\\rm spin})$ is our prior probability density for $\\\\overline{T}_{\\\\rm spin}$ and $p(\\\\mathcal{N})$ is the marginal probability of the number of detections, which can be treated as a normalizing constant. The minimally informative Jeffreys prior for the mean value $\\\\mu$ of a Poisson distribution is $1/\\\\sqrt{\\\\mu}$ (\\\\citealt{Jeffreys:1946})\\\\footnote{A suitable alternative choice for the prior is the standard scale-invariant form $1/\\\\mu$ (e.g. \\\\citealt{Jeffreys:1961, Novick:1965, Villegas:1977}). While we find that our choice of non-informative prior has negligible effect on the spin temperature posterior for the full \\\\mbox{H\\\\,{\\\\sc i}} absorption survey, as one would expect this choice becomes more important for smaller surveys. For the early-science 1000\\\\,deg$^{2}$ survey discussed in \\\\autoref{section:tspin_results} we find that the difference in these two priors produces a $\\\\sim 2$ to 20\\\\,per\\\\,cent effect in the posterior. However, in all cases considered this change is smaller than the 68.3\\\\,per\\\\,cent credible interval spanned by the posterior.}. From \\\\autoref{equation:harmonic_spin_temp} it therefore follows that a suitable form for the non-informative spin temperature prior is $p(\\\\overline{T}_{\\\\rm spin}) = 1/\\\\sqrt{\\\\overline{\\\\mu}}$, so that \\\\begin{equation}\\\\label{equation:tspin_prob} p(\\\\overline{T}_{\\\\rm spin}|\\\\mathcal{N}) = C^{-1}\\\\,{\\\\overline{\\\\mu}^{(\\\\mathcal{N}-1/2)} \\\\over \\\\mathcal{N}!} \\\\mathrm{e}^{-\\\\overline{\\\\mu}}, \\\\end{equation} where the distribution is normalised to unit total probability by evaluating the integral \\\\begin{equation} C = \\\\int{{\\\\overline{\\\\mu}^{(\\\\mathcal{N}-1/2)} \\\\over \\\\mathcal{N}!} \\\\mathrm{e}^{-\\\\overline{\\\\mu}}}\\\\,\\\\mathrm{d}\\\\overline{T}_{\\\\rm spin}. \\\\end{equation} The probabilistic relationship given by \\\\autoref{equation:tspin_prob} and the expected detection yield derived in \\\\autoref{section:all_sky_survey} can be used as a frame-work for inferring the harmonic-mean spin temperature using the results of any homogeneous 21-cm survey. We have assumed that we can accurately distinguish intervening absorbing systems from those associated with the host galaxy of the radio source. However, any 21-cm survey will be accompanied by follow-up observations, at optical and sub-mm wavelengths, which will aid identification. Furthermore, future implementation of probabilistic techniques to either use photometric redshift information or distinguish between line profiles should provide further disambiguation. Of course we have not yet accounted for any error in our estimate of $\\\\overline{\\\\mu}$, which will increase our uncertainty in $\\\\overline{T}_\\\\mathrm{spin}$. In the following section we discuss these possible sources of error and their effect on the result. \\\\section{Sources of error}\\\\label{section:errors} Our estimate of the expected number of 21\\\\,cm absorbers is dependent upon several distributions describing the properties of the foreground absorbing gas and the background source distribution. For future large-scale 21\\\\,cm surveys, the accuracy to which we can infer the harmonic mean of the spin temperature distribution will eventually be limited by the accuracy to which we can measure these other distributions. In this section, we describe these errors and their propagation through to the estimate of $\\\\overline{T}_\\\\mathrm{spin}$, summarizing our results in \\\\autoref{table:tspin_uncertainties}. \\\\subsection{The covering factor}\\\\label{section:covering_factor} \\\\subsubsection{Deviation from a uniform distribution between 0 and 1} The fraction $c_{\\\\rm f}$ by which the foreground gas subtends the background radiation source is difficult to measure directly and is thereby a significant source of error for 21\\\\,cm absorption surveys. In this work, we have assumed a uniform distribution for $c_{\\\\rm f}$, taking random values between 0 and 1. In \\\\autoref{section:expected_number}, we tested this assumption by comparing it with the distribution of flux density core fractions in a sample of 37 quasars, used by \\\\cite{Kanekar:2014a} as a proxy for the covering factor. By carrying out a two-tailed KS test, we found some evidence (at the 0.05 level) that this quasar sample was inconsistent with our assumption of a uniform distribution between 0 and 1. Noticeably there seems to be an under-representation of quasars in the Kanekar et al. sample with estimated $c_{\\\\rm f} \\\\lesssim 0.2$. In the low optical depth limit, the detection rate is dependent on the ratio of spin temperature to covering factor, in which case a fractional deviation in $c_{\\\\rm f}$ will propagate as an equal fractional deviation in $\\\\overline{T}_\\\\mathrm{spin}$. Based on the difference seen in the covering factor distribution of the Kanekar et al. sample and the uniform distribution, we assume that the spin temperature can deviate by as much as $\\\\pm$10\\\\,per\\\\,cent. \\\\subsubsection{Evolution with redshift} We also consider that the covering factor distribution may evolve with redshift, which would mimic a perceived evolution in the average spin temperature. Such an effect was proposed by \\\\cite{Curran:2006b} and \\\\cite{Curran:2012b}, who claimed that the relative change in angular-scale behaviour of absorbers and radio sources between low- and high-redshift samples could explain the apparent evolution of the spin temperature found by \\\\cite{Kanekar:2003b}. To test for this effect in their larger DLA sample, \\\\cite{Kanekar:2014a} considered a sub-sample at redshifts greater than $z = 1$, for which the relative evolution of the absorber and source angular sizes should be minimal. While the significance of their result was reduced by removing the lower redshift absorbers from their sample, they still found a difference at $3.5\\\\,\\\\sigma$ significance between spin temperature distributions in the two DLA sub-samples separated by a median redshift of $z = 2.683$. Future surveys with ASKAP and the other SKA pathfinders will search for \\\\mbox{H\\\\,{\\\\sc i}} absorption at intermediate redshifts ($z \\\\sim 1$), where the relative evolution of the absorber and source angular sizes is expected to be more significant than for the higher redshift DLA sample considered by \\\\cite{Kanekar:2014a}. We therefore consider the potential effect of this cosmological evolution on the inferred value of $\\\\overline{T}_\\\\mathrm{spin}$. We approximate the covering factor using the following model of \\\\cite{Curran:2006b} \\\\begin{equation}\\\\label{equation:covering_factor} c_{f} \\\\approx \\\\begin{cases} \\\\left({\\\\theta_{\\\\rm abs}\\\\over \\\\theta_{\\\\rm src}}\\\\right)^{2}, & \\\\text{if}\\\\ \\\\theta_{\\\\rm abs} < \\\\theta_{\\\\rm src}, \\\\\\\\ 1, & \\\\text{otherwise}, \\\\end{cases} \\\\end{equation} where $\\\\theta_{\\\\rm abs}$ and $\\\\theta_{\\\\rm src}$ are the angular sizes of the absorber and background source, respectively. Under the small-angle approximation $\\\\theta_{\\\\rm abs} \\\\approx {d_{\\\\rm abs}/D_{\\\\rm abs}}$ and $\\\\theta_{\\\\rm abs} \\\\approx {d_{\\\\rm src}/D_{\\\\rm src}}$, where $d_{\\\\rm abs}$ and $D_{\\\\rm abs}$ are the linear size and angular diameter distance of the absorber, and likewise $d_{\\\\rm src}$ and $D_{\\\\rm src}$ are the linear size and angular diameter distance of the background source. Assuming that the ratio $d_{\\\\rm abs}/d_{\\\\rm src}$ is randomly distributed and independent of redshift, any evolution in the covering factor is therefore dominated by relative changes in the angular diameter distances. We calculate the expected angular diameter distance ratio at a redshift $z$ by \\\\begin{equation} \\\\left\\\\langle{D_{\\\\rm abs}\\\\over D_{\\\\rm src}}\\\\right\\\\rangle_{z} = D_{\\\\rm abs}(z){\\\\int_{z}^{\\\\infty} \\\\mathcal{N}_{\\\\rm src}(z^{\\\\prime})D_{\\\\rm src}(z^{\\\\prime})^{-1}\\\\mathrm{d}z^{\\\\prime}\\\\over\\\\int_{z}^{\\\\infty}\\\\mathcal{N}_{\\\\rm src} (z^{\\\\prime})\\\\mathrm{d}z^{\\\\prime}}, \\\\end{equation} which, for the source redshift distribution model given by \\\\cite{deZotti:2010}, evolves from 0.7 at $z = 0.4$ to 1.0 at $z = 1.0$ (see \\\\autoref{figure:dang_ratio}). We note that this is consistent with the behaviour measured by \\\\cite{Curran:2012b} for the total sample of DLAs observed at 21\\\\,cm wavelengths. By applying this as a correction to the otherwise uniformly distributed covering factor (using \\\\autoref{equation:covering_factor}), we find that the inferred value of $\\\\overline{T}_{\\\\rm spin}$ systematically increases by approximately 30 per\\\\,cent. \\\\begin{figure} \\\\centering \\\\includegraphics[width=0.475\\\\textwidth]{dang_ratio.pdf} \\\\caption{The expected redshift behaviour of $D_{\\\\rm abs}/D_{\\\\rm src}$ based on the \\\\citet{deZotti:2010} model for the radio source redshift distribution.}\\\\label{figure:dang_ratio} \\\\end{figure} \\\\subsection{The $\\\\bmath{N_{\\\\rm HI}}$ frequency distribution} \\\\subsubsection{Uncertainty in the measurement of $f(N_{\\\\rm HI},X)$} We assume that $f(N_{\\\\rm HI}, X)$ is relatively well understood as a function of redshift by interpolating between model gamma functions fitted to the distributions at $z = 0$ and $3$. However, these distributions were measured from finite samples of galaxies, which of course have associated uncertainties that need to be considered. In the case of the data presented by \\\\cite{Zwaan:2005} and \\\\cite{Noterdaeme:2009}, both have typical measurement uncertainties in $f(N_{\\\\rm HI}, X)$ of approximately 10\\\\,per\\\\,cent over the range of column densities for which our simulated ASKAP survey is sensitive (see \\\\autoref{figure:ndetections_nhi}). This will propagate as a 10\\\\,per\\\\,cent fractional error in the expected number of absorber detections, and contribute a similar percentage uncertainty in the inferred average spin temperature. \\\\subsubsection{Correcting for 21\\\\,cm self-absorption} In the local Universe, \\\\cite{Braun:2012} showed that self-absorption from opaque \\\\mbox{H\\\\,{\\\\sc i}} clouds identified in high-resolution images of the Local Group galaxies M31, M33 and the Large Magellanic Cloud may necessitate a correction to the local atomic mass density of up to 30\\\\,per\\\\,cent. Although it is not yet clear whether this small sample of Local Group galaxies is representative of the low-redshift population, it is useful to understand how this effect might propagate through to our average spin temperature measurement. We therefore replace the gamma-function parametrization of the local $f(N_{\\\\rm HI})$ given by \\\\cite{Zwaan:2005} with the non-parametric values given in table\\\\,2 of \\\\cite{Braun:2012}, and recalculate $\\\\overline{T}_{\\\\rm spin}$. For an all-sky survey with the full 36-antenna ASKAP we find that $\\\\overline{T}_{\\\\rm spin}$ increases by $\\\\sim$30 for 100 detections and $\\\\sim$10\\\\,per\\\\,cent for 1000 detections. Note that the correction increases for low numbers of detections, which are dominated by the highest column density systems. \\\\subsubsection{Dust obscuration bias in optically-selected DLAs} At higher redshifts, it is possible that the number density of optically-selected DLAs could be significantly underestimated as a result of dust obscuration of the background quasar (\\\\citealt{Ostriker:1984}). This would cause a reduction in the $f(\\\\mbox{H\\\\,{\\\\sc i}}, X)$ measured from optical surveys, thereby significantly underestimating the expected number of intervening 21\\\\,cm absorbers at high redshifts. The issue is further compounded by the expectation that the highest column density DLAs ($N_{\\\\rm HI} \\\\gtrsim 10^{21}$\\\\,cm$^{-2}$), for which future wide-field 21\\\\,cm surveys are most sensitive (see \\\\autoref{figure:ndetections_nhi}), may contain more dust than their less-dense counterparts. This conclusion was supported by early analyses of the existing quasar surveys at that time (e.g. \\\\citealt{Fall:1993}), which indicated that up to 70\\\\,per\\\\,cent of quasars could be missing from optical surveys through the effect of dust obscuration, albeit with large uncertainties. However, subsequent optical and infrared observations of radio-selected quasars (e.g. \\\\citealt{Ellison:2001}; \\\\citealt*{Ellison:2005}; \\\\citealt{Jorgenson:2006}), which are free of the potential selection biases associated with these optical surveys, found that the severity of this issue was substantially over-estimated and that there was minimal evidence in support of a correlation between the presence of DLAs and dust reddening. Furthermore, the \\\\mbox{H\\\\,{\\\\sc i}} column density frequency distribution measured by \\\\cite{Jorgenson:2006} was found to be consistent with the optically-determined gamma-function parametrization of \\\\cite{Prochaska:2005}, with no evidence of DLA systems missing from the SDSS sample at a sensitivity of $N_{\\\\rm HI} \\\\lesssim 5 \\\\times 10^{21}$\\\\,cm$^{-2}$. Although radio-selected surveys of quasars are free of the selection biases associated with optical surveys, they do typically suffer from smaller sample sizes and are therefore less sensitive to the rarer DLAs with the highest column densities. Another approach is to directly test whether optically-selected quasars with intervening DLAs, selected from the SDSS sample, are systematically more dust reddened than a control sample of non-DLA quasars. Comparisons in the literature are based on several different colour indicators, which include the spectral index (e.g. \\\\citealt{Murphy:2004,Murphy:2016}), spectral stacking (e.g. \\\\citealt{Frank:2010, Khare:2012}) and direct photometry (e.g. \\\\citealt*{Vladilo:2008}; \\\\citealt{Fukugita:2015}). The current status of these efforts is summarized by \\\\citet{Murphy:2016}, showing broad support for a missing DLA population at the level of $\\\\sim$5\\\\,per\\\\,cent but highlighting that tension still exists between different dust measurements. No substantial evidence has yet been found to support a correlation between the dust reddening and \\\\mbox{H\\\\,{\\\\sc i}} column density in these optically selected DLA surveys (e.g. \\\\citealt{Vladilo:2008, Khare:2012, Murphy:2016}). In an attempt to reconcile the differences and myriad biases associated with these techniques, \\\\cite{Pontzen:2009} carried out a statistically-robust meta-analysis of the available optical and radio data, using a Bayesian parameter estimation approach to model the dust as a function of column density and metallicity. They found that the expected fraction of DLAs missing from optical surveys is 7\\\\,per\\\\,cent, with fewer than 28\\\\,per\\\\,cent missing at 3\\\\,$\\\\sigma$ confidence. Based on this body of work we therefore assume that approximately 10\\\\,per\\\\,cent of DLAs are missing from the SDSS sample of \\\\cite{Noterdaeme:2009} and consider the affect on our estimate of $\\\\overline{T}_{\\\\rm spin}$. We further assume that there is no dependance on column density, an assumption which is supported by the aforementioned observational data for the range of column densities to which our 21\\\\,cm survey is sensitive. We find that increasing the high-redshift column density frequency distribution by 10\\\\,per\\\\,cent introduces a systematic increase of approximately 3\\\\,per\\\\,cent in the expected number of detections for the redshifts covered by our ASKAP surveys. We note that this error will increase significantly for 21\\\\,cm surveys at higher redshifts where the optically derived $f(\\\\mbox{H\\\\,{\\\\sc i}}, X)$ dominates the calculation of the expected detection rate. \\\\subsection{The radio source background} As described in \\\\autoref{section:all_sky_survey}, we weight the comoving path-length for each sight-line by a statistical redshift distribution in order to account for evolution in the radio source background. We use the parametric model of \\\\cite{deZotti:2010}, which is derived from fitting the measured redshifts of \\\\cite{Brookes:2008} for CENSORS sources brighter than 10\\\\,mJy, and assume that this applies to all sources in the range 10 - 1000\\\\,mJy. In \\\\autoref{figure:zdist}, we show the cumulative distribution of sources located behind a given redshift and the associated measurement uncertainty given by the errorbars. For the intermediate redshifts covered by the ASKAP survey, the fractional uncertainty in this distribution increases from $\\\\sigma_{\\\\mathcal{F}_{\\\\rm src}}/\\\\mathcal{F}_{\\\\rm src} \\\\approx 3.5$ to 8\\\\,per\\\\,cent between $z = 0.4$ and 1.0, which propagates through to a similar fractional uncertainty in $\\\\overline{T}_{\\\\rm spin}$. However, for higher redshifts this fractional uncertainty increases rapidly at $z > 2$, to more than 50\\\\,per\\\\,cent at $z = 3$, reflecting the paucity of optical spectroscopic data for the high-redshift radio source population. Understanding how the radio source population is distributed at lower flux densities and at higher redshifts is therefore a concern for the future 21\\\\,cm absorption surveys undertaken with the SKA mid- and low-frequency telescopes (see \\\\citealt{Kanekar:2004} and \\\\citealt*{Morganti:2015} for reviews). \\\\begin{table} \\\\begin{threeparttable} \\\\caption{An account of errors in our estimate of $\\\\overline{T}_{\\\\rm spin}$ due to the accuracy to which we can determine the expected number of absorber detections.}\\\\label{table:tspin_uncertainties} \\\\begin{tabular}{l@{\\\\hspace{0.05in}}l@{\\\\hspace{0.05in}}l@{\\\\hspace{0.05in}}l@{\\\\hspace{0.05in}}l} \\\\hline & Source of error & $\\\\mathrm{err}(\\\\overline{T}_{\\\\rm spin})$ & Refs. \\\\\\\\ & & [per\\\\,cent] & \\\\\\\\ \\\\hline Covering factor & Distribution uncertainty & $\\\\pm10$ & $a$ \\\\\\\\ Covering factor & Systematic evolution & +30 & $a$, $b$\\\\\\\\ $f(N_{\\\\rm HI}, X)$ & Measurement uncertainty & $\\\\pm10$ & $c, d$\\\\\\\\ Low-$z$ $f(N_{\\\\rm HI}, X)$ & Systematic self-absorption & $+(10-30)$ & $e$ \\\\\\\\ High-$z$ $f(N_{\\\\rm HI}, X)$ & Systematic dust-obscuration & $+3$ & $f$, $g$ \\\\\\\\ $\\\\mathcal{F}_{\\\\rm src}(z^{\\\\prime} \\\\geq z)$ & Measurement uncertainty & $\\\\pm 5$ & $h$, $i$ \\\\\\\\ \\\\hline \\\\end{tabular} \\\\begin{tablenotes} \\\\item[] References: $^{a}${\\\\citet{Kanekar:2014a}}, $^{b}${\\\\citet{Curran:2012b}}, $^{c}${\\\\citet{Zwaan:2005}}, $^{d}${\\\\citet{Noterdaeme:2009}} , $^{e}${\\\\citet{Braun:2012}}, $^{f}${\\\\citet{Pontzen:2009}}, $^{g}${\\\\citet{Murphy:2016}}, $^{h}${\\\\citet{Brookes:2008}}, $^{i}${\\\\citet{deZotti:2010}}. \\\\end{tablenotes} \\\\end{threeparttable} \\\\end{table} \\\\section{Expected results for future 21-cm absorption surveys}\\\\label{section:tspin_results} \\\\begin{figure} \\\\centering \\\\includegraphics[width=0.475\\\\textwidth]{tspin_prob.pdf} \\\\caption{The posterior probability density of the average spin temperature, as a function of absorber detection yield ($\\\\mathcal{N}$). We show results for our simulated all-southern-sky survey with 2-h per pointing using the full 36-antenna ASKAP (top panel) and a smaller 1000\\\\,deg$^{2}$ survey with 12-h per pointing and 12 antennas of ASKAP (bottom panel). The dashed curves show the cumulative effect of the systematic errors discussed in \\\\autoref{section:errors}. $\\\\overline{\\\\mathcal{F}}_{\\\\rm CNM}$ is the average CNM fraction assuming a simple two-phase neutral ISM with $T_{\\\\rm spin,CNM} = 100$\\\\,K and $T_{\\\\rm spin,WNM} = 1800$\\\\,K (\\\\citealt{Liszt:2001}).}\\\\label{figure:tspin_prob} \\\\end{figure} In the top panel of \\\\autoref{figure:tspin_prob} we show the results of applying our method for inferring $\\\\overline{T}_{\\\\rm spin}$ to the simulated all-southern-sky \\\\mbox{H\\\\,{\\\\sc i}} absorption survey with ASKAP described in \\\\autoref{section:all_sky_survey}. We account for the uncertainties in the expected detection rate $\\\\overline{\\\\mu}$, discussed in \\\\autoref{section:errors}, by using a Monte Carlo approach and marginalizing over many realizations. A yield of 1000 absorbers from such a survey would imply an average spin temperature of $\\\\overline{T}_\\\\mathrm{spin} = 127^{+14}_{-14}\\\\,(193^{+23}_{-23})$\\\\,K\\\\footnote{We give the 68.3\\\\,per\\\\,cent interval about the median value measured from the posterior distributions shown in \\\\autoref{figure:tspin_prob}.}, where values in parentheses denote the alternative posterior probability resulting from the systematic errors discussed in \\\\autoref{section:errors}. This scenario would indicate that a large fraction of the atomic gas in DLAs at these intermediate redshifts is in the classical stable CNM phase. Conversely, a yield of only 100 detections would imply that $\\\\overline{T}_\\\\mathrm{spin} = 679^{+64}_{-65}\\\\,(1184^{+116}_{-120})$\\\\,K, indicating that less than 10\\\\,per\\\\,cent of the atomic gas is in the CNM and that the bulk of the neutral gas in galaxies is significantly different at intermediate redshifts compared with the local Universe. We also consider the effect of reducing the sky area and array size, which is relevant for planned early science surveys with ASKAP and other SKA pathfinder telescopes. In the bottom panel of \\\\autoref{figure:tspin_prob}, we show the spin temperatures inferred when observing a random 1000\\\\,deg$^{2}$ field for 12\\\\,h per pointing, between $z_{\\\\rm HI} = 0.4$ and $1.0$, using a 12-antenna version of ASKAP. We find that detection yields of 30 and 3 from such a survey would give inferred spin temperatures of $\\\\overline{T}_{\\\\rm spin} =134^{+23}_{-27}\\\\,(209^{+40}_{-47})$ and $848^{+270}_{-430}\\\\,(1535^{+513}_{-837})$\\\\,K, respectively. The significant reduction in telescope sensitivity and sky-area, compensated by the increase in integration time per pointing planned for early-science, results in a factor of 30 decrease in the expected number of detections and therefore an increase in the sample variance and uncertainty in $\\\\overline{T}_{\\\\rm spin}$. However, this result demonstrates that we expect to be able to distinguish between the limiting cases of CNM-rich or deficient DLA populations even during the early-science phases of the SKA pathfinders. For example 30 detections with the early ASKAP survey rules out an average spin temperature of 1000\\\\,K at high probability. \\\\section{Conclusions} We have demonstrated a statistical method for measuring the average spin temperature of the neutral ISM in distant galaxies, using the expected detection yields from future wide-field 21\\\\,cm absorption surveys. The spin temperature is a crucial property of the ISM that can be used to determine the fraction of the cold ($T_{\\\\rm k} \\\\sim 100$\\\\,K) and dense ($n \\\\sim 100$\\\\,cm$^{-2}$) atomic gas that provides sites for the future formation of cold molecular gas clouds and star formation. Recent 21\\\\,cm surveys for \\\\mbox{H\\\\,{\\\\sc i}} absorption in \\\\mbox{Mg\\\\,{\\\\sc ii}} absorbers and DLAs towards distant quasars have yielded some evidence of an evolution in the average spin temperature that might reveal a decrease in the fraction of cold dense atomic gas at high redshift (e.g. \\\\citealt{Gupta:2009, Kanekar:2014a}). By combining recent specifications for ASKAP, with available information for the population of background radio sources, we show that strong statistical constraints (approximately $\\\\pm10$\\\\,per\\\\,cent) in the average spin temperature can be achieved by carrying out a shallow 2-h per pointing survey of the southern sky between redshifts of $z = 0.4$ and $1.0$. However, we find that the accuracy to which we can measure the average spin temperature is ultimately limited by the accuracy to which we can measure the distribution of the covering factor, the $N_{\\\\rm HI}$ frequency distribution function and the evolution of the radio source population as a function of redshift. By improving our understanding of these distributions we will be able to leverage the order-of-magnitude increases in sensitivity and redshift coverage of the future SKA telescope, allowing us to measure the evolution of the average spin temperature to much higher redshifts. \\\\section*{Acknowledgements} We thank Robert Allison, Elaine Sadler and Michael Pracy for useful discussions, and the anonymous referee for providing comments that helped improve this paper. JRA acknowledges support from a Bolton Fellowship. We have made use of \\\\texttt{Astropy}, a community-developed core \\\\texttt{Python} package for astronomy (\\\\citealt{Astropy:2013}); NASA's Astrophysics Data System Bibliographic Services; and the VizieR catalogue access tool, operated at CDS, Strasbourg, France. \\\\bibliographystyle{mnras} {'timestamp': '2016-08-09T02:04:29', 'yymm': '1607', 'arxiv_id': '1607.04828', 'language': 'en', 'url': 'https://arxiv.org/abs/1607.04828'} arxiv \\\\section{Introduction} Given $\\\\rho>0$, we consider the problem \\\\begin{equation}\\\\label{eq:main_prob_U} \\\\begin{cases} -\\\\Delta U + \\\\lambda U = |U|^{p-1}U & \\\\text{in }\\\\Omega,\\\\smallskip\\\\\\\\ \\\\int_\\\\Omega U^2\\\\,dx = \\\\rho, \\\\quad U=0 & \\\\text{on }\\\\partial\\\\Omega, \\\\end{cases} \\\\end{equation} where $\\\\Omega\\\\subset{\\\\mathbb{R}}^N$ is a Lipschitz, bounded domain, $1<p<2^*-1$, $\\\\rho>0$ is a fixed parameter, and both $U\\\\in H^1_0(\\\\Omega)$ and $\\\\lambda\\\\in{\\\\mathbb{R}}$ are unknown. More precisely, we investigate conditions on $p$ and $\\\\rho$ (and also $\\\\Omega$) for the solvability of the problem. The main interest in \\\\eqref{eq:main_prob_U} relies on the investigation of standing wave solutions for the nonlinear Schr\\\\\\\"odinger equation \\\\[ i\\\\frac{\\\\partial \\\\Phi}{\\\\partial t}+\\\\Delta \\\\Phi+ |\\\\Phi|^{p-1}\\\\Phi=0,\\\\qquad (t,x)\\\\in {\\\\mathbb{R}}\\\\times \\\\Omega \\\\] with Dirichlet boundary conditions on $\\\\partial\\\\Omega$. This equation appears in several different physical models, both in the case $\\\\Omega={\\\\mathbb{R}}^N$ \\\\cite{MR2002047}, and on bounded domains \\\\cite{MR1837207}. In particular, the latter case appears in nonlinear optics and in the theory of Bose-Einstein condensation, also as a limiting case of the equation on ${\\\\mathbb{R}}^N$ with confining potential. When searching for solutions having the wave function $\\\\Phi$ factorized as $\\\\Phi(x,t)=e^{i\\\\lambda t} U(x)$, one obtains that the real valued function $U$ must solve \\\\begin{equation}\\\\label{eq:NLS} -\\\\Delta U + \\\\lambda U = |U|^{p-1}U ,\\\\qquad U\\\\in H^1_0(\\\\Omega), \\\\end{equation} and two points of view are available. The first possibility is to assign the chemical potential $\\\\lambda\\\\in{\\\\mathbb{R}}$, and search for solutions of \\\\eqref{eq:NLS} as critical points of the related action functional. The literature concerning this approach is huge and we do not even make an attempt to summarize it here. On the contrary, we focus on the second possibility, which consists in considering $\\\\lambda$ as part of the unknown and prescribing the mass (or charge) $\\\\|U\\\\|_{L^2(\\\\Omega)}^2$ as a natural additional condition. Up to our knowledge, the only previous paper dealing with this case, in bounded domains, is \\\\cite{MR3318740}, which we describe below. The problem of searching for normalized solutions in ${\\\\mathbb{R}}^N$, with non-homogeneous nonlinearities, is more investigated \\\\cite{MR3009665,MR1430506}, even though the methods used there can not be easily extended to bounded domains, where dilations are not allowed. Very recently, also the case of partial confinement has been considered \\\\cite{BeBoJeVi_2016}. Solutions of \\\\eqref{eq:main_prob_U} can be identified with critical points of the associated energy functional \\\\[ \\\\mathcal{E}(U) = \\\\frac12\\\\int_\\\\Omega|\\\\nabla U|^2\\\\,dx - \\\\frac{1}{p+1} \\\\int_\\\\Omega|U|^{p+1}\\\\,dx \\\\] restricted to the mass constraint \\\\[ {\\\\mathcal{M}}_\\\\rho=\\\\{U\\\\in H_0^1(\\\\Omega) : \\\\|U\\\\|_{L^2(\\\\Omega)}=\\\\rho\\\\}, \\\\] with $\\\\lambda$ playing the role of a Lagrange multiplier. A cricial role in the discussion of the above problem is played by the Gagliardo-Nirenberg inequality: for any $\\\\Omega$ and for any $v\\\\in H^1_0(\\\\Omega)$, \\\\begin{equation} \\\\label{sobest} \\\\|v\\\\|^{p+1}_{L^{p+1}(\\\\Omega)} \\\\leq C_{N,p} \\\\| \\\\nabla v \\\\|_{L^2(\\\\Omega)}^{N(p-1)/2} \\\\| v \\\\|_{L^2(\\\\Omega)} ^{(p+1)-N(p-1)/2}, \\\\end{equation} the equality holding only when $\\\\Omega={\\\\mathbb{R}}^N$ and $v=Z_{N,p}$, the positive solution of $-\\\\Delta Z + Z = Z^{p}$ (which is unique up to translations \\\\cite{MR969899}). Accordingly, the exponent $p$ can be classified in relation with the so called \\\\emph{$L^2$-critical exponent} $1+4/N$ (throughout all the paper, $p$ will be always Sobolev-subcritical and its criticality will be understood in the $L^2$ sense). Indeed we have that ${\\\\mathcal{E}}$ is bounded below and coercive on ${\\\\mathcal{M}}_\\\\rho$ if and only if either $p$ is subcritical, or it is critical and $\\\\rho$ is sufficiently small. The recent paper \\\\cite{MR3318740} deals with problem \\\\eqref{eq:main_prob_U} in the case of the spherical domain $\\\\Omega = B_1$, when searching for positive solutions $U$. In particular, it is shown that the solvability of \\\\eqref{eq:main_prob_U} is strongly influenced by the exponent $p$, indeed: \\\\begin{itemize} \\\\item in the subcritical case $1<p<1+4/N$, \\\\eqref{eq:main_prob_U} admits a unique positive solution for every $\\\\rho>0$; \\\\item if $p=1+4/N$ then \\\\eqref{eq:main_prob_U} admits a unique positive solution for \\\\[ 0<\\\\rho<\\\\rho^*=\\\\left(\\\\frac{p+1}{2C_{N,p}}\\\\right)^{N/2}=\\\\|Z_{N,p}\\\\|^2_{L^2({\\\\mathbb{R}}^N)}, \\\\] and no positive solutions for $\\\\rho\\\\geq\\\\rho^*$; \\\\item finally, in the supercritical regime $1+4/N<p<2^*-1$, \\\\eqref{eq:main_prob_U} admits positive solutions if and only if $0<\\\\rho\\\\leq\\\\rho^*$ (the threshold $\\\\rho^*$ depending on $p$), and such solutions are at least two for $\\\\rho<\\\\rho^*$. \\\\end{itemize} In this paper we carry on such analysis, dealing with a general domain $\\\\Omega$ and with solutions which are not necessarily positive. More precisely, let us recall that for any $U$ solving \\\\eqref{eq:main_prob_U} for some $\\\\lambda$, it is well-defined the Morse index \\\\[ m(U) = \\\\max\\\\left\\\\{k : \\\\begin{array}{l} \\\\exists V\\\\subset H^1_0(\\\\Omega),\\\\,\\\\dim(V)= k:\\\\forall v\\\\in V\\\\setminus\\\\{0\\\\}\\\\smallskip\\\\\\\\ \\\\displaystyle\\\\int_\\\\Omega |\\\\nabla v|^2 + \\\\lambda v^2 - p|U|^{p-1}v^2\\\\,dx<0 \\\\end{array} \\\\right\\\\}\\\\in{\\\\mathbb{N}}. \\\\] Then, if $\\\\Omega=B_1$, it is well known that a solution $U$ of \\\\eqref{eq:main_prob_U} is positive if and only if $m(U)=1$. Under this perspective, the results in \\\\cite{MR3318740} can be read in terms of Morse index one--solutions, rather than positive ones: introducing the sets of admissible masses \\\\[ {\\\\mathfrak{A}}_k ={\\\\mathfrak{A}}_k(p,\\\\Omega) := \\\\left\\\\{\\\\rho>0 : \\\\begin{array}{l} \\\\eqref{eq:main_prob_U} \\\\text{ admits a solution $U$ (for some $\\\\lambda$)}\\\\\\\\ \\\\text{having Morse index }m(U)\\\\leq k \\\\end{array} \\\\right\\\\}, \\\\] then \\\\cite{MR3318740} implies that ${\\\\mathfrak{A}}_1(p,B_1)$ is a bounded interval if and only if $p$ is critical or supercritical, while ${\\\\mathfrak{A}}_1(p,B_1)={\\\\mathbb{R}}^+$ in the subcritical case. On the contrary, when considering general domains and higher Morse index, the situation may become much more complicated. We collect some examples in the following remark. \\\\begin{remark}\\\\label{rem:specialdomains} In the case of a symmetric domain, one can use any solution as a building block to construct other solutions with a more complex behavior, obtaining the so-called necklace solitary waves. Such kind of solutions are constructed in \\\\cite{MR3426917}, even though in such paper the focus is on stability, rather than on normalization conditions. For instance, by scaling argument, any Dirichlet solution of $-\\\\Delta U + \\\\lambda U = |U|^{p-1}U$ in a rectangle $R=\\\\prod_{i=1}^N(a_i,b_i)$ can be scaled to a solution of $-\\\\Delta U + k^2\\\\lambda U = |U|^{p-1}U$ in $R/k$, $k\\\\in{\\\\mathbb{N}}_+$, and then $k^N$ copies of it can be juxtaposed, with alternating sign. In this way one obtains a new solution on $R$ having $k^{4/(p-1)}$ times the mass of the starting one, and eventually solutions in $R$ with arbitrarily high mass (but with higher Morse index) can be constructed even in the critical and supercritical case. An analogous construction can be performed in the disk, using solutions in circular sectors as building blocks, even though in this case explicit bounds on the mass obtained are more delicate. Also, instead of symmetric domains, singular perturbed ones can be considered, such as dumbbell domains \\\\cite{MR949628}: for instance, using \\\\cite[Theorem 3.5]{MR2997381}, one can show that for any $k$, there exists a domain $\\\\Omega$, which is close in a suitable sense to the disjoint union of $k$ domains, such that \\\\eqref{eq:main_prob_U} has a \\\\emph{positive} solution on $\\\\Omega$ with Morse index $k$ and $\\\\rho=\\\\rho_k\\\\to+\\\\infty$ as $k\\\\to+\\\\infty$. This kind of results justifies the choice of classifying the solutions in terms of their Morse index, rather than in terms of their nodal properties. \\\\end{remark} Motivated by the previous remark, the first question we address in this paper concerns the boundedness of ${\\\\mathfrak{A}}_k$. We provide the following complete classification. \\\\begin{theorem}\\\\label{thm:bbd_index} For every $\\\\Omega\\\\subset{\\\\mathbb{R}}^N$ bounded $C^1$ domain, $k\\\\ge1$, $1<p<2^*-1$, \\\\[ \\\\sup{\\\\mathfrak{A}}_k(p,\\\\Omega) < +\\\\infty \\\\qquad\\\\iff\\\\qquad p \\\\ge 1+\\\\frac{4}{N}. \\\\] \\\\end{theorem} The proof of such result, which is outlined in Section \\\\ref{sec:blow-up}, is obtained by a detailed blow-up analysis of sequences of solutions with bounded Morse index, via suitable a priori pointwise estimates (see \\\\cite{MR2063399}). In this respect, the regularity assumption on $\\\\partial\\\\Omega$ simplifies the treatment of possible concentration phenomena towards the boundary. The argument, which holds for solutions which possibly change sign, is inspired by \\\\cite{MR2825606}, where the case of positive solutions is treated. Once Theorem \\\\ref{thm:bbd_index} is established, in case $p\\\\geq 1 + 4/N$ two questions arise, namely: \\\\begin{enumerate} \\\\item is it possible to provide lower bounds for $\\\\sup{\\\\mathfrak{A}}_k$? Is it true that $\\\\sup{\\\\mathfrak{A}}_k$ is strictly increasing in $k$, or, at least, that $\\\\sup{\\\\mathfrak{A}}_k > \\\\sup{\\\\mathfrak{A}}_1$ for some $k$? \\\\item is \\\\eqref{eq:main_prob_U} solvable for every $\\\\rho\\\\in(0,\\\\sup{\\\\mathfrak{A}}_k)$, or at least can we characterize some subinterval of solvability? \\\\end{enumerate} It is clear that both issues can be addressed by characterizing values of $\\\\rho$ for which existence (and multiplicity) of solutions with bounded Morse index can be guaranteed. To this aim, it can be useful to restate problem \\\\eqref{eq:main_prob_U} as \\\\begin{equation}\\\\label{eq:main_prob_u} \\\\begin{cases} -\\\\Delta u + \\\\lambda u = \\\\mu|u|^{p-1}u & \\\\text{in }\\\\Omega,\\\\\\\\ \\\\int_\\\\Omega u^2\\\\,dx = 1, \\\\quad u=0 & \\\\text{on }\\\\partial\\\\Omega, \\\\end{cases} \\\\qquad\\\\text{where}\\\\quad \\\\begin{cases} U=\\\\sqrt{\\\\rho} u\\\\\\\\ \\\\mu = \\\\rho^{(p-1)/2}, \\\\end{cases} \\\\end{equation} where now $\\\\mu>0$ is prescribed. Since \\\\begin{equation} \\\\label{Emu} \\\\text{both } \\\\mathcal{E}_{\\\\mu}(u) := \\\\frac{1}{2}\\\\int_{\\\\Omega}|\\\\nabla u|^2- \\\\frac{\\\\mu}{p+1}\\\\int_{\\\\Omega}| u|^{p+1} \\\\qquad \\\\text{and }{\\\\mathcal{M}}={\\\\mathcal{M}}_1=\\\\{u : \\\\|u\\\\|_{L^2(\\\\Omega)}=1\\\\} \\\\end{equation} are invariant under the ${\\\\mathbb{Z}}_2$-action of the involution $u\\\\mapsto -u$, solutions of \\\\eqref{eq:main_prob_u} can be found via min-max principles in the framework of index theories (see e.g. \\\\cite[Ch. II.5]{St_2008}). Notice that in the supercritical case ${\\\\mathcal{E}}_\\\\mu$ is not bounded from below on ${\\\\mathcal{M}}$. Following \\\\cite{MR3318740}, it can be convenient to parameterize solutions to \\\\eqref{eq:main_prob_u} with respect to the $H^1_0$-norm, therefore we introduce the sets \\\\begin{equation}\\\\label{eq:defBU} \\\\mathcal{B}_\\\\alpha:=\\\\left\\\\{u\\\\in {\\\\mathcal{M}}:\\\\,\\\\int_\\\\Omega |\\\\nabla u|^2\\\\,dx<\\\\alpha\\\\right\\\\},\\\\quad\\\\quad \\\\mathcal{U}_\\\\alpha:=\\\\left\\\\{u\\\\in {\\\\mathcal{M}}:\\\\,\\\\int_\\\\Omega |\\\\nabla u|^2\\\\,dx=\\\\alpha\\\\right\\\\}. \\\\end{equation} Introducing the first Dirichlet eigenvalue of $-\\\\Delta$ in $H^1_0(\\\\Omega)$, $ ...\n\n(The output of the tool call (shortuuid identifier: FP7NamGQoejPNY8jydopcX) is too long! Only the first 100000 characters are shown here. The original output length is 183316 characters. The full output has been saved to the file /workspace/dumps/workspace/.overlong_tool_outputs/FP7NamGQoejPNY8jydopcX.json. Please check this file carefully, as it may be very long!)",
            "tool_call_id": "toolu_01CrzQumLftYygHNuBKWWZvs"
        },
        {
            "role": "assistant",
            "content": "Let me now search for more specific information about the sizes of individual datasets:",
            "tool_calls": [
                {
                    "id": "toolu_01PVvpyAVur2irpDMkMmMHZa",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"\\\"StackExchange dataset\\\" \\\"20 GB\\\" \\\"14 GB\\\" size\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "No search results found.",
            "tool_call_id": "toolu_01PVvpyAVur2irpDMkMmMHZa"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01Sbm6ocw2FW1ipgMqQxmPRT",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"\\\"GitHub dataset\\\" \\\"59 GB\\\" \\\"100 GB\\\" \\\"200 GB\\\" RedPajama programming\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "No search results found.",
            "tool_call_id": "toolu_01Sbm6ocw2FW1ipgMqQxmPRT"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01TRDeEUs6jwEjqgiKFBBdk1",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\": \"\\\"ArXiv dataset\\\" \\\"28 GB\\\" \\\"33 GB\\\" academic papers scientific\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "No search results found.",
            "tool_call_id": "toolu_01TRDeEUs6jwEjqgiKFBBdk1"
        },
        {
            "role": "assistant",
            "content": "Based on my research, I now have enough information to compile the pre-training data for LLaMA and GPT-Neo models. Let me organize this data and populate the spreadsheet:",
            "tool_calls": [
                {
                    "id": "toolu_01GwSQSwjJDSufsxDSCyBEN2",
                    "type": "function",
                    "function": {
                        "name": "google_sheet-update_cells",
                        "arguments": "{\"spreadsheet_id\": \"1pkvieQSe2snpTOS94pnlHoGDwCXO2Fsx0Ct-_iGnQhM\", \"sheet\": \"ptdata\", \"range\": \"A1:D15\", \"data\": [[\"name\",\"use in llm\",\"size\",\"link\"],[\"Common Crawl\",\"llama\",1342,\"https://huggingface.co/datasets/commoncrawl/statistics\"],[\"The Pile\",\"gpt-neo\",825,\"https://huggingface.co/datasets/EleutherAI/pile\"],[\"C4\",\"llama\",750,\"https://huggingface.co/datasets/c4\"],[\"GitHub\",\"llama\",212,\"https://huggingface.co/datasets/codeparrot/github-code\"],[\"Wikipedia\",\"llama\",112,\"https://huggingface.co/datasets/wikimedia/wikipedia\"],[\"Books3\",\"llama\",101,\"https://huggingface.co/datasets/the_pile/books3\"],[\"ArXiv\",\"llama\",33,\"https://huggingface.co/datasets/arxiv\"],[\"StackExchange\",\"llama\",20,\"https://huggingface.co/datasets/ArmelR/stack-exchange-instruction\"],[\"Project Gutenberg\",\"llama\",11,\"https://huggingface.co/datasets/sedthh/gutenberg_english\"],[\"HackerNews\",\"gpt-neo\",4,\"https://huggingface.co/datasets/the_pile/hackernews\"],[\"OpenWebText2\",\"gpt-neo\",62,\"https://huggingface.co/datasets/openwebtext2\"],[\"FreeLaw\",\"gpt-neo\",51,\"https://huggingface.co/datasets/the_pile/freelaw\"],[\"PubMed Central\",\"gpt-neo\",90,\"https://huggingface.co/datasets/the_pile/pubmed_central\"],[\"USPTO Backgrounds\",\"gpt-neo\",22,\"https://huggingface.co/datasets/the_pile/uspto_backgrounds\"]]}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"{\\n  \\\"spreadsheetId\\\": \\\"1pkvieQSe2snpTOS94pnlHoGDwCXO2Fsx0Ct-_iGnQhM\\\",\\n  \\\"updatedRange\\\": \\\"ptdata!A1:D15\\\",\\n  \\\"updatedRows\\\": 15,\\n  \\\"updatedColumns\\\": 4,\\n  \\\"updatedCells\\\": 60\\n}\",\"annotations\":null}",
            "tool_call_id": "toolu_01GwSQSwjJDSufsxDSCyBEN2"
        },
        {
            "role": "assistant",
            "content": "Now let me verify the data has been entered correctly and check if we need to sort it by size in descending order:",
            "tool_calls": [
                {
                    "id": "toolu_01BA7moAvJY8egEiJ7TWSRcX",
                    "type": "function",
                    "function": {
                        "name": "google_sheet-get_sheet_data",
                        "arguments": "{\"spreadsheet_id\": \"1pkvieQSe2snpTOS94pnlHoGDwCXO2Fsx0Ct-_iGnQhM\", \"sheet\": \"ptdata\", \"range\": \"A1:D15\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"{\\n  \\\"spreadsheetId\\\": \\\"1pkvieQSe2snpTOS94pnlHoGDwCXO2Fsx0Ct-_iGnQhM\\\",\\n  \\\"properties\\\": {\\n    \\\"title\\\": \\\"LLM Pre-training Data\\\",\\n    \\\"locale\\\": \\\"zh_CN\\\",\\n    \\\"autoRecalc\\\": \\\"ON_CHANGE\\\",\\n    \\\"timeZone\\\": \\\"Asia/Shanghai\\\",\\n    \\\"defaultFormat\\\": {\\n      \\\"backgroundColor\\\": {\\n        \\\"red\\\": 1,\\n        \\\"green\\\": 1,\\n        \\\"blue\\\": 1\\n      },\\n      \\\"padding\\\": {\\n        \\\"top\\\": 2,\\n        \\\"right\\\": 3,\\n        \\\"bottom\\\": 2,\\n        \\\"left\\\": 3\\n      },\\n      \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n      \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n      \\\"textFormat\\\": {\\n        \\\"foregroundColor\\\": {},\\n        \\\"fontFamily\\\": \\\"arial,sans,sans-serif\\\",\\n        \\\"fontSize\\\": 10,\\n        \\\"bold\\\": false,\\n        \\\"italic\\\": false,\\n        \\\"strikethrough\\\": false,\\n        \\\"underline\\\": false,\\n        \\\"foregroundColorStyle\\\": {\\n          \\\"rgbColor\\\": {}\\n        }\\n      },\\n      \\\"backgroundColorStyle\\\": {\\n        \\\"rgbColor\\\": {\\n          \\\"red\\\": 1,\\n          \\\"green\\\": 1,\\n          \\\"blue\\\": 1\\n        }\\n      }\\n    },\\n    \\\"spreadsheetTheme\\\": {\\n      \\\"primaryFontFamily\\\": \\\"Arial\\\",\\n      \\\"themeColors\\\": [\\n        {\\n          \\\"colorType\\\": \\\"TEXT\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {}\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"BACKGROUND\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 1,\\n              \\\"green\\\": 1,\\n              \\\"blue\\\": 1\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"ACCENT1\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 0.25882354,\\n              \\\"green\\\": 0.52156866,\\n              \\\"blue\\\": 0.95686275\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"ACCENT2\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 0.91764706,\\n              \\\"green\\\": 0.2627451,\\n              \\\"blue\\\": 0.20784314\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"ACCENT3\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 0.9843137,\\n              \\\"green\\\": 0.7372549,\\n              \\\"blue\\\": 0.015686275\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"ACCENT4\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 0.20392157,\\n              \\\"green\\\": 0.65882355,\\n              \\\"blue\\\": 0.3254902\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"ACCENT5\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 1,\\n              \\\"green\\\": 0.42745098,\\n              \\\"blue\\\": 0.003921569\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"ACCENT6\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 0.27450982,\\n              \\\"green\\\": 0.7411765,\\n              \\\"blue\\\": 0.7764706\\n            }\\n          }\\n        },\\n        {\\n          \\\"colorType\\\": \\\"LINK\\\",\\n          \\\"color\\\": {\\n            \\\"rgbColor\\\": {\\n              \\\"red\\\": 0.06666667,\\n              \\\"green\\\": 0.33333334,\\n              \\\"blue\\\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  },\\n  \\\"sheets\\\": [\\n    {\\n      \\\"properties\\\": {\\n        \\\"sheetId\\\": 0,\\n        \\\"title\\\": \\\"ptdata\\\",\\n        \\\"index\\\": 0,\\n        \\\"sheetType\\\": \\\"GRID\\\",\\n        \\\"gridProperties\\\": {\\n          \\\"rowCount\\\": 1000,\\n          \\\"columnCount\\\": 26\\n        }\\n      },\\n      \\\"data\\\": [\\n        {\\n          \\\"rowData\\\": [\\n            {\\n              \\\"values\\\": [\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"name\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"name\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"name\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"use in llm\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"use in llm\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"use in llm\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"size\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"size\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"size\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"link\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"link\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"link\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                }\\n              ]\\n            },\\n            {\\n              \\\"values\\\": [\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"Common Crawl\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"Common Crawl\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"Common Crawl\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"llama\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"numberValue\\\": 1342\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"numberValue\\\": 1342\\n                  },\\n                  \\\"formattedValue\\\": \\\"1342\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"RIGHT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/commoncrawl/statistics\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/commoncrawl/statistics\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"https://huggingface.co/datasets/commoncrawl/statistics\\\",\\n                  \\\"userEnteredFormat\\\": {\\n                    \\\"textFormat\\\": {\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/commoncrawl/statistics\\\"\\n                      }\\n                    }\\n                  },\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {\\n                        \\\"red\\\": 0.06666667,\\n                        \\\"green\\\": 0.33333334,\\n                        \\\"blue\\\": 0.8\\n                      },\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": true,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {\\n                          \\\"red\\\": 0.06666667,\\n                          \\\"green\\\": 0.33333334,\\n                          \\\"blue\\\": 0.8\\n                        }\\n                      },\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/commoncrawl/statistics\\\"\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"LINKED\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  },\\n                  \\\"hyperlink\\\": \\\"https://huggingface.co/datasets/commoncrawl/statistics\\\"\\n                }\\n              ]\\n            },\\n            {\\n              \\\"values\\\": [\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"The Pile\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"The Pile\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"The Pile\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"gpt-neo\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"gpt-neo\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"gpt-neo\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"numberValue\\\": 825\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"numberValue\\\": 825\\n                  },\\n                  \\\"formattedValue\\\": \\\"825\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"RIGHT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/EleutherAI/pile\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/EleutherAI/pile\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"https://huggingface.co/datasets/EleutherAI/pile\\\",\\n                  \\\"userEnteredFormat\\\": {\\n                    \\\"textFormat\\\": {\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/EleutherAI/pile\\\"\\n                      }\\n                    }\\n                  },\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {\\n                        \\\"red\\\": 0.06666667,\\n                        \\\"green\\\": 0.33333334,\\n                        \\\"blue\\\": 0.8\\n                      },\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": true,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {\\n                          \\\"red\\\": 0.06666667,\\n                          \\\"green\\\": 0.33333334,\\n                          \\\"blue\\\": 0.8\\n                        }\\n                      },\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/EleutherAI/pile\\\"\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"LINKED\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  },\\n                  \\\"hyperlink\\\": \\\"https://huggingface.co/datasets/EleutherAI/pile\\\"\\n                }\\n              ]\\n            },\\n            {\\n              \\\"values\\\": [\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"C4\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"C4\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"C4\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"llama\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"numberValue\\\": 750\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"numberValue\\\": 750\\n                  },\\n                  \\\"formattedValue\\\": \\\"750\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"RIGHT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/c4\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/c4\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"https://huggingface.co/datasets/c4\\\",\\n                  \\\"userEnteredFormat\\\": {\\n                    \\\"textFormat\\\": {\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/c4\\\"\\n                      }\\n                    }\\n                  },\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {\\n                        \\\"red\\\": 0.06666667,\\n                        \\\"green\\\": 0.33333334,\\n                        \\\"blue\\\": 0.8\\n                      },\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": true,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {\\n                          \\\"red\\\": 0.06666667,\\n                          \\\"green\\\": 0.33333334,\\n                          \\\"blue\\\": 0.8\\n                        }\\n                      },\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/c4\\\"\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"LINKED\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  },\\n                  \\\"hyperlink\\\": \\\"https://huggingface.co/datasets/c4\\\"\\n                }\\n              ]\\n            },\\n            {\\n              \\\"values\\\": [\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"GitHub\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"GitHub\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"GitHub\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"llama\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"numberValue\\\": 212\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"numberValue\\\": 212\\n                  },\\n                  \\\"formattedValue\\\": \\\"212\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"RIGHT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/codeparrot/github-code\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/codeparrot/github-code\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"https://huggingface.co/datasets/codeparrot/github-code\\\",\\n                  \\\"userEnteredFormat\\\": {\\n                    \\\"textFormat\\\": {\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/codeparrot/github-code\\\"\\n                      }\\n                    }\\n                  },\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {\\n                        \\\"red\\\": 0.06666667,\\n                        \\\"green\\\": 0.33333334,\\n                        \\\"blue\\\": 0.8\\n                      },\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": true,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {\\n                          \\\"red\\\": 0.06666667,\\n                          \\\"green\\\": 0.33333334,\\n                          \\\"blue\\\": 0.8\\n                        }\\n                      },\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/codeparrot/github-code\\\"\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"LINKED\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  },\\n                  \\\"hyperlink\\\": \\\"https://huggingface.co/datasets/codeparrot/github-code\\\"\\n                }\\n              ]\\n            },\\n            {\\n              \\\"values\\\": [\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"Wikipedia\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"Wikipedia\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"Wikipedia\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"llama\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"numberValue\\\": 112\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"numberValue\\\": 112\\n                  },\\n                  \\\"formattedValue\\\": \\\"112\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"RIGHT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/wikimedia/wikipedia\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/wikimedia/wikipedia\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"https://huggingface.co/datasets/wikimedia/wikipedia\\\",\\n                  \\\"userEnteredFormat\\\": {\\n                    \\\"textFormat\\\": {\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/wikimedia/wikipedia\\\"\\n                      }\\n                    }\\n                  },\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {\\n                        \\\"red\\\": 0.06666667,\\n                        \\\"green\\\": 0.33333334,\\n                        \\\"blue\\\": 0.8\\n                      },\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": true,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {\\n                          \\\"red\\\": 0.06666667,\\n                          \\\"green\\\": 0.33333334,\\n                          \\\"blue\\\": 0.8\\n                        }\\n                      },\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/wikimedia/wikipedia\\\"\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"LINKED\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  },\\n                  \\\"hyperlink\\\": \\\"https://huggingface.co/datasets/wikimedia/wikipedia\\\"\\n                }\\n              ]\\n            },\\n            {\\n              \\\"values\\\": [\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"Books3\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"Books3\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"Books3\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"llama\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"numberValue\\\": 101\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"numberValue\\\": 101\\n                  },\\n                  \\\"formattedValue\\\": \\\"101\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"RIGHT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/the_pile/books3\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/the_pile/books3\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"https://huggingface.co/datasets/the_pile/books3\\\",\\n                  \\\"userEnteredFormat\\\": {\\n                    \\\"textFormat\\\": {\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/the_pile/books3\\\"\\n                      }\\n                    }\\n                  },\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {\\n                        \\\"red\\\": 0.06666667,\\n                        \\\"green\\\": 0.33333334,\\n                        \\\"blue\\\": 0.8\\n                      },\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": true,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {\\n                          \\\"red\\\": 0.06666667,\\n                          \\\"green\\\": 0.33333334,\\n                          \\\"blue\\\": 0.8\\n                        }\\n                      },\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/the_pile/books3\\\"\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"LINKED\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  },\\n                  \\\"hyperlink\\\": \\\"https://huggingface.co/datasets/the_pile/books3\\\"\\n                }\\n              ]\\n            },\\n            {\\n              \\\"values\\\": [\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"ArXiv\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"ArXiv\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"ArXiv\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"llama\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"numberValue\\\": 33\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"numberValue\\\": 33\\n                  },\\n                  \\\"formattedValue\\\": \\\"33\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"RIGHT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/arxiv\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/arxiv\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"https://huggingface.co/datasets/arxiv\\\",\\n                  \\\"userEnteredFormat\\\": {\\n                    \\\"textFormat\\\": {\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/arxiv\\\"\\n                      }\\n                    }\\n                  },\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {\\n                        \\\"red\\\": 0.06666667,\\n                        \\\"green\\\": 0.33333334,\\n                        \\\"blue\\\": 0.8\\n                      },\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": true,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {\\n                          \\\"red\\\": 0.06666667,\\n                          \\\"green\\\": 0.33333334,\\n                          \\\"blue\\\": 0.8\\n                        }\\n                      },\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/arxiv\\\"\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"LINKED\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  },\\n                  \\\"hyperlink\\\": \\\"https://huggingface.co/datasets/arxiv\\\"\\n                }\\n              ]\\n            },\\n            {\\n              \\\"values\\\": [\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"StackExchange\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"StackExchange\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"StackExchange\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"llama\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"numberValue\\\": 20\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"numberValue\\\": 20\\n                  },\\n                  \\\"formattedValue\\\": \\\"20\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"RIGHT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/ArmelR/stack-exchange-instruction\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/ArmelR/stack-exchange-instruction\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"https://huggingface.co/datasets/ArmelR/stack-exchange-instruction\\\",\\n                  \\\"userEnteredFormat\\\": {\\n                    \\\"textFormat\\\": {\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/ArmelR/stack-exchange-instruction\\\"\\n                      }\\n                    }\\n                  },\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {\\n                        \\\"red\\\": 0.06666667,\\n                        \\\"green\\\": 0.33333334,\\n                        \\\"blue\\\": 0.8\\n                      },\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": true,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {\\n                          \\\"red\\\": 0.06666667,\\n                          \\\"green\\\": 0.33333334,\\n                          \\\"blue\\\": 0.8\\n                        }\\n                      },\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/ArmelR/stack-exchange-instruction\\\"\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"LINKED\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  },\\n                  \\\"hyperlink\\\": \\\"https://huggingface.co/datasets/ArmelR/stack-exchange-instruction\\\"\\n                }\\n              ]\\n            },\\n            {\\n              \\\"values\\\": [\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"Project Gutenberg\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"Project Gutenberg\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"Project Gutenberg\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"llama\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"llama\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"numberValue\\\": 11\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"numberValue\\\": 11\\n                  },\\n                  \\\"formattedValue\\\": \\\"11\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"RIGHT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/sedthh/gutenberg_english\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/sedthh/gutenberg_english\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"https://huggingface.co/datasets/sedthh/gutenberg_english\\\",\\n                  \\\"userEnteredFormat\\\": {\\n                    \\\"textFormat\\\": {\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/sedthh/gutenberg_english\\\"\\n                      }\\n                    }\\n                  },\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {\\n                        \\\"red\\\": 0.06666667,\\n                        \\\"green\\\": 0.33333334,\\n                        \\\"blue\\\": 0.8\\n                      },\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": true,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {\\n                          \\\"red\\\": 0.06666667,\\n                          \\\"green\\\": 0.33333334,\\n                          \\\"blue\\\": 0.8\\n                        }\\n                      },\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/sedthh/gutenberg_english\\\"\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"LINKED\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  },\\n                  \\\"hyperlink\\\": \\\"https://huggingface.co/datasets/sedthh/gutenberg_english\\\"\\n                }\\n              ]\\n            },\\n            {\\n              \\\"values\\\": [\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"HackerNews\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"HackerNews\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"HackerNews\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"gpt-neo\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"gpt-neo\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"gpt-neo\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"numberValue\\\": 4\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"numberValue\\\": 4\\n                  },\\n                  \\\"formattedValue\\\": \\\"4\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"RIGHT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/the_pile/hackernews\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/the_pile/hackernews\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"https://huggingface.co/datasets/the_pile/hackernews\\\",\\n                  \\\"userEnteredFormat\\\": {\\n                    \\\"textFormat\\\": {\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/the_pile/hackernews\\\"\\n                      }\\n                    }\\n                  },\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {\\n                        \\\"red\\\": 0.06666667,\\n                        \\\"green\\\": 0.33333334,\\n                        \\\"blue\\\": 0.8\\n                      },\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": true,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {\\n                          \\\"red\\\": 0.06666667,\\n                          \\\"green\\\": 0.33333334,\\n                          \\\"blue\\\": 0.8\\n                        }\\n                      },\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/the_pile/hackernews\\\"\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"LINKED\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  },\\n                  \\\"hyperlink\\\": \\\"https://huggingface.co/datasets/the_pile/hackernews\\\"\\n                }\\n              ]\\n            },\\n            {\\n              \\\"values\\\": [\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"OpenWebText2\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"OpenWebText2\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"OpenWebText2\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"gpt-neo\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"gpt-neo\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"gpt-neo\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"numberValue\\\": 62\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"numberValue\\\": 62\\n                  },\\n                  \\\"formattedValue\\\": \\\"62\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"RIGHT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  }\\n                },\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/openwebtext2\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"https://huggingface.co/datasets/openwebtext2\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"https://huggingface.co/datasets/openwebtext2\\\",\\n                  \\\"userEnteredFormat\\\": {\\n                    \\\"textFormat\\\": {\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/openwebtext2\\\"\\n                      }\\n                    }\\n                  },\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {\\n                        \\\"red\\\": 0.06666667,\\n                        \\\"green\\\": 0.33333334,\\n                        \\\"blue\\\": 0.8\\n                      },\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": true,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {\\n                          \\\"red\\\": 0.06666667,\\n                          \\\"green\\\": 0.33333334,\\n                          \\\"blue\\\": 0.8\\n                        }\\n                      },\\n                      \\\"link\\\": {\\n                        \\\"uri\\\": \\\"https://huggingface.co/datasets/openwebtext2\\\"\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"LINKED\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                        \\\"red\\\": 1,\\n                        \\\"green\\\": 1,\\n                        \\\"blue\\\": 1\\n                      }\\n                    }\\n                  },\\n                  \\\"hyperlink\\\": \\\"https://huggingface.co/datasets/openwebtext2\\\"\\n                }\\n              ]\\n            },\\n            {\\n              \\\"values\\\": [\\n                {\\n                  \\\"userEnteredValue\\\": {\\n                    \\\"stringValue\\\": \\\"FreeLaw\\\"\\n                  },\\n                  \\\"effectiveValue\\\": {\\n                    \\\"stringValue\\\": \\\"FreeLaw\\\"\\n                  },\\n                  \\\"formattedValue\\\": \\\"FreeLaw\\\",\\n                  \\\"effectiveFormat\\\": {\\n                    \\\"backgroundColor\\\": {\\n                      \\\"red\\\": 1,\\n                      \\\"green\\\": 1,\\n                      \\\"blue\\\": 1\\n                    },\\n                    \\\"padding\\\": {\\n                      \\\"top\\\": 2,\\n                      \\\"right\\\": 3,\\n                      \\\"bottom\\\": 2,\\n                      \\\"left\\\": 3\\n                    },\\n                    \\\"horizontalAlignment\\\": \\\"LEFT\\\",\\n                    \\\"verticalAlignment\\\": \\\"BOTTOM\\\",\\n                    \\\"wrapStrategy\\\": \\\"OVERFLOW_CELL\\\",\\n                    \\\"textFormat\\\": {\\n                      \\\"foregroundColor\\\": {},\\n                      \\\"fontFamily\\\": \\\"Arial\\\",\\n                      \\\"fontSize\\\": 10,\\n                      \\\"bold\\\": false,\\n                      \\\"italic\\\": false,\\n                      \\\"strikethrough\\\": false,\\n                      \\\"underline\\\": false,\\n                      \\\"foregroundColorStyle\\\": {\\n                        \\\"rgbColor\\\": {}\\n                      }\\n                    },\\n                    \\\"hyperlinkDisplayType\\\": \\\"PLAIN_TEXT\\\",\\n                    \\\"backgroundColorStyle\\\": {\\n                      \\\"rgbColor\\\": {\\n                 ...\n\n(The output of the tool call (shortuuid identifier: 2GPYCvqE4eXY3SiLrS5g7u) is too long! Only the first 100000 characters are shown here. The original output length is 123942 characters. The full output has been saved to the file /workspace/dumps/workspace/.overlong_tool_outputs/2GPYCvqE4eXY3SiLrS5g7u.json. Please check this file carefully, as it may be very long!)",
            "tool_call_id": "toolu_01BA7moAvJY8egEiJ7TWSRcX"
        },
        {
            "role": "assistant",
            "content": "I can see the data has been entered correctly, but it's not sorted by size in descending order as requested. Let me update the data with the correct sorting:",
            "tool_calls": [
                {
                    "id": "toolu_011bcp8H8TzzDwFQN8ipZTzd",
                    "type": "function",
                    "function": {
                        "name": "google_sheet-update_cells",
                        "arguments": "{\"spreadsheet_id\": \"1pkvieQSe2snpTOS94pnlHoGDwCXO2Fsx0Ct-_iGnQhM\", \"sheet\": \"ptdata\", \"range\": \"A1:D15\", \"data\": [[\"name\",\"use in llm\",\"size\",\"link\"],[\"Common Crawl\",\"llama\",1342,\"https://huggingface.co/datasets/commoncrawl/statistics\"],[\"The Pile\",\"gpt-neo\",825,\"https://huggingface.co/datasets/EleutherAI/pile\"],[\"C4\",\"llama\",750,\"https://huggingface.co/datasets/c4\"],[\"GitHub\",\"llama\",212,\"https://huggingface.co/datasets/codeparrot/github-code\"],[\"Wikipedia\",\"llama\",112,\"https://huggingface.co/datasets/wikimedia/wikipedia\"],[\"Books3\",\"llama\",101,\"https://huggingface.co/datasets/the_pile/books3\"],[\"PubMed Central\",\"gpt-neo\",90,\"https://huggingface.co/datasets/the_pile/pubmed_central\"],[\"OpenWebText2\",\"gpt-neo\",62,\"https://huggingface.co/datasets/openwebtext2\"],[\"FreeLaw\",\"gpt-neo\",51,\"https://huggingface.co/datasets/the_pile/freelaw\"],[\"ArXiv\",\"llama\",33,\"https://huggingface.co/datasets/arxiv\"],[\"USPTO Backgrounds\",\"gpt-neo\",22,\"https://huggingface.co/datasets/the_pile/uspto_backgrounds\"],[\"StackExchange\",\"llama\",20,\"https://huggingface.co/datasets/ArmelR/stack-exchange-instruction\"],[\"Project Gutenberg\",\"llama\",11,\"https://huggingface.co/datasets/sedthh/gutenberg_english\"],[\"HackerNews\",\"gpt-neo\",4,\"https://huggingface.co/datasets/the_pile/hackernews\"]]}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"{\\n  \\\"spreadsheetId\\\": \\\"1pkvieQSe2snpTOS94pnlHoGDwCXO2Fsx0Ct-_iGnQhM\\\",\\n  \\\"updatedRange\\\": \\\"ptdata!A1:D15\\\",\\n  \\\"updatedRows\\\": 15,\\n  \\\"updatedColumns\\\": 4,\\n  \\\"updatedCells\\\": 60\\n}\",\"annotations\":null}",
            "tool_call_id": "toolu_011bcp8H8TzzDwFQN8ipZTzd"
        },
        {
            "role": "assistant",
            "content": "Perfect! I have successfully organized the pre-training data for LLaMA and GPT-Neo models in the `ptdata` sheet of the \"LLM Pre-training Data\" spreadsheet. Here's a summary of what I've accomplished:\n\n## Summary of Pre-training Data Organization:\n\n**Structure:** The data is organized with the following columns as requested:\n- `name`: Dataset name\n- `use in llm`: Either \"llama\" or \"gpt-neo\" \n- `size`: Numeric value in GB (without \"GB\" suffix)\n- `link`: Hugging Face dataset links (preferred) or other relevant links\n\n**Sorting:** The data is sorted in descending order by size, from largest to smallest:\n\n1. **Common Crawl** (1,342 GB) - LLaMA\n2. **The Pile** (825 GB) - GPT-Neo\n3. **C4** (750 GB) - LLaMA\n4. **GitHub** (212 GB) - LLaMA\n5. **Wikipedia** (112 GB) - LLaMA\n6. **Books3** (101 GB) - LLaMA\n7. **PubMed Central** (90 GB) - GPT-Neo\n8. **OpenWebText2** (62 GB) - GPT-Neo\n9. **FreeLaw** (51 GB) - GPT-Neo\n10. **ArXiv** (33 GB) - LLaMA\n11. **USPTO Backgrounds** (22 GB) - GPT-Neo\n12. **StackExchange** (20 GB) - LLaMA\n13. **Project Gutenberg** (11 GB) - LLaMA\n14. **HackerNews** (4 GB) - GPT-Neo\n\n**Key Features:**\n- All links point to Hugging Face datasets where available\n- Size values are numeric only (no \"GB\" units as requested)\n- Each dataset is assigned to either LLaMA or GPT-Neo based on research findings\n- The data covers the major pre-training datasets used by both models\n\nThis organization will help you understand the scale and variety of data used to train these large language models, with Common Crawl being the largest single dataset and The Pile being the comprehensive collection used by GPT-Neo.",
            "tool_calls": [
                {
                    "id": "toolu_018Y7oHHUAFweWb2nyroJ7uy",
                    "type": "function",
                    "function": {
                        "name": "local-claim_done",
                        "arguments": ""
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "you have claimed the task is done!",
            "tool_call_id": "toolu_018Y7oHHUAFweWb2nyroJ7uy"
        }
    ]
}