---
title: "Git Repo"
description: "Summarize the ImageNet 256 experimental results from my image generation papers into a LaTeX table."
---

<Card>
<div className="tools-container">
<div className="mcp-servers-container">
<div className="mcp-servers-title">
MCP Servers
</div>
<div className="mcp-servers-grid">
<div className="mcp-server-item">
<svg width="20" height="20" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
<span className="mcp-server-name">github</span>
</div>
<div className="mcp-server-item">
<svg width="20px" height="20px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg>
<span className="mcp-server-name">pdf-tools</span>
</div>
<div className="mcp-server-item">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="20px" height="20px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg>
<span className="mcp-server-name">filesystem</span>
</div>
</div>
</div>
<div className="local-tools-container">
<div className="mcp-servers-title">
Local Tools
</div>
<div className="local-tools-grid">
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 100 100" enable-background="new 0 0 100 100" id="Layer_1" version="1.1" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><g><rect clip-rule="evenodd" fill="#F2F2F2" fill-rule="evenodd" height="83.437" width="67.025" x="9.012" y="7.604"/><path d="M77.454,92.458H7.595V6.187h69.859V92.458z M10.429,89.624H74.62V9.021H10.429V89.624z"/></g><g><rect clip-rule="evenodd" fill="#FF7C24" fill-rule="evenodd" height="10.481" width="17.952" x="46.695" y="34.866"/></g><g><rect height="2.834" width="19.463" x="20.504" y="35.575"/></g><g><rect height="2.834" width="15.561" x="20.718" y="42.508"/></g><g><rect height="2.833" width="15.562" x="20.813" y="49.514"/></g><g><rect height="2.833" width="27.128" x="20.718" y="56.753"/></g><g><rect height="2.833" width="23.51" x="20.718" y="63.688"/></g><g><rect clip-rule="evenodd" fill="#39B6CC" fill-rule="evenodd" height="9.004" width="44.095" x="20.553" y="19.064"/></g><g><rect height="2.833" width="26.272" x="20.718" y="70.32"/></g><g><rect height="2.834" width="32.8" x="20.718" y="77.253"/></g><g><rect height="2.834" width="3.235" x="38.304" y="42.508"/></g><g><path clip-rule="evenodd" d="M77.931,71.902l4.287,4.427l-6.644,6.437l-4.309-4.457    C74.147,76.998,76.504,74.726,77.931,71.902L77.931,71.902z" fill="#F2F2F2" fill-rule="evenodd"/><path d="M75.542,84.77l-6.692-6.92l1.828-0.831c2.579-1.174,4.706-3.218,5.989-5.756l0.897-1.776l6.656,6.874L75.542,84.77z     M73.584,78.669l2.023,2.091l4.605-4.463l-2.007-2.074C76.994,76.012,75.414,77.531,73.584,78.669z"/></g><g><polygon clip-rule="evenodd" fill="#39B6CC" fill-rule="evenodd" points="83.267,75.319 91.984,84.338 83.247,92.779     74.535,83.761   "/><path d="M83.213,94.783L72.531,83.726l10.771-10.41l10.687,11.056L83.213,94.783z M76.538,83.794l6.744,6.981l6.698-6.472    l-6.748-6.981L76.538,83.794z"/></g><g><path clip-rule="evenodd" d="M66.124,50.799c7.742,0,14.018,6.276,14.018,14.019    s-6.275,14.019-14.018,14.019c-7.743,0-14.019-6.276-14.019-14.019S58.381,50.799,66.124,50.799L66.124,50.799z" fill="#F2F2F2" fill-rule="evenodd"/><path d="M66.124,80.253c-8.511,0-15.435-6.924-15.435-15.435s6.924-15.435,15.435-15.435S81.56,56.307,81.56,64.818    S74.635,80.253,66.124,80.253z M66.124,52.216c-6.949,0-12.601,5.653-12.601,12.602s5.651,12.601,12.601,12.601    c6.948,0,12.602-5.652,12.602-12.601S73.072,52.216,66.124,52.216z"/></g><g><rect height="2.833" width="10.313" x="39.902" y="49.514"/></g><g><path d="M76.404,65.586H73.57c0-0.636-0.068-1.255-0.205-1.84c-0.043-0.186-0.096-0.385-0.169-0.63l2.717-0.808    c0.091,0.304,0.158,0.559,0.215,0.801C76.31,63.901,76.404,64.735,76.404,65.586z M72.438,61.433    c-1.489-2.5-4.203-4.058-7.084-4.061l0.004-2.834c3.871,0.005,7.518,2.091,9.516,5.445L72.438,61.433z"/></g></g></svg>
<span className="local-tool-name">history</span>
</div>
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M861.588238 240.133873v-65.792823c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.631049H314.659414v-65.631049c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.792823c-36.317212 0.868255-65.631049 30.539428-65.63105 67.061417v543.745565c0 37.06772 30.155471 67.223191 67.223191 67.223191h696.886045c37.06772 0 67.223191-30.155471 67.223191-67.223191V307.19529c-0.001024-36.52199-29.315885-66.193162-65.633097-67.061417z m-109.385765-65.792823c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m-546.928824 0c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m678.191947 676.600829c0 12.935767-10.532708 23.468476-23.468476 23.468475H163.111076c-12.935767 0-23.468476-10.532708-23.468476-23.468475V307.19529c0-12.402323 9.677764-22.593054 21.877358-23.415233v65.577807c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.631049h393.789368v65.631049c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.577807c12.19857 0.82218 21.877358 11.012911 21.877358 23.415233v543.746589z" fill="#22C67F" /><path d="M706.719439 478.272194l-48.01715-44.741741-182.28128 195.621482-111.468348-122.615387-48.563905 44.148911 159.469116 172.685427z" fill="#74E8AE" /></svg>
<span className="local-tool-name">claim_done</span>
</div>
<div className="local-tool-item">
<Icon icon="bars-progress" size={20} color="#4286f6" />
<span className="local-tool-name">manage_context</span>
</div>
<div className="local-tool-item">
<Icon icon="filter-list" size={20} color="#4286f6" />
<span className="local-tool-name">handle_overlong_tool_outputs</span>
</div>
</div>
</div>
</div>
</Card>



## Instruction

I'm trying to reproduce the experiments from a deep learning paper (paper.pdf). Unfortunately, the paper does not provide an official implementation code repository. To work more efficiently, please help me find the unofficial implementation repository with the most stars on GitHub and update the existing `result.json` with the information.

## Initial State

### Local Workspace

<div className="file-tree">
<a href="https://github.com/hkust-nlp/mcpbench_dev/tree/finalpool_dev/tasks/finalpool/git-repo/initial_workspace">workspace</a>/
└── result.json
</div>

## Model Trajectory

<AccordionGroup>
<Accordion title="claude4-sonnet">

<Columns cols={3}>
<Card title="Task Completion" icon="check">
Completed
</Card>
<Card title="Tool Calls" icon="wrench">
18
</Card>
<Card title="Turns" icon="arrows-rotate">
19
</Card>
</Columns>

<div className="thinking-box">
🧐`Agent`

I'll help you find the unofficial implementation repository for the deep learning paper. First, let me read the paper to understand what we're looking for, then search GitHub for implementations.
</div>

<div className="tool-call-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools get_pdf_info`

```json
{
	"pdf_file_path": "/workspace/dumps/workspace/paper.pdf"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
PDF file information:
Total pages: 14
Title: 
Author: 
Creator: LaTeX with hyperref
Creation date: Unknown

```
</details>
</div>

<div className="tool-call-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

```json
{
	"pdf_file_path": "/workspace/dumps/workspace/paper.pdf",
	 "start_page": 1,
	 "end_page": 3
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
PDF total pages: 14
Extracted page range: 1-3

=== Page 1 ===
Mixture-of-Depths: Dynamically allocating
compute in transformer-based language
models
David Raposo1*, Sam Ritter1, Blake Richards1,2, Timothy Lillicrap1, Peter Conway Humphreys1and Adam
Santoro1*
1Google DeepMind,2McGill University & Mila,*Equal Contribution
Transformer-based language models spread FLOPs uniformly across input sequences. In this work we
demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute ) to specific
positions in a sequence, optimising the allocation along the sequence for different layers across the
model depth. Our method enforces a total compute budget by capping the number of tokens ( 𝑘) that
can participate in the self-attention and MLP computations at a given layer. The tokens to be processed
are determined by the network using a top- 𝑘routing mechanism. Since 𝑘is defined a priori, this
simple procedure uses a static computation graph with known tensor sizes, unlike other conditional
computation techniques. Nevertheless, since the identities of the 𝑘tokens are fluid, this method can
expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure
is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do
models trained in this way learn to dynamically allocate compute, they do so efficiently. These models
match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of
the FLOPs per forward pass, and can be upwards of 50% faster to step during post-training sampling.
1. Introduction
Notallproblemsrequirethesameamountoftimeorefforttosolve. Analogously, inlanguagemodeling
not all tokens and sequences require the same time or effort to accurately make a prediction. And
yet, transformer models expend the same amount of compute per token in a forward pass. Ideally,
transformers would use smaller total compute budgets by not spending compute unnecessarily.
Conditional computation is a technique that tries to reduce total compute by expending it only
when needed (Bengio et al., 2016; Bengio, 2013; Bengio et al., 2013). Various algorithms offer
solutions to when and how much compute should be used (Ainslie et al., 2023; Bapna et al., 2020;
Fedus et al., 2022). However, general formulations of this challenging problem may not work well with
existing hardware constraints since they tend to introduce dynamic computation graphs (Dehghani
et al., 2018; Graves, 2016). The most promising conditional computation methods may instead be
those that are harmonious with our current hardware stack, which prioritizes static computation
graphs, and known tensor sizes that are selected to maximize hardware utilization.
Here we consider the problem of language modeling using a staticcompute budget that can be
made less than that used by a vanilla transformer. The network must learn how to dynamically allocate
the available compute by making decisions per-token, in each layer, about where to spend compute
from the available budget. In our implementation total compute is user defined and unchanging
prior to training, rather than being a function of the network’s on-the-fly decisions. Thus, hardware
efficiency gains—such as reduced memory footprint, or reduced FLOPs per forward pass—can be
anticipated and exploited ahead of time. As we will show, these gains can be had without sacrificing
overall performance.
Corresponding author(s): draposo@google.com, adamsantoro@google.com
©2024 Google DeepMind. All rights reservedarXiv:2404.02258v1  [cs.LG]  2 Apr 2024

=== Page 2 ===
Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
We leverage an approach akin to Mixture of Experts (MoE) transformers, in which dynamic
token-level routing decisions are made across the network depth. Departing from MoE, we choose
to either apply a computation to a token (as would be the case for a standard transformer), or pass
it through a residual connection (remaining unchanged and saving compute). Also in contrast to
MoE, we apply this routing to both forward MLPs and multi-head attention. Since this therefore
also impacts the keys and queries we process, the routing makes decisions not only about which
tokens to update, but also which tokens are made available to attend to. We refer to this strategy
as Mixture-of-Depths (MoD) to emphasize how individual tokens pass through different numbers of
layers, or blocks, through the depth of the transformer (see figure 1).
The MoD technique also allows one to trade-off performance with speed. On the one hand, one
can train an MoD transformer that improves upon vanilla transformers by as much as 1.5%on the
final log probability training objective for equivalent training FLOPs (isoFLOP), and while taking an
equivalent amount of wall-clock time to train. On the other hand, one can train an MoD transformer
that achieves training loss parity with an isoFLOP optimal vanilla transformer, but which uses a
fraction of the FLOPs (upwards of 50%) per forward pass, and hence is faster to step. Together, these
results imply that MoD transformers learn to route intelligently (i.e., skipping computations that are
unnecessary) since they can achieve equal or better log probabilities per sequence despite a smaller
FLOP footprint per forward pass.
2. Background
The transformer architecture has become the workhorse of a revolution in practical artificial in-
telligence, bringing unprecedented capabilities at the cost of expensive training runs and serving
procedures. This has spurred tremendous interest in making transformer architectures more effi-
cient (Gupta and Agrawal, 2021; Tay et al., 2020). One of the promising approaches is conditional
computation , whereby learned mechanisms determine when and how to expend computation. This
terminology was introduced by Bengio (2013), and the concept was explored further over the next
several years (Bengio et al., 2016, 2013; Cho and Bengio, 2014; Graves, 2016; Jernite et al., 2017;
Wang et al., 2017).
A wide variety of recent work has developed conditional computation methods for transformers.
Some of this work focuses on "early exiting", that is, learning to decide when to end computation on
a given token, allowing the token to skip any remaining transformer layers after the exit decision
is made (Elbayad et al., 2019; Liu et al., 2021; Schuster et al., 2022). In MoD, unlike in early-exit
methods, a token can skip middle layers, then be updated via self-attention with tokens that that
have gone through all the middle layers. We speculate that this might be a useful property.
Other work has developed methods for iterating transformer layers with shared weights for
an adaptive number of steps (Dehghani et al., 2018; Simoulin and Crabbé, 2021). Bolya et al.
(2023) developed a method for choosing tokens to merge when running inference on a trained vision
transformer which notably requires no learning. Lei et al. (2023) make use of conditional computation
in a fine tuning setting by building on adapter approaches (He et al., 2021) to learn to skip blocks of
frozen pre-trained weights in favor of running only a small fine-tuned adapter.
CoLT5 (Ainslie et al., 2023) uses conditional routing to select whether a given token will pass
through a heavy or light pathway for each feedforward layer. Further, they use the same routing
mechanism to select whether a token will attend to all other tokens or to a select few, as in Guo et al.
(2022). Like MoD, CoLT5 uses soft top-k for making routing decisions. However, CoLT5 focuses on
a encoder-decoder setting, and thus does need to contend with the problem of efficient sequential
decoding given the non-causal nature of the top-k operation. In contrast, our current work with
2

=== Page 3 ===
Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
Figure 1|Mixture-of-Depths Transformer. As in mixture-of-experts (MoE) transformers we use a
router to choose among potential computational paths. But unlike in MoE transformers the possible
choices are a standard block’s computation (i.e., self-attention and MLP) or a residual connection.
Since some tokens take this second route, Mixture-of-Depths (MoD) transformers have a smaller
total FLOP footprint compared to vanilla or MoE transformers. On the top right is depicted a trained
model’sroutingdecisionsforashortsequencetruncatedto64tokensforvisualizationpurposes. When
examining the choices one can find tokens processed by later blocks’ layers, despite passing through
relatively few total blocks throughout the model’s depth. This is a unique feature of MoD compared
to conventional halting-based, or "early-exit" conditional computation, which instead engage blocks
serially, or vanilla transformers, which engage every block.
MoD focuses on the decoder-only setting, and so we propose a predictive router to enable efficient
inference for conditional computation in transformers.
One successful formulation of conditional computation is the the "mixture-of-experts" layer (MoE)
as introduced by Shazeer et al. (2017). Developed initially in the context of LSTMs, later work showed
compelling empirical results for MoE with transformers (Fedus et al., 2022; Lepikhin et al., 2020;
Zoph et al., 2022). Unlike other conditional computation approaches that try to conserve or expend
additional compute, MoE transformers use conditional logic to route tokens to one of many expert
MLPs while keeping total compute expenditure constant. Our mixture-of-depths method can be
thought of as using the routing logic from MoE transformers, but rather than having multiple experts,
MoD deploys a single expert which can be dynamically skipped.
3. Implementing Mixture-of-Depths Transformers
Our high-level strategy is as follows:
•Set a static compute budget that is less than that of an equivalent vanilla transformer by limiting
3
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me read more of the paper to get the full title and understand the key concepts:
</div>

<div className="tool-call-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

```json
{
	"pdf_file_path": "/workspace/dumps/workspace/paper.pdf",
	 "start_page": 4,
	 "end_page": 6
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
PDF total pages: 14
Extracted page range: 4-6

=== Page 4 ===
Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
the number of tokens in a sequence that can participate in a block’s computations (i.e., self-
attention and subsequent MLP). For example, while a vanilla transformer might permit all the
tokens in a sequence to participate in self-attention, we might limit the number to 50% of the
tokens in a sequence. See section 3.1.
•Use a per-block router to emit a scalar weight for each token, which expresses the router’s
preference for that token to participate in a block’s computations or to route around it. See
section 3.2.
•Identify the top- 𝑘scalar weights (per sequence, per block) to select those tokens that will
participate in a block’s computations. Since precisely 𝑘tokens will participate in the block’s
computations, the computation graph and tensor sizes remain static throughout training; it is
merely the tokens’ participation that is dynamic and context-sensitive, as determined by the
router. See section 3.3.
We then discuss some complications when sampling post-training in section 3.5.
3.1. Defining a compute budget
To enforce a total compute budget per forward pass we leverage the notion of capacity , which
defines the total number of tokens that comprise the input to a given computation (e.g., the tokens
participatinginself-attention, agivenexpertinMoEtransformers, etc). Forexample, theself-attention
and MLP in each vanilla transformer block have a capacity of 𝑇—the total number of tokens across
the sequence and batch. MoE transformers, on the other hand, use a capacity less than 𝑇per expert
MLP so as to more evenly divide the total compute across each expert. But, since they use multiple
experts per block, their total capacity is approximately equal to that of a vanilla transformer.
Generally, it is the token capacity that determines the total FLOPs for transformers that use
conditional computation, rather than the outcomes of any routing decisions. This is because static-
graph implementations account for the worst-case scenarios decisions; e.g., a computation’s inputs
will be padded to its capacity amount even if relatively few tokens actually end up routing to it,
and/or tokens will be dropped from the computation if the capacity is exceeded.
We can achieve our goal of using a smaller compute budget per forward pass compared to a
vanilla transformer by lowering the capacity of the computations. However, using a smaller compute
budget haphazardly will result in a performance degradation. We hypothesize that certaintokens
might not require as much processing as others, and these tokens can be identified through learning.
Therefore, if the network learns to choose the right tokens to fill up its capacities, then it may preserve
its performance. In the following we describe routing schemes that can be used for this purpose.
3.2. Routing around transformer blocks
We consider the setting whereby we route tokens to one of two computational paths: (1) self-attention
and MLP blocks, and (2) a residual connection. The latter is computationally cheap, and results in a
block output that is entirely determined by the value of its input. The former path is computationally
expensive.
The total number of FLOPs per forward pass will be fewer than that in a vanilla transformer if
we set the capacity for path (1) to be anything less than 𝑇(the total number of tokens across the
sequence and batch). For example, if we were to set a block’s capacity to𝑇
2(i.e., half the number
of tokens as would be the case in a vanilla transformer) then query-times-key matrix multiplication
during self-attention becomes 25%as FLOP-intensive as in a vanilla transformer ( (𝑇
2)2vs.𝑇2). Similar
calculations can determine the FLOP-savings for the MLP.
4

=== Page 5 ===
Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
Intuitively, the total FLOPs per forward pass decreases (and the time to complete a forward pass
decreases) in proportion to how aggressively we shrink the blocks’ capacities. However, downstream
performance will also be affected by how aggressively we shrink the blocks capacities, and by the
routing algorithm we implement.
At one extreme, if we leave each block’s capacity at 𝑇and route every token to (rather than
around) each block, then we recover a vanilla transformer. At the other extreme, if we set each
block’s capacity to 0and route all tokens aroundeach block, then we’re left with a very fast model
that doesn’t engage with the vast majority of the transformer’s parameters, and undoubtedly has
poor downstream performance. We hypothesize that somewhere between these two extremes is an
optimal model that is faster than a vanilla Transformer and performs as well, if not better, all while
being faster to step.
3.3. Routing schemes
Naively, one can leverage stochasticity to route tokens, akin to layer or block “dropout”. We present
this routing scheme as a control, and will show that it significantly under-performs relative to vanilla
transformers.
We hypothesize that learnedrouting is preferable. Intuitively, the network should be able to learn
which tokens require more or less processing than others. If we are correct that Transformers often
expend more compute than they need to make their predictions, then it is an empirical question as to
how aggressively we can shrink each block’s capacity, and hence, how many tokens we can afford to
route around each block.
There are two learned routing schemes we consider (see figure 2): token-choice and expert-choice.
In token-choice routing, a router produces per-token probability distributions across computational
paths (e.g., across expert identities in MoE Transformers). Tokens are then shuttled to the path
they prefer—i.e., that with the highest probability—and auxiliary losses ensure that all tokens don’t
converge to the same path. Token-choice routing can have load balancing problems since there isn’t a
guarantee that tokens divide themselves appropriately between the possible paths. “Expert choice
routing” flips this recipe on its head: rather than having tokens choose the path they prefer, each
path instead chooses the top- 𝑘tokens based on the tokens’ preferences. This ensures a perfect load
balance since 𝑘tokens are guaranteed to be shuttled to each path. However, it could result in over- or
under-processing of some tokens, since some tokens may be among the top- 𝑘for multiple paths, or
for none of them.
We decided to leverage expert-choice routing for a few reasons. First, it obviates the need for an
auxiliary balancing loss. Second, since the top- 𝑘operation depends on the magnitude of the router
weights, this routing scheme allows for relative routing weights to help determine which tokens most
need the block’s computations; routers can try to ensurethat the most critical tokens are among the
top-𝑘by setting their weight appropriately, which is not possible with token-choice routing schemes.
For our specific use-case, wherein one computational path is essentially a null operation, it might
be critical that important tokens are routed away from the null operation. Third, because we only
route through two paths, a single top- 𝑘operation can efficiently split the tokens into two mutually
exclusive sets, one for each computational path, preventing the over- or under-processing problem
mentioned above.
5

=== Page 6 ===
Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
Figure 2|Routing schemes. Tokens are funnelled to the computational path of their choice when
using token-choice routing (left). If a given path exceeds its capacity (e.g., more than two tokens in
this example) then surplus tokens must be dropped (purple token). The exact token that is ultimately
dropped depends on the precise implementation in the underlying code. For example, priority is
often given to those tokens that come earlier in the sequence or batch order. With expert-choice
routing (middle), precisely 𝑘(in this case, two) tokens are chosen per path using a top- 𝑘mechanism
across the tokens’ router weights. Here, tokens are dropped if they are not among the top- 𝑘with
respect to any given path (orange token), and some tokens may even be funnelled to multiple paths
(yellow token). In this work we deploy expert-choice routing (right). However, because we use just
a single path, we leverage the implicit knowledge that tokens will be dropped if 𝑘is less than the
sequence length so that we can route tokens away from the self-attention and MLP computations,
thus expending fewer FLOPs in a given forward pass of the model.
3.4. Routing implementation
As a reminder of the high-level intuition, each token is processed by a router to produce a scalar
weight, and the top- 𝑘weights are then used to choose the token identities that will route through a
transformer’s block, which comprises self-attention and the subsequent MLP.
Suppose we have the set of token embeddings in a sequence of length 𝑆for a given layer 𝑙; that
is𝑋𝑙={𝑥𝑙
𝑖|𝑖is an integer, 1≤𝑖≤𝑆}. The router weight for a given token embedding is a scalar
produced as a result of a linear projection, 𝑟𝑙
𝑖=𝑤𝑇
𝜃𝑥𝑙
𝑖.
Our goal is to use these router weights to determine the output of a block’s computation of each
token. Suppose 𝑃𝛽(𝑅𝑙)is the𝛽-th percentile of the set of router weights 𝑅𝑙, where𝛽=1−𝐶/𝑆and𝐶
is the user-defined capacity per batch element (an integer < 𝑆that defines the number of tokens from
a sequence that will be processed by a given function). A block’s output for a given token is:
𝑥𝑙+1
𝑖=(
𝑟𝑙
𝑖𝑓𝑖(˜𝑋𝑙)+𝑥𝑙
𝑖,if𝑟𝑙
𝑖> 𝑃𝛽(𝑅𝑙)
𝑥𝑙
𝑖, if𝑟𝑙
𝑖< 𝑃𝛽(𝑅𝑙)(1)
Here, ˜𝑋𝑙is the set of tokens whose router values 𝑟𝑙
𝑖> 𝑃𝛽(𝑅𝑙)(that is, the “top-k” tokens), and 𝑓
comprises self-attention and the subsequent MLP. Note that the output for a given token 𝑥𝑙+1
𝑖might
depend on other tokens 𝑥𝑙
𝑖≠𝑗because of the self-attention operation. The cardinality of ˜𝑋𝑙is𝐶(or𝑘):
the user-defined capacity. Therefore, the mixture-of-depths transformer accrues compute savings
6
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Perfect! Now I have a clear understanding of the paper. This is about "Mixture-of-Depths" (MoD) transformers by David Raposo et al. from Google DeepMind. Let me search GitHub for unofficial implementations of this paper.
</div>

<div className="tool-call-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github search_repositories`

```json
{
	"query": "mixture of depths transformer implementation",
	 "perPage": 20
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
{"total_count":6,"incomplete_results":false,"items":[{"id":785664688,"name":"Mixture-of-depths","full_name":"astramind-ai/Mixture-of-depths","description":"Unofficial implementation for the paper \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/astramind-ai/Mixture-of-depths","language":"Python","stargazers_count":171,"forks_count":7,"open_issues_count":5,"updated_at":"2025-09-19T02:56:57Z","created_at":"2024-04-12T11:07:48Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":782269816,"name":"Mixture-of-Depths","full_name":"kyegomez/Mixture-of-Depths","description":"Implementation of the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/kyegomez/Mixture-of-Depths","language":"Python","stargazers_count":106,"forks_count":7,"open_issues_count":7,"updated_at":"2025-09-19T03:52:12Z","created_at":"2024-04-05T00:51:20Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":782847289,"name":"mixture-of-depths","full_name":"sramshetty/mixture-of-depths","description":"An unofficial implementation of \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/sramshetty/mixture-of-depths","language":"Python","stargazers_count":36,"forks_count":3,"open_issues_count":0,"updated_at":"2025-09-05T01:19:59Z","created_at":"2024-04-06T07:51:03Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":874565778,"name":"Router-Tuning-Mixture-of-Depths","full_name":"CASE-Lab-UMD/Router-Tuning-Mixture-of-Depths","description":"The open-source Mixture of Depths code and the official implementation of the paper \"Router-Tuning: A Simple and Effective Approach for Enabling Dynamic Depth in Transformers. (EMNLP 2025)\"","html_url":"https://github.com/CASE-Lab-UMD/Router-Tuning-Mixture-of-Depths","language":"Python","stargazers_count":16,"forks_count":2,"open_issues_count":0,"updated_at":"2025-09-15T08:40:03Z","created_at":"2024-10-18T04:11:52Z","topics":["deep-learning","large-language-models","machine-learning","mixture-of-depths","neural-network","parameter-efficient-fine-tuning"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":783121754,"name":"MoE-LLM","full_name":"eclouder/MoE-LLM","description":"Implementation of the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/eclouder/MoE-LLM","language":"Python","stargazers_count":2,"forks_count":0,"open_issues_count":0,"updated_at":"2024-04-22T08:48:24Z","created_at":"2024-04-07T02:04:36Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":809894170,"name":"MH-MoDE","full_name":"dav3794/MH-MoDE","description":"Implementation of Multi-Head Mixture-of-Depths-and-Experts Transformer for NLP course.","html_url":"https://github.com/dav3794/MH-MoDE","language":"Jupyter Notebook","stargazers_count":0,"forks_count":0,"open_issues_count":0,"updated_at":"2024-06-18T11:31:36Z","created_at":"2024-06-03T16:48:16Z","private":false,"fork":false,"archived":false,"default_branch":"main"}]}
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Great! I found several implementations. Let me also search with different keywords to make sure I'm not missing any popular ones:
</div>

<div className="tool-call-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github search_repositories`

```json
{
	"query": "mixture-of-depths MoD transformer",
	 "perPage": 20
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
{"total_count":1,"incomplete_results":false,"items":[{"id":809894170,"name":"MH-MoDE","full_name":"dav3794/MH-MoDE","description":"Implementation of Multi-Head Mixture-of-Depths-and-Experts Transformer for NLP course.","html_url":"https://github.com/dav3794/MH-MoDE","language":"Jupyter Notebook","stargazers_count":0,"forks_count":0,"open_issues_count":0,"updated_at":"2024-06-18T11:31:36Z","created_at":"2024-06-03T16:48:16Z","private":false,"fork":false,"archived":false,"default_branch":"main"}]}
```
</details>
</div>

<div className="tool-call-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github search_repositories`

```json
{
	"query": "\"Mixture-of-Depths\" \"Dynamically allocating compute\"",
	 "perPage": 20
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
{"total_count":5,"incomplete_results":false,"items":[{"id":785664688,"name":"Mixture-of-depths","full_name":"astramind-ai/Mixture-of-depths","description":"Unofficial implementation for the paper \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/astramind-ai/Mixture-of-depths","language":"Python","stargazers_count":171,"forks_count":7,"open_issues_count":5,"updated_at":"2025-09-19T02:56:57Z","created_at":"2024-04-12T11:07:48Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":782269816,"name":"Mixture-of-Depths","full_name":"kyegomez/Mixture-of-Depths","description":"Implementation of the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/kyegomez/Mixture-of-Depths","language":"Python","stargazers_count":106,"forks_count":7,"open_issues_count":7,"updated_at":"2025-09-19T03:52:12Z","created_at":"2024-04-05T00:51:20Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":782847289,"name":"mixture-of-depths","full_name":"sramshetty/mixture-of-depths","description":"An unofficial implementation of \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/sramshetty/mixture-of-depths","language":"Python","stargazers_count":36,"forks_count":3,"open_issues_count":0,"updated_at":"2025-09-05T01:19:59Z","created_at":"2024-04-06T07:51:03Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":784596511,"name":"Mixture-of-Depths-Jax","full_name":"dtunai/Mixture-of-Depths-Jax","description":"Jax module for the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/dtunai/Mixture-of-Depths-Jax","language":"Python","stargazers_count":3,"forks_count":0,"open_issues_count":13,"updated_at":"2024-07-18T20:50:24Z","created_at":"2024-04-10T06:56:02Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":783121754,"name":"MoE-LLM","full_name":"eclouder/MoE-LLM","description":"Implementation of the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/eclouder/MoE-LLM","language":"Python","stargazers_count":2,"forks_count":0,"open_issues_count":0,"updated_at":"2024-04-22T08:48:24Z","created_at":"2024-04-07T02:04:36Z","private":false,"fork":false,"archived":false,"default_branch":"main"}]}
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Let me also search for any repositories that might reference the paper authors:
</div>

<div className="tool-call-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github search_repositories`

```json
{
	"query": "\"David Raposo\" \"Mixture-of-Depths\"",
	 "perPage": 10
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
{"total_count":0,"incomplete_results":false,"items":[]}
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me check if there are any other variations or if I missed anything by searching for "MoD transformer":
</div>

<div className="tool-call-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github search_repositories`

```json
{
	"query": "MoD transformer conditional compute",
	 "perPage": 20
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
{"total_count":0,"incomplete_results":false,"items":[]}
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Let me also check for repositories that might be implementing the paper with different naming:
</div>

<div className="tool-call-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github search_repositories`

```json
{
	"query": "\"dynamic compute allocation\" transformer",
	 "perPage": 10
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
{"total_count":0,"incomplete_results":false,"items":[]}
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me get more details about the top repositories to make sure I identify the best one. Let me examine the top candidates:
</div>

<div className="tool-call-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github get_file_contents`

```json
{
	"owner": "astramind-ai",
	 "repo": "Mixture-of-depths",
	 "path": "/"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
[{"type":"file","name":".gitignore","path":".gitignore","size":260,"sha":"db6273ba8530b15d4224a3cb55e8462fefec64ce","url":"https://api.github.com/repos/astramind-ai/Mixture-of-depths/contents/.gitignore?ref=aff9e74fc9c5a30d2c59dc36767f1f0fd86255e8","html_url":"https://github.com/astramind-ai/Mixture-of-depths/blob/aff9e74fc9c5a30d2c59dc36767f1f0fd86255e8/.gitignore","download_url":"https://raw.githubusercontent.com/astramind-ai/Mixture-of-depths/aff9e74fc9c5a30d2c59dc36767f1f0fd86255e8/.gitignore"},{"type":"dir","name":"MoD","path":"MoD","sha":"42275bd76e3823e5ad1050c5566e0819fe9d7ecb","url":"https://api.github.com/repos/astramind-ai/Mixture-of-depths/contents/MoD?ref=aff9e74fc9c5a30d2c59dc36767f1f0fd86255e8","html_url":"https://github.com/astramind-ai/Mixture-of-depths/tree/aff9e74fc9c5a30d2c59dc36767f1f0fd86255e8/MoD"},{"type":"file","name":"README.md","path":"README.md","size":2539,"sha":"3b64b82fd62acd1dc75a368ccc2d87f640ad0b58","url":"https://api.github.com/repos/astramind-ai/Mixture-of-depths/contents/README.md?ref=aff9e74fc9c5a30d2c59dc36767f1f0fd86255e8","html_url":"https://github.com/astramind-ai/Mixture-of-depths/blob/aff9e74fc9c5a30d2c59dc36767f1f0fd86255e8/README.md","download_url":"https://raw.githubusercontent.com/astramind-ai/Mixture-of-depths/aff9e74fc9c5a30d2c59dc36767f1f0fd86255e8/README.md"},{"type":"file","name":"setup.py","path":"setup.py","size":769,"sha":"5f2a08831a8b8684c321081fee35b98f66adc9c1","url":"https://api.github.com/repos/astramind-ai/Mixture-of-depths/contents/setup.py?ref=aff9e74fc9c5a30d2c59dc36767f1f0fd86255e8","html_url":"https://github.com/astramind-ai/Mixture-of-depths/blob/aff9e74fc9c5a30d2c59dc36767f1f0fd86255e8/setup.py","download_url":"https://raw.githubusercontent.com/astramind-ai/Mixture-of-depths/aff9e74fc9c5a30d2c59dc36767f1f0fd86255e8/setup.py"}]
```
</details>
</div>

<div className="tool-call-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github get_file_contents`

```json
{
	"owner": "kyegomez",
	 "repo": "Mixture-of-Depths",
	 "path": "/"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
[{"type":"file","name":".DS_Store","path":".DS_Store","size":6148,"sha":"5008ddfcf53c02e82d7eee2e57c38e5672ef89f6","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/.DS_Store?ref=597febe17e8942ec5c999dff5de680208fa55659","html_url":"https://github.com/kyegomez/Mixture-of-Depths/blob/597febe17e8942ec5c999dff5de680208fa55659/.DS_Store","download_url":"https://raw.githubusercontent.com/kyegomez/Mixture-of-Depths/597febe17e8942ec5c999dff5de680208fa55659/.DS_Store"},{"type":"dir","name":".github","path":".github","sha":"2ef6e78ea3e51ddafb2e577882a429a5c002bd90","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/.github?ref=597febe17e8942ec5c999dff5de680208fa55659","html_url":"https://github.com/kyegomez/Mixture-of-Depths/tree/597febe17e8942ec5c999dff5de680208fa55659/.github"},{"type":"file","name":".gitignore","path":".gitignore","size":3108,"sha":"36b36efb57965d11e32006f54bcaff44832fd65b","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/.gitignore?ref=597febe17e8942ec5c999dff5de680208fa55659","html_url":"https://github.com/kyegomez/Mixture-of-Depths/blob/597febe17e8942ec5c999dff5de680208fa55659/.gitignore","download_url":"https://raw.githubusercontent.com/kyegomez/Mixture-of-Depths/597febe17e8942ec5c999dff5de680208fa55659/.gitignore"},{"type":"file","name":".pre-commit-config.yaml","path":".pre-commit-config.yaml","size":466,"sha":"ae0a4fc0c2dd9408228bb959699e10b94dc39c1b","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/.pre-commit-config.yaml?ref=597febe17e8942ec5c999dff5de680208fa55659","html_url":"https://github.com/kyegomez/Mixture-of-Depths/blob/597febe17e8942ec5c999dff5de680208fa55659/.pre-commit-config.yaml","download_url":"https://raw.githubusercontent.com/kyegomez/Mixture-of-Depths/597febe17e8942ec5c999dff5de680208fa55659/.pre-commit-config.yaml"},{"type":"file","name":".readthedocs.yml","path":".readthedocs.yml","size":159,"sha":"fbdc74ec77285c26f01ba309c565c0018c36b1e7","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/.readthedocs.yml?ref=597febe17e8942ec5c999dff5de680208fa55659","html_url":"https://github.com/kyegomez/Mixture-of-Depths/blob/597febe17e8942ec5c999dff5de680208fa55659/.readthedocs.yml","download_url":"https://raw.githubusercontent.com/kyegomez/Mixture-of-Depths/597febe17e8942ec5c999dff5de680208fa55659/.readthedocs.yml"},{"type":"file","name":"LICENSE","path":"LICENSE","size":1074,"sha":"ca69c7eb4955d82bcc187d8b510746d618eb0471","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/LICENSE?ref=597febe17e8942ec5c999dff5de680208fa55659","html_url":"https://github.com/kyegomez/Mixture-of-Depths/blob/597febe17e8942ec5c999dff5de680208fa55659/LICENSE","download_url":"https://raw.githubusercontent.com/kyegomez/Mixture-of-Depths/597febe17e8942ec5c999dff5de680208fa55659/LICENSE"},{"type":"file","name":"README.md","path":"README.md","size":808,"sha":"2ddff931477c167010a58be340cb85f65acebafa","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/README.md?ref=597febe17e8942ec5c999dff5de680208fa55659","html_url":"https://github.com/kyegomez/Mixture-of-Depths/blob/597febe17e8942ec5c999dff5de680208fa55659/README.md","download_url":"https://raw.githubusercontent.com/kyegomez/Mixture-of-Depths/597febe17e8942ec5c999dff5de680208fa55659/README.md"},{"type":"file","name":"agorabanner.png","path":"agorabanner.png","size":198637,"sha":"030ad15560bb5b6154592b392fd594dceeccd4e7","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/agorabanner.png?ref=597febe17e8942ec5c999dff5de680208fa55659","html_url":"https://github.com/kyegomez/Mixture-of-Depths/blob/597febe17e8942ec5c999dff5de680208fa55659/agorabanner.png","download_url":"https://raw.githubusercontent.com/kyegomez/Mixture-of-Depths/597febe17e8942ec5c999dff5de680208fa55659/agorabanner.png"},{"type":"file","name":"example.py","path":"example.py","size":269,"sha":"84c0909bea1ba53d4f7a44a8e2382f7b8b1d4c49","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/example.py?ref=597febe17e8942ec5c999dff5de680208fa55659","html_url":"https://github.com/kyegomez/Mixture-of-Depths/blob/597febe17e8942ec5c999dff5de680208fa55659/example.py","download_url":"https://raw.githubusercontent.com/kyegomez/Mixture-of-Depths/597febe17e8942ec5c999dff5de680208fa55659/example.py"},{"type":"dir","name":"mixture_of_depths","path":"mixture_of_depths","sha":"7e8ac710a6b66fd26db01717d11ddfcebfae0bd4","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/mixture_of_depths?ref=597febe17e8942ec5c999dff5de680208fa55659","html_url":"https://github.com/kyegomez/Mixture-of-Depths/tree/597febe17e8942ec5c999dff5de680208fa55659/mixture_of_depths"},{"type":"file","name":"pyproject.toml","path":"pyproject.toml","size":1406,"sha":"fe792f05bf7b93739d9b4a6d67d9356a5ab64a69","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/pyproject.toml?ref=597febe17e8942ec5c999dff5de680208fa55659","html_url":"https://github.com/kyegomez/Mixture-of-Depths/blob/597febe17e8942ec5c999dff5de680208fa55659/pyproject.toml","download_url":"https://raw.githubusercontent.com/kyegomez/Mixture-of-Depths/597febe17e8942ec5c999dff5de680208fa55659/pyproject.toml"},{"type":"file","name":"requirements.txt","path":"requirements.txt","size":45,"sha":"a4f54ed2318c83ed474c011cf7306272bad6c8b0","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/requirements.txt?ref=597febe17e8942ec5c999dff5de680208fa55659","html_url":"https://github.com/kyegomez/Mixture-of-Depths/blob/597febe17e8942ec5c999dff5de680208fa55659/requirements.txt","download_url":"https://raw.githubusercontent.com/kyegomez/Mixture-of-Depths/597febe17e8942ec5c999dff5de680208fa55659/requirements.txt"},{"type":"dir","name":"scripts","path":"scripts","sha":"ce651d419f3a1eef38ad90ae9f4d1981b9a6edd6","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/scripts?ref=597febe17e8942ec5c999dff5de680208fa55659","html_url":"https://github.com/kyegomez/Mixture-of-Depths/tree/597febe17e8942ec5c999dff5de680208fa55659/scripts"}]
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Let me read the README files to understand the quality and completeness of these implementations:
</div>

<div className="tool-call-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github get_file_contents`

```json
{
	"owner": "astramind-ai",
	 "repo": "Mixture-of-depths",
	 "path": "README.md"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
{"content":"# Unofficial implementation for the paper \"Mixture-of-Depths\"\n\n\n## Introduction\nThis is an unofficial implementation for the paper [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/abs/2404.02258)\n\n## Currently supported models\n\n| Model  | Supported? |\n| ------------- | ------------- |\n| Mistral  |  ✅ |\n| Mixtral  |  ✅ |\n| LLama  |  ✅ |\n| LLama2  |  ✅ |\n| LLama3  |  ✅ |\n| Gemma  |  ✅ |\n| BLOOMZ  |  ✅ |\n| BLOOM  |  ✅ |\n| DeepSeek  |  ✅ |\n| Phi (1.5 \u0026 2)  |  ✅ |\n| Qwen2  |  ✅ |\n| StarCoder2  |  ✅ |\n| Qwen2-MoE  |  ❓ |\n| Solar  |  ❓ |\n| Baichuan  |  ❌ |\n| ChatGLM3  |  ❌ |\n| InternLM  |  ❌ |\n| Olmo  |  ❌ |\n| XVERSE  |  ❌ |\n| Yi  |  ❌ |\n| Yuan  |  ❌ |\n\n## 💾 Installation\nbash\npip install mixture-of-depth\n\nBoth **Linux**, **Windows** and **MacOS** are supported.\n## 🏁 Quick Start\n\n### High-level API (tranformers-compatible)\npython\nfrom transformers import AutoModelForCausalLM\nfrom MoD import apply_mod_to_hf\n\n# Initialize your model from an available hf model\nmodel= AutoModelForCausalLM.from_pretrained(\"some-repo/some-model\")\n# Convert the model to include the mixture of depths layers\nmodel = apply_mod_to_hf(model)\n# train the model\n# ...\n# save the model\nmodel.save_pretrained('some_local_directory')\n\n### Loading the converted Model\nTo utilize the converted model, you will need to load the model from the AutoClass. Below is an example demonstrating how to load the model from a local directory:\npython\nfrom MoD import AutoMoDModelForCausalLM\n\n# Replace 'path_to_your_model' with the actual path to your model's directory\nmodel = AutoMoDModelForCausalLM.from_pretrained('path_to_your_model')\n\n### Using generate()\nBefore calling the hf generate() method please explicitly use eval() on the model\n\n## 🫱🏼‍🫲🏽 Contributing\nWe welcome contributions from the community, whether it's adding new features, improving documentation, or reporting bugs. Please refer to our contribution guidelines before making a pull request.\n\n## 📜 License\nThis repo is open-sourced under the Apache-2.0 license.\n\n## Citation\nIf you use our code in your research, please cite it using the following Bibtex entry:\n\nbibtex\n@article{MoD2024,\n  title={Unofficial implementation for the paper \"Mixture-of-Depths\"},\n  author={AstraMind AI},\n  journal={https://github.com/astramind-ai/Mixture-of-depths},\n  year={2024}\n}\n\n## Support\nFor questions, issues, or support, please open an issue on our GitHub repository.\n","download_url":"https://raw.githubusercontent.com/astramind-ai/Mixture-of-depths/main/README.md","encoding":"text","git_url":"https://api.github.com/repos/astramind-ai/Mixture-of-depths/git/blobs/3b64b82fd62acd1dc75a368ccc2d87f640ad0b58","html_url":"https://github.com/astramind-ai/Mixture-of-depths/blob/main/README.md","name":"README.md","path":"README.md","sha":"3b64b82fd62acd1dc75a368ccc2d87f640ad0b58","size":2539,"type":"file","url":"https://api.github.com/repos/astramind-ai/Mixture-of-depths/contents/README.md?ref=main"}
```
</details>
</div>

<div className="tool-call-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github get_file_contents`

```json
{
	"owner": "kyegomez",
	 "repo": "Mixture-of-Depths",
	 "path": "README.md"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
{"content":"[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)\n\n## Mixture of Depths Scaling\nImplementation of the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\". From the paper: \"These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50% faster to step during post-training sampling.\"\n\n\n## install \n`pip3 install mixture-of-depths`\n\n## usage\npython\nimport torch\nfrom mixture_of_depths.main import MoD\n\nx = torch.randn(1, 1000, 512)\n# mask = torch.ones(1)\n\n# Model\nmodel = MoD(\n    seq_len=1000,\n    dim=512,\n    capacity_factor=0.12,\n    vocab_size=10000,\n    transformer_depth=8,\n)\n\n# Model\nout = model(x)\nprint(out)\n\n\n# License\nMIT\n","download_url":"https://raw.githubusercontent.com/kyegomez/Mixture-of-Depths/main/README.md","encoding":"text","git_url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/git/blobs/2ddff931477c167010a58be340cb85f65acebafa","html_url":"https://github.com/kyegomez/Mixture-of-Depths/blob/main/README.md","name":"README.md","path":"README.md","sha":"2ddff931477c167010a58be340cb85f65acebafa","size":808,"type":"file","url":"https://api.github.com/repos/kyegomez/Mixture-of-Depths/contents/README.md?ref=main"}
```
</details>
</div>

<div className="tool-call-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github get_file_contents`

```json
{
	"owner": "sramshetty",
	 "repo": "mixture-of-depths",
	 "path": "README.md"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
{"content":"# Mixture of Depths\nAn unofficial implementation of [\"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"](https://arxiv.org/abs/2404.02258)\n\n\n## Setup\n- First follow instructions for setting up your environment for Llama 2 [here](https://github.com/meta-llama/llama).\n- Then:\nbash\npip install einops\n\n\n\n## Details\n- Implementing MoD in Llama 2\n- Follow paper's configuration with some assumptions.\n    - Route every other layer\n    - Training configurations for both causal inference methods proposed\n- Notes on auxiliary router for causal inference:\n    - Currently, we train it separately after MoD Llama is trained.\n    - Simple task as we achieve high token prediction accuracy quickly, which is further simplified by using a simple dataset.\n\n- `MoD_training.ipynb` demonstrates training and was used for the results below.\n- `MoD_sampling.ipynb` demonstrates generation with each method.\n\n## Results\n- 50 million parameter model\n    - C4\n        - Baseline after 1 epoch:\n            - Loss: 3.73\n            - Samples/sec: 6.79\n        - MoD w/ Auxiliary Loss after 1 epoch:\n            - Loss: 3.81\n            - Samples/sec: 8.15\n        - MoD w/ Auxiliary Router after 1 epoch:\n            - Loss: 4.19\n            - Samples/sec: 7.64\n    - Tiny Stories\n        - Baseline after 5 epochs:\n            - Loss: 2.46\n            - Samples/sec: 11.22\n        - MoD w/ Auxiliary Loss after 5 epochs:\n            - Loss: 2.55\n            - Samples/sec: 11.33\n        - MoD w/ Auxiliary Router after 5 epochs:\n            - Loss: 2.48\n            - Auxilairy Router Causal Loss: 0.15 \n            - Samples/sec: 11.54\n\n## TODO\n- [x] Validate\n- [x] Sampling methods\n    - [x] Auxiliary loss\n    - [x] \"Second\" router\n\n\n## Citations\nbibtex\n@misc{raposo2024mixtureofdepths,\n    title={Mixture-of-Depths: Dynamically allocating compute in transformer-based language models}, \n    author={David Raposo and Sam Ritter and Blake Richards and Timothy Lillicrap and Peter Conway Humphreys and Adam Santoro},\n    year={2024},\n    eprint={2404.02258},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n\n","download_url":"https://raw.githubusercontent.com/sramshetty/mixture-of-depths/main/README.md","encoding":"text","git_url":"https://api.github.com/repos/sramshetty/mixture-of-depths/git/blobs/a651911f3a4287a1b84c1627805645f5d92a7012","html_url":"https://github.com/sramshetty/mixture-of-depths/blob/main/README.md","name":"README.md","path":"README.md","sha":"a651911f3a4287a1b84c1627805645f5d92a7012","size":2147,"type":"file","url":"https://api.github.com/repos/sramshetty/mixture-of-depths/contents/README.md?ref=main"}
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me check if there's a result.json file in the workspace to understand the expected format:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem list_directory`

```json
{
	"path": "/workspace/dumps/workspace"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
[DIR] .pdf_tools_tempfiles
[FILE] paper.pdf
[FILE] result.json
```
</details>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

```json
{
	"path": "/workspace/dumps/workspace/result.json"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
{
    "URL": ""
}
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Perfect! Now I have all the information I need. Based on my analysis of the GitHub search results, I can identify the repository with the most stars that implements the "Mixture-of-Depths" paper.

From the search results, I found several unofficial implementations:

1. **astramind-ai/Mixture-of-depths** - 171 stars (highest)
2. **kyegomez/Mixture-of-Depths** - 106 stars  
3. **sramshetty/mixture-of-depths** - 36 stars
4. **CASE-Lab-UMD/Router-Tuning-Mixture-of-Depths** - 16 stars
5. **dtunai/Mixture-of-Depths-Jax** - 3 stars
6. **eclouder/MoE-LLM** - 2 stars

The repository with the most stars is **astramind-ai/Mixture-of-depths** with 171 stars. This implementation also appears to be quite comprehensive, supporting multiple model architectures (Mistral, Mixtral, LLama, LLama2, LLama3, Gemma, BLOOMZ, BLOOM, DeepSeek, Phi, Qwen2, StarCoder2) and provides a transformers-compatible high-level API.

Let me update the result.json file:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem-write_file`

```text workspace/result.json
{
    "URL": "https://github.com/astramind-ai/Mixture-of-depths"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
Successfully wrote to /workspace/dumps/workspace/result.json
```
</details>
</div>

<div className="tool-call-box">
<svg width="14px" height="14px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M861.588238 240.133873v-65.792823c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.631049H314.659414v-65.631049c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.792823c-36.317212 0.868255-65.631049 30.539428-65.63105 67.061417v543.745565c0 37.06772 30.155471 67.223191 67.223191 67.223191h696.886045c37.06772 0 67.223191-30.155471 67.223191-67.223191V307.19529c-0.001024-36.52199-29.315885-66.193162-65.633097-67.061417z m-109.385765-65.792823c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m-546.928824 0c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m678.191947 676.600829c0 12.935767-10.532708 23.468476-23.468476 23.468475H163.111076c-12.935767 0-23.468476-10.532708-23.468476-23.468475V307.19529c0-12.402323 9.677764-22.593054 21.877358-23.415233v65.577807c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.631049h393.789368v65.631049c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.577807c12.19857 0.82218 21.877358 11.012911 21.877358 23.415233v543.746589z" fill="#22C67F" /><path d="M706.719439 478.272194l-48.01715-44.741741-182.28128 195.621482-111.468348-122.615387-48.563905 44.148911 159.469116 172.685427z" fill="#74E8AE" /></svg> `claim_done `

```json
{}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
you have claimed the task is done!
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

I have successfully completed the task! Here's what I found:

**Paper Analysis:**
- The paper is "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models" by David Raposo et al. from Google DeepMind
- It introduces a method to dynamically allocate compute in transformers by using a top-k routing mechanism to select which tokens participate in self-attention and MLP computations at each layer

**GitHub Search Results:**
I searched GitHub comprehensively and found several unofficial implementations. The repository with the most stars is:

**astramind-ai/Mixture-of-depths** with **171 stars**

This implementation stands out because it:
- Has the highest star count (171 stars)
- Supports multiple popular model architectures (Mistral, Mixtral, LLama variants, Gemma, BLOOMZ, BLOOM, DeepSeek, Phi, Qwen2, StarCoder2)
- Provides a transformers-compatible high-level API
- Is actively maintained (last updated September 2025)
- Has proper documentation and installation instructions

I have updated the `result.json` file with the URL of this repository: `https://github.com/astramind-ai/Mixture-of-depths`
</div>

</Accordion>

<Accordion title="gemini-2.5-pro">

<Columns cols={3}>
<Card title="Task Completion" icon="x">
Failed
</Card>
<Card title="Tool Calls" icon="wrench">
1
</Card>
<Card title="Turns" icon="arrows-rotate">
2
</Card>
</Columns>

<div className="tool-call-box">
🛠 `read_pdf_pages `

```json
{
	"end_page":1,
	"pdf_file_path":"paper.pdf"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
Click to expand result
</summary>
```json
Tool read_pdf_pages not found in agent Assistant
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

I am sorry, but I was unable to read the content of the `paper.pdf` file. To find the unofficial implementation with the most stars on GitHub, I need the title of the paper. Please provide me with the title of the paper.
</div>

</Accordion>

</AccordionGroup>
