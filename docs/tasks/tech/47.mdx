---
title: "Git Repo"
description: "Summarize the ImageNet 256 experimental results from my image generation papers into a LaTeX table."
mode: wide
---

<Card>
<div className="tools-container">
<div className="mcp-servers-container">
<div className="mcp-servers-title">
MCP Servers
</div>
<div className="mcp-servers-grid">
<div className="mcp-server-item">
<svg width="20" height="20" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
<span className="mcp-server-name">github</span>
</div>
<div className="mcp-server-item">
<svg width="20px" height="20px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg>
<span className="mcp-server-name">pdf-tools</span>
</div>
<div className="mcp-server-item">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="20px" height="20px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg>
<span className="mcp-server-name">filesystem</span>
</div>
</div>
</div>
<div className="local-tools-container">
<div className="mcp-servers-title">
Local Tools
</div>
<div className="local-tools-grid">
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 100 100" enable-background="new 0 0 100 100" id="Layer_1" version="1.1" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><g><rect clip-rule="evenodd" fill="#F2F2F2" fill-rule="evenodd" height="83.437" width="67.025" x="9.012" y="7.604"/><path d="M77.454,92.458H7.595V6.187h69.859V92.458z M10.429,89.624H74.62V9.021H10.429V89.624z"/></g><g><rect clip-rule="evenodd" fill="#FF7C24" fill-rule="evenodd" height="10.481" width="17.952" x="46.695" y="34.866"/></g><g><rect height="2.834" width="19.463" x="20.504" y="35.575"/></g><g><rect height="2.834" width="15.561" x="20.718" y="42.508"/></g><g><rect height="2.833" width="15.562" x="20.813" y="49.514"/></g><g><rect height="2.833" width="27.128" x="20.718" y="56.753"/></g><g><rect height="2.833" width="23.51" x="20.718" y="63.688"/></g><g><rect clip-rule="evenodd" fill="#39B6CC" fill-rule="evenodd" height="9.004" width="44.095" x="20.553" y="19.064"/></g><g><rect height="2.833" width="26.272" x="20.718" y="70.32"/></g><g><rect height="2.834" width="32.8" x="20.718" y="77.253"/></g><g><rect height="2.834" width="3.235" x="38.304" y="42.508"/></g><g><path clip-rule="evenodd" d="M77.931,71.902l4.287,4.427l-6.644,6.437l-4.309-4.457    C74.147,76.998,76.504,74.726,77.931,71.902L77.931,71.902z" fill="#F2F2F2" fill-rule="evenodd"/><path d="M75.542,84.77l-6.692-6.92l1.828-0.831c2.579-1.174,4.706-3.218,5.989-5.756l0.897-1.776l6.656,6.874L75.542,84.77z     M73.584,78.669l2.023,2.091l4.605-4.463l-2.007-2.074C76.994,76.012,75.414,77.531,73.584,78.669z"/></g><g><polygon clip-rule="evenodd" fill="#39B6CC" fill-rule="evenodd" points="83.267,75.319 91.984,84.338 83.247,92.779     74.535,83.761   "/><path d="M83.213,94.783L72.531,83.726l10.771-10.41l10.687,11.056L83.213,94.783z M76.538,83.794l6.744,6.981l6.698-6.472    l-6.748-6.981L76.538,83.794z"/></g><g><path clip-rule="evenodd" d="M66.124,50.799c7.742,0,14.018,6.276,14.018,14.019    s-6.275,14.019-14.018,14.019c-7.743,0-14.019-6.276-14.019-14.019S58.381,50.799,66.124,50.799L66.124,50.799z" fill="#F2F2F2" fill-rule="evenodd"/><path d="M66.124,80.253c-8.511,0-15.435-6.924-15.435-15.435s6.924-15.435,15.435-15.435S81.56,56.307,81.56,64.818    S74.635,80.253,66.124,80.253z M66.124,52.216c-6.949,0-12.601,5.653-12.601,12.602s5.651,12.601,12.601,12.601    c6.948,0,12.602-5.652,12.602-12.601S73.072,52.216,66.124,52.216z"/></g><g><rect height="2.833" width="10.313" x="39.902" y="49.514"/></g><g><path d="M76.404,65.586H73.57c0-0.636-0.068-1.255-0.205-1.84c-0.043-0.186-0.096-0.385-0.169-0.63l2.717-0.808    c0.091,0.304,0.158,0.559,0.215,0.801C76.31,63.901,76.404,64.735,76.404,65.586z M72.438,61.433    c-1.489-2.5-4.203-4.058-7.084-4.061l0.004-2.834c3.871,0.005,7.518,2.091,9.516,5.445L72.438,61.433z"/></g></g></svg>
<span className="local-tool-name">history</span>
</div>
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M861.588238 240.133873v-65.792823c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.631049H314.659414v-65.631049c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.792823c-36.317212 0.868255-65.631049 30.539428-65.63105 67.061417v543.745565c0 37.06772 30.155471 67.223191 67.223191 67.223191h696.886045c37.06772 0 67.223191-30.155471 67.223191-67.223191V307.19529c-0.001024-36.52199-29.315885-66.193162-65.633097-67.061417z m-109.385765-65.792823c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m-546.928824 0c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m678.191947 676.600829c0 12.935767-10.532708 23.468476-23.468476 23.468475H163.111076c-12.935767 0-23.468476-10.532708-23.468476-23.468475V307.19529c0-12.402323 9.677764-22.593054 21.877358-23.415233v65.577807c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.631049h393.789368v65.631049c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.577807c12.19857 0.82218 21.877358 11.012911 21.877358 23.415233v543.746589z" fill="#22C67F" /><path d="M706.719439 478.272194l-48.01715-44.741741-182.28128 195.621482-111.468348-122.615387-48.563905 44.148911 159.469116 172.685427z" fill="#74E8AE" /></svg>
<span className="local-tool-name">claim_done</span>
</div>
<div className="local-tool-item">
<Icon icon="bars-progress" size={20} color="#4286f6" />
<span className="local-tool-name">manage_context</span>
</div>
<div className="local-tool-item">
<Icon icon="filter-list" size={20} color="#4286f6" />
<span className="local-tool-name">handle_overlong_tool_outputs</span>
</div>
</div>
</div>
</div>
</Card>









## Instruction

I'm trying to reproduce the experiments from a deep learning paper (paper.pdf). Unfortunately, the paper does not provide an official implementation code repository. To work more efficiently, please help me find the unofficial implementation repository with the most stars on GitHub and update the existing `result.json` with the information.

## Initial State

### Local Workspace

<div className="file-tree">
<a href="https://github.com/hkust-nlp/mcpbench_dev/tree/finalpool_dev/tasks/finalpool/git-repo/initial_workspace">workspace</a>/
‚îî‚îÄ‚îÄ result.json
</div>

## Model Trajectory

<AccordionGroup>
<Accordion title="claude-4.5-sonnet">

<Columns cols={3}>
<Card title="Task Completion" icon="check">
Completed
</Card>
<Card title="Tool Calls" icon="wrench">
5
</Card>
<Card title="Turns" icon="arrows-rotate">
6
</Card>
</Columns>

<div className="thinking-box">
üßê`Agent`

I'll help you find the unofficial implementation repository with the most stars on GitHub for the paper. Let me start by reading the paper to understand what we're looking for.
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools get_pdf_info`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/paper.pdf"
}

```

```json output_result
PDF file information:
Total pages: 14
Title: 
Author: 
Creator: LaTeX with hyperref
Creation date: Unknown

```

</Expandable>
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/paper.pdf",
	 "start_page": 1,
	 "end_page": 2
}

```

```json output_result
PDF total pages: 14
Extracted page range: 1-2

=== Page 1 ===
Mixture-of-Depths: Dynamically allocating
compute in transformer-based language
models
David Raposo1*, Sam Ritter1, Blake Richards1,2, Timothy Lillicrap1, Peter Conway Humphreys1and Adam
Santoro1*
1Google DeepMind,2McGill University & Mila,*Equal Contribution
Transformer-based language models spread FLOPs uniformly across input sequences. In this work we
demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute ) to specific
positions in a sequence, optimising the allocation along the sequence for different layers across the
model depth. Our method enforces a total compute budget by capping the number of tokens ( ùëò) that
can participate in the self-attention and MLP computations at a given layer. The tokens to be processed
are determined by the network using a top- ùëòrouting mechanism. Since ùëòis defined a priori, this
simple procedure uses a static computation graph with known tensor sizes, unlike other conditional
computation techniques. Nevertheless, since the identities of the ùëòtokens are fluid, this method can
expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure
is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do
models trained in this way learn to dynamically allocate compute, they do so efficiently. These models
match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of
the FLOPs per forward pass, and can be upwards of 50% faster to step during post-training sampling.
1. Introduction
Notallproblemsrequirethesameamountoftimeorefforttosolve. Analogously, inlanguagemodeling
not all tokens and sequences require the same time or effort to accurately make a prediction. And
yet, transformer models expend the same amount of compute per token in a forward pass. Ideally,
transformers would use smaller total compute budgets by not spending compute unnecessarily.
Conditional computation is a technique that tries to reduce total compute by expending it only
when needed (Bengio et al., 2016; Bengio, 2013; Bengio et al., 2013). Various algorithms offer
solutions to when and how much compute should be used (Ainslie et al., 2023; Bapna et al., 2020;
Fedus et al., 2022). However, general formulations of this challenging problem may not work well with
existing hardware constraints since they tend to introduce dynamic computation graphs (Dehghani
et al., 2018; Graves, 2016). The most promising conditional computation methods may instead be
those that are harmonious with our current hardware stack, which prioritizes static computation
graphs, and known tensor sizes that are selected to maximize hardware utilization.
Here we consider the problem of language modeling using a staticcompute budget that can be
made less than that used by a vanilla transformer. The network must learn how to dynamically allocate
the available compute by making decisions per-token, in each layer, about where to spend compute
from the available budget. In our implementation total compute is user defined and unchanging
prior to training, rather than being a function of the network‚Äôs on-the-fly decisions. Thus, hardware
efficiency gains‚Äîsuch as reduced memory footprint, or reduced FLOPs per forward pass‚Äîcan be
anticipated and exploited ahead of time. As we will show, these gains can be had without sacrificing
overall performance.
Corresponding author(s): draposo@google.com, adamsantoro@google.com
¬©2024 Google DeepMind. All rights reservedarXiv:2404.02258v1  [cs.LG]  2 Apr 2024

=== Page 2 ===
Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
We leverage an approach akin to Mixture of Experts (MoE) transformers, in which dynamic
token-level routing decisions are made across the network depth. Departing from MoE, we choose
to either apply a computation to a token (as would be the case for a standard transformer), or pass
it through a residual connection (remaining unchanged and saving compute). Also in contrast to
MoE, we apply this routing to both forward MLPs and multi-head attention. Since this therefore
also impacts the keys and queries we process, the routing makes decisions not only about which
tokens to update, but also which tokens are made available to attend to. We refer to this strategy
as Mixture-of-Depths (MoD) to emphasize how individual tokens pass through different numbers of
layers, or blocks, through the depth of the transformer (see figure 1).
The MoD technique also allows one to trade-off performance with speed. On the one hand, one
can train an MoD transformer that improves upon vanilla transformers by as much as 1.5%on the
final log probability training objective for equivalent training FLOPs (isoFLOP), and while taking an
equivalent amount of wall-clock time to train. On the other hand, one can train an MoD transformer
that achieves training loss parity with an isoFLOP optimal vanilla transformer, but which uses a
fraction of the FLOPs (upwards of 50%) per forward pass, and hence is faster to step. Together, these
results imply that MoD transformers learn to route intelligently (i.e., skipping computations that are
unnecessary) since they can achieve equal or better log probabilities per sequence despite a smaller
FLOP footprint per forward pass.
2. Background
The transformer architecture has become the workhorse of a revolution in practical artificial in-
telligence, bringing unprecedented capabilities at the cost of expensive training runs and serving
procedures. This has spurred tremendous interest in making transformer architectures more effi-
cient (Gupta and Agrawal, 2021; Tay et al., 2020). One of the promising approaches is conditional
computation , whereby learned mechanisms determine when and how to expend computation. This
terminology was introduced by Bengio (2013), and the concept was explored further over the next
several years (Bengio et al., 2016, 2013; Cho and Bengio, 2014; Graves, 2016; Jernite et al., 2017;
Wang et al., 2017).
A wide variety of recent work has developed conditional computation methods for transformers.
Some of this work focuses on "early exiting", that is, learning to decide when to end computation on
a given token, allowing the token to skip any remaining transformer layers after the exit decision
is made (Elbayad et al., 2019; Liu et al., 2021; Schuster et al., 2022). In MoD, unlike in early-exit
methods, a token can skip middle layers, then be updated via self-attention with tokens that that
have gone through all the middle layers. We speculate that this might be a useful property.
Other work has developed methods for iterating transformer layers with shared weights for
an adaptive number of steps (Dehghani et al., 2018; Simoulin and Crabb√©, 2021). Bolya et al.
(2023) developed a method for choosing tokens to merge when running inference on a trained vision
transformer which notably requires no learning. Lei et al. (2023) make use of conditional computation
in a fine tuning setting by building on adapter approaches (He et al., 2021) to learn to skip blocks of
frozen pre-trained weights in favor of running only a small fine-tuned adapter.
CoLT5 (Ainslie et al., 2023) uses conditional routing to select whether a given token will pass
through a heavy or light pathway for each feedforward layer. Further, they use the same routing
mechanism to select whether a token will attend to all other tokens or to a select few, as in Guo et al.
(2022). Like MoD, CoLT5 uses soft top-k for making routing decisions. However, CoLT5 focuses on
a encoder-decoder setting, and thus does need to contend with the problem of efficient sequential
decoding given the non-causal nature of the top-k operation. In contrast, our current work with
2
```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

Great! This is the "Mixture-of-Depths" paper by Raposo et al. from Google DeepMind (arXiv:2404.02258). Now let me search GitHub for unofficial implementations of this paper.
</div>

<div className="result-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github search_repositories`

<Expandable title="Details">
```json arguments
{
	"query": "mixture of depths transformer"
}

```

```json output_result
{"total_count":7,"incomplete_results":false,"items":[{"id":785664688,"name":"Mixture-of-depths","full_name":"astramind-ai/Mixture-of-depths","description":"Unofficial implementation for the paper \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/astramind-ai/Mixture-of-depths","language":"Python","stargazers_count":173,"forks_count":8,"open_issues_count":5,"updated_at":"2025-10-11T08:37:33Z","created_at":"2024-04-12T11:07:48Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":782269816,"name":"Mixture-of-Depths","full_name":"kyegomez/Mixture-of-Depths","description":"Implementation of the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/kyegomez/Mixture-of-Depths","language":"Python","stargazers_count":108,"forks_count":7,"open_issues_count":8,"updated_at":"2025-10-17T06:11:07Z","created_at":"2024-04-05T00:51:20Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":782847289,"name":"mixture-of-depths","full_name":"sramshetty/mixture-of-depths","description":"An unofficial implementation of \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/sramshetty/mixture-of-depths","language":"Python","stargazers_count":36,"forks_count":3,"open_issues_count":0,"updated_at":"2025-09-05T01:19:59Z","created_at":"2024-04-06T07:51:03Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":874565778,"name":"Router-Tuning-Mixture-of-Depths","full_name":"CASE-Lab-UMD/Router-Tuning-Mixture-of-Depths","description":"The open-source Mixture of Depths code and the official implementation of the paper \"Router-Tuning: A Simple and Effective Approach for Enabling Dynamic Depth in Transformers. (EMNLP 2025)\"","html_url":"https://github.com/CASE-Lab-UMD/Router-Tuning-Mixture-of-Depths","language":"Python","stargazers_count":16,"forks_count":3,"open_issues_count":0,"updated_at":"2025-10-01T23:31:40Z","created_at":"2024-10-18T04:11:52Z","topics":["deep-learning","large-language-models","machine-learning","mixture-of-depths","neural-network","parameter-efficient-fine-tuning"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":784596511,"name":"Mixture-of-Depths-Jax","full_name":"dtunai/Mixture-of-Depths-Jax","description":"Jax module for the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/dtunai/Mixture-of-Depths-Jax","language":"Python","stargazers_count":3,"forks_count":0,"open_issues_count":13,"updated_at":"2024-07-18T20:50:24Z","created_at":"2024-04-10T06:56:02Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":783121754,"name":"MoE-LLM","full_name":"eclouder/MoE-LLM","description":"Implementation of the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/eclouder/MoE-LLM","language":"Python","stargazers_count":2,"forks_count":0,"open_issues_count":0,"updated_at":"2024-04-22T08:48:24Z","created_at":"2024-04-07T02:04:36Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":809894170,"name":"MH-MoDE","full_name":"dav3794/MH-MoDE","description":"Implementation of Multi-Head Mixture-of-Depths-and-Experts Transformer for NLP course.","html_url":"https://github.com/dav3794/MH-MoDE","language":"Jupyter Notebook","stargazers_count":0,"forks_count":0,"open_issues_count":0,"updated_at":"2024-06-18T11:31:36Z","created_at":"2024-06-03T16:48:16Z","private":false,"fork":false,"archived":false,"default_branch":"main"}]}
```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

Perfect! I found several unofficial implementations. The repository with the most stars is **astramind-ai/Mixture-of-depths** with 173 stars. Now let me read the existing result.json file and update it with this information.
</div>

<div className="result-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

<Expandable title="Details">
```json arguments
{
	"path": "/workspace/dumps/workspace/result.json"
}

```

```json output_result
{
    "URL": ""
}
```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

Now I'll update the result.json file with the information about the most starred unofficial implementation repository.
</div>

<div className="result-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

<Expandable title="Details">
```json arguments
workspace/result.json
{
    "URL": "https://github.com/astramind-ai/Mixture-of-depths",
    "repository_name": "astramind-ai/Mixture-of-depths",
    "stars": 173,
    "description": "Unofficial implementation for the paper \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"",
    "language": "Python",
    "paper_title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models",
    "paper_arxiv": "arXiv:2404.02258v1"
}

```

```json output_result
Successfully wrote to /workspace/dumps/workspace/result.json
```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

Perfect! I've successfully completed the task. Here's what I found:

**Paper**: "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models" by Raposo et al. from Google DeepMind (arXiv:2404.02258v1)

**Most Starred Unofficial Implementation**:
- **Repository**: astramind-ai/Mixture-of-depths
- **URL**: https://github.com/astramind-ai/Mixture-of-depths
- **Stars**: 173 ‚≠ê
- **Language**: Python
- **Description**: Unofficial implementation for the paper "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"

I've updated the `result.json` file with this information, including the repository URL, name, star count, description, programming language, and paper details. This repository has significantly more stars than the other unofficial implementations (the second most starred has 108 stars).
</div>

</Accordion>

<Accordion title="deepseek-v3.2">

<Columns cols={3}>
<Card title="Task Completion" icon="check">
Completed
</Card>
<Card title="Tool Calls" icon="wrench">
9
</Card>
<Card title="Turns" icon="arrows-rotate">
9
</Card>
</Columns>

<div className="thinking-box">
üßê`Agent`

I'll help you find the unofficial implementation repository for the deep learning paper. First, let me check if there's a `paper.pdf` file in your workspace and then search for implementations on GitHub.
</div>

<div className="result-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem list_directory`

<Expandable title="Details">
```json arguments
{
	"path": "/workspace/dumps/workspace"
}

```

```json output_result
[DIR] .pdf_tools_tempfiles
[FILE] paper.pdf
[FILE] result.json
```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

Great! I can see both `paper.pdf` and `result.json` files. Let me first read the paper to understand what it's about, then search for implementations on GitHub.
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools get_pdf_info`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/paper.pdf"
}

```

```json output_result
PDF file information:
Total pages: 14
Title: 
Author: 
Creator: LaTeX with hyperref
Creation date: Unknown

```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

Now let me read the content of the paper to understand what it's about:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/paper.pdf",
	 "start_page": 1,
	 "end_page": 3
}

```

```json output_result
PDF total pages: 14
Extracted page range: 1-3

=== Page 1 ===
Mixture-of-Depths: Dynamically allocating
compute in transformer-based language
models
David Raposo1*, Sam Ritter1, Blake Richards1,2, Timothy Lillicrap1, Peter Conway Humphreys1and Adam
Santoro1*
1Google DeepMind,2McGill University & Mila,*Equal Contribution
Transformer-based language models spread FLOPs uniformly across input sequences. In this work we
demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute ) to specific
positions in a sequence, optimising the allocation along the sequence for different layers across the
model depth. Our method enforces a total compute budget by capping the number of tokens ( ùëò) that
can participate in the self-attention and MLP computations at a given layer. The tokens to be processed
are determined by the network using a top- ùëòrouting mechanism. Since ùëòis defined a priori, this
simple procedure uses a static computation graph with known tensor sizes, unlike other conditional
computation techniques. Nevertheless, since the identities of the ùëòtokens are fluid, this method can
expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure
is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do
models trained in this way learn to dynamically allocate compute, they do so efficiently. These models
match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of
the FLOPs per forward pass, and can be upwards of 50% faster to step during post-training sampling.
1. Introduction
Notallproblemsrequirethesameamountoftimeorefforttosolve. Analogously, inlanguagemodeling
not all tokens and sequences require the same time or effort to accurately make a prediction. And
yet, transformer models expend the same amount of compute per token in a forward pass. Ideally,
transformers would use smaller total compute budgets by not spending compute unnecessarily.
Conditional computation is a technique that tries to reduce total compute by expending it only
when needed (Bengio et al., 2016; Bengio, 2013; Bengio et al., 2013). Various algorithms offer
solutions to when and how much compute should be used (Ainslie et al., 2023; Bapna et al., 2020;
Fedus et al., 2022). However, general formulations of this challenging problem may not work well with
existing hardware constraints since they tend to introduce dynamic computation graphs (Dehghani
et al., 2018; Graves, 2016). The most promising conditional computation methods may instead be
those that are harmonious with our current hardware stack, which prioritizes static computation
graphs, and known tensor sizes that are selected to maximize hardware utilization.
Here we consider the problem of language modeling using a staticcompute budget that can be
made less than that used by a vanilla transformer. The network must learn how to dynamically allocate
the available compute by making decisions per-token, in each layer, about where to spend compute
from the available budget. In our implementation total compute is user defined and unchanging
prior to training, rather than being a function of the network‚Äôs on-the-fly decisions. Thus, hardware
efficiency gains‚Äîsuch as reduced memory footprint, or reduced FLOPs per forward pass‚Äîcan be
anticipated and exploited ahead of time. As we will show, these gains can be had without sacrificing
overall performance.
Corresponding author(s): draposo@google.com, adamsantoro@google.com
¬©2024 Google DeepMind. All rights reservedarXiv:2404.02258v1  [cs.LG]  2 Apr 2024

=== Page 2 ===
Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
We leverage an approach akin to Mixture of Experts (MoE) transformers, in which dynamic
token-level routing decisions are made across the network depth. Departing from MoE, we choose
to either apply a computation to a token (as would be the case for a standard transformer), or pass
it through a residual connection (remaining unchanged and saving compute). Also in contrast to
MoE, we apply this routing to both forward MLPs and multi-head attention. Since this therefore
also impacts the keys and queries we process, the routing makes decisions not only about which
tokens to update, but also which tokens are made available to attend to. We refer to this strategy
as Mixture-of-Depths (MoD) to emphasize how individual tokens pass through different numbers of
layers, or blocks, through the depth of the transformer (see figure 1).
The MoD technique also allows one to trade-off performance with speed. On the one hand, one
can train an MoD transformer that improves upon vanilla transformers by as much as 1.5%on the
final log probability training objective for equivalent training FLOPs (isoFLOP), and while taking an
equivalent amount of wall-clock time to train. On the other hand, one can train an MoD transformer
that achieves training loss parity with an isoFLOP optimal vanilla transformer, but which uses a
fraction of the FLOPs (upwards of 50%) per forward pass, and hence is faster to step. Together, these
results imply that MoD transformers learn to route intelligently (i.e., skipping computations that are
unnecessary) since they can achieve equal or better log probabilities per sequence despite a smaller
FLOP footprint per forward pass.
2. Background
The transformer architecture has become the workhorse of a revolution in practical artificial in-
telligence, bringing unprecedented capabilities at the cost of expensive training runs and serving
procedures. This has spurred tremendous interest in making transformer architectures more effi-
cient (Gupta and Agrawal, 2021; Tay et al., 2020). One of the promising approaches is conditional
computation , whereby learned mechanisms determine when and how to expend computation. This
terminology was introduced by Bengio (2013), and the concept was explored further over the next
several years (Bengio et al., 2016, 2013; Cho and Bengio, 2014; Graves, 2016; Jernite et al., 2017;
Wang et al., 2017).
A wide variety of recent work has developed conditional computation methods for transformers.
Some of this work focuses on "early exiting", that is, learning to decide when to end computation on
a given token, allowing the token to skip any remaining transformer layers after the exit decision
is made (Elbayad et al., 2019; Liu et al., 2021; Schuster et al., 2022). In MoD, unlike in early-exit
methods, a token can skip middle layers, then be updated via self-attention with tokens that that
have gone through all the middle layers. We speculate that this might be a useful property.
Other work has developed methods for iterating transformer layers with shared weights for
an adaptive number of steps (Dehghani et al., 2018; Simoulin and Crabb√©, 2021). Bolya et al.
(2023) developed a method for choosing tokens to merge when running inference on a trained vision
transformer which notably requires no learning. Lei et al. (2023) make use of conditional computation
in a fine tuning setting by building on adapter approaches (He et al., 2021) to learn to skip blocks of
frozen pre-trained weights in favor of running only a small fine-tuned adapter.
CoLT5 (Ainslie et al., 2023) uses conditional routing to select whether a given token will pass
through a heavy or light pathway for each feedforward layer. Further, they use the same routing
mechanism to select whether a token will attend to all other tokens or to a select few, as in Guo et al.
(2022). Like MoD, CoLT5 uses soft top-k for making routing decisions. However, CoLT5 focuses on
a encoder-decoder setting, and thus does need to contend with the problem of efficient sequential
decoding given the non-causal nature of the top-k operation. In contrast, our current work with
2

=== Page 3 ===
Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
Figure 1|Mixture-of-Depths Transformer. As in mixture-of-experts (MoE) transformers we use a
router to choose among potential computational paths. But unlike in MoE transformers the possible
choices are a standard block‚Äôs computation (i.e., self-attention and MLP) or a residual connection.
Since some tokens take this second route, Mixture-of-Depths (MoD) transformers have a smaller
total FLOP footprint compared to vanilla or MoE transformers. On the top right is depicted a trained
model‚Äôsroutingdecisionsforashortsequencetruncatedto64tokensforvisualizationpurposes. When
examining the choices one can find tokens processed by later blocks‚Äô layers, despite passing through
relatively few total blocks throughout the model‚Äôs depth. This is a unique feature of MoD compared
to conventional halting-based, or "early-exit" conditional computation, which instead engage blocks
serially, or vanilla transformers, which engage every block.
MoD focuses on the decoder-only setting, and so we propose a predictive router to enable efficient
inference for conditional computation in transformers.
One successful formulation of conditional computation is the the "mixture-of-experts" layer (MoE)
as introduced by Shazeer et al. (2017). Developed initially in the context of LSTMs, later work showed
compelling empirical results for MoE with transformers (Fedus et al., 2022; Lepikhin et al., 2020;
Zoph et al., 2022). Unlike other conditional computation approaches that try to conserve or expend
additional compute, MoE transformers use conditional logic to route tokens to one of many expert
MLPs while keeping total compute expenditure constant. Our mixture-of-depths method can be
thought of as using the routing logic from MoE transformers, but rather than having multiple experts,
MoD deploys a single expert which can be dynamically skipped.
3. Implementing Mixture-of-Depths Transformers
Our high-level strategy is as follows:
‚Ä¢Set a static compute budget that is less than that of an equivalent vanilla transformer by limiting
3
```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

Now I understand the paper is about "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models" by Google DeepMind. Let me search for implementations on GitHub:
</div>

<div className="result-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github search_repositories`

<Expandable title="Details">
```json arguments
{
	"query": "Mixture-of-Depths transformer implementation",
	 "perPage": 10
}

```

```json output_result
{"total_count":7,"incomplete_results":false,"items":[{"id":785664688,"name":"Mixture-of-depths","full_name":"astramind-ai/Mixture-of-depths","description":"Unofficial implementation for the paper \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/astramind-ai/Mixture-of-depths","language":"Python","stargazers_count":173,"forks_count":8,"open_issues_count":5,"updated_at":"2025-10-11T08:37:33Z","created_at":"2024-04-12T11:07:48Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":782269816,"name":"Mixture-of-Depths","full_name":"kyegomez/Mixture-of-Depths","description":"Implementation of the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/kyegomez/Mixture-of-Depths","language":"Python","stargazers_count":107,"forks_count":7,"open_issues_count":8,"updated_at":"2025-10-11T08:37:35Z","created_at":"2024-04-05T00:51:20Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":782847289,"name":"mixture-of-depths","full_name":"sramshetty/mixture-of-depths","description":"An unofficial implementation of \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/sramshetty/mixture-of-depths","language":"Python","stargazers_count":36,"forks_count":3,"open_issues_count":0,"updated_at":"2025-09-05T01:19:59Z","created_at":"2024-04-06T07:51:03Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":786779390,"name":"infini-transformer","full_name":"dingo-actual/infini-transformer","description":"PyTorch implementation of Infini-Transformer from \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\" (https://arxiv.org/abs/2404.07143)","html_url":"https://github.com/dingo-actual/infini-transformer","language":"Python","stargazers_count":291,"forks_count":23,"open_issues_count":2,"updated_at":"2025-10-04T19:51:39Z","created_at":"2024-04-15T09:37:36Z","topics":["attention-mechanism","deep-learning","infini-attention","long-context","mixture-of-depths","pytorch","transformers"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":874565778,"name":"Router-Tuning-Mixture-of-Depths","full_name":"CASE-Lab-UMD/Router-Tuning-Mixture-of-Depths","description":"The open-source Mixture of Depths code and the official implementation of the paper \"Router-Tuning: A Simple and Effective Approach for Enabling Dynamic Depth in Transformers. (EMNLP 2025)\"","html_url":"https://github.com/CASE-Lab-UMD/Router-Tuning-Mixture-of-Depths","language":"Python","stargazers_count":16,"forks_count":3,"open_issues_count":0,"updated_at":"2025-10-01T23:31:40Z","created_at":"2024-10-18T04:11:52Z","topics":["deep-learning","large-language-models","machine-learning","mixture-of-depths","neural-network","parameter-efficient-fine-tuning"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":783121754,"name":"MoE-LLM","full_name":"eclouder/MoE-LLM","description":"Implementation of the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/eclouder/MoE-LLM","language":"Python","stargazers_count":2,"forks_count":0,"open_issues_count":0,"updated_at":"2024-04-22T08:48:24Z","created_at":"2024-04-07T02:04:36Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":809894170,"name":"MH-MoDE","full_name":"dav3794/MH-MoDE","description":"Implementation of Multi-Head Mixture-of-Depths-and-Experts Transformer for NLP course.","html_url":"https://github.com/dav3794/MH-MoDE","language":"Jupyter Notebook","stargazers_count":0,"forks_count":0,"open_issues_count":0,"updated_at":"2024-06-18T11:31:36Z","created_at":"2024-06-03T16:48:16Z","private":false,"fork":false,"archived":false,"default_branch":"main"}]}
```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

Perfect! I found several implementations. Let me also search with a broader query to make sure I don't miss any:
</div>

<div className="result-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github search_repositories`

<Expandable title="Details">
```json arguments
{
	"query": "\"Mixture-of-Depths\" transformer implementation",
	 "perPage": 10
}

```

```json output_result
{"total_count":7,"incomplete_results":false,"items":[{"id":785664688,"name":"Mixture-of-depths","full_name":"astramind-ai/Mixture-of-depths","description":"Unofficial implementation for the paper \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/astramind-ai/Mixture-of-depths","language":"Python","stargazers_count":173,"forks_count":8,"open_issues_count":5,"updated_at":"2025-10-11T08:37:33Z","created_at":"2024-04-12T11:07:48Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":782269816,"name":"Mixture-of-Depths","full_name":"kyegomez/Mixture-of-Depths","description":"Implementation of the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/kyegomez/Mixture-of-Depths","language":"Python","stargazers_count":107,"forks_count":7,"open_issues_count":8,"updated_at":"2025-10-11T08:37:35Z","created_at":"2024-04-05T00:51:20Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":782847289,"name":"mixture-of-depths","full_name":"sramshetty/mixture-of-depths","description":"An unofficial implementation of \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/sramshetty/mixture-of-depths","language":"Python","stargazers_count":36,"forks_count":3,"open_issues_count":0,"updated_at":"2025-09-05T01:19:59Z","created_at":"2024-04-06T07:51:03Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":786779390,"name":"infini-transformer","full_name":"dingo-actual/infini-transformer","description":"PyTorch implementation of Infini-Transformer from \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\" (https://arxiv.org/abs/2404.07143)","html_url":"https://github.com/dingo-actual/infini-transformer","language":"Python","stargazers_count":291,"forks_count":23,"open_issues_count":2,"updated_at":"2025-10-04T19:51:39Z","created_at":"2024-04-15T09:37:36Z","topics":["attention-mechanism","deep-learning","infini-attention","long-context","mixture-of-depths","pytorch","transformers"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":874565778,"name":"Router-Tuning-Mixture-of-Depths","full_name":"CASE-Lab-UMD/Router-Tuning-Mixture-of-Depths","description":"The open-source Mixture of Depths code and the official implementation of the paper \"Router-Tuning: A Simple and Effective Approach for Enabling Dynamic Depth in Transformers. (EMNLP 2025)\"","html_url":"https://github.com/CASE-Lab-UMD/Router-Tuning-Mixture-of-Depths","language":"Python","stargazers_count":16,"forks_count":3,"open_issues_count":0,"updated_at":"2025-10-01T23:31:40Z","created_at":"2024-10-18T04:11:52Z","topics":["deep-learning","large-language-models","machine-learning","mixture-of-depths","neural-network","parameter-efficient-fine-tuning"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":783121754,"name":"MoE-LLM","full_name":"eclouder/MoE-LLM","description":"Implementation of the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/eclouder/MoE-LLM","language":"Python","stargazers_count":2,"forks_count":0,"open_issues_count":0,"updated_at":"2024-04-22T08:48:24Z","created_at":"2024-04-07T02:04:36Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":809894170,"name":"MH-MoDE","full_name":"dav3794/MH-MoDE","description":"Implementation of Multi-Head Mixture-of-Depths-and-Experts Transformer for NLP course.","html_url":"https://github.com/dav3794/MH-MoDE","language":"Jupyter Notebook","stargazers_count":0,"forks_count":0,"open_issues_count":0,"updated_at":"2024-06-18T11:31:36Z","created_at":"2024-06-03T16:48:16Z","private":false,"fork":false,"archived":false,"default_branch":"main"}]}
```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

Now let me check the current content of `result.json` to understand its structure:
</div>

<div className="result-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

<Expandable title="Details">
```json arguments
{
	"path": "/workspace/dumps/workspace/result.json"
}

```

```json output_result
{
    "URL": ""
}
```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

Perfect! Now I can see that the repository with the most stars (173 stars) is `astramind-ai/Mixture-of-depths`. Let me update the `result.json` file with this information:
</div>

<div className="result-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

<Expandable title="Details">
```json arguments
workspace/result.json
{
    "URL": "https://github.com/astramind-ai/Mixture-of-depths"
}

```

```json output_result
Successfully wrote to /workspace/dumps/workspace/result.json
```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

Let me verify the update was successful:
</div>

<div className="result-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

<Expandable title="Details">
```json arguments
{
	"path": "/workspace/dumps/workspace/result.json"
}

```

```json output_result
{
    "URL": "https://github.com/astramind-ai/Mixture-of-depths"
}
```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

Perfect! I have successfully found the unofficial implementation repository with the most stars on GitHub for the "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models" paper and updated the `result.json` file.

## Summary

I found the unofficial implementation repository for the Mixture-of-Depths paper:

- **Repository**: `astramind-ai/Mixture-of-depths`
- **URL**: https://github.com/astramind-ai/Mixture-of-depths
- **Stars**: 173 (highest among all implementations)
- **Description**: "Unofficial implementation for the paper 'Mixture-of-Depths: Dynamically allocating compute in transformer-based language models'"
- **Language**: Python

This repository has the most stars (173) among all the unofficial implementations I found, making it the most popular and likely well-maintained implementation available on GitHub. The `result.json` file has been updated with the repository URL.
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M861.588238 240.133873v-65.792823c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.631049H314.659414v-65.631049c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.792823c-36.317212 0.868255-65.631049 30.539428-65.63105 67.061417v543.745565c0 37.06772 30.155471 67.223191 67.223191 67.223191h696.886045c37.06772 0 67.223191-30.155471 67.223191-67.223191V307.19529c-0.001024-36.52199-29.315885-66.193162-65.633097-67.061417z m-109.385765-65.792823c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m-546.928824 0c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m678.191947 676.600829c0 12.935767-10.532708 23.468476-23.468476 23.468475H163.111076c-12.935767 0-23.468476-10.532708-23.468476-23.468475V307.19529c0-12.402323 9.677764-22.593054 21.877358-23.415233v65.577807c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.631049h393.789368v65.631049c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.577807c12.19857 0.82218 21.877358 11.012911 21.877358 23.415233v543.746589z" fill="#22C67F" /><path d="M706.719439 478.272194l-48.01715-44.741741-182.28128 195.621482-111.468348-122.615387-48.563905 44.148911 159.469116 172.685427z" fill="#74E8AE" /></svg> `claim_done `

<Expandable title="Details">
```json arguments
{}

```

```json output_result
you have claimed the task is done!
```

</Expandable>
</div>

</Accordion>

<Accordion title="gemini-2.5-flash">

<Columns cols={3}>
<Card title="Task Completion" icon="x">
Failed
</Card>
<Card title="Tool Calls" icon="wrench">
1
</Card>
<Card title="Turns" icon="arrows-rotate">
2
</Card>
</Columns>

<div className="error-box">
‚ùì `read_pdf_pages `
<Expandable title="Details">
```json arguments
{
	"start_page":1,
	"end_page":2,
	"pdf_file_path":"paper.pdf"
}

```
```json error_message
Tool read_pdf_pages not found in agent Assistant
```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

I apologize, but it seems I cannot directly read the `paper.pdf` file. To help you find the unofficial implementation repository, could you please tell me the title of the paper or any other key information that I can use to search on GitHub?
</div>

</Accordion>

<Accordion title="gpt-5">

<Columns cols={3}>
<Card title="Task Completion" icon="check">
Completed
</Card>
<Card title="Tool Calls" icon="wrench">
8
</Card>
<Card title="Turns" icon="arrows-rotate">
9
</Card>
</Columns>

<div className="result-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem list_directory`

<Expandable title="Details">
```json arguments
{
	"path":"/workspace/dumps/workspace"
}

```

```json output_result
[DIR] .pdf_tools_tempfiles
[FILE] paper.pdf
[FILE] result.json
```

</Expandable>
</div>

<div className="result-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

<Expandable title="Details">
```json arguments
{
	"path":"/workspace/dumps/workspace/result.json",
	"tail":200,
	"head":200
}

```

```json output_result
Error: Cannot specify both head and tail parameters simultaneously
```

</Expandable>
</div>

<div className="result-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

<Expandable title="Details">
```json arguments
{
	"path":"/workspace/dumps/workspace/result.json",
	"tail":2000,
	"head":0
}

```

```json output_result
{
    "URL": ""
}
```

</Expandable>
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools get_pdf_info`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path":"/workspace/dumps/workspace/paper.pdf"
}

```

```json output_result
PDF file information:
Total pages: 14
Title: 
Author: 
Creator: LaTeX with hyperref
Creation date: Unknown

```

</Expandable>
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path":"/workspace/dumps/workspace/paper.pdf",
	"start_page":1,
	"end_page":2
}

```

```json output_result
PDF total pages: 14
Extracted page range: 1-2

=== Page 1 ===
Mixture-of-Depths: Dynamically allocating
compute in transformer-based language
models
David Raposo1*, Sam Ritter1, Blake Richards1,2, Timothy Lillicrap1, Peter Conway Humphreys1and Adam
Santoro1*
1Google DeepMind,2McGill University & Mila,*Equal Contribution
Transformer-based language models spread FLOPs uniformly across input sequences. In this work we
demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute ) to specific
positions in a sequence, optimising the allocation along the sequence for different layers across the
model depth. Our method enforces a total compute budget by capping the number of tokens ( ùëò) that
can participate in the self-attention and MLP computations at a given layer. The tokens to be processed
are determined by the network using a top- ùëòrouting mechanism. Since ùëòis defined a priori, this
simple procedure uses a static computation graph with known tensor sizes, unlike other conditional
computation techniques. Nevertheless, since the identities of the ùëòtokens are fluid, this method can
expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure
is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do
models trained in this way learn to dynamically allocate compute, they do so efficiently. These models
match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of
the FLOPs per forward pass, and can be upwards of 50% faster to step during post-training sampling.
1. Introduction
Notallproblemsrequirethesameamountoftimeorefforttosolve. Analogously, inlanguagemodeling
not all tokens and sequences require the same time or effort to accurately make a prediction. And
yet, transformer models expend the same amount of compute per token in a forward pass. Ideally,
transformers would use smaller total compute budgets by not spending compute unnecessarily.
Conditional computation is a technique that tries to reduce total compute by expending it only
when needed (Bengio et al., 2016; Bengio, 2013; Bengio et al., 2013). Various algorithms offer
solutions to when and how much compute should be used (Ainslie et al., 2023; Bapna et al., 2020;
Fedus et al., 2022). However, general formulations of this challenging problem may not work well with
existing hardware constraints since they tend to introduce dynamic computation graphs (Dehghani
et al., 2018; Graves, 2016). The most promising conditional computation methods may instead be
those that are harmonious with our current hardware stack, which prioritizes static computation
graphs, and known tensor sizes that are selected to maximize hardware utilization.
Here we consider the problem of language modeling using a staticcompute budget that can be
made less than that used by a vanilla transformer. The network must learn how to dynamically allocate
the available compute by making decisions per-token, in each layer, about where to spend compute
from the available budget. In our implementation total compute is user defined and unchanging
prior to training, rather than being a function of the network‚Äôs on-the-fly decisions. Thus, hardware
efficiency gains‚Äîsuch as reduced memory footprint, or reduced FLOPs per forward pass‚Äîcan be
anticipated and exploited ahead of time. As we will show, these gains can be had without sacrificing
overall performance.
Corresponding author(s): draposo@google.com, adamsantoro@google.com
¬©2024 Google DeepMind. All rights reservedarXiv:2404.02258v1  [cs.LG]  2 Apr 2024

=== Page 2 ===
Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
We leverage an approach akin to Mixture of Experts (MoE) transformers, in which dynamic
token-level routing decisions are made across the network depth. Departing from MoE, we choose
to either apply a computation to a token (as would be the case for a standard transformer), or pass
it through a residual connection (remaining unchanged and saving compute). Also in contrast to
MoE, we apply this routing to both forward MLPs and multi-head attention. Since this therefore
also impacts the keys and queries we process, the routing makes decisions not only about which
tokens to update, but also which tokens are made available to attend to. We refer to this strategy
as Mixture-of-Depths (MoD) to emphasize how individual tokens pass through different numbers of
layers, or blocks, through the depth of the transformer (see figure 1).
The MoD technique also allows one to trade-off performance with speed. On the one hand, one
can train an MoD transformer that improves upon vanilla transformers by as much as 1.5%on the
final log probability training objective for equivalent training FLOPs (isoFLOP), and while taking an
equivalent amount of wall-clock time to train. On the other hand, one can train an MoD transformer
that achieves training loss parity with an isoFLOP optimal vanilla transformer, but which uses a
fraction of the FLOPs (upwards of 50%) per forward pass, and hence is faster to step. Together, these
results imply that MoD transformers learn to route intelligently (i.e., skipping computations that are
unnecessary) since they can achieve equal or better log probabilities per sequence despite a smaller
FLOP footprint per forward pass.
2. Background
The transformer architecture has become the workhorse of a revolution in practical artificial in-
telligence, bringing unprecedented capabilities at the cost of expensive training runs and serving
procedures. This has spurred tremendous interest in making transformer architectures more effi-
cient (Gupta and Agrawal, 2021; Tay et al., 2020). One of the promising approaches is conditional
computation , whereby learned mechanisms determine when and how to expend computation. This
terminology was introduced by Bengio (2013), and the concept was explored further over the next
several years (Bengio et al., 2016, 2013; Cho and Bengio, 2014; Graves, 2016; Jernite et al., 2017;
Wang et al., 2017).
A wide variety of recent work has developed conditional computation methods for transformers.
Some of this work focuses on "early exiting", that is, learning to decide when to end computation on
a given token, allowing the token to skip any remaining transformer layers after the exit decision
is made (Elbayad et al., 2019; Liu et al., 2021; Schuster et al., 2022). In MoD, unlike in early-exit
methods, a token can skip middle layers, then be updated via self-attention with tokens that that
have gone through all the middle layers. We speculate that this might be a useful property.
Other work has developed methods for iterating transformer layers with shared weights for
an adaptive number of steps (Dehghani et al., 2018; Simoulin and Crabb√©, 2021). Bolya et al.
(2023) developed a method for choosing tokens to merge when running inference on a trained vision
transformer which notably requires no learning. Lei et al. (2023) make use of conditional computation
in a fine tuning setting by building on adapter approaches (He et al., 2021) to learn to skip blocks of
frozen pre-trained weights in favor of running only a small fine-tuned adapter.
CoLT5 (Ainslie et al., 2023) uses conditional routing to select whether a given token will pass
through a heavy or light pathway for each feedforward layer. Further, they use the same routing
mechanism to select whether a token will attend to all other tokens or to a select few, as in Guo et al.
(2022). Like MoD, CoLT5 uses soft top-k for making routing decisions. However, CoLT5 focuses on
a encoder-decoder setting, and thus does need to contend with the problem of efficient sequential
decoding given the non-causal nature of the top-k operation. In contrast, our current work with
2
```

</Expandable>
</div>

<div className="result-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github search_repositories`

<Expandable title="Details">
```json arguments
{
	"minimal_output": true,
	 "page": 1,
	 "perPage": 50,
	 "query": "Mixture-of-Depths in:name,
	description,
	readme"
}

```

```json output_result
{"total_count":733,"incomplete_results":false,"items":[{"id":785664688,"name":"Mixture-of-depths","full_name":"astramind-ai/Mixture-of-depths","description":"Unofficial implementation for the paper \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/astramind-ai/Mixture-of-depths","language":"Python","stargazers_count":173,"forks_count":8,"open_issues_count":5,"updated_at":"2025-10-11T08:37:33Z","created_at":"2024-04-12T11:07:48Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":782269816,"name":"Mixture-of-Depths","full_name":"kyegomez/Mixture-of-Depths","description":"Implementation of the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/kyegomez/Mixture-of-Depths","language":"Python","stargazers_count":107,"forks_count":7,"open_issues_count":8,"updated_at":"2025-10-11T08:37:35Z","created_at":"2024-04-05T00:51:20Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":1002668918,"name":"mixture_of_recursions","full_name":"raymin0223/mixture_of_recursions","description":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation (NeurIPS 2025)","html_url":"https://github.com/raymin0223/mixture_of_recursions","language":"Python","stargazers_count":481,"forks_count":70,"open_issues_count":1,"updated_at":"2025-10-15T08:47:44Z","created_at":"2025-06-16T00:20:03Z","topics":["adaptive-computation","early-exiting","kv-cache","llm","recursive-transformers","router"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":783744120,"name":"mergoo","full_name":"Leeroo-AI/mergoo","description":"A library for easily merging multiple LLM experts, and efficiently train the merged LLM.","html_url":"https://github.com/Leeroo-AI/mergoo","language":"Python","stargazers_count":493,"forks_count":35,"open_issues_count":11,"updated_at":"2025-10-14T21:34:15Z","created_at":"2024-04-08T13:40:41Z","topics":["artificial-intelligence","fine-tuning","generative-ai","large-language-models","llm","lora","merge","mixture-of-adapters","mixture-of-experts","multi-model","nlp","open-source","transformers"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":1009063652,"name":"Awesome-Efficient-Arch","full_name":"weigao266/Awesome-Efficient-Arch","description":"Speed Always Wins: A Survey on Efficient Architectures for Large Language Models","html_url":"https://github.com/weigao266/Awesome-Efficient-Arch","stargazers_count":343,"forks_count":30,"open_issues_count":0,"updated_at":"2025-10-16T13:25:01Z","created_at":"2025-06-26T14:15:07Z","topics":["diffusion-llm","efficient-architectures","linear-attention","linear-models","linear-rnn","mamba","mixture-of-experts","moe","sparse-attention","state-space-model"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":782847289,"name":"mixture-of-depths","full_name":"sramshetty/mixture-of-depths","description":"An unofficial implementation of \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/sramshetty/mixture-of-depths","language":"Python","stargazers_count":36,"forks_count":3,"open_issues_count":0,"updated_at":"2025-09-05T01:19:59Z","created_at":"2024-04-06T07:51:03Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":317358544,"name":"GaMMA","full_name":"AI4EPS/GaMMA","description":"GaMMA: Earthquake Phase Association using a Bayesian Gaussian Mixture Model","html_url":"https://github.com/AI4EPS/GaMMA","language":"Jupyter Notebook","stargazers_count":88,"forks_count":35,"open_issues_count":15,"updated_at":"2025-09-22T02:52:20Z","created_at":"2020-11-30T22:10:02Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":786911662,"name":"A-Survey-on-Mixture-of-Experts-in-LLMs","full_name":"withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs","description":"[TKDE'25] The official GitHub page for the survey paper \"A Survey on Mixture of Experts in Large Language Models\".","html_url":"https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs","stargazers_count":432,"forks_count":22,"open_issues_count":5,"updated_at":"2025-10-15T12:38:51Z","created_at":"2024-04-15T14:35:32Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":786779390,"name":"infini-transformer","full_name":"dingo-actual/infini-transformer","description":"PyTorch implementation of Infini-Transformer from \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\" (https://arxiv.org/abs/2404.07143)","html_url":"https://github.com/dingo-actual/infini-transformer","language":"Python","stargazers_count":291,"forks_count":23,"open_issues_count":2,"updated_at":"2025-10-04T19:51:39Z","created_at":"2024-04-15T09:37:36Z","topics":["attention-mechanism","deep-learning","infini-attention","long-context","mixture-of-depths","pytorch","transformers"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":119560854,"name":"Trimix-analyzer","full_name":"captainigloo/Trimix-analyzer","description":"Trimix-analyser","html_url":"https://github.com/captainigloo/Trimix-analyzer","language":"C++","stargazers_count":21,"forks_count":11,"open_issues_count":4,"updated_at":"2025-04-23T02:41:31Z","created_at":"2018-01-30T16:13:01Z","private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":333108833,"name":"SATC","full_name":"popgenDK/SATC","description":"Sex assignment through coverage","html_url":"https://github.com/popgenDK/SATC","language":"R","stargazers_count":20,"forks_count":10,"open_issues_count":11,"updated_at":"2025-08-29T15:50:56Z","created_at":"2021-01-26T14:23:33Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":803133604,"name":"Mixture-of-Depths","full_name":"Mixture-AI/Mixture-of-Depths","description":"Google DeepMind: Mixture of Depths Unofficial Implementation.","html_url":"https://github.com/Mixture-AI/Mixture-of-Depths","language":"Python","stargazers_count":12,"forks_count":3,"open_issues_count":0,"updated_at":"2025-08-18T02:56:53Z","created_at":"2024-05-20T06:14:28Z","topics":["conditional-computation","llm","mixture-of-depths"],"private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":629520282,"name":"Efficient-LLMs-Survey","full_name":"AIoT-MLSys-Lab/Efficient-LLMs-Survey","description":"[TMLR 2024] Efficient Large Language Models: A Survey","html_url":"https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey","stargazers_count":1220,"forks_count":97,"open_issues_count":2,"updated_at":"2025-10-09T08:07:18Z","created_at":"2023-04-18T13:32:01Z","topics":["efficient-deep-learning","generative-ai","large-language-models","machine-learning-systems","survey"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":351354962,"name":"CAPTRA","full_name":"HalfSummer11/CAPTRA","description":"[ICCV 2021, Oral] Official PyTorch implementation of CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds","html_url":"https://github.com/HalfSummer11/CAPTRA","language":"Python","stargazers_count":123,"forks_count":21,"open_issues_count":1,"updated_at":"2025-10-16T06:22:30Z","created_at":"2021-03-25T08:04:37Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":874565778,"name":"Router-Tuning-Mixture-of-Depths","full_name":"CASE-Lab-UMD/Router-Tuning-Mixture-of-Depths","description":"The open-source Mixture of Depths code and the official implementation of the paper \"Router-Tuning: A Simple and Effective Approach for Enabling Dynamic Depth in Transformers. (EMNLP 2025)\"","html_url":"https://github.com/CASE-Lab-UMD/Router-Tuning-Mixture-of-Depths","language":"Python","stargazers_count":16,"forks_count":3,"open_issues_count":0,"updated_at":"2025-10-01T23:31:40Z","created_at":"2024-10-18T04:11:52Z","topics":["deep-learning","large-language-models","machine-learning","mixture-of-depths","neural-network","parameter-efficient-fine-tuning"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":564867527,"name":"SHIKE","full_name":"jinyan-06/SHIKE","description":"Long-Tailed Visual Recognition via Self-Heterogeneous Integration with Knowledge Excavation (CVPR 2023)","html_url":"https://github.com/jinyan-06/SHIKE","language":"Python","stargazers_count":39,"forks_count":5,"open_issues_count":5,"updated_at":"2025-09-06T05:52:27Z","created_at":"2022-11-11T17:32:28Z","topics":["cvpr2023","imbalanced-learning","long-tailed-learning","long-tailed-recognition"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":646410686,"name":"LLaMA-Factory","full_name":"hiyouga/LLaMA-Factory","description":"Unified Efficient Fine-Tuning of 100+ LLMs \u0026 VLMs (ACL 2024)","html_url":"https://github.com/hiyouga/LLaMA-Factory","language":"Python","stargazers_count":60290,"forks_count":7306,"open_issues_count":708,"updated_at":"2025-10-16T13:33:08Z","created_at":"2023-05-28T10:09:12Z","topics":["agent","ai","deepseek","fine-tuning","gemma","gpt","instruction-tuning","large-language-models","llama","llama3","llm","lora","moe","nlp","peft","qlora","quantization","qwen","rlhf","transformers"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":853227707,"name":"Awesome-Token-Compress","full_name":"daixiangzi/Awesome-Token-Compress","description":"A paper list of some recent  works about Token Compress for Vit and VLM","html_url":"https://github.com/daixiangzi/Awesome-Token-Compress","stargazers_count":693,"forks_count":30,"open_issues_count":1,"updated_at":"2025-10-16T08:40:18Z","created_at":"2024-09-06T08:35:07Z","private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":897319488,"name":"p-MoD","full_name":"MCG-NJU/p-MoD","description":"[ICCV 2025] p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay","html_url":"https://github.com/MCG-NJU/p-MoD","language":"Python","stargazers_count":43,"forks_count":2,"open_issues_count":1,"updated_at":"2025-10-03T13:42:44Z","created_at":"2024-12-02T12:31:35Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":110621731,"name":"Symbolic-Regression","full_name":"twhughes/Symbolic-Regression","description":"predicting equations from raw data with deep learning","html_url":"https://github.com/twhughes/Symbolic-Regression","language":"Python","stargazers_count":57,"forks_count":10,"open_issues_count":0,"updated_at":"2025-08-28T09:18:47Z","created_at":"2017-11-14T01:01:39Z","topics":["decoding","equation","neural-network","symbolic-regression"],"private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":43090439,"name":"carcassonne-shading","full_name":"tomkinsc/carcassonne-shading","description":"Small JS app to manipulate SVG files for printing or laser-cutting tiles for the board game Carcassonne.","html_url":"https://github.com/tomkinsc/carcassonne-shading","language":"JavaScript","stargazers_count":24,"forks_count":6,"open_issues_count":0,"updated_at":"2024-12-07T12:18:28Z","created_at":"2015-09-24T20:08:33Z","private":false,"fork":false,"archived":false,"default_branch":"gh-pages"},{"id":214648555,"name":"HumanPoseMemo","full_name":"chingswy/HumanPoseMemo","description":"Memo about 3d human pose estimation, record of datasets, papers, codes.","html_url":"https://github.com/chingswy/HumanPoseMemo","language":"TeX","stargazers_count":175,"forks_count":20,"open_issues_count":1,"updated_at":"2025-07-29T07:46:26Z","created_at":"2019-10-12T13:04:52Z","private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":460351353,"name":"Research-Space","full_name":"DicardoX/Research-Space","description":"This repository is established to store personal notes and annotated papers during daily research.","html_url":"https://github.com/DicardoX/Research-Space","stargazers_count":155,"forks_count":13,"open_issues_count":0,"updated_at":"2025-10-14T07:49:44Z","created_at":"2022-02-17T08:45:55Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":138352180,"name":"mimic_classify","full_name":"rajatsen91/mimic_classify","description":"Mimic \u0026 Classify CI Test","html_url":"https://github.com/rajatsen91/mimic_classify","language":"Python","stargazers_count":6,"forks_count":5,"open_issues_count":0,"updated_at":"2022-03-14T18:44:33Z","created_at":"2018-06-22T22:07:46Z","private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":816686884,"name":"pytorch-mixtures","full_name":"jaisidhsingh/pytorch-mixtures","description":"One-stop solutions for Mixture of Experts and Mixture of Depth modules in PyTorch.","html_url":"https://github.com/jaisidhsingh/pytorch-mixtures","language":"Python","stargazers_count":25,"forks_count":1,"open_issues_count":0,"updated_at":"2025-10-14T22:09:37Z","created_at":"2024-06-18T08:05:26Z","topics":["language-model","large-language-models","mixture-of-depths","mixture-of-experts","nlp","pip-package","pytorch"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":255601979,"name":"Awesome-Self-Supervised-Papers","full_name":"dev-sungman/Awesome-Self-Supervised-Papers","description":"Paper bank for Self-Supervised Learning","html_url":"https://github.com/dev-sungman/Awesome-Self-Supervised-Papers","stargazers_count":580,"forks_count":58,"open_issues_count":1,"updated_at":"2025-10-15T20:29:22Z","created_at":"2020-04-14T12:23:38Z","topics":["awesome","cvpr2020","self-supervised","self-supervised-learning","sota"],"private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":218470804,"name":"awesome-image-registration","full_name":"Awesome-Image-Registration-Organization/awesome-image-registration","description":"image registration related books, papers, videos, and toolboxes ","html_url":"https://github.com/Awesome-Image-Registration-Organization/awesome-image-registration","stargazers_count":1451,"forks_count":174,"open_issues_count":0,"updated_at":"2025-10-15T07:15:15Z","created_at":"2019-10-30T07:47:02Z","topics":["awesome","awesome-list","image","image-processing","image-registration","medical-images","natural-images","point-cloud-registration","remote-sensing-image","slam"],"private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":623618719,"name":"Awesome-Foundation-Models","full_name":"uncbiag/Awesome-Foundation-Models","description":"A curated list of foundation models for vision and language tasks","html_url":"https://github.com/uncbiag/Awesome-Foundation-Models","stargazers_count":1102,"forks_count":57,"open_issues_count":0,"updated_at":"2025-10-16T12:53:54Z","created_at":"2023-04-04T18:27:06Z","topics":["foundation-models","large-language-models","multimodal-models","transformer-models","vision-transformer"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":785695282,"name":"voltronformers","full_name":"OpenAccess-AI-Collective/voltronformers","description":"Assembling the best SotA AI techniques into a unified model","html_url":"https://github.com/OpenAccess-AI-Collective/voltronformers","stargazers_count":12,"forks_count":1,"open_issues_count":1,"updated_at":"2025-01-29T21:36:09Z","created_at":"2024-04-12T12:30:21Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":165509352,"name":"awesome-transformer-nlp","full_name":"cedrickchee/awesome-transformer-nlp","description":"A curated list of NLP resources focused on Transformer networks, attention mechanism, GPT, BERT, ChatGPT, LLMs, and transfer learning.","html_url":"https://github.com/cedrickchee/awesome-transformer-nlp","stargazers_count":1117,"forks_count":132,"open_issues_count":1,"updated_at":"2025-10-04T12:42:05Z","created_at":"2019-01-13T13:37:08Z","topics":["attention-mechanism","awesome","awesome-list","bert","chatgpt","gpt-2","gpt-3","gpt-4","language-model","llama","natural-language-processing","neural-networks","nlp","pre-trained-language-models","transfer-learning","transformer","xlnet"],"private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":18563333,"name":"romstools","full_name":"trondkr/romstools","description":"A variety of tools for plotting and analysing output from the Regional Ocean Model (ROMS)","html_url":"https://github.com/trondkr/romstools","language":"Jupyter Notebook","stargazers_count":35,"forks_count":20,"open_issues_count":1,"updated_at":"2025-02-03T23:15:46Z","created_at":"2014-04-08T15:19:59Z","private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":421092333,"name":"tapas","full_name":"wfinsinger/tapas","description":"Trend And PeAkS (TAPAS) analysis of temporal series (e.g. paleoecological records) to assess long-term trend and detect events (and eventually estimate event-return intervals)","html_url":"https://github.com/wfinsinger/tapas","language":"R","stargazers_count":11,"forks_count":2,"open_issues_count":0,"updated_at":"2024-08-19T08:12:43Z","created_at":"2021-10-25T16:02:09Z","topics":["paleoecology","paleofire","peak-detection","timeseries-analysis","trend"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":849748717,"name":"praxis","full_name":"0-5788719150923125/praxis","description":"for all, none","html_url":"https://github.com/0-5788719150923125/praxis","language":"Python","stargazers_count":15,"forks_count":2,"open_issues_count":14,"updated_at":"2025-10-15T04:48:03Z","created_at":"2024-08-30T07:05:55Z","topics":["ai","devops","docker","hivemind","ml","moe","python","ryan","transformers"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":930300532,"name":"UniMoD","full_name":"showlab/UniMoD","description":"The code repository of UniMoD","html_url":"https://github.com/showlab/UniMoD","stargazers_count":10,"forks_count":0,"open_issues_count":1,"updated_at":"2025-06-29T04:19:10Z","created_at":"2025-02-10T12:14:15Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":90380488,"name":"CNVcaller","full_name":"JiangYuLab/CNVcaller","html_url":"https://github.com/JiangYuLab/CNVcaller","language":"Perl","stargazers_count":42,"forks_count":14,"open_issues_count":13,"updated_at":"2025-07-19T15:44:21Z","created_at":"2017-05-05T13:49:32Z","private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":679673318,"name":"awesome-adaptive-computation","full_name":"koayon/awesome-adaptive-computation","description":"A curated reading list of research in Adaptive Computation, Inference-Time Computation \u0026 Mixture of Experts (MoE).","html_url":"https://github.com/koayon/awesome-adaptive-computation","stargazers_count":154,"forks_count":10,"open_issues_count":1,"updated_at":"2025-10-04T14:21:32Z","created_at":"2023-08-17T11:11:32Z","topics":["adaptive-computation","computer-vision","machine-learning","mixture-of-experts","nlp","pytorch","tensorflow","transformers"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":784596511,"name":"Mixture-of-Depths-Jax","full_name":"dtunai/Mixture-of-Depths-Jax","description":"Jax module for the paper: \"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\"","html_url":"https://github.com/dtunai/Mixture-of-Depths-Jax","language":"Python","stargazers_count":3,"forks_count":0,"open_issues_count":13,"updated_at":"2024-07-18T20:50:24Z","created_at":"2024-04-10T06:56:02Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":126408408,"name":"ScubaDiving_SE","full_name":"garc0062/ScubaDiving_SE","html_url":"https://github.com/garc0062/ScubaDiving_SE","language":"Java","stargazers_count":0,"forks_count":3,"open_issues_count":0,"updated_at":"2018-05-29T12:00:09Z","created_at":"2018-03-22T23:53:41Z","private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":172594443,"name":"BAMMSC","full_name":"CHPGenetics/BAMMSC","html_url":"https://github.com/CHPGenetics/BAMMSC","language":"C++","stargazers_count":10,"forks_count":3,"open_issues_count":1,"updated_at":"2024-01-06T13:08:49Z","created_at":"2019-02-25T22:19:34Z","private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":513566064,"name":"wf-human-variation","full_name":"epi2me-labs/wf-human-variation","html_url":"https://github.com/epi2me-labs/wf-human-variation","language":"Nextflow","stargazers_count":134,"forks_count":72,"open_issues_count":19,"updated_at":"2025-09-12T06:00:35Z","created_at":"2022-07-13T15:02:36Z","private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":783865510,"name":"Mixture-Of-Depths","full_name":"paulilioaica/Mixture-Of-Depths","html_url":"https://github.com/paulilioaica/Mixture-Of-Depths","language":"Python","stargazers_count":2,"forks_count":0,"open_issues_count":0,"updated_at":"2025-06-29T09:06:21Z","created_at":"2024-04-08T18:16:21Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":808493339,"name":"Efficient-Diffusion-Model-Survey","full_name":"AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey","description":"[TMLR 2025] Efficient Diffusion Models: A Survey","html_url":"https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey","stargazers_count":117,"forks_count":5,"open_issues_count":0,"updated_at":"2025-10-15T10:01:10Z","created_at":"2024-05-31T07:26:39Z","topics":["diffusion-models","efficient-deep-learning","generative-ai","machine-learning-systems","survey"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":95475048,"name":"remixt","full_name":"amcpherson/remixt","description":"Clone-specific genomic structure estimation in cancer","html_url":"https://github.com/amcpherson/remixt","language":"Python","stargazers_count":8,"forks_count":3,"open_issues_count":18,"updated_at":"2025-07-03T17:39:56Z","created_at":"2017-06-26T18:08:10Z","private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":814422048,"name":"Easy_Fintune_LLM_using_SageMaker_with_LLama_Factory","full_name":"aws-samples/Easy_Fintune_LLM_using_SageMaker_with_LLama_Factory","html_url":"https://github.com/aws-samples/Easy_Fintune_LLM_using_SageMaker_with_LLama_Factory","language":"Jupyter Notebook","stargazers_count":14,"forks_count":2,"open_issues_count":1,"updated_at":"2025-07-27T12:21:13Z","created_at":"2024-06-13T01:44:44Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":223818793,"name":"awesome-bioinformatics-benchmarks","full_name":"j-andrews7/awesome-bioinformatics-benchmarks","description":"A curated and summarized list of bioinformatics bench-marking papers and resources.","html_url":"https://github.com/j-andrews7/awesome-bioinformatics-benchmarks","stargazers_count":344,"forks_count":62,"open_issues_count":23,"updated_at":"2025-10-11T10:12:42Z","created_at":"2019-11-24T22:20:17Z","topics":["benchmarking-study","bioinformatics","bioinformatics-benchmarks","collection","genomics"],"private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":215084762,"name":"heinsen_routing","full_name":"glassroom/heinsen_routing","description":"Reference implementation of \"An Algorithm for Routing Vectors in Sequences\" (Heinsen, 2022) and \"An Algorithm for Routing Capsules in All Domains\" (Heinsen, 2019), for composing deep neural networks.","html_url":"https://github.com/glassroom/heinsen_routing","language":"Python","stargazers_count":172,"forks_count":17,"open_issues_count":0,"updated_at":"2025-07-23T21:12:52Z","created_at":"2019-10-14T15:52:45Z","topics":["associative-memory","capsule-network","capsules","deep-learning","dense-associative-memory","em-routing","paper","pytorch","routing-algorithm","society-of-mind","vector-routing"],"private":false,"fork":false,"archived":false,"default_branch":"master"},{"id":736966436,"name":"cfDNA_deconv_benchmark","full_name":"LiymLab/cfDNA_deconv_benchmark","html_url":"https://github.com/LiymLab/cfDNA_deconv_benchmark","language":"Jupyter Notebook","stargazers_count":3,"forks_count":2,"open_issues_count":0,"updated_at":"2025-09-20T12:55:47Z","created_at":"2023-12-29T11:55:13Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":318541083,"name":"so-fronts","full_name":"so-wise/so-fronts","description":"Full scripts to generate figures for \"Defining Southern Ocean fronts using unsupervised classification\" https://doi.org/10.5194/os-17-1545-2021","html_url":"https://github.com/so-wise/so-fronts","language":"Python","stargazers_count":10,"forks_count":2,"open_issues_count":2,"updated_at":"2025-08-13T09:15:47Z","created_at":"2020-12-04T14:29:10Z","topics":["fronts","gaussian-mixture-models","oceanography","sobel-edge-detector"],"private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":922129389,"name":"Notable-LLM-Research-Papers","full_name":"itsual/Notable-LLM-Research-Papers","description":"Curated list of research papers published in 2024 related to Large Language Models (LLM)","html_url":"https://github.com/itsual/Notable-LLM-Research-Papers","stargazers_count":30,"forks_count":4,"open_issues_count":0,"updated_at":"2025-10-07T02:01:54Z","created_at":"2025-01-25T11:53:42Z","private":false,"fork":false,"archived":false,"default_branch":"main"},{"id":855021859,"name":"mixplat","full_name":"seasideGargantua/mixplat","html_url":"https://github.com/seasideGargantua/mixplat","language":"Cuda","stargazers_count":10,"forks_count":0,"open_issues_count":0,"updated_at":"2025-09-30T06:23:28Z","created_at":"2024-09-10T07:02:47Z","private":false,"fork":false,"archived":false,"default_branch":"main"}]}
```

</Expandable>
</div>

<div className="result-box">
<svg width="14" height="14" viewBox="0 0 24 24" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg> `github search_repositories`

<Expandable title="Details">
```json arguments
workspace/result.json
{
    "URL": "https://github.com/astramind-ai/Mixture-of-depths"
}

```

```json output_result
Successfully wrote to /workspace/dumps/workspace/result.json
```

</Expandable>
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M861.588238 240.133873v-65.792823c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.631049H314.659414v-65.631049c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.792823c-36.317212 0.868255-65.631049 30.539428-65.63105 67.061417v543.745565c0 37.06772 30.155471 67.223191 67.223191 67.223191h696.886045c37.06772 0 67.223191-30.155471 67.223191-67.223191V307.19529c-0.001024-36.52199-29.315885-66.193162-65.633097-67.061417z m-109.385765-65.792823c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m-546.928824 0c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m678.191947 676.600829c0 12.935767-10.532708 23.468476-23.468476 23.468475H163.111076c-12.935767 0-23.468476-10.532708-23.468476-23.468475V307.19529c0-12.402323 9.677764-22.593054 21.877358-23.415233v65.577807c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.631049h393.789368v65.631049c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.577807c12.19857 0.82218 21.877358 11.012911 21.877358 23.415233v543.746589z" fill="#22C67F" /><path d="M706.719439 478.272194l-48.01715-44.741741-182.28128 195.621482-111.468348-122.615387-48.563905 44.148911 159.469116 172.685427z" fill="#74E8AE" /></svg> `claim_done `

<Expandable title="Details">
```json arguments
{}

```

```json output_result
you have claimed the task is done!
```

</Expandable>
</div>

<div className="thinking-box">
üßê`Agent`

I found the most-starred unofficial implementation on GitHub for the paper ‚ÄúMixture-of-Depths: Dynamically allocating compute in transformer-based language models‚Äù and updated result.json accordingly.

Top unofficial repo:
- https://github.com/astramind-ai/Mixture-of-depths (most stars among Mixture-of-Depths unofficial implementations)

result.json has been updated to:
\{
    "URL": "https://github.com/astramind-ai/Mixture-of-depths"
\}
</div>

</Accordion>

</AccordionGroup>
