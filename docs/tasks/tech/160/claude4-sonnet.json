{"config": {"task_dir": "finalpool/logical-datasets-collection", "id": "finalpool-logical-datasets-collection", "needed_mcp_servers": ["filesystem", "scholarly", "arxiv_local", "pdf-tools", "fetch"], "needed_local_tools": ["web_search", "claim_done", "handle_overlong_tool_outputs", "manage_context", "history"], "task_root": "/workspace/dumps", "task_str": "Hello! I am a PhD student engaged in deep learning research. Recently, my collaborators and I have proposed a logical reasoning dataset and are writing a paper to introduce this new dataset to the community. To demonstrate the differences between our work and previous work, I need you to help me summarize a latex table and save it in the workspace with name as `datasets.tex`, the tex file should only contain table content (no commented lines), without any other content. The table needs to include four columns: Dataset (dataset name), Tasks (number of tasks included in the dataset), Trainable (whether it includes a training set, filled with \\ding{55} or \\ding{51}), and Adjustable Difficulty (whether it includes different difficulty levels, filled with \\ding{55} or \\ding{51}). The table format is as follows:\n\n```tex\n\\begin{table}[!ht]\n    \\begin{center}\n    \\begin{tabular}{lccc}\n        \\toprule\n        Dataset & Tasks & Trainable & Adjustable Difficulty\\\\\n        \\midrule\n        % content\n        \\bottomrule\n    \\end{tabular}\n  \\end{center}\n\\end{table}\n```\n\nThe names of the datasets we need to include are (from top to bottom):\n- BBH\n- Zebra Logic\n- KOR-Bench (# of tasks should based the broader categorization in the paper.)\n- K&K (https://arxiv.org/abs/2410.23123)\n- BBEH", "log_file": "/workspace/dumps/traj_log.json", "agent_workspace": "/workspace/dumps/workspace", "launch_time": "2025-09-21 08:01:26 Sunday", "max_turns": 50, "max_steps_under_single_turn_mode": 100, "single_turn_mode": true, "cn_mode": false, "system_prompts": {"agent": "Accessible workspace directory: /workspace/dumps/workspace\nWhen processing tasks, if you need to read/write local files and the user provides a relative path, you need to combine it with the above workspace directory to get the complete path.\nIf you believe the task is completed, you can either call the `local-claim_done` tool or respond without calling any tool to indicate completion. This will immediately terminate the task, and you will have no further opportunity to work on it.\nPlease complete the given task independently. Do not seek confirmation or additional feedback from the user. You should handle all situations on your own, as the user will not provide any further information.", "user": null}, "initialization": {"workspace": null, "process_command": null}, "stop": {"user_phrases": ["#### STOP"], "tool_names": ["local-claim_done"]}, "evaluation": {"groundtruth_workspace": "tasks/finalpool/logical-datasets-collection/groundtruth_workspace", "evaluation_command": "uv run -m tasks.finalpool.logical-datasets-collection.evaluation.main"}, "meta": {}, "local_token_key_session": null}, "request_id": "7aa3936d-49f4-4a7e-a0cf-2f8475d8bd18", "initial_run_time": "2025-09-21 08:01:28", "completion_time": "2025-09-21 08:03:11", "tool_calls": {"tools": [{"type": "function", "function": {"name": "filesystem-read_file", "description": "Read the complete contents of a file from the file system. Handles various text encodings and provides detailed error messages if the file cannot be read. Use this tool when you need to examine the contents of a single file. Use the 'head' parameter to read only the first N lines of a file, or the 'tail' parameter to read only the last N lines of a file. Only works within allowed directories.", "parameters": {"type": "object", "properties": {"path": {"type": "string"}, "tail": {"type": "number", "description": "If provided, returns only the last N lines of the file"}, "head": {"type": "number", "description": "If provided, returns only the first N lines of the file"}}, "required": ["path"], "additionalProperties": false, "$schema": "http://json-schema.org/draft-07/schema#"}}}, {"type": "function", "function": {"name": "filesystem-read_multiple_files", "description": "Read the contents of multiple files simultaneously. This is more efficient than reading files one by one when you need to analyze or compare multiple files. Each file's content is returned with its path as a reference. Failed reads for individual files won't stop the entire operation. Only works within allowed directories.", "parameters": {"type": "object", "properties": {"paths": {"type": "array", "items": {"type": "string"}}}, "required": ["paths"], "additionalProperties": false, "$schema": "http://json-schema.org/draft-07/schema#"}}}, {"type": "function", "function": {"name": "filesystem-write_file", "description": "Create a new file or completely overwrite an existing file with new content. Use with caution as it will overwrite existing files without warning. Handles text content with proper encoding. Only works within allowed directories.", "parameters": {"type": "object", "properties": {"path": {"type": "string"}, "content": {"type": "string"}}, "required": ["path", "content"], "additionalProperties": false, "$schema": "http://json-schema.org/draft-07/schema#"}}}, {"type": "function", "function": {"name": "filesystem-edit_file", "description": "Make line-based edits to a text file. Each edit replaces exact line sequences with new content. Returns a git-style diff showing the changes made. Only works within allowed directories.", "parameters": {"type": "object", "properties": {"path": {"type": "string"}, "edits": {"type": "array", "items": {"type": "object", "properties": {"oldText": {"type": "string", "description": "Text to search for - must match exactly"}, "newText": {"type": "string", "description": "Text to replace with"}}, "required": ["oldText", "newText"], "additionalProperties": false}}, "dryRun": {"type": "boolean", "default": false, "description": "Preview changes using git-style diff format"}}, "required": ["path", "edits"], "additionalProperties": false, "$schema": "http://json-schema.org/draft-07/schema#"}}}, {"type": "function", "function": {"name": "filesystem-create_directory", "description": "Create a new directory or ensure a directory exists. Can create multiple nested directories in one operation. If the directory already exists, this operation will succeed silently. Perfect for setting up directory structures for projects or ensuring required paths exist. Only works within allowed directories.", "parameters": {"type": "object", "properties": {"path": {"type": "string"}}, "required": ["path"], "additionalProperties": false, "$schema": "http://json-schema.org/draft-07/schema#"}}}, {"type": "function", "function": {"name": "filesystem-list_directory", "description": "Get a detailed listing of all files and directories in a specified path. Results clearly distinguish between files and directories with [FILE] and [DIR] prefixes. This tool is essential for understanding directory structure and finding specific files within a directory. Only works within allowed directories.", "parameters": {"type": "object", "properties": {"path": {"type": "string"}}, "required": ["path"], "additionalProperties": false, "$schema": "http://json-schema.org/draft-07/schema#"}}}, {"type": "function", "function": {"name": "filesystem-list_directory_with_sizes", "description": "Get a detailed listing of all files and directories in a specified path, including sizes. Results clearly distinguish between files and directories with [FILE] and [DIR] prefixes. This tool is useful for understanding directory structure and finding specific files within a directory. Only works within allowed directories.", "parameters": {"type": "object", "properties": {"path": {"type": "string"}, "sortBy": {"type": "string", "enum": ["name", "size"], "default": "name", "description": "Sort entries by name or size"}}, "required": ["path"], "additionalProperties": false, "$schema": "http://json-schema.org/draft-07/schema#"}}}, {"type": "function", "function": {"name": "filesystem-directory_tree", "description": "Get a recursive tree view of files and directories as a JSON structure. Each entry includes 'name', 'type' (file/directory), and 'children' for directories. Files have no children array, while directories always have a children array (which may be empty). The output is formatted with 2-space indentation for readability. Only works within allowed directories.", "parameters": {"type": "object", "properties": {"path": {"type": "string"}}, "required": ["path"], "additionalProperties": false, "$schema": "http://json-schema.org/draft-07/schema#"}}}, {"type": "function", "function": {"name": "filesystem-move_file", "description": "Move or rename files and directories. Can move files between directories and rename them in a single operation. If the destination exists, the operation will fail. Works across different directories and can be used for simple renaming within the same directory. Both source and destination must be within allowed directories.", "parameters": {"type": "object", "properties": {"source": {"type": "string"}, "destination": {"type": "string"}}, "required": ["source", "destination"], "additionalProperties": false, "$schema": "http://json-schema.org/draft-07/schema#"}}}, {"type": "function", "function": {"name": "filesystem-search_files", "description": "Recursively search for files and directories matching a pattern. Searches through all subdirectories from the starting path. The search is case-insensitive and matches partial names. Returns full paths to all matching items. Great for finding files when you don't know their exact location. Only searches within allowed directories.", "parameters": {"type": "object", "properties": {"path": {"type": "string"}, "pattern": {"type": "string"}, "excludePatterns": {"type": "array", "items": {"type": "string"}, "default": []}}, "required": ["path", "pattern"], "additionalProperties": false, "$schema": "http://json-schema.org/draft-07/schema#"}}}, {"type": "function", "function": {"name": "filesystem-get_file_info", "description": "Retrieve detailed metadata about a file or directory. Returns comprehensive information including size, creation time, last modified time, permissions, and type. This tool is perfect for understanding file characteristics without reading the actual content. Only works within allowed directories.", "parameters": {"type": "object", "properties": {"path": {"type": "string"}}, "required": ["path"], "additionalProperties": false, "$schema": "http://json-schema.org/draft-07/schema#"}}}, {"type": "function", "function": {"name": "filesystem-list_allowed_directories", "description": "Returns the list of directories that this server is allowed to access. Use this to understand which directories are available before trying to access files.", "parameters": {"type": "object", "properties": {}, "required": []}}}, {"type": "function", "function": {"name": "fetch-fetch_html", "description": "Fetch a website and return the content as HTML", "parameters": {"type": "object", "properties": {"url": {"type": "string", "description": "URL of the website to fetch"}, "headers": {"type": "object", "description": "Optional headers to include in the request"}}, "required": ["url"]}}}, {"type": "function", "function": {"name": "fetch-fetch_markdown", "description": "Fetch a website and return the content as Markdown", "parameters": {"type": "object", "properties": {"url": {"type": "string", "description": "URL of the website to fetch"}, "headers": {"type": "object", "description": "Optional headers to include in the request"}}, "required": ["url"]}}}, {"type": "function", "function": {"name": "fetch-fetch_txt", "description": "Fetch a website, return the content as plain text (no HTML)", "parameters": {"type": "object", "properties": {"url": {"type": "string", "description": "URL of the website to fetch"}, "headers": {"type": "object", "description": "Optional headers to include in the request"}}, "required": ["url"]}}}, {"type": "function", "function": {"name": "fetch-fetch_json", "description": "Fetch a JSON file from a URL", "parameters": {"type": "object", "properties": {"url": {"type": "string", "description": "URL of the JSON to fetch"}, "headers": {"type": "object", "description": "Optional headers to include in the request"}}, "required": ["url"]}}}, {"type": "function", "function": {"name": "arxiv_local-search_papers", "description": "Search for papers on arXiv with advanced filtering", "parameters": {"type": "object", "properties": {"query": {"type": "string"}, "max_results": {"type": "integer"}, "date_from": {"type": "string"}, "date_to": {"type": "string"}, "categories": {"type": "array", "items": {"type": "string"}}}, "required": ["query"]}}}, {"type": "function", "function": {"name": "arxiv_local-download_paper", "description": "Download a paper and create a resource for it", "parameters": {"type": "object", "properties": {"paper_id": {"type": "string", "description": "The arXiv ID of the paper to download"}, "check_status": {"type": "boolean", "description": "If true, only check conversion status without downloading", "default": false}}, "required": ["paper_id"]}}}, {"type": "function", "function": {"name": "arxiv_local-list_papers", "description": "List all existing papers available as resources", "parameters": {"type": "object", "properties": {}, "required": []}}}, {"type": "function", "function": {"name": "arxiv_local-read_paper", "description": "Read the full content of a stored paper in markdown format", "parameters": {"type": "object", "properties": {"paper_id": {"type": "string", "description": "The arXiv ID of the paper to read"}}, "required": ["paper_id"]}}}, {"type": "function", "function": {"name": "pdf-tools-read_pdf_pages", "description": "Read content from PDF file for specified page range.\n\nSupports both local file paths and URLs. For URLs, the PDF will be downloaded\nto a temporary directory and cached for future use.\n\nNote: Avoid reading too many pages at once (recommended: <50 pages) to prevent errors.\n\nArgs:\n    pdf_file_path: Path to the PDF file or URL to PDF\n    start_page: Starting page number (default: 1)\n    end_page: Ending page number (default: 1)\n    \nReturns:\n    Extracted text content from the specified pages\n", "parameters": {"properties": {"pdf_file_path": {"title": "Pdf File Path", "type": "string"}, "start_page": {"default": 1, "title": "Start Page", "type": "integer"}, "end_page": {"default": 1, "title": "End Page", "type": "integer"}}, "required": ["pdf_file_path"], "title": "read_pdf_pagesArguments", "type": "object"}}}, {"type": "function", "function": {"name": "pdf-tools-get_pdf_info", "description": "Get basic information about a PDF file including page count.\n\nSupports both local file paths and URLs. For URLs, the PDF will be downloaded\nto a temporary directory and cached for future use.\n\nArgs:\n    pdf_file_path: Path to the PDF file or URL to PDF\n    \nReturns:\n    Basic information about the PDF file\n", "parameters": {"properties": {"pdf_file_path": {"title": "Pdf File Path", "type": "string"}}, "required": ["pdf_file_path"], "title": "get_pdf_infoArguments", "type": "object"}}}, {"type": "function", "function": {"name": "pdf-tools-merge_pdfs", "description": "Merge multiple PDF files into one.\n\nSupports both local file paths and URLs. URLs will be downloaded to temporary\nfiles before merging. Mixed local and URL paths are supported.\n\nArgs:\n    pdf_paths: List of paths to PDF files to merge (in order) - can include URLs\n    output_path: Path where the merged PDF will be saved (must be local path)\n    \nReturns:\n    Success message with merge details or error message\n", "parameters": {"properties": {"pdf_paths": {"items": {"type": "string"}, "title": "Pdf Paths", "type": "array"}, "output_path": {"title": "Output Path", "type": "string"}}, "required": ["pdf_paths", "output_path"], "title": "merge_pdfsArguments", "type": "object"}}}, {"type": "function", "function": {"name": "pdf-tools-extract_pdf_pages", "description": "Extract specific pages from a PDF and create a new PDF.\n\nSupports URLs for source PDF. The source PDF will be downloaded to a temporary\ndirectory if it's a URL. Output path must be a local file path.\n\nArgs:\n    source_path: Path to the source PDF file or URL to PDF\n    page_numbers: List of page numbers to extract (1-indexed)\n    output_path: Path where the new PDF will be saved (must be local path)\n    \nReturns:\n    Success message with extraction details or error message\n", "parameters": {"properties": {"source_path": {"title": "Source Path", "type": "string"}, "page_numbers": {"items": {"type": "integer"}, "title": "Page Numbers", "type": "array"}, "output_path": {"title": "Output Path", "type": "string"}}, "required": ["source_path", "page_numbers", "output_path"], "title": "extract_pdf_pagesArguments", "type": "object"}}}, {"type": "function", "function": {"name": "pdf-tools-search_pdf_content", "description": "Search for regex pattern in PDF content and return paginated results.\n\nSupports both local file paths and URLs. For URLs, the PDF will be downloaded\nto a temporary directory and cached for future use.\n\nArgs:\n    pdf_file_path: Path to the PDF file or URL to PDF\n    pattern: Regular expression pattern to search for\n    page_size: Number of results per page (10-50, default: 10)\n\nReturns:\n    Search results with UUID for pagination, or error message\n", "parameters": {"properties": {"pdf_file_path": {"title": "Pdf File Path", "type": "string"}, "pattern": {"title": "Pattern", "type": "string"}, "page_size": {"default": 10, "title": "Page Size", "type": "integer"}}, "required": ["pdf_file_path", "pattern"], "title": "search_pdf_contentArguments", "type": "object"}}}, {"type": "function", "function": {"name": "pdf-tools-search_pdf_next_page", "description": "Get next page of search results.\n\nArgs:\n    search_id: Search session ID from previous search\n\nReturns:\n    Next page of search results or error message\n", "parameters": {"properties": {"search_id": {"title": "Search Id", "type": "string"}}, "required": ["search_id"], "title": "search_pdf_next_pageArguments", "type": "object"}}}, {"type": "function", "function": {"name": "pdf-tools-search_pdf_prev_page", "description": "Get previous page of search results.\n\nArgs:\n    search_id: Search session ID from previous search\n\nReturns:\n    Previous page of search results or error message\n", "parameters": {"properties": {"search_id": {"title": "Search Id", "type": "string"}}, "required": ["search_id"], "title": "search_pdf_prev_pageArguments", "type": "object"}}}, {"type": "function", "function": {"name": "pdf-tools-search_pdf_go_page", "description": "Go to specific page of search results.\n\nArgs:\n    search_id: Search session ID from previous search\n    page_number: Page number to go to (1-indexed)\n\nReturns:\n    Specified page of search results or error message\n", "parameters": {"properties": {"search_id": {"title": "Search Id", "type": "string"}, "page_number": {"title": "Page Number", "type": "integer"}}, "required": ["search_id", "page_number"], "title": "search_pdf_go_pageArguments", "type": "object"}}}, {"type": "function", "function": {"name": "pdf-tools-search_pdf_info", "description": "Get information about a search session.\n\nArgs:\n    search_id: Search session ID from previous search\n\nReturns:\n    Information about the search session\n", "parameters": {"properties": {"search_id": {"title": "Search Id", "type": "string"}}, "required": ["search_id"], "title": "search_pdf_infoArguments", "type": "object"}}}, {"type": "function", "function": {"name": "scholarly-search-arxiv", "description": "Search arxiv for articles related to the given keyword.", "parameters": {"type": "object", "properties": {"keyword": {"type": "string"}}, "required": ["keyword"]}}}, {"type": "function", "function": {"name": "scholarly-search-google-scholar", "description": "Search google scholar for articles related to the given keyword.", "parameters": {"type": "object", "properties": {"keyword": {"type": "string"}}, "required": ["keyword"]}}}, {"type": "function", "function": {"name": "local-web_search", "description": "Search the web using Google Serper API with concurrency control and retry mechanisms. Supports various Google search operators.", "parameters": {"type": "object", "properties": {"query": {"type": "string", "description": "Search query with optional Google search operators."}, "num_results": {"type": "integer", "description": "Number of results to return, default 10, max 50", "default": 10, "minimum": 1, "maximum": 50}}, "required": ["query"]}}}, {"type": "function", "function": {"name": "local-claim_done", "description": "claim the task is done", "parameters": {"type": "object", "properties": {}}}}, {"type": "function", "function": {"name": "local-search_overlong_tooloutput", "description": "Search within overlong tool output content using regex patterns and return first page with session ID", "parameters": {"type": "object", "properties": {"shortuuid": {"type": "string", "description": "The shortuuid identifier for the overlong tool output"}, "pattern": {"type": "string", "description": "The regex pattern to search for in the content"}, "page_size": {"type": "integer", "description": "Number of matches per page (default: 10, max: 50)", "minimum": 1, "maximum": 50}, "context_size": {"type": "integer", "description": "Characters of context around each match (default: 1000)", "minimum": 100, "maximum": 5000}}, "required": ["shortuuid", "pattern"]}}}, {"type": "function", "function": {"name": "local-search_overlong_tooloutput_navigate", "description": "Navigate through search results using search session ID", "parameters": {"type": "object", "properties": {"search_session_id": {"type": "string", "description": "The search session ID returned from search_overlong_tool"}, "action": {"type": "string", "description": "Navigation action to perform", "enum": ["next_page", "prev_page", "jump_to_page", "first_page", "last_page"]}, "target_page": {"type": "integer", "description": "Target page number (required for jump_to_page action)", "minimum": 1}}, "required": ["search_session_id"]}}}, {"type": "function", "function": {"name": "local-view_overlong_tooloutput", "description": "View overlong tool output content with pagination and return first page with session ID", "parameters": {"type": "object", "properties": {"shortuuid": {"type": "string", "description": "The shortuuid identifier for the overlong tool output"}, "page_size": {"type": "integer", "description": "Number of characters per page (default: 10000, max: 100000)", "minimum": 1, "maximum": 100000}}, "required": ["shortuuid"]}}}, {"type": "function", "function": {"name": "local-view_overlong_tooloutput_navigate", "description": "Navigate through view content using view session ID", "parameters": {"type": "object", "properties": {"view_session_id": {"type": "string", "description": "The view session ID returned from view_overlong_tool"}, "action": {"type": "string", "description": "Navigation action to perform", "enum": ["next_page", "prev_page", "jump_to_page", "first_page", "last_page"]}, "target_page": {"type": "integer", "description": "Target page number (required for jump_to_page action)", "minimum": 1}}, "required": ["view_session_id"]}}}, {"type": "function", "function": {"name": "local-check_context_status", "description": "Query current conversation context status, including turn statistics, token usage, truncation history and other information", "parameters": {"type": "object", "properties": {}, "required": []}}}, {"type": "function", "function": {"name": "local-manage_context", "description": "Manage conversation context by deleting historical messages to free up space. Supports multiple strategies:\n- keep_recent_turns: Keep the most recent N turns of conversation\n- keep_recent_percent: Keep the most recent X% of conversation  \n- delete_first_turns: Delete the earliest N turns of conversation\n- delete_first_percent: Delete the earliest X% of conversation", "parameters": {"type": "object", "properties": {"action": {"type": "string", "enum": ["truncate"], "description": "Operation to execute, currently only supports truncate", "default": "truncate"}, "method": {"type": "string", "enum": ["keep_recent_turns", "keep_recent_percent", "delete_first_turns", "delete_first_percent"], "description": "Truncation strategy"}, "value": {"type": "number", "description": "Numeric parameter, for turns methods it's number of turns, for percent methods it's percentage (0-100)", "minimum": 0}, "preserve_system": {"type": "boolean", "description": "Whether to preserve system messages", "default": true}}, "required": ["method", "value"]}}}, {"type": "function", "function": {"name": "local-smart_context_truncate", "description": "Smart context truncation tool that precisely controls retained content by specifying ranges.\nAccepts 2D list [[start1,end1],[start2,end2],...,[startN,endN]], each sublist represents a closed range to retain (both ends included).\nIndexing starts from 0, ranges cannot overlap, must be arranged in order.", "parameters": {"type": "object", "properties": {"ranges": {"type": "array", "description": "List of ranges to retain, format: [[start1,end1],[start2,end2],...], indexing starts from 0", "items": {"type": "array", "minItems": 2, "maxItems": 2, "items": {"type": "integer", "minimum": 0}}, "minItems": 1}, "preserve_system": {"type": "boolean", "description": "Whether to preserve system messages", "default": true}}, "required": ["ranges"]}}}, {"type": "function", "function": {"name": "local-search_history", "description": "Search history conversation records. Support multiple keyword search or regular expression search, return records containing all keywords. Support paging to browse all results.", "parameters": {"type": "object", "properties": {"keywords": {"type": "array", "items": {"type": "string"}, "description": "Search keyword list or regular expression list, will find records matching all patterns"}, "use_regex": {"type": "boolean", "description": "Whether to treat keywords as regular expressions", "default": false}, "page": {"type": "integer", "description": "Page number, starting from 1", "default": 1, "minimum": 1}, "per_page": {"type": "integer", "description": "Number of results per page", "default": 10, "minimum": 1, "maximum": 50}, "search_id": {"type": "string", "description": "Continue previous search (for paging)"}}, "required": []}}}, {"type": "function", "function": {"name": "local-view_history_turn", "description": "View the complete conversation content of a specific turn, including the context of previous and subsequent turns. Support content truncation to view long content.", "parameters": {"type": "object", "properties": {"turn": {"type": "integer", "description": "Turn number to view", "minimum": 0}, "context_turns": {"type": "integer", "description": "Display the context of previous and subsequent turns", "default": 2, "minimum": 0, "maximum": 10}, "truncate": {"type": "boolean", "description": "Whether to truncate long content (keep the first 500 and last 500 characters)", "default": true}}, "required": ["turn"]}}}, {"type": "function", "function": {"name": "local-browse_history", "description": "Browse history records in chronological order, support forward or backward browsing. Can choose whether to truncate long content.", "parameters": {"type": "object", "properties": {"start_turn": {"type": "integer", "description": "Start turn (inclusive), default from earliest", "minimum": 0}, "end_turn": {"type": "integer", "description": "End turn (inclusive), default to latest", "minimum": 0}, "limit": {"type": "integer", "description": "Maximum number of turns returned", "default": 20, "minimum": 1, "maximum": 100}, "direction": {"type": "string", "enum": ["forward", "backward"], "description": "Browse direction: forward from early to late, backward from late to early", "default": "forward"}, "truncate": {"type": "boolean", "description": "Whether to truncate long content display", "default": true}}, "required": []}}}, {"type": "function", "function": {"name": "local-history_stats", "description": "Get statistics of history records, including total turns, time range, message type distribution, etc.", "parameters": {"type": "object", "properties": {}, "required": []}}}, {"type": "function", "function": {"name": "local-search_in_turn", "description": "Search content within a specific turn, support regular expressions. Used to find specific information in long content (such as tool output).", "parameters": {"type": "object", "properties": {"turn": {"type": "integer", "description": "Turn number to search", "minimum": 0}, "pattern": {"type": "string", "description": "Search pattern (support regular expressions)"}, "page": {"type": "integer", "description": "Page number, starting from 1", "default": 1, "minimum": 1}, "per_page": {"type": "integer", "description": "Number of results per page", "default": 10, "minimum": 1, "maximum": 20}, "search_id": {"type": "string", "description": "Search session ID (for paging)"}, "jump_to": {"oneOf": [{"type": "string", "enum": ["first", "last", "next", "prev"]}, {"type": "integer", "minimum": 1}], "description": "Jump to: 'first'(first page), 'last'(last page), 'next'(next page), 'prev'(previous page), or specific page number"}}, "required": ["turn"]}}}], "tool_choice": "auto"}, "status": "success", "messages": [{"role": "user", "content": "Hello! I am a PhD student engaged in deep learning research. Recently, my collaborators and I have proposed a logical reasoning dataset and are writing a paper to introduce this new dataset to the community. To demonstrate the differences between our work and previous work, I need you to help me summarize a latex table and save it in the workspace with name as `datasets.tex`, the tex file should only contain table content (no commented lines), without any other content. The table needs to include four columns: Dataset (dataset name), Tasks (number of tasks included in the dataset), Trainable (whether it includes a training set, filled with \\ding{55} or \\ding{51}), and Adjustable Difficulty (whether it includes different difficulty levels, filled with \\ding{55} or \\ding{51}). The table format is as follows:\n\n```tex\n\\begin{table}[!ht]\n    \\begin{center}\n    \\begin{tabular}{lccc}\n        \\toprule\n        Dataset & Tasks & Trainable & Adjustable Difficulty\\\\\n        \\midrule\n        % content\n        \\bottomrule\n    \\end{tabular}\n  \\end{center}\n\\end{table}\n```\n\nThe names of the datasets we need to include are (from top to bottom):\n- BBH\n- Zebra Logic\n- KOR-Bench (# of tasks should based the broader categorization in the paper.)\n- K&K (https://arxiv.org/abs/2410.23123)\n- BBEH"}, {"role": "assistant", "content": "I'll help you create a comprehensive table summarizing these logical reasoning datasets. Let me search for information about each dataset to gather the required details.", "tool_calls": [{"id": "toolu_01KNeKSqNUm6RCbspBtYP6dk", "type": "function", "function": {"name": "scholarly-search-arxiv", "arguments": "{\"keyword\": \"BBH Big-Bench Hard logical reasoning\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"Search articles for BBH Big-Bench Hard logical reasoning:\\nTitle: Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting\\nAuthors: Rylan Schaeffer, Kateryna Pistunova, Samar Khanna, Sarthak Consul, Sanmi Koyejo\\nPublished: 2023-07-20\\nUpdated: 2023-07-23\\nPrimary Category: cs.AI\\nAll Categories: cs.AI\\nDOI: N/A\\nJournal Reference: N/A\\nComment: ICML 2023 Workshop: Knowledge and Logical Reasoning in the Era of\\n  Data-driven Learning\\nEntry ID: http://arxiv.org/abs/2307.10573v2\\nPDF URL: http://arxiv.org/pdf/2307.10573v2\\nAll Links: http://arxiv.org/abs/2307.10573v2||http://arxiv.org/pdf/2307.10573v2\\nSummary: Language models can be prompted to reason through problems in a manner that\\nsignificantly improves performance. However, \\\\textit{why} such prompting\\nimproves performance is unclear. Recent work showed that using logically\\n\\\\textit{invalid} Chain-of-Thought (CoT) prompting improves performance almost\\nas much as logically \\\\textit{valid} CoT prompting, and that editing CoT prompts\\nto replace problem-specific information with abstract information or\\nout-of-distribution information typically doesn't harm performance. Critics\\nhave responded that these findings are based on too few and too easily solved\\ntasks to draw meaningful conclusions. To resolve this dispute, we test whether\\nlogically invalid CoT prompts offer the same level of performance gains as\\nlogically valid prompts on the hardest tasks in the BIG-Bench benchmark, termed\\nBIG-Bench Hard (BBH). We find that the logically \\\\textit{invalid} reasoning\\nprompts do indeed achieve similar performance gains on BBH tasks as logically\\nvalid reasoning prompts. We also discover that some CoT prompts used by\\nprevious works contain logical errors. This suggests that covariates beyond\\nlogically valid reasoning are responsible for performance improvements.\\n\\n\\nTitle: AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models\\nAuthors: Yanxi Zhang, Xin Cong, Zhong Zhang, Xiao Liu, Dongyan Zhao, Yesai Wu\\nPublished: 2025-05-13\\nUpdated: 2025-05-13\\nPrimary Category: cs.CL\\nAll Categories: cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2505.08750v1\\nPDF URL: http://arxiv.org/pdf/2505.08750v1\\nAll Links: http://arxiv.org/abs/2505.08750v1||http://arxiv.org/pdf/2505.08750v1\\nSummary: Actual causality (AC), a fundamental aspect of causal reasoning (CR), is\\nresponsible for attribution and responsibility assignment in real-world\\nscenarios. However, existing LLM-based methods lack grounding in formal AC\\ntheory, resulting in limited interpretability. Therefore, we propose AC-Reason,\\na semi-formal reasoning framework that identifies causally relevant events\\nwithin an AC scenario, infers the values of their formal causal factors (e.g.,\\nsufficiency, necessity, and normality), and answers AC queries via a\\ntheory-guided algorithm with explanations. While AC-Reason does not explicitly\\nconstruct a causal graph, it operates over variables in the underlying causal\\nstructure to support principled reasoning. To enable comprehensive evaluation,\\nwe introduce AC-Bench, a new benchmark built upon and substantially extending\\nBig-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully\\nannotated samples, each with detailed reasoning steps and focuses solely on\\nactual causation. The case study shows that synthesized samples in AC-Bench\\npresent greater challenges for LLMs. Extensive experiments on BBH-CJ and\\nAC-Bench show that AC-Reason consistently improves LLM performance over\\nbaselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy\\nof 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 +\\nAC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further\\nenables fine-grained analysis of reasoning faithfulness, revealing that only\\nQwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful\\nreasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation\\nstudy proves that integrating AC theory into LLMs is highly effective, with the\\nproposed algorithm contributing the most significant performance gains.\\n\\n\\nTitle: Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus\\nAuthors: Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, Yasuhiro Sogawa\\nPublished: 2024-11-19\\nUpdated: 2024-12-23\\nPrimary Category: cs.LG\\nAll Categories: cs.LG, cs.AI, cs.LO\\nDOI: N/A\\nJournal Reference: N/A\\nComment: NeurIPS 2024\\nEntry ID: http://arxiv.org/abs/2411.12498v2\\nPDF URL: http://arxiv.org/pdf/2411.12498v2\\nAll Links: http://arxiv.org/abs/2411.12498v2||http://arxiv.org/pdf/2411.12498v2\\nSummary: Large language models (LLMs) are capable of solving a wide range of tasks,\\nyet they have struggled with reasoning. To address this, we propose\\n$\\\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs'\\nreasoning capabilities by program-generated logical reasoning samples. We first\\nestablish principles for designing high-quality samples by integrating symbolic\\nlogic theory and previous empirical insights. Then, based on these principles,\\nwe construct a synthetic corpus named $\\\\textbf{Formal Logic Deduction Diverse}$\\n($\\\\textbf{FLD}$$_{\\\\times 2}$), comprising numerous samples of multi-step\\ndeduction with unknown facts, diverse reasoning rules, diverse linguistic\\nexpressions, and challenging distractors. Finally, we empirically show that ALT\\non FLD$_{\\\\times2}$ substantially enhances the reasoning capabilities of\\nstate-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of\\nup to 30 points on logical reasoning benchmarks, up to 10 points on math and\\ncoding benchmarks, and 5 points on the benchmark suite BBH.\\n\\n\\nTitle: BIG-Bench Extra Hard\\nAuthors: Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, Orhan Firat\\nPublished: 2025-02-26\\nUpdated: 2025-05-06\\nPrimary Category: cs.CL\\nAll Categories: cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2502.19187v2\\nPDF URL: http://arxiv.org/pdf/2502.19187v2\\nAll Links: http://arxiv.org/abs/2502.19187v2||http://arxiv.org/pdf/2502.19187v2\\nSummary: Large language models (LLMs) are increasingly deployed in everyday\\napplications, demanding robust general reasoning capabilities and diverse\\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\\nwhich has served as a crucial benchmark for evaluating the general reasoning\\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\\nallowed for a comprehensive assessment of general reasoning across various\\nskills within a unified framework. However, recent advances in LLMs have led to\\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\\nprobes a similar reasoning capability but exhibits significantly increased\\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\\naccuracy of 9.8\\\\% for the best general-purpose model and 44.8\\\\% for the best\\nreasoning-specialized model, indicating substantial room for improvement and\\nhighlighting the ongoing challenge of achieving robust general reasoning in\\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.\\n\\n\\nTitle: Gravitational waves from BBH-systems? A (doubly) vain quest\\nAuthors: A. Loinger\\nPublished: 2006-02-06\\nUpdated: 2006-02-06\\nPrimary Category: physics.gen-ph\\nAll Categories: physics.gen-ph\\nDOI: N/A\\nJournal Reference: N/A\\nComment: 7 pages, LaTeX, to be published on Spacetime & Substance\\nEntry ID: http://arxiv.org/abs/physics/0602040v1\\nPDF URL: http://arxiv.org/pdf/physics/0602040v1\\nAll Links: http://arxiv.org/abs/physics/0602040v1||http://arxiv.org/pdf/physics/0602040v1\\nSummary: The theoretical reasons at the root of LIGO's experimental failure in\\nsearching gravitational waves (GW's) from binary black hole (BBH) inspirals.\\n\\n\\nTitle: Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them\\nAuthors: Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, Jason Wei\\nPublished: 2022-10-17\\nUpdated: 2022-10-17\\nPrimary Category: cs.CL\\nAll Categories: cs.CL, cs.AI\\nDOI: N/A\\nJournal Reference: N/A\\nComment: GitHub repository: https://github.com/suzgunmirac/BIG-Bench-Hard\\nEntry ID: http://arxiv.org/abs/2210.09261v1\\nPDF URL: http://arxiv.org/pdf/2210.09261v1\\nAll Links: http://arxiv.org/abs/2210.09261v1||http://arxiv.org/pdf/2210.09261v1\\nSummary: BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that\\nfocuses on tasks believed to be beyond the capabilities of current language\\nmodels. Language models have already made good progress on this benchmark, with\\nthe best model in the BIG-Bench paper outperforming average reported\\nhuman-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But\\non what tasks do language models fall short of average human-rater performance,\\nand are those tasks actually unsolvable by current language models?\\n  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we\\ncall BIG-Bench Hard (BBH). These are the task for which prior language model\\nevaluations did not outperform the average human-rater. We find that applying\\nchain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the\\naverage human-rater performance on 10 of the 23 tasks, and Codex\\n(code-davinci-002) to surpass the average human-rater performance on 17 of the\\n23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot\\nprompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al.,\\n2022), substantially underestimates the best performance and capabilities of\\nlanguage models, which is better captured via CoT prompting. As further\\nanalysis, we explore the interaction between CoT and model scale on BBH,\\nfinding that CoT enables emergent task performance on several BBH tasks with\\notherwise flat scaling curves.\\n\\n\\nTitle: Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search\\nAuthors: Dongge Han, Menglin Xia, Daniel Madrigal Diaz, Samuel Kessler, Ankur Mallick, Xuchao Zhang, Mirian Del Carmen Hipolito Garcia, Jin Xu, Victor R\u00fchle, Saravan Rajmohan\\nPublished: 2025-06-10\\nUpdated: 2025-06-10\\nPrimary Category: cs.LG\\nAll Categories: cs.LG, cs.AI\\nDOI: N/A\\nJournal Reference: N/A\\nComment: TTODLer-FM Workshop@ICML'25 (Tiny Titans: The next wave of On-Device\\n  Learning for Foundational Models)\\nEntry ID: http://arxiv.org/abs/2506.08669v1\\nPDF URL: http://arxiv.org/pdf/2506.08669v1\\nAll Links: http://arxiv.org/abs/2506.08669v1||http://arxiv.org/pdf/2506.08669v1\\nSummary: Small language models (SLMs) offer promising and efficient alternatives to\\nlarge language models (LLMs). However, SLMs' limited capacity restricts their\\nreasoning capabilities and makes them sensitive to prompt variations. To\\naddress these challenges, we propose a novel framework that enhances SLM\\nreasoning capabilities through LLM generated blueprints. The blueprints provide\\nstructured, high-level reasoning guides that help SLMs systematically tackle\\nrelated problems. Furthermore, our framework integrates a prompt template\\nsearch mechanism to mitigate the SLMs' sensitivity to prompt variations. Our\\nframework demonstrates improved SLM performance across various tasks, including\\nmath (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves\\nthe reasoning capabilities of SLMs without increasing model size or requiring\\nadditional training, offering a lightweight and deployment-friendly solution\\nfor on-device or resource-constrained environments.\\n\\n\\nTitle: Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text\\nAuthors: Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, Nan Duan\\nPublished: 2021-05-08\\nUpdated: 2021-05-08\\nPrimary Category: cs.CL\\nAll Categories: cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: 10 pages, 4 figures\\nEntry ID: http://arxiv.org/abs/2105.03659v1\\nPDF URL: http://arxiv.org/pdf/2105.03659v1\\nAll Links: http://arxiv.org/abs/2105.03659v1||http://arxiv.org/pdf/2105.03659v1\\nSummary: Logical reasoning of text requires understanding critical logical information\\nin the text and performing inference over them. Large-scale pre-trained models\\nfor logical reasoning mainly focus on word-level semantics of text while\\nstruggling to capture symbolic logic. In this paper, we propose to understand\\nlogical symbols and expressions in the text to arrive at the answer. Based on\\nsuch logical information, we not only put forward a context extension framework\\nbut also propose a data augmentation algorithm. The former extends the context\\nto cover implicit logical expressions following logical equivalence laws. The\\nlatter augments literally similar but logically different instances to better\\ncapture logical information, especially logical negative and conditional\\nrelationships. We conduct experiments on ReClor dataset. The results show that\\nour method achieves the state-of-the-art performance, and both logic-driven\\ncontext extension framework and data augmentation algorithm can help improve\\nthe accuracy. And our multi-model ensemble system is the first to surpass human\\nperformance on both EASY set and HARD set of ReClor.\\n\\n\\nTitle: Evaluating the Logical Reasoning Abilities of Large Reasoning Models\\nAuthors: Hanmeng Liu, Yiran Ding, Zhizhang Fu, Chaoli Zhang, Xiaozhang Liu, Yue Zhang\\nPublished: 2025-05-17\\nUpdated: 2025-05-17\\nPrimary Category: cs.AI\\nAll Categories: cs.AI\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2505.11854v1\\nPDF URL: http://arxiv.org/pdf/2505.11854v1\\nAll Links: http://arxiv.org/abs/2505.11854v1||http://arxiv.org/pdf/2505.11854v1\\nSummary: Large reasoning models, often post-trained on long chain-of-thought (long\\nCoT) data with reinforcement learning, achieve state-of-the-art performance on\\nmathematical, coding, and domain-specific reasoning benchmarks. However, their\\nlogical reasoning capabilities - fundamental to human cognition and independent\\nof domain knowledge - remain understudied. To address this gap, we introduce\\nLogiEval, a holistic benchmark for evaluating logical reasoning in large\\nreasoning models. LogiEval spans diverse reasoning types (deductive, inductive,\\nanalogical, and abductive) and task formats (e.g., logical sequence, argument\\nanalysis), sourced from high-quality human examinations (e.g., LSAT, GMAT). Our\\nexperiments demonstrate that modern reasoning models excel at 4-choice argument\\nanalysis problems and analogical reasoning, surpassing human performance, yet\\nexhibit uneven capabilities across reasoning types and formats, highlighting\\nlimitations in their generalization. Our analysis reveals that human\\nperformance does not mirror model failure distributions. To foster further\\nresearch, we curate LogiEval-Hard, a challenging subset identified through a\\nnovel screening paradigm where small-model failures (Qwen3-30B-A3B) reliably\\npredict difficulties for larger models. Modern models show striking, consistent\\nfailures on LogiEval-Hard. This demonstrates that fundamental reasoning\\nbottlenecks persist across model scales, and establishes LogiEval-Hard as both\\na diagnostic tool and a rigorous testbed for advancing logical reasoning in\\nLLMs.\\n\\n\\nTitle: Training Step-Level Reasoning Verifiers with Formal Verification Tools\\nAuthors: Ryo Kamoi, Yusen Zhang, Nan Zhang, Sarkar Snigdha Sarathi Das, Rui Zhang\\nPublished: 2025-05-21\\nUpdated: 2025-05-21\\nPrimary Category: cs.CL\\nAll Categories: cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: Datasets, models, and code are provided at\\n  https://github.com/psunlpgroup/FoVer. Please also refer to our project\\n  website at https://fover-prm.github.io/\\nEntry ID: http://arxiv.org/abs/2505.15960v1\\nPDF URL: http://arxiv.org/pdf/2505.15960v1\\nAll Links: http://arxiv.org/abs/2505.15960v1||http://arxiv.org/pdf/2505.15960v1\\nSummary: Process Reward Models (PRMs), which provide step-by-step feedback on the\\nreasoning generated by Large Language Models (LLMs), are receiving increasing\\nattention. However, two key research gaps remain: collecting accurate\\nstep-level error labels for training typically requires costly human\\nannotation, and existing PRMs are limited to math reasoning problems. In\\nresponse to these gaps, this paper aims to address the challenges of automatic\\ndataset creation and the generalization of PRMs to diverse reasoning tasks. To\\nachieve this goal, we propose FoVer, an approach for training PRMs on\\nstep-level error labels automatically annotated by formal verification tools,\\nsuch as Z3 for formal logic and Isabelle for theorem proof, which provide\\nautomatic and accurate verification for symbolic tasks. Using this approach, we\\nsynthesize a training dataset with error labels on LLM responses for formal\\nlogic and theorem proof tasks without human annotation. Although this data\\nsynthesis is feasible only for tasks compatible with formal verification, we\\nobserve that LLM-based PRMs trained on our dataset exhibit cross-task\\ngeneralization, improving verification across diverse reasoning tasks.\\nSpecifically, PRMs trained with FoVer significantly outperform baseline PRMs\\nbased on the original LLMs and achieve competitive or superior results compared\\nto state-of-the-art PRMs trained on labels annotated by humans or stronger\\nmodels, as measured by step-level verification on ProcessBench and Best-of-K\\nperformance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU,\\nand BBH. The datasets, models, and code are provided at\\nhttps://github.com/psunlpgroup/FoVer.\",\"annotations\":null}", "tool_call_id": "toolu_01KNeKSqNUm6RCbspBtYP6dk"}, {"role": "assistant", "content": null, "tool_calls": [{"id": "toolu_01Uwir8XwN6aYmXbUmy818Mm", "type": "function", "function": {"name": "scholarly-search-arxiv", "arguments": "{\"keyword\": \"Zebra Logic puzzle reasoning dataset\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"Search articles for Zebra Logic puzzle reasoning dataset:\\nTitle: Measuring reasoning capabilities of ChatGPT\\nAuthors: Adrian Groza\\nPublished: 2023-10-08\\nUpdated: 2023-10-08\\nPrimary Category: cs.AI\\nAll Categories: cs.AI\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2310.05993v1\\nPDF URL: http://arxiv.org/pdf/2310.05993v1\\nAll Links: http://arxiv.org/abs/2310.05993v1||http://arxiv.org/pdf/2310.05993v1\\nSummary: I shall quantify the logical faults generated by ChatGPT when applied to\\nreasoning tasks. For experiments, I use the 144 puzzles from the library\\n\\\\url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\\\\cite{groza:fol}. The\\nlibrary contains puzzles of various types, including arithmetic puzzles,\\nlogical equations, Sudoku-like puzzles, zebra-like puzzles, truth-telling\\npuzzles, grid puzzles, strange numbers, or self-reference puzzles. The correct\\nsolutions for these puzzles were checked using the theorem prover\\nProver9~\\\\cite{mccune2005release} and the finite models finder\\nMace4~\\\\cite{mccune2003mace4} based on human-modelling in Equational First Order\\nLogic. A first output of this study is the benchmark of 100 logical puzzles.\\nFor this dataset ChatGPT provided both correct answer and justification for 7\\\\%\\nonly. %, while BARD for 5\\\\%. Since the dataset seems challenging, the\\nresearchers are invited to test the dataset on more advanced or tuned models\\nthan ChatGPT3.5 with more crafted prompts. A second output is the\\nclassification of reasoning faults conveyed by ChatGPT. This classification\\nforms a basis for a taxonomy of reasoning faults generated by large language\\nmodels. I have identified 67 such logical faults, among which: inconsistencies,\\nimplication does not hold, unsupported claim, lack of commonsense, wrong\\njustification. The 100 solutions generated by ChatGPT contain 698 logical\\nfaults. That is on average, 7 fallacies for each reasoning task. A third ouput\\nis the annotated answers of the ChatGPT with the corresponding logical faults.\\nEach wrong statement within the ChatGPT answer was manually annotated, aiming\\nto quantify the amount of faulty text generated by the language model. On\\naverage, 26.03\\\\% from the generated text was a logical fault.\\n\\n\\nTitle: Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning\\nAuthors: Ang Li, Charles Wang, Kaiyu Yue, Zikui Cai, Ollie Liu, Deqing Fu, Peng Guo, Wang Bill Zhu, Vatsal Sharan, Robin Jia, Willie Neiswanger, Furong Huang, Tom Goldstein, Micah Goldblum\\nPublished: 2025-07-22\\nUpdated: 2025-07-22\\nPrimary Category: cs.CV\\nAll Categories: cs.CV, cs.CL, cs.LG\\nDOI: N/A\\nJournal Reference: N/A\\nComment: dataset link:\\n  https://huggingface.co/datasets/multimodal-reasoning-lab/Zebra-CoT\\nEntry ID: http://arxiv.org/abs/2507.16746v1\\nPDF URL: http://arxiv.org/pdf/2507.16746v1\\nAll Links: http://arxiv.org/abs/2507.16746v1||http://arxiv.org/pdf/2507.16746v1\\nSummary: Humans often use visual aids, for example diagrams or sketches, when solving\\ncomplex problems. Training multimodal models to do the same, known as Visual\\nChain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf\\nvisual CoT performance, which hinders reinforcement learning, and (2) the lack\\nof high-quality visual CoT training data. We introduce $\\\\textbf{Zebra-CoT}$, a\\ndiverse large-scale dataset with 182,384 samples, containing logically coherent\\ninterleaved text-image reasoning traces. We focus on four categories of tasks\\nwhere sketching or visual reasoning is especially natural, spanning scientific\\nquestions such as geometry, physics, and algorithms; 2D visual reasoning tasks\\nlike visual search and jigsaw puzzles; 3D reasoning tasks including 3D\\nmulti-hop inference, embodied and robot planning; visual logic problems and\\nstrategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT\\ntraining corpus results in an improvement of +12% in our test-set accuracy and\\nyields up to +13% performance gain on standard VLM benchmark evaluations.\\nFine-tuning Bagel-7B yields a model that generates high-quality interleaved\\nvisual reasoning chains, underscoring Zebra-CoT's effectiveness for developing\\nmultimodal reasoning abilities. We open-source our dataset and models to\\nsupport development and evaluation of visual CoT.\\n\\n\\nTitle: A Puzzle-Based Dataset for Natural Language Inference\\nAuthors: Roxana Szomiu, Adrian Groza\\nPublished: 2021-12-10\\nUpdated: 2021-12-10\\nPrimary Category: cs.AI\\nAll Categories: cs.AI\\nDOI: 10.13140/RG.2.2.19206.09289\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2112.05742v1\\nPDF URL: http://arxiv.org/pdf/2112.05742v1\\nAll Links: http://dx.doi.org/10.13140/RG.2.2.19206.09289||http://arxiv.org/abs/2112.05742v1||http://arxiv.org/pdf/2112.05742v1\\nSummary: We provide here a dataset for tasks related to natural language understanding\\nand natural language inference. The dataset contains logical puzzles in natural\\nlanguage from three domains: comparing puzzles, knighs and knaves, and zebra\\npuzzles. Each puzzle is associated with the entire set of atomic questions that\\ncan be generated based on the relations and individuals occurring in the text.\\nFor each question we provide the correct answer: entailment, contradiction or\\nambiguity. The answer's correctness is verified against theorem provers. Good\\npuzzles have two properties: (i) each piece of information is necessary and\\n(ii) no unnecessary information is provided. These properties make puzzles\\ninteresting candidates for machine comprehension tasks.\\n\\n\\nTitle: Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles\\nAuthors: Kulin Shah, Nishanth Dikkala, Xin Wang, Rina Panigrahy\\nPublished: 2024-09-16\\nUpdated: 2024-09-16\\nPrimary Category: cs.LG\\nAll Categories: cs.LG, cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: 26 pages\\nEntry ID: http://arxiv.org/abs/2409.10502v1\\nPDF URL: http://arxiv.org/pdf/2409.10502v1\\nAll Links: http://arxiv.org/abs/2409.10502v1||http://arxiv.org/pdf/2409.10502v1\\nSummary: Causal language modeling using the Transformer architecture has yielded\\nremarkable capabilities in Large Language Models (LLMs) over the last few\\nyears. However, the extent to which fundamental search and reasoning\\ncapabilities emerged within LLMs remains a topic of ongoing debate. In this\\nwork, we study if causal language modeling can learn a complex task such as\\nsolving Sudoku puzzles. To solve a Sudoku, the model is first required to\\nsearch over all empty cells of the puzzle to decide on a cell to fill and then\\napply an appropriate strategy to fill the decided cell. Sometimes, the\\napplication of a strategy only results in thinning down the possible values in\\na cell rather than concluding the exact value of the cell. In such cases,\\nmultiple strategies are applied one after the other to fill a single cell. We\\nobserve that Transformer models trained on this synthetic task can indeed learn\\nto solve Sudokus (our model solves $94.21\\\\%$ of the puzzles fully correctly)\\nwhen trained on a logical sequence of steps taken by a solver. We find that\\ntraining Transformers with the logical sequence of steps is necessary and\\nwithout such training, they fail to learn Sudoku. We also extend our analysis\\nto Zebra puzzles (known as Einstein puzzles) and show that the model solves\\n$92.04 \\\\%$ of the puzzles fully correctly. In addition, we study the internal\\nrepresentations of the trained Transformer and find that through linear\\nprobing, we can decode information about the set of possible values in any\\ngiven cell from them, pointing to the presence of a strong reasoning engine\\nimplicit in the Transformer weights.\\n\\n\\nTitle: How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning\\nAuthors: Hongyi James Cai, Junlin Wang, Xiaoyin Chen, Bhuwan Dhingra\\nPublished: 2025-05-30\\nUpdated: 2025-05-30\\nPrimary Category: cs.AI\\nAll Categories: cs.AI\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2505.24273v1\\nPDF URL: http://arxiv.org/pdf/2505.24273v1\\nAll Links: http://arxiv.org/abs/2505.24273v1||http://arxiv.org/pdf/2505.24273v1\\nSummary: Recent breakthroughs in large language models (LLMs) have effectively\\nimproved their reasoning abilities, particularly on mathematical and logical\\nproblems that have verifiable answers, through techniques such as supervised\\nfinetuning (SFT) and reinforcement learning (RL). Prior research indicates that\\nRL effectively internalizes search strategies, enabling long chain-of-thought\\n(CoT) reasoning, with backtracking emerging naturally as a learned capability.\\nHowever, the precise benefits of backtracking, specifically, how significantly\\nit contributes to reasoning improvements and the optimal extent of its use,\\nremain poorly understood. In this work, we systematically investigate the\\ndynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc\\n1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self\\nReference. Our findings highlight that short CoT sequences used in SFT as a\\nwarm-up do have moderate contribution to RL training, compared with cold-start\\nRL; however such contribution diminishes when tasks become increasingly\\ndifficult. Motivated by this observation, we construct synthetic datasets\\nvarying systematically in the number of backtracking steps and conduct\\ncontrolled experiments to isolate the influence of either the correctness\\n(content) or the structure (i.e., backtrack frequency). We find that (1) longer\\nCoT with backtracks generally induce better and more stable RL training, (2)\\nmore challenging problems with larger search space tend to need higher numbers\\nof backtracks during the SFT stage. Additionally, we demonstrate through\\nexperiments on distilled data that RL training is largely unaffected by the\\ncorrectness of long CoT sequences, suggesting that RL prioritizes structural\\npatterns over content correctness. Collectively, our results offer practical\\ninsights into designing optimal training strategies to effectively scale\\nreasoning in LLMs.\\n\\n\\nTitle: Solving Zebra Puzzles Using Constraint-Guided Multi-Agent Systems\\nAuthors: Shmuel Berman, Kathleen McKeown, Baishakhi Ray\\nPublished: 2024-07-04\\nUpdated: 2024-07-09\\nPrimary Category: cs.MA\\nAll Categories: cs.MA, cs.CL, 68T01, 68T20, 68T27,, I.2.3; I.2.6; I.2.7; I.2.11\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2407.03956v2\\nPDF URL: http://arxiv.org/pdf/2407.03956v2\\nAll Links: http://arxiv.org/abs/2407.03956v2||http://arxiv.org/pdf/2407.03956v2\\nSummary: Prior research has enhanced the ability of Large Language Models (LLMs) to\\nsolve logic puzzles using techniques such as chain-of-thought prompting or\\nintroducing a symbolic representation. These frameworks are still usually\\ninsufficient to solve complicated logical problems, such as Zebra puzzles, due\\nto the inherent complexity of translating natural language clues into logical\\nstatements. We introduce a multi-agent system, ZPS, that integrates LLMs with\\nan off the shelf theorem prover. This system tackles the complex puzzle-solving\\ntask by breaking down the problem into smaller, manageable parts, generating\\nSMT (Satisfiability Modulo Theories) code to solve them with a theorem prover,\\nand using feedback between the agents to repeatedly improve their answers. We\\nalso introduce an automated grid puzzle grader to assess the correctness of our\\npuzzle solutions and show that the automated grader is reliable by evaluating\\nit in a user-study. Our approach shows improvement in all three LLMs we tested,\\nwith GPT-4 showing 166% improvement in the number of fully correct solutions.\\n\\n\\nTitle: VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL\\nAuthors: Yichen Feng, Zhangchen Xu, Fengqing Jiang, Yuetai Li, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran\\nPublished: 2025-05-29\\nUpdated: 2025-05-29\\nPrimary Category: cs.CV\\nAll Categories: cs.CV, cs.AI, cs.LG\\nDOI: N/A\\nJournal Reference: N/A\\nComment: Project page at https://visualsphinx.github.io/\\nEntry ID: http://arxiv.org/abs/2505.23977v1\\nPDF URL: http://arxiv.org/pdf/2505.23977v1\\nAll Links: http://arxiv.org/abs/2505.23977v1||http://arxiv.org/pdf/2505.23977v1\\nSummary: Vision language models (VLMs) are expected to perform effective multimodal\\nreasoning and make logically coherent decisions, which is critical to tasks\\nsuch as diagram understanding and spatial problem solving. However, current VLM\\nreasoning lacks large-scale and well-structured training datasets. To bridge\\nthis gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic\\nvisual logical reasoning training data. To tackle the challenge of image\\nsynthesis with grounding answers, we propose a rule-to-image synthesis\\npipeline, which extracts and expands puzzle rules from seed questions and\\ngenerates the code of grounding synthesis image synthesis for puzzle sample\\nassembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx\\nbenefit from logical coherence and readability of our dataset and exhibit\\nimproved performance on logical reasoning tasks. The enhanced reasoning\\ncapabilities developed from VisualSphinx also benefit other reasoning tasks\\nsuch as algebraic reasoning, arithmetic reasoning and geometry reasoning.\\n\\n\\nTitle: SPaRC: A Spatial Pathfinding Reasoning Challenge\\nAuthors: Lars Benedikt Kaesberg, Jan Philip Wahle, Terry Ruas, Bela Gipp\\nPublished: 2025-05-22\\nUpdated: 2025-05-22\\nPrimary Category: cs.AI\\nAll Categories: cs.AI, cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2505.16686v1\\nPDF URL: http://arxiv.org/pdf/2505.16686v1\\nAll Links: http://arxiv.org/abs/2505.16686v1||http://arxiv.org/pdf/2505.16686v1\\nSummary: Existing reasoning datasets saturate and fail to test abstract, multi-step\\nproblems, especially pathfinding and complex rule constraint satisfaction. We\\nintroduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000\\n2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,\\nrequiring step-by-step planning with arithmetic and geometric rules. Humans\\nachieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best\\nreasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).\\nModels often generate invalid paths (>50% of puzzles for o4-mini), and\\nreasoning tokens reveal they make errors in navigation and spatial logic.\\nUnlike humans, who take longer on hard puzzles, models fail to scale test-time\\ncompute with difficulty. Allowing models to make multiple solution attempts\\nimproves accuracy, suggesting potential for better spatial reasoning with\\nimproved training and efficient test-time scaling methods. SPaRC can be used as\\na window into models' spatial reasoning limitations and drive research toward\\nnew methods that excel in abstract, multi-step problem-solving.\\n\\n\\nTitle: Puzzle Solving using Reasoning of Large Language Models: A Survey\\nAuthors: Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou\\nPublished: 2024-02-17\\nUpdated: 2024-09-14\\nPrimary Category: cs.CL\\nAll Categories: cs.CL, cs.AI\\nDOI: 10.18653/v1/2024.emnlp-main.646\\nJournal Reference: Proceedings of the 2024 Conference on Empirical Methods in Natural\\n  Language Processing, pages 11574-11591, Miami, Florida, USA. Association for\\n  Computational Linguistics, 2024\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2402.11291v3\\nPDF URL: http://arxiv.org/pdf/2402.11291v3\\nAll Links: http://dx.doi.org/10.18653/v1/2024.emnlp-main.646||http://arxiv.org/abs/2402.11291v3||http://arxiv.org/pdf/2402.11291v3\\nSummary: Exploring the capabilities of Large Language Models (LLMs) in puzzle solving\\nunveils critical insights into their potential and challenges in AI, marking a\\nsignificant step towards understanding their applicability in complex reasoning\\ntasks. This survey leverages a unique taxonomy -- dividing puzzles into\\nrule-based and rule-less categories -- to critically assess LLMs through\\nvarious methodologies, including prompting techniques, neuro-symbolic\\napproaches, and fine-tuning. Through a critical review of relevant datasets and\\nbenchmarks, we assess LLMs' performance, identifying significant challenges in\\ncomplex puzzle scenarios. Our findings highlight the disparity between LLM\\ncapabilities and human-like reasoning, particularly in those requiring advanced\\nlogical inference. The survey underscores the necessity for novel strategies\\nand richer datasets to advance LLMs' puzzle-solving proficiency and contribute\\nto AI's logical reasoning and creative problem-solving advancements.\\n\\n\\nTitle: Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning\\nAuthors: Deepanway Ghosal, Vernon Toh Yan Han, Chia Yew Ken, Soujanya Poria\\nPublished: 2024-03-06\\nUpdated: 2024-03-13\\nPrimary Category: cs.CV\\nAll Categories: cs.CV, cs.AI\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2403.03864v3\\nPDF URL: http://arxiv.org/pdf/2403.03864v3\\nAll Links: http://arxiv.org/abs/2403.03864v3||http://arxiv.org/pdf/2403.03864v3\\nSummary: This paper introduces the novel task of multimodal puzzle solving, framed\\nwithin the context of visual question-answering. We present a new dataset,\\nAlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal\\nlanguage models in solving algorithmic puzzles that necessitate both visual\\nunderstanding, language understanding, and complex algorithmic reasoning. We\\ncreate the puzzles to encompass a diverse array of mathematical and algorithmic\\ntopics such as boolean logic, combinatorics, graph theory, optimization,\\nsearch, etc., aiming to evaluate the gap between visual data interpretation and\\nalgorithmic problem-solving skills. The dataset is generated automatically from\\ncode authored by humans. All our puzzles have exact solutions that can be found\\nfrom the algorithm without tedious human calculations. It ensures that our\\ndataset can be scaled up arbitrarily in terms of reasoning complexity and\\ndataset size. Our investigation reveals that large language models (LLMs) such\\nas GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We\\nfind that their performance is near random in a multi-choice question-answering\\nsetup for a significant number of puzzles. The findings emphasize the\\nchallenges of integrating visual, language, and algorithmic knowledge for\\nsolving complex reasoning problems.\",\"annotations\":null}", "tool_call_id": "toolu_01Uwir8XwN6aYmXbUmy818Mm"}, {"role": "assistant", "content": null, "tool_calls": [{"id": "toolu_01SstPwD8sdNQTz26jYACgKZ", "type": "function", "function": {"name": "scholarly-search-arxiv", "arguments": "{\"keyword\": \"KOR-Bench Korean reasoning benchmark\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"Search articles for KOR-Bench Korean reasoning benchmark:\\nTitle: KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks\\nAuthors: Kaijing Ma, Xinrun Du, Yunran Wang, Haoran Zhang, Zhoufutu Wen, Xingwei Qu, Jian Yang, Jiaheng Liu, Minghao Liu, Xiang Yue, Wenhao Huang, Ge Zhang\\nPublished: 2024-10-09\\nUpdated: 2025-03-01\\nPrimary Category: cs.DB\\nAll Categories: cs.DB\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2410.06526v3\\nPDF URL: http://arxiv.org/pdf/2410.06526v3\\nAll Links: http://arxiv.org/abs/2410.06526v3||http://arxiv.org/pdf/2410.06526v3\\nSummary: In this paper, we introduce Knowledge-Orthogonal Reasoning (KOR), a concept\\naimed at minimizing reliance on domain-specific knowledge, enabling more\\naccurate evaluation of models' reasoning abilities in out-of-distribution\\nsettings. Based on this concept, we propose the Knowledge-Orthogonal Reasoning\\nBenchmark (KOR-Bench), encompassing five task categories: Operation, Logic,\\nCipher, Puzzle, and Counterfactual. KOR-Bench emphasizes models' effectiveness\\nin applying new rule descriptions to solve novel rule-driven questions.\\nO1-Preview and O1-Mini achieve accuracies of 72.88% and 70.16%, surpassing\\nClaude-3.5-Sonnet and GPT-4o (58.96% and 58.00%), highlighting the\\neffectiveness of KOR-Bench. We perform detailed analyses, identifying\\nbottlenecks in the Cipher task with Stepwise Prompting, where two rounds of\\nSelf-Correction yield optimal results. We evaluate performance across three\\nintegrated tasks, explore the impact of Tricks on the Puzzle task, and\\nvisualize rule-focused attention. Additionally, we conduct an ablation study on\\ndataset size, benchmark correlations, and zero-shot and three-shot \\\"only\\nquestions\\\" experiments. KOR-Bench aims to enhance reasoning evaluation and\\nsupport further research in this area.\\n\\n\\nTitle: Making Qwen3 Think in Korean with Reinforcement Learning\\nAuthors: Jungyup Lee, Jemin Kim, Sang Park, SeungJae Lee\\nPublished: 2025-08-14\\nUpdated: 2025-08-14\\nPrimary Category: cs.CL\\nAll Categories: cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2508.10355v1\\nPDF URL: http://arxiv.org/pdf/2508.10355v1\\nAll Links: http://arxiv.org/abs/2508.10355v1||http://arxiv.org/pdf/2508.10355v1\\nSummary: We present a two-stage fine-tuning approach to make the large language model\\nQwen3 14B \\\"think\\\" natively in Korean. In the first stage, supervised\\nfine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a\\nstrong foundation in Korean logical reasoning, yielding notable improvements in\\nKorean-language tasks and even some gains in general reasoning ability. In the\\nsecond stage, we employ reinforcement learning with a customized Group Relative\\nPolicy Optimization (GRPO) algorithm to further enhance both Korean reasoning\\nalignment and overall problem-solving performance. We address critical\\nstability challenges in GRPO training - such as reward hacking and policy\\ncollapse - by introducing an oracle judge model that calibrates the reward\\nsignal. Our approach achieves stable learning (avoiding the collapse observed\\nin naive GRPO) and leads to steady, incremental performance gains. The final\\nRL-tuned model demonstrates substantially improved results on advanced\\nreasoning benchmarks (particularly math and coding tasks) while maintaining\\nknowledge and language proficiency, successfully conducting its internal\\nchain-of-thought entirely in Korean.\\n\\n\\nTitle: KAIO: A Collection of More Challenging Korean Questions\\nAuthors: Nahyun Lee, Guijin Son, Hyunwoo Ko, Kyubeen Han\\nPublished: 2025-09-18\\nUpdated: 2025-09-18\\nPrimary Category: cs.CL\\nAll Categories: cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: 4 pages paper\\nEntry ID: http://arxiv.org/abs/2509.14752v1\\nPDF URL: http://arxiv.org/pdf/2509.14752v1\\nAll Links: http://arxiv.org/abs/2509.14752v1||http://arxiv.org/pdf/2509.14752v1\\nSummary: With the advancement of mid/post-training techniques, LLMs are pushing their\\nboundaries at an accelerated pace. Legacy benchmarks saturate quickly (e.g.,\\nbroad suites like MMLU over the years, newer ones like GPQA-D even faster),\\nwhich makes frontier progress hard to track. The problem is especially acute in\\nKorean: widely used benchmarks are fewer, often translated or narrow in scope,\\nand updated more slowly, so saturation and contamination arrive sooner.\\nAccordingly, at this moment, there is no Korean benchmark capable of evaluating\\nand ranking frontier models. To bridge this gap, we introduce KAIO, a Korean,\\nmath-centric benchmark that stresses long-chain reasoning. Unlike recent Korean\\nsuites that are at or near saturation, KAIO remains far from saturated: the\\nbest-performing model, GPT-5, attains 62.8, followed by Gemini-2.5-Pro (52.3).\\nOpen models such as Qwen3-235B and DeepSeek-R1 cluster falls below 30,\\ndemonstrating substantial headroom, enabling robust tracking of frontier\\nprogress in Korean. To reduce contamination, KAIO will remain private and be\\nserved via a held-out evaluator until the best publicly known model reaches at\\nleast 80% accuracy, after which we will release the set and iterate to a harder\\nversion.\\n\\n\\nTitle: KOBEST: Korean Balanced Evaluation of Significant Tasks\\nAuthors: Dohyeong Kim, Myeongjun Jang, Deuk Sin Kwon, Eric Davis\\nPublished: 2022-04-09\\nUpdated: 2022-04-09\\nPrimary Category: cs.CL\\nAll Categories: cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: 9 pages\\nEntry ID: http://arxiv.org/abs/2204.04541v1\\nPDF URL: http://arxiv.org/pdf/2204.04541v1\\nAll Links: http://arxiv.org/abs/2204.04541v1||http://arxiv.org/pdf/2204.04541v1\\nSummary: A well-formulated benchmark plays a critical role in spurring advancements in\\nthe natural language processing (NLP) field, as it allows objective and precise\\nevaluation of diverse models. As modern language models (LMs) have become more\\nelaborate and sophisticated, more difficult benchmarks that require linguistic\\nknowledge and reasoning have been proposed. However, most of these benchmarks\\nonly support English, and great effort is necessary to construct benchmarks for\\nother low resource languages. To this end, we propose a new benchmark named\\nKorean balanced evaluation of significant tasks (KoBEST), which consists of\\nfive Korean-language downstream tasks. Professional Korean linguists designed\\nthe tasks that require advanced Korean linguistic knowledge. Moreover, our data\\nis purely annotated by humans and thoroughly reviewed to guarantee high data\\nquality. We also provide baseline models and human performance results. Our\\ndataset is available on the Huggingface.\\n\\n\\nTitle: Nunchi-Bench: Benchmarking Language Models on Cultural Reasoning with a Focus on Korean Superstition\\nAuthors: Kyuhee Kim, Sangah Lee\\nPublished: 2025-07-05\\nUpdated: 2025-07-05\\nPrimary Category: cs.CL\\nAll Categories: cs.CL, cs.AI, cs.CY\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2507.04014v1\\nPDF URL: http://arxiv.org/pdf/2507.04014v1\\nAll Links: http://arxiv.org/abs/2507.04014v1||http://arxiv.org/pdf/2507.04014v1\\nSummary: As large language models (LLMs) become key advisors in various domains, their\\ncultural sensitivity and reasoning skills are crucial in multicultural\\nenvironments. We introduce Nunchi-Bench, a benchmark designed to evaluate LLMs'\\ncultural understanding, with a focus on Korean superstitions. The benchmark\\nconsists of 247 questions spanning 31 topics, assessing factual knowledge,\\nculturally appropriate advice, and situational interpretation. We evaluate\\nmultilingual LLMs in both Korean and English to analyze their ability to reason\\nabout Korean cultural contexts and how language variations affect performance.\\nTo systematically assess cultural reasoning, we propose a novel evaluation\\nstrategy with customized scoring metrics that capture the extent to which\\nmodels recognize cultural nuances and respond appropriately. Our findings\\nhighlight significant challenges in LLMs' cultural reasoning. While models\\ngenerally recognize factual information, they struggle to apply it in practical\\nscenarios. Furthermore, explicit cultural framing enhances performance more\\neffectively than relying solely on the language of the prompt. To support\\nfurther research, we publicly release Nunchi-Bench alongside a leaderboard.\\n\\n\\nTitle: Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context\\nAuthors: Dasol Choi, Jungwhan Kim, Guijin Son\\nPublished: 2025-09-14\\nUpdated: 2025-09-14\\nPrimary Category: cs.CL\\nAll Categories: cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2509.11303v1\\nPDF URL: http://arxiv.org/pdf/2509.11303v1\\nAll Links: http://arxiv.org/abs/2509.11303v1||http://arxiv.org/pdf/2509.11303v1\\nSummary: Physical commonsense reasoning datasets like PIQA are predominantly\\nEnglish-centric and lack cultural diversity. We introduce Ko-PIQA, a Korean\\nphysical commonsense reasoning dataset that incorporates cultural context.\\nStarting from 3.01 million web-crawled questions, we employed a multi-stage\\nfiltering approach using three language models to identify 11,553 PIQA-style\\nquestions. Through GPT-4o refinement and human validation, we obtained 441\\nhigh-quality question-answer pairs. A key feature of Ko-PIQA is its cultural\\ngrounding: 19.7\\\\% of questions contain culturally specific elements like\\ntraditional Korean foods (kimchi), clothing (hanbok), and specialized\\nappliances (kimchi refrigerators) that require culturally-aware reasoning\\nbeyond direct translation. We evaluate seven language models on Ko-PIQA, with\\nthe best model achieving 83.22\\\\% accuracy while the weakest reaches only\\n59.86\\\\%, demonstrating significant room for improvement. Models particularly\\nstruggle with culturally specific scenarios, highlighting the importance of\\nculturally diverse datasets. Ko-PIQA serves as both a benchmark for Korean\\nlanguage models and a foundation for more inclusive commonsense reasoning\\nresearch. The dataset and code will be publicly available.\\n\\n\\nTitle: Understand, Solve and Translate: Bridging the Multilingual Mathematical Reasoning Gap\\nAuthors: Hyunwoo Ko, Guijin Son, Dasol Choi\\nPublished: 2025-01-05\\nUpdated: 2025-01-31\\nPrimary Category: cs.CL\\nAll Categories: cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: 18 pages, 14 figures, 9 tables\\nEntry ID: http://arxiv.org/abs/2501.02448v2\\nPDF URL: http://arxiv.org/pdf/2501.02448v2\\nAll Links: http://arxiv.org/abs/2501.02448v2||http://arxiv.org/pdf/2501.02448v2\\nSummary: Large language models (LLMs) demonstrate exceptional performance on complex\\nreasoning tasks. However, despite their strong reasoning capabilities in\\nhigh-resource languages (e.g., English and Chinese), a significant performance\\ngap persists in other languages. To investigate this gap in Korean, we\\nintroduce HRM8K, a benchmark comprising 8,011 English-Korean parallel bilingual\\nmath problems. Through systematic analysis of model behaviors, we identify a\\nkey finding: these performance disparities stem primarily from difficulties in\\ncomprehending non-English inputs, rather than limitations in reasoning\\ncapabilities. Based on these findings, we propose UST (Understand, Solve, and\\nTranslate), a method that strategically uses English as an anchor for reasoning\\nand solution generation. By fine-tuning the model on 130k synthetically\\ngenerated data points, UST achieves a 10.91% improvement on the HRM8K benchmark\\nand reduces the multilingual performance gap from 11.6% to 0.7%. Additionally,\\nwe show that improvements from UST generalize effectively to different Korean\\ndomains, demonstrating that capabilities acquired from machine-verifiable\\ncontent can be generalized to other areas. We publicly release the benchmark,\\ntraining dataset, and models.\\n\\n\\nTitle: KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts\\nAuthors: Taebaek Hwang, Minseo Kim, Gisang Lee, Seonuk Kim, Hyunjun Eun\\nPublished: 2025-08-27\\nUpdated: 2025-08-31\\nPrimary Category: cs.CV\\nAll Categories: cs.CV, cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: Accepted to EMNLP 2025 (Main Conference)\\nEntry ID: http://arxiv.org/abs/2508.19944v2\\nPDF URL: http://arxiv.org/pdf/2508.19944v2\\nAll Links: http://arxiv.org/abs/2508.19944v2||http://arxiv.org/pdf/2508.19944v2\\nSummary: Understanding and reasoning over text within visual contexts poses a\\nsignificant challenge for Vision-Language Models (VLMs), given the complexity\\nand diversity of real-world scenarios. To address this challenge, text-rich\\nVisual Question Answering (VQA) datasets and benchmarks have emerged for\\nhigh-resource languages like English. However, a critical gap persists for\\nlow-resource languages such as Korean, where the lack of comprehensive\\nbenchmarks hinders robust model evaluation and comparison. To bridge this gap,\\nwe introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich\\nVQA Attuned to diverse visual contexts. KRETA facilitates an in-depth\\nevaluation of both visual text understanding and reasoning capabilities, while\\nalso supporting a multifaceted assessment across 15 domains and 26 image types.\\nAdditionally, we introduce a semi-automated VQA generation pipeline\\nspecifically optimized for text-rich settings, leveraging refined stepwise\\nimage decomposition and a rigorous seven-metric evaluation protocol to ensure\\ndata quality. While KRETA is tailored for Korean, we hope our adaptable and\\nextensible pipeline will facilitate the development of similar benchmarks in\\nother languages, thereby accelerating multilingual VLM research. The code and\\ndataset for KRETA are available at https://github.com/tabtoyou/KRETA.\\n\\n\\nTitle: KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations\\nAuthors: Sunjun Kweon, Byungjin Choi, Gyouk Chu, Junyeong Song, Daeun Hyeon, Sujin Gan, Jueon Kim, Minkyu Kim, Rae Woong Park, Edward Choi\\nPublished: 2024-03-03\\nUpdated: 2024-12-09\\nPrimary Category: cs.CL\\nAll Categories: cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2403.01469v3\\nPDF URL: http://arxiv.org/pdf/2403.01469v3\\nAll Links: http://arxiv.org/abs/2403.01469v3||http://arxiv.org/pdf/2403.01469v3\\nSummary: We present KorMedMCQA, the first Korean Medical Multiple-Choice Question\\nAnswering benchmark, derived from professional healthcare licensing\\nexaminations conducted in Korea between 2012 and 2024. The dataset contains\\n7,469 questions from examinations for doctor, nurse, pharmacist, and dentist,\\ncovering a wide range of medical disciplines. We evaluate the performance of 59\\nlarge language models, spanning proprietary and open-source models,\\nmultilingual and Korean-specialized models, and those fine-tuned for clinical\\napplications. Our results show that applying Chain of Thought (CoT) reasoning\\ncan enhance the model performance by up to 4.5% compared to direct answering\\napproaches. We also investigate whether MedQA, one of the most widely used\\nmedical benchmarks derived from the U.S. Medical Licensing Examination, can\\nserve as a reliable proxy for evaluating model performance in other regions-in\\nthis case, Korea. Our correlation analysis between model scores on KorMedMCQA\\nand MedQA reveals that these two benchmarks align no better than benchmarks\\nfrom entirely different domains (e.g., MedQA and MMLU-Pro). This finding\\nunderscores the substantial linguistic and clinical differences between Korean\\nand U.S. medical contexts, reinforcing the need for region-specific medical QA\\nbenchmarks. To support ongoing research in Korean healthcare AI, we publicly\\nrelease the KorMedMCQA via Huggingface.\\n\\n\\nTitle: HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models\\nAuthors: Guijin Son, Hanwool Lee, Suwan Kim, Huiseo Kim, Jaecheol Lee, Je Won Yeom, Jihyu Jung, Jung Woo Kim, Songseong Kim\\nPublished: 2023-09-06\\nUpdated: 2024-03-20\\nPrimary Category: cs.CL\\nAll Categories: cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: Accepted at LREC-COLING 2024\\nEntry ID: http://arxiv.org/abs/2309.02706v5\\nPDF URL: http://arxiv.org/pdf/2309.02706v5\\nAll Links: http://arxiv.org/abs/2309.02706v5||http://arxiv.org/pdf/2309.02706v5\\nSummary: Large language models (LLMs) trained on massive corpora demonstrate\\nimpressive capabilities in a wide range of tasks. While there are ongoing\\nefforts to adapt these models to languages beyond English, the attention given\\nto their evaluation methodologies remains limited. Current multilingual\\nbenchmarks often rely on back translations or re-implementations of English\\ntests, limiting their capacity to capture unique cultural and linguistic\\nnuances. To bridge this gap for the Korean language, we introduce the HAE-RAE\\nBench, a dataset curated to challenge models lacking Korean cultural and\\ncontextual depth. The dataset encompasses six downstream tasks across four\\ndomains: vocabulary, history, general knowledge, and reading comprehension.\\nUnlike traditional evaluation suites focused on token and sequence\\nclassification or mathematical and logical reasoning, the HAE-RAE Bench\\nemphasizes a model's aptitude for recalling Korean-specific knowledge and\\ncultural contexts. Comparative analysis with prior Korean benchmarks indicates\\nthat the HAE-RAE Bench presents a greater challenge to non-Korean models by\\ndisturbing abilities and knowledge learned from English being transferred.\",\"annotations\":null}", "tool_call_id": "toolu_01SstPwD8sdNQTz26jYACgKZ"}, {"role": "assistant", "content": null, "tool_calls": [{"id": "toolu_01EQSHdQ7WjcAF6DgPejqrfd", "type": "function", "function": {"name": "arxiv_local-download_paper", "arguments": "{\"paper_id\": \"2410.23123\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"{\\\"status\\\": \\\"converting\\\", \\\"message\\\": \\\"Paper downloaded, conversion started\\\", \\\"started_at\\\": \\\"2025-09-21T08:01:45.550828\\\"}\",\"annotations\":null}", "tool_call_id": "toolu_01EQSHdQ7WjcAF6DgPejqrfd"}, {"role": "assistant", "content": null, "tool_calls": [{"id": "toolu_01SFoC477gA2npoja2eyim1W", "type": "function", "function": {"name": "arxiv_local-read_paper", "arguments": "{\"paper_id\": \"2410.23123\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"{\\\"status\\\": \\\"error\\\", \\\"message\\\": \\\"Paper 2410.23123 not found in storage. You may need to download it first using download_paper.\\\"}\",\"annotations\":null}", "tool_call_id": "toolu_01SFoC477gA2npoja2eyim1W"}, {"role": "assistant", "content": "Let me wait a moment and try again, as the paper is still converting.", "tool_calls": [{"id": "toolu_01V9rSZ7LfYWJecwta5FH6eK", "type": "function", "function": {"name": "arxiv_local-download_paper", "arguments": "{\"paper_id\": \"2410.23123\", \"check_status\": true}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"{\\\"status\\\": \\\"converting\\\", \\\"started_at\\\": \\\"2025-09-21T08:01:45.550828\\\", \\\"completed_at\\\": null, \\\"error\\\": null, \\\"message\\\": \\\"Paper conversion converting\\\"}\",\"annotations\":null}", "tool_call_id": "toolu_01V9rSZ7LfYWJecwta5FH6eK"}, {"role": "assistant", "content": "Let me search for information about BBEH while waiting:", "tool_calls": [{"id": "toolu_01HatanpBLDnSQKJGUphb2Ne", "type": "function", "function": {"name": "scholarly-search-arxiv", "arguments": "{\"keyword\": \"BBEH Big-Bench Extra Hard\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"Search articles for BBEH Big-Bench Extra Hard:\\nTitle: BIG-Bench Extra Hard\\nAuthors: Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, Orhan Firat\\nPublished: 2025-02-26\\nUpdated: 2025-05-06\\nPrimary Category: cs.CL\\nAll Categories: cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2502.19187v2\\nPDF URL: http://arxiv.org/pdf/2502.19187v2\\nAll Links: http://arxiv.org/abs/2502.19187v2||http://arxiv.org/pdf/2502.19187v2\\nSummary: Large language models (LLMs) are increasingly deployed in everyday\\napplications, demanding robust general reasoning capabilities and diverse\\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\\nwhich has served as a crucial benchmark for evaluating the general reasoning\\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\\nallowed for a comprehensive assessment of general reasoning across various\\nskills within a unified framework. However, recent advances in LLMs have led to\\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\\nprobes a similar reasoning capability but exhibits significantly increased\\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\\naccuracy of 9.8\\\\% for the best general-purpose model and 44.8\\\\% for the best\\nreasoning-specialized model, indicating substantial room for improvement and\\nhighlighting the ongoing challenge of achieving robust general reasoning in\\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.\\n\\n\\nTitle: Superluminal neutrinos and extra dimensions: constraints from the null energy condition\\nAuthors: Steven S. Gubser\\nPublished: 2011-09-26\\nUpdated: 2011-09-30\\nPrimary Category: hep-th\\nAll Categories: hep-th\\nDOI: 10.1016/j.physletb.2011.10.028\\nJournal Reference: N/A\\nComment: 8 pages, 1 figure. v2: References added, small additions\\nEntry ID: http://arxiv.org/abs/1109.5687v2\\nPDF URL: http://arxiv.org/pdf/1109.5687v2\\nAll Links: http://dx.doi.org/10.1016/j.physletb.2011.10.028||http://arxiv.org/abs/1109.5687v2||http://arxiv.org/pdf/1109.5687v2\\nSummary: In light of the recent results from the OPERA collaboration, indicating that\\nneutrinos can travel superluminally, I review a simple extra-dimensional\\nstrategy for accommodating such behavior; and I also explain why it is hard in\\nthis strategy to avoid violating the null energy condition somewhere in the\\nextra dimensions.\\n\\n\\nTitle: Hard Diagrams of the Unknot\\nAuthors: Benjamin A. Burton, Hsien-Chih Chang, Maarten L\u00f6ffler, Arnaud de Mesmay, Cl\u00e9ment Maria, Saul Schleimer, Eric Sedgwick, Jonathan Spreer\\nPublished: 2021-04-29\\nUpdated: 2021-04-29\\nPrimary Category: math.GT\\nAll Categories: math.GT, math.CO, 57K10, 05C12\\nDOI: N/A\\nJournal Reference: N/A\\nComment: 11 pages + appendix, 7 figures\\nEntry ID: http://arxiv.org/abs/2104.14076v1\\nPDF URL: http://arxiv.org/pdf/2104.14076v1\\nAll Links: http://arxiv.org/abs/2104.14076v1||http://arxiv.org/pdf/2104.14076v1\\nSummary: We present three \\\"hard\\\" diagrams of the unknot. They require (at least) three\\nextra crossings before they can be simplified to the trivial unknot diagram via\\nReidemeister moves in $\\\\mathbb{S}^2$. Both examples are constructed by applying\\npreviously proposed methods. The proof of their hardness uses significant\\ncomputational resources. We also determine that no small \\\"standard\\\" example of\\na hard unknot diagram requires more than one extra crossing for Reidemeister\\nmoves in $\\\\mathbb{S}^2$.\\n\\n\\nTitle: Improving Object Detection with Inverted Attention\\nAuthors: Zeyi Huang, Wei Ke, Dong Huang\\nPublished: 2019-03-28\\nUpdated: 2019-03-28\\nPrimary Category: cs.CV\\nAll Categories: cs.CV\\nDOI: N/A\\nJournal Reference: N/A\\nComment: 9 pages, 7 figures, 6 tables\\nEntry ID: http://arxiv.org/abs/1903.12255v1\\nPDF URL: http://arxiv.org/pdf/1903.12255v1\\nAll Links: http://arxiv.org/abs/1903.12255v1||http://arxiv.org/pdf/1903.12255v1\\nSummary: Improving object detectors against occlusion, blur and noise is a critical\\nstep to deploy detectors in real applications. Since it is not possible to\\nexhaust all image defects through data collection, many researchers seek to\\ngenerate hard samples in training. The generated hard samples are either images\\nor feature maps with coarse patches dropped out in the spatial dimensions.\\nSignificant overheads are required in training the extra hard samples and/or\\nestimating drop-out patches using extra network branches. In this paper, we\\nimprove object detectors using a highly efficient and fine-grain mechanism\\ncalled Inverted Attention (IA). Different from the original detector network\\nthat only focuses on the dominant part of objects, the detector network with IA\\niteratively inverts attention on feature maps and puts more attention on\\ncomplementary object parts, feature channels and even context. Our approach (1)\\noperates along both the spatial and channels dimensions of the feature maps;\\n(2) requires no extra training on hard samples, no extra network parameters for\\nattention estimation, and no testing overheads. Experiments show that our\\napproach consistently improved both two-stage and single-stage detectors on\\nbenchmark databases.\\n\\n\\nTitle: A two-hump spectrum in the prompt emission of GRB 240825A\\nAuthors: Hai-Ming Zhang, Zi-Qi Wang, Cui-Yuan Dai, Yi-Yun Huang, Ruo-Yu Liu, En-Wei Liang, Xiang-Yu Wang\\nPublished: 2025-01-06\\nUpdated: 2025-04-22\\nPrimary Category: astro-ph.HE\\nAll Categories: astro-ph.HE\\nDOI: 10.3847/2041-8213/adcf1a\\nJournal Reference: N/A\\nComment: Accepted for publication in the ApJ Letters\\nEntry ID: http://arxiv.org/abs/2501.03082v2\\nPDF URL: http://arxiv.org/pdf/2501.03082v2\\nAll Links: http://dx.doi.org/10.3847/2041-8213/adcf1a||http://arxiv.org/abs/2501.03082v2||http://arxiv.org/pdf/2501.03082v2\\nSummary: An extra hard spectral component that extends to GeV energies, in additional\\nto the typical sub- MeV Band component, appears in several gamma-ray burst\\n(GRBs) detected by Fermi Large Area Telescopes (LAT). Only in one case (i.e.,\\nGRB 090926A), a spectral break feature at the high energy end is identified in\\nthe extra hard component, but the photon counts are not enough to distinguish\\nbetween the cutoff model and the broken power law model for the spectral break.\\nIn this work, we report the detection of an extra hard component showing the\\nspectral break in GRB 240825A. We find that a broken power-law model fits the\\nspectral data of the extra component better than a single power-law with an\\nexponential cutoff in the time resolved spectrum for the second emission pulse,\\nwith a break at about 50 MeV. This spectral feature disfavors the gamma-ray\\nopacity to pair creation as the origin of the spectral break, but points to an\\nintrinsic peak for the extra component. The low ratio between the peak of the\\nextra hard component and that of the Band component challenges the synchrotron\\nself-Compton origin for the extra component. Alternative scenarios, such as the\\ninverse Compton scattering of the photosphere emission, are discussed. In\\naddition, we find a clear transition from the prompt emission to afterglow\\nemission at GeV energies in GRB 240825A, manifested by a temporal steep decay\\nand an unique spectral evolution.\\n\\n\\nTitle: Dilepton + jet signature of Split-UED at the LHC\\nAuthors: Swarup Kumar Majee, Seong Chan Park\\nPublished: 2013-01-28\\nUpdated: 2013-01-28\\nPrimary Category: hep-ph\\nAll Categories: hep-ph\\nDOI: N/A\\nJournal Reference: N/A\\nComment: 1 Table, 5 Figures, 13 pages\\nEntry ID: http://arxiv.org/abs/1301.6421v1\\nPDF URL: http://arxiv.org/pdf/1301.6421v1\\nAll Links: http://arxiv.org/abs/1301.6421v1||http://arxiv.org/pdf/1301.6421v1\\nSummary: We study the signature of dilepton and a hard jet ($\\\\ell^+\\\\ell^-j$) via heavy\\nnew gauge boson production in split universal extra dimension scenario where\\nthe Kaluza-Klein parity is conserved but the Kaluza-Klein number is not. A hard\\ncut to the jet energy effectively removes virtually all possible backgrounds\\nand provides a handle to search of new physics involving new neutral heavy\\nstates as the Kaluza-Klein $Z$ boson. The signature can be more generically\\nused in search of other new states such as graviton and radion in warped extra\\ndimension models.\\n\\n\\nTitle: Not All Pixels Are Equal: Learning Pixel Hardness for Semantic Segmentation\\nAuthors: Xin Xiao, Daiguo Zhou, Jiagao Hu, Yi Hu, Yongchao Xu\\nPublished: 2023-05-15\\nUpdated: 2023-05-15\\nPrimary Category: cs.CV\\nAll Categories: cs.CV, cs.AI\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2305.08462v1\\nPDF URL: http://arxiv.org/pdf/2305.08462v1\\nAll Links: http://arxiv.org/abs/2305.08462v1||http://arxiv.org/pdf/2305.08462v1\\nSummary: Semantic segmentation has recently witnessed great progress. Despite the\\nimpressive overall results, the segmentation performance in some hard areas\\n(e.g., small objects or thin parts) is still not promising. A straightforward\\nsolution is hard sample mining, which is widely used in object detection. Yet,\\nmost existing hard pixel mining strategies for semantic segmentation often rely\\non pixel's loss value, which tends to decrease during training. Intuitively,\\nthe pixel hardness for segmentation mainly depends on image structure and is\\nexpected to be stable. In this paper, we propose to learn pixel hardness for\\nsemantic segmentation, leveraging hardness information contained in global and\\nhistorical loss values. More precisely, we add a gradient-independent branch\\nfor learning a hardness level (HL) map by maximizing hardness-weighted\\nsegmentation loss, which is minimized for the segmentation head. This\\nencourages large hardness values in difficult areas, leading to appropriate and\\nstable HL map. Despite its simplicity, the proposed method can be applied to\\nmost segmentation methods with no and marginal extra cost during inference and\\ntraining, respectively. Without bells and whistles, the proposed method\\nachieves consistent/significant improvement (1.37% mIoU on average) over most\\npopular semantic segmentation methods on Cityscapes dataset, and demonstrates\\ngood generalization ability across domains. The source codes are available at\\nhttps://github.com/Menoly-xin/Hardness-Level-Learning .\\n\\n\\nTitle: Phenomenological Constraints on Extra-Dimensional Scalars\\nAuthors: G. Azuelos, P. -H. Beauchemin, C. P. Burgess\\nPublished: 2004-01-16\\nUpdated: 2004-01-16\\nPrimary Category: hep-ph\\nAll Categories: hep-ph\\nDOI: 10.1088/0954-3899/31/1/001\\nJournal Reference: J.Phys.G31:1-20,2005\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/hep-ph/0401125v1\\nPDF URL: http://arxiv.org/pdf/hep-ph/0401125v1\\nAll Links: http://dx.doi.org/10.1088/0954-3899/31/1/001||http://arxiv.org/abs/hep-ph/0401125v1||http://arxiv.org/pdf/hep-ph/0401125v1\\nSummary: We examine whether the ATLAS detector has sensitivity to extra-dimensional\\nscalars (as opposed to components of higher-dimensional tensors which look like\\n4D scalars), in scenarios having the extra-dimensional Planck scale in the TeV\\nrange and $n \\\\ge 2$ nonwarped extra dimensions. Such scalars appear as partners\\nof the graviton in virtually all higher-dimensional supersymmetric theories.\\nUsing the scalar's lowest-dimensional effective couplings to quarks and gluons,\\nwe compute the rate for the production of a hard jet together with missing\\nenergy. We find a nontrivial range of graviscalar couplings to which ATLAS\\ncould be sensitive, with experiments being more sensitive to couplings to\\ngluons than to quarks. Graviscalar emission increases the missing-energy signal\\nby adding to graviton production, and so complicates the inference of the\\nextra-dimensional Planck scale from an observed rate. Because graviscalar\\ndifferential cross sections resemble those for gravitons, it is unlikely that\\nthese can be experimentally distinguished from one another should a missing\\nenergy signal be observed.\\n\\n\\nTitle: Hadronic Models for the Extra Spectral Component in the short GRB 090510\\nAuthors: Katsuaki Asano, Sylvain Guiriec, Peter M\u00e9sz\u00e1ros\\nPublished: 2009-09-02\\nUpdated: 2009-10-17\\nPrimary Category: astro-ph.HE\\nAll Categories: astro-ph.HE, astro-ph.CO\\nDOI: 10.1088/0004-637X/705/2/L191\\nJournal Reference: Astrophys.J.705:L191-L194,2009\\nComment: 12pages, 4 figures. Accepted for publication in ApJL\\nEntry ID: http://arxiv.org/abs/0909.0306v2\\nPDF URL: http://arxiv.org/pdf/0909.0306v2\\nAll Links: http://dx.doi.org/10.1088/0004-637X/705/2/L191||http://arxiv.org/abs/0909.0306v2||http://arxiv.org/pdf/0909.0306v2\\nSummary: A short gamma-ray burst GRB 090510 detected by {\\\\it Fermi} shows an extra\\nspectral component between 10 MeV and 30 GeV, an addition to a more usual\\nlow-energy ($<10$ MeV) Band component. In general, such an extra component\\ncould originate from accelerated protons. In particular, inverse Compton\\nemission from secondary electron-positron pairs and proton synchrotron emission\\nare competitive models for reproducing the hard spectrum of the extra component\\nin GRB 090510. Here, using Monte Carlo simulations, we test the hadronic\\nscenarios against the observed properties. To reproduce the extra component\\naround GeV with these models, the proton injection isotropic-equivalent\\nluminosity is required to be larger than $10^{55}$ erg/s. Such large proton\\nluminosities are a challenge for the hadronic models.\\n\\n\\nTitle: General-Reasoner: Advancing LLM Reasoning Across All Domains\\nAuthors: Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen\\nPublished: 2025-05-20\\nUpdated: 2025-06-09\\nPrimary Category: cs.CL\\nAll Categories: cs.CL\\nDOI: N/A\\nJournal Reference: N/A\\nComment: N/A\\nEntry ID: http://arxiv.org/abs/2505.14652v5\\nPDF URL: http://arxiv.org/pdf/2505.14652v5\\nAll Links: http://arxiv.org/abs/2505.14652v5||http://arxiv.org/pdf/2505.14652v5\\nSummary: Reinforcement learning (RL) has recently demonstrated strong potential in\\nenhancing the reasoning capabilities of large language models (LLMs).\\nParticularly, the \\\"Zero\\\" reinforcement learning introduced by Deepseek-R1-Zero,\\nenables direct RL training of base LLMs without relying on an intermediate\\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\\nreasoning mainly focus on mathematical and coding domains, largely due to data\\nabundance and the ease of answer verification. This limits the applicability\\nand generalization of such models to broader domains, where questions often\\nhave diverse answer representations, and data is more scarce. In this paper, we\\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\\nreasoning capabilities across diverse domains. Our key contributions include:\\n(1) constructing a large-scale, high-quality dataset of questions with\\nverifiable answers curated by web crawling, covering a wide range of\\ndisciplines; and (2) developing a generative model-based answer verifier, which\\nreplaces traditional rule-based verification with the capability of\\nchain-of-thought and context-awareness. We train a series of models and\\nevaluate them on a wide range of datasets covering wide domains like physics,\\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\\ndemonstrates that General-Reasoner outperforms existing baseline methods,\\nachieving robust and generalizable reasoning performance while maintaining\\nsuperior effectiveness in mathematical reasoning tasks.\",\"annotations\":null}", "tool_call_id": "toolu_01HatanpBLDnSQKJGUphb2Ne"}, {"role": "assistant", "content": "Let me try to access the K&K paper again:", "tool_calls": [{"id": "toolu_01F7BELnA7on9HLbRLMw6eSx", "type": "function", "function": {"name": "arxiv_local-download_paper", "arguments": "{\"paper_id\": \"2410.23123\", \"check_status\": true}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"{\\\"status\\\": \\\"converting\\\", \\\"started_at\\\": \\\"2025-09-21T08:01:45.550828\\\", \\\"completed_at\\\": null, \\\"error\\\": null, \\\"message\\\": \\\"Paper conversion converting\\\"}\",\"annotations\":null}", "tool_call_id": "toolu_01F7BELnA7on9HLbRLMw6eSx"}, {"role": "assistant", "content": "Let me search for more information about the K&K paper directly:", "tool_calls": [{"id": "toolu_01Ws5Vzbfdd98bq7kZMEtMya", "type": "function", "function": {"name": "fetch-fetch_html", "arguments": "{\"url\": \"https://arxiv.org/abs/2410.23123\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"<!DOCTYPE html>\\n<html lang=\\\"en\\\">\\n\\n<head>  <title>[2410.23123] On Memorization of Large Language Models in Logical Reasoning</title>\\n  <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1\\\">\\n  <link rel=\\\"apple-touch-icon\\\" sizes=\\\"180x180\\\" href=\\\"/static/browse/0.3.4/images/icons/apple-touch-icon.png\\\">\\n  <link rel=\\\"icon\\\" type=\\\"image/png\\\" sizes=\\\"32x32\\\" href=\\\"/static/browse/0.3.4/images/icons/favicon-32x32.png\\\">\\n  <link rel=\\\"icon\\\" type=\\\"image/png\\\" sizes=\\\"16x16\\\" href=\\\"/static/browse/0.3.4/images/icons/favicon-16x16.png\\\">\\n  <link rel=\\\"manifest\\\" href=\\\"/static/browse/0.3.4/images/icons/site.webmanifest\\\">\\n  <link rel=\\\"mask-icon\\\" href=\\\"/static/browse/0.3.4/images/icons/safari-pinned-tab.svg\\\" color=\\\"#5bbad5\\\">\\n  <meta name=\\\"msapplication-TileColor\\\" content=\\\"#da532c\\\">\\n  <meta name=\\\"theme-color\\\" content=\\\"#ffffff\\\">\\n  <link rel=\\\"stylesheet\\\" type=\\\"text/css\\\" media=\\\"screen\\\" href=\\\"/static/browse/0.3.4/css/arXiv.css?v=20241206\\\" />\\n  <link rel=\\\"stylesheet\\\" type=\\\"text/css\\\" media=\\\"print\\\" href=\\\"/static/browse/0.3.4/css/arXiv-print.css?v=20200611\\\" />\\n  <link rel=\\\"stylesheet\\\" type=\\\"text/css\\\" media=\\\"screen\\\" href=\\\"/static/browse/0.3.4/css/browse_search.css\\\" />\\n  <script language=\\\"javascript\\\" src=\\\"/static/browse/0.3.4/js/accordion.js\\\" /></script>\\n  <link rel=\\\"canonical\\\" href=\\\"https://arxiv.org/abs/2410.23123\\\"/>\\n  <meta name=\\\"description\\\" content=\\\"Abstract page for arXiv paper 2410.23123: On Memorization of Large Language Models in Logical Reasoning\\\"><meta property=\\\"og:type\\\" content=\\\"website\\\" />\\n<meta property=\\\"og:site_name\\\" content=\\\"arXiv.org\\\" />\\n<meta property=\\\"og:title\\\" content=\\\"On Memorization of Large Language Models in Logical Reasoning\\\" />\\n<meta property=\\\"og:url\\\" content=\\\"https://arxiv.org/abs/2410.23123v2\\\" />\\n<meta property=\\\"og:image\\\" content=\\\"/static/browse/0.3.4/images/arxiv-logo-fb.png\\\" />\\n<meta property=\\\"og:image:secure_url\\\" content=\\\"/static/browse/0.3.4/images/arxiv-logo-fb.png\\\" />\\n<meta property=\\\"og:image:width\\\" content=\\\"1200\\\" />\\n<meta property=\\\"og:image:height\\\" content=\\\"700\\\" />\\n<meta property=\\\"og:image:alt\\\" content=\\\"arXiv logo\\\"/>\\n<meta property=\\\"og:description\\\" content=\\\"Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs&#39; reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&amp;K) puzzles. We find that LLMs could interpolate and memorize the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet they struggle with slight variations of these puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. Through in-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers, we establish that LLMs develop reasoning skills on K&amp;K puzzles alongside memorization. Finally, our analysis based on a per-sample memorization score sheds light on how LLMs switch between reasoning and memorization when solving logical puzzles. Our code and data are available at https://memkklogic.github.io.\\\"/>\\n<meta name=\\\"twitter:site\\\" content=\\\"@arxiv\\\"/>\\n<meta name=\\\"twitter:card\\\" content=\\\"summary\\\"/>\\n<meta name=\\\"twitter:title\\\" content=\\\"On Memorization of Large Language Models in Logical Reasoning\\\"/>\\n<meta name=\\\"twitter:description\\\" content=\\\"Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to...\\\"/>\\n<meta name=\\\"twitter:image\\\" content=\\\"https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png\\\"/>\\n<meta name=\\\"twitter:image:alt\\\" content=\\\"arXiv logo\\\"/>\\n  <link rel=\\\"stylesheet\\\" media=\\\"screen\\\" type=\\\"text/css\\\" href=\\\"/static/browse/0.3.4/css/tooltip.css\\\"/><link rel=\\\"stylesheet\\\" media=\\\"screen\\\" type=\\\"text/css\\\" href=\\\"https://static.arxiv.org/js/bibex-dev/bibex.css?20200709\\\"/>  <script src=\\\"/static/browse/0.3.4/js/mathjaxToggle.min.js\\\" type=\\\"text/javascript\\\"></script>  <script src=\\\"//code.jquery.com/jquery-latest.min.js\\\" type=\\\"text/javascript\\\"></script>\\n  <script src=\\\"//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js\\\" type=\\\"text/javascript\\\"></script>\\n  <script src=\\\"//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js\\\"></script>\\n  <script src=\\\"/static/browse/0.3.4/js/toggle-labs.js?20241022\\\" type=\\\"text/javascript\\\"></script>\\n  <script src=\\\"/static/browse/0.3.4/js/cite.js\\\" type=\\\"text/javascript\\\"></script><meta name=\\\"citation_title\\\" content=\\\"On Memorization of Large Language Models in Logical Reasoning\\\" /><meta name=\\\"citation_author\\\" content=\\\"Xie, Chulin\\\" /><meta name=\\\"citation_author\\\" content=\\\"Huang, Yangsibo\\\" /><meta name=\\\"citation_author\\\" content=\\\"Zhang, Chiyuan\\\" /><meta name=\\\"citation_author\\\" content=\\\"Yu, Da\\\" /><meta name=\\\"citation_author\\\" content=\\\"Chen, Xinyun\\\" /><meta name=\\\"citation_author\\\" content=\\\"Lin, Bill Yuchen\\\" /><meta name=\\\"citation_author\\\" content=\\\"Li, Bo\\\" /><meta name=\\\"citation_author\\\" content=\\\"Ghazi, Badih\\\" /><meta name=\\\"citation_author\\\" content=\\\"Kumar, Ravi\\\" /><meta name=\\\"citation_date\\\" content=\\\"2024/10/30\\\" /><meta name=\\\"citation_online_date\\\" content=\\\"2025/03/04\\\" /><meta name=\\\"citation_pdf_url\\\" content=\\\"http://arxiv.org/pdf/2410.23123\\\" /><meta name=\\\"citation_arxiv_id\\\" content=\\\"2410.23123\\\" /><meta name=\\\"citation_abstract\\\" content=\\\"Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs&#39; reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&amp;K) puzzles. We find that LLMs could interpolate and memorize the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet they struggle with slight variations of these puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. Through in-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers, we establish that LLMs develop reasoning skills on K&amp;K puzzles alongside memorization. Finally, our analysis based on a per-sample memorization score sheds light on how LLMs switch between reasoning and memorization when solving logical puzzles. Our code and data are available at https://memkklogic.github.io.\\\" />\\n</head>\\n\\n<body  class=\\\"with-cu-identity\\\">\\n  \\n  <div class=\\\"flex-wrap-footer\\\">\\n    <header>\\n      <a href=\\\"#content\\\" class=\\\"is-sr-only\\\">Skip to main content</a>\\n      <!-- start desktop header -->\\n      <div class=\\\"columns is-vcentered is-hidden-mobile\\\" id=\\\"cu-identity\\\">\\n        <div class=\\\"column\\\" id=\\\"cu-logo\\\">\\n          <a href=\\\"https://www.cornell.edu/\\\"><img src=\\\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\\\" alt=\\\"Cornell University\\\" /></a>\\n        </div>\\n\\n\\n        <!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class=\\\"column\\\" id=\\\"support-ack\\\">\\n          <span id=\\\"support-ack-url\\\">We gratefully acknowledge support from the Simons Foundation, <a href=\\\"https://info.arxiv.org/about/ourmembers.html\\\">member institutions</a>, and all contributors.</span>\\n          <a href=\\\"https://info.arxiv.org/about/donate.html\\\" class=\\\"btn-header-donate\\\">Donate</a>\\n        </div>\\n      </div>\\n\\n      <div id=\\\"header\\\" class=\\\"is-hidden-mobile\\\">\\n<a aria-hidden=\\\"true\\\" tabindex=\\\"-1\\\" href=\\\"/IgnoreMe\\\"></a>\\n  <div class=\\\"header-breadcrumbs is-hidden-mobile\\\">\\n    <a href=\\\"/\\\"><img src=\\\"/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg\\\" alt=\\\"arxiv logo\\\" style=\\\"height:40px;\\\"/></a> <span>&gt;</span> <a href=\\\"/list/cs/recent\\\">cs</a> <span>&gt;</span> arXiv:2410.23123\\n  </div>\\n          <div class=\\\"search-block level-right\\\">\\n    <form class=\\\"level-item mini-search\\\" method=\\\"GET\\\" action=\\\"https://arxiv.org/search\\\">\\n      <div class=\\\"field has-addons\\\">\\n        <div class=\\\"control\\\">\\n          <input class=\\\"input is-small\\\" type=\\\"text\\\" name=\\\"query\\\" placeholder=\\\"Search...\\\" aria-label=\\\"Search term or terms\\\" />\\n          <p class=\\\"help\\\"><a href=\\\"https://info.arxiv.org/help\\\">Help</a> | <a href=\\\"https://arxiv.org/search/advanced\\\">Advanced Search</a></p>\\n        </div>\\n        <div class=\\\"control\\\">\\n          <div class=\\\"select is-small\\\">\\n            <select name=\\\"searchtype\\\" aria-label=\\\"Field to search\\\">\\n              <option value=\\\"all\\\" selected=\\\"selected\\\">All fields</option>\\n              <option value=\\\"title\\\">Title</option>\\n              <option value=\\\"author\\\">Author</option>\\n              <option value=\\\"abstract\\\">Abstract</option>\\n              <option value=\\\"comments\\\">Comments</option>\\n              <option value=\\\"journal_ref\\\">Journal reference</option>\\n              <option value=\\\"acm_class\\\">ACM classification</option>\\n              <option value=\\\"msc_class\\\">MSC classification</option>\\n              <option value=\\\"report_num\\\">Report number</option>\\n              <option value=\\\"paper_id\\\">arXiv identifier</option>\\n              <option value=\\\"doi\\\">DOI</option>\\n              <option value=\\\"orcid\\\">ORCID</option>\\n              <option value=\\\"author_id\\\">arXiv author ID</option>\\n              <option value=\\\"help\\\">Help pages</option>\\n              <option value=\\\"full_text\\\">Full text</option>\\n            </select>\\n          </div>\\n        </div>\\n        <input type=\\\"hidden\\\" name=\\\"source\\\" value=\\\"header\\\">\\n        <button class=\\\"button is-small is-cul-darker\\\">Search</button>\\n      </div>\\n    </form>\\n  </div>\\n     </div><!-- /end desktop header -->\\n\\n      <div class=\\\"mobile-header\\\">\\n        <div class=\\\"columns is-mobile\\\">\\n          <div class=\\\"column logo-arxiv\\\"><a href=\\\"https://arxiv.org/\\\"><img src=\\\"/static/browse/0.3.4/images/arxiv-logomark-small-white.svg\\\" alt=\\\"arXiv logo\\\" style=\\\"height:60px;\\\" /></a></div>\\n          <div class=\\\"column logo-cornell\\\"><a href=\\\"https://www.cornell.edu/\\\">\\n            <picture>\\n              <source media=\\\"(min-width: 501px)\\\"\\n                srcset=\\\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w\\\"\\n                sizes=\\\"400w\\\" />\\n              <source srcset=\\\"/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x\\\" />\\n              <img src=\\\"/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg\\\" alt=\\\"Cornell University Logo\\\" />\\n            </picture>\\n          </a></div>\\n          <div class=\\\"column nav\\\" id=\\\"toggle-container\\\" role=\\\"menubar\\\">\\n            <button class=\\\"toggle-control\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\" class=\\\"icon filter-white\\\"><title>open search</title><path d=\\\"M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z\\\"/></svg></button>\\n            <div class=\\\"mobile-toggle-block toggle-target\\\">\\n              <form class=\\\"mobile-search-form\\\" method=\\\"GET\\\" action=\\\"https://arxiv.org/search\\\">\\n                <div class=\\\"field has-addons\\\">\\n                  <input class=\\\"input\\\" type=\\\"text\\\" name=\\\"query\\\" placeholder=\\\"Search...\\\" aria-label=\\\"Search term or terms\\\" />\\n                  <input type=\\\"hidden\\\" name=\\\"source\\\" value=\\\"header\\\">\\n                  <input type=\\\"hidden\\\" name=\\\"searchtype\\\" value=\\\"all\\\">\\n                  <button class=\\\"button\\\">GO</button>\\n                </div>\\n              </form>\\n            </div>\\n\\n            <button class=\\\"toggle-control\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 448 512\\\" class=\\\"icon filter-white\\\" role=\\\"menu\\\"><title>open navigation menu</title><path d=\\\"M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z\\\"/ ></svg></button>\\n            <div class=\\\"mobile-toggle-block toggle-target\\\">\\n              <nav class=\\\"mobile-menu\\\" aria-labelledby=\\\"mobilemenulabel\\\">\\n                <h2 id=\\\"mobilemenulabel\\\">quick links</h2>\\n                <ul>\\n                    <li><a href=\\\"https://arxiv.org/login\\\">Login</a></li>\\n                    <li><a href=\\\"https://info.arxiv.org/help\\\">Help Pages</a></li>\\n                    <li><a href=\\\"https://info.arxiv.org/about\\\">About</a></li>\\n                </ul>\\n              </nav>\\n            </div>\\n          </div>\\n        </div>\\n      </div><!-- /end mobile-header -->\\n    </header>\\n\\n    <main>\\n      <div id=\\\"content\\\">\\n<!--\\nrdf:RDF xmlns:rdf=\\\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\\\"\\n         xmlns:dc=\\\"http://purl.org/dc/elements/1.1/\\\"\\n         xmlns:trackback=\\\"http://madskills.com/public/xml/rss/module/trackback/\\\">\\n    <rdf:Description\\n        rdf:about=\\\"/abs/2410.23123\\\"\\n        dc:identifier=\\\"/abs/2410.23123\\\"\\n        dc:title=\\\"On Memorization of Large Language Models in Logical Reasoning\\\"\\n        trackback:ping=\\\"/trackback/2410.23123\\\" />\\n    </rdf:RDF>\\n--><div id=\\\"abs-outer\\\">\\n\\n  <div class=\\\"leftcolumn\\\">\\n    <div class=\\\"subheader\\\">\\n      <h1>Computer Science > Computation and Language</h1>\\n    </div>\\n\\n    <div class=\\\"header-breadcrumbs-mobile\\\">\\n      <strong>arXiv:2410.23123</strong> (cs)\\n    </div>\\n<link rel=\\\"stylesheet\\\" type=\\\"text/css\\\" href=\\\"/static/base/1.0.1/css/abs.css\\\">\\n<div id=\\\"content-inner\\\">\\n  <div id=\\\"abs\\\">\\n    <div class=\\\"dateline\\\">\\n  [Submitted on 30 Oct 2024 (<a href=\\\"https://arxiv.org/abs/2410.23123v1\\\">v1</a>), last revised 4 Mar 2025 (this version, v2)]</div>\\n    <h1 class=\\\"title mathjax\\\"><span class=\\\"descriptor\\\">Title:</span>On Memorization of Large Language Models in Logical Reasoning</h1>\\n    <div class=\\\"authors\\\"><span class=\\\"descriptor\\\">Authors:</span><a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+C\\\" rel=\\\"nofollow\\\">Chulin Xie</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+Y\\\" rel=\\\"nofollow\\\">Yangsibo Huang</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+C\\\" rel=\\\"nofollow\\\">Chiyuan Zhang</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+D\\\" rel=\\\"nofollow\\\">Da Yu</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+X\\\" rel=\\\"nofollow\\\">Xinyun Chen</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+B+Y\\\" rel=\\\"nofollow\\\">Bill Yuchen Lin</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+B\\\" rel=\\\"nofollow\\\">Bo Li</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Ghazi,+B\\\" rel=\\\"nofollow\\\">Badih Ghazi</a>, <a href=\\\"https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+R\\\" rel=\\\"nofollow\\\">Ravi Kumar</a></div>            <div id=\\\"download-button-info\\\" hidden>View a PDF of the paper titled On Memorization of Large Language Models in Logical Reasoning, by Chulin Xie and 8 other authors</div>\\n    <a class=\\\"mobile-submission-download\\\" href=\\\"/pdf/2410.23123\\\">View PDF</a>\\n    <a class=\\\"mobile-submission-download\\\" href=\\\"https://arxiv.org/html/2410.23123v2\\\">HTML (experimental)</a>\\n\\n\\n\\n    <blockquote class=\\\"abstract mathjax\\\">\\n            <span class=\\\"descriptor\\\">Abstract:</span>Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs&#39; reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&amp;K) puzzles. We find that LLMs could interpolate and memorize the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet they struggle with slight variations of these puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. Through in-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers, we establish that LLMs develop reasoning skills on K&amp;K puzzles alongside memorization. Finally, our analysis based on a per-sample memorization score sheds light on how LLMs switch between reasoning and memorization when solving logical puzzles. Our code and data are available at <a href=\\\"https://memkklogic.github.io\\\" rel=\\\"external noopener nofollow\\\" class=\\\"link-external link-https\\\">this https URL</a>.\\n    </blockquote>\\n\\n    <!--CONTEXT-->\\n    <div class=\\\"metatable\\\">\\n      <table summary=\\\"Additional metadata\\\"><tr>\\n          <td class=\\\"tablecell label\\\">Subjects:</td>\\n          <td class=\\\"tablecell subjects\\\">\\n            <span class=\\\"primary-subject\\\">Computation and Language (cs.CL)</span></td>\\n        </tr><tr>\\n          <td class=\\\"tablecell label\\\">Cite as:</td>\\n          <td class=\\\"tablecell arxivid\\\"><span class=\\\"arxivid\\\"><a href=\\\"https://arxiv.org/abs/2410.23123\\\">arXiv:2410.23123</a> [cs.CL]</span></td>\\n        </tr>\\n        <tr>\\n          <td class=\\\"tablecell label\\\">&nbsp;</td>\\n          <td class=\\\"tablecell arxividv\\\">(or <span class=\\\"arxivid\\\">\\n              <a href=\\\"https://arxiv.org/abs/2410.23123v2\\\">arXiv:2410.23123v2</a> [cs.CL]</span> for this version)\\n          </td>\\n        </tr>\\n        <tr>\\n          <td class=\\\"tablecell label\\\">&nbsp;</td>\\n          <td class=\\\"tablecell arxivdoi\\\">              <a href=\\\"https://doi.org/10.48550/arXiv.2410.23123\\\"  id=\\\"arxiv-doi-link\\\">https://doi.org/10.48550/arXiv.2410.23123</a><div class=\\\"button-and-tooltip\\\">\\n              <button class=\\\"more-info\\\" aria-describedby=\\\"more-info-desc-1\\\">\\n                <svg height=\\\"15\\\" role=\\\"presentation\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\"><path fill=\\\"currentColor\\\" d=\\\"M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z\\\" class=\\\"\\\"></path></svg>\\n                <span class=\\\"visually-hidden\\\">Focus to learn more</span>\\n              </button>\\n              <!-- tooltip description -->\\n              <div role=\\\"tooltip\\\" id=\\\"more-info-desc-1\\\">\\n                <span class=\\\"left-corner\\\"></span>                  arXiv-issued DOI via DataCite</div>\\n            </div>\\n          </td>\\n        </tr></table>\\n    </div>\\n  </div>\\n</div>\\n    <div class=\\\"submission-history\\\">\\n      <h2>Submission history</h2> From: Chulin Xie [<a href=\\\"/show-email/d1541218/2410.23123\\\" rel=\\\"nofollow\\\">view email</a>]      <br/>            <strong><a href=\\\"/abs/2410.23123v1\\\" rel=\\\"nofollow\\\">[v1]</a></strong>\\n        Wed, 30 Oct 2024 15:31:54 UTC (8,530 KB)<br/>\\n    <strong>[v2]</strong>\\n        Tue, 4 Mar 2025 06:22:40 UTC (9,623 KB)<br/>\\n</div>\\n  </div>\\n  <!--end leftcolumn-->\\n<div class=\\\"extra-services\\\">    <div class=\\\"full-text\\\">\\n      <a name=\\\"other\\\"></a>\\n      <span class=\\\"descriptor\\\">Full-text links:</span>\\n      <h2>Access Paper:</h2>\\n      <ul>\\n  <div id=\\\"download-button-info\\\" hidden>\\nView a PDF of the paper titled On Memorization of Large Language Models in Logical Reasoning, by Chulin Xie and 8 other authors</div><li><a href=\\\"/pdf/2410.23123\\\" aria-describedby=\\\"download-button-info\\\" accesskey=\\\"f\\\" class=\\\"abs-button download-pdf\\\">View PDF</a></li><li><a href=\\\"https://arxiv.org/html/2410.23123v2\\\" class=\\\"abs-button\\\" id=\\\"latexml-download-link\\\">HTML (experimental)</a></li><li><a href=\\\"/src/2410.23123\\\" class=\\\"abs-button download-eprint\\\">TeX Source</a></li><li><a href=\\\"/format/2410.23123\\\" class=\\\"abs-button download-format\\\">Other Formats</a></li></ul>\\n      <div class=\\\"abs-license\\\"><a href=\\\"http://arxiv.org/licenses/nonexclusive-distrib/1.0/\\\" title=\\\"Rights to this article\\\">view license</a></div>\\n    </div>\\n    <!--end full-text-->    <div class=\\\"browse\\\">\\n    Current browse context: <div class=\\\"current\\\">cs.CL</div>\\n\\n  <div class=\\\"prevnext\\\">\\n<span class=\\\"arrow\\\">\\n      <a class=\\\"abs-button prev-url\\\" href=\\\"/prevnext?id=2410.23123&amp;function=prev&amp;context=cs.CL\\\"\\n         accesskey=\\\"p\\\" title=\\\"previous in cs.CL (accesskey p)\\\" rel=\\\"nofollow\\\">&lt;&nbsp;prev</a>\\n    </span>\\n    <span class=\\\"is-hidden-mobile\\\">&nbsp; | &nbsp;</span>    <span class=\\\"arrow\\\">\\n      <a class=\\\"abs-button next-url\\\" href=\\\"/prevnext?id=2410.23123&amp;function=next&amp;context=cs.CL\\\" accesskey=\\\"n\\\"\\n         title=\\\"next in cs.CL (accesskey n)\\\"  rel=\\\"nofollow\\\">next&nbsp;&gt;</a>\\n    </span><br/>\\n  </div><div class=\\\"list\\\">\\n    <a class=\\\"abs-button abs-button-grey abs-button-small context-new\\\" href=\\\"/list/cs.CL/new\\\"  rel=\\\"nofollow\\\">new</a>\\n    <span class=\\\"is-hidden-mobile\\\"> | </span>\\n    <a class=\\\"abs-button abs-button-grey abs-button-small context-recent\\\" href=\\\"/list/cs.CL/recent\\\" rel=\\\"nofollow\\\">recent</a>\\n    <span class=\\\"is-hidden-mobile\\\"> | </span><a class=\\\"abs-button abs-button-grey abs-button-small context-id\\\" href=\\\"/list/cs.CL/2024-10\\\" rel=\\\"nofollow\\\">2024-10</a>\\n  </div><div class=\\\"abs-switch-cat\\\">\\n    Change to browse by:\\n    <div class=\\\"switch context-change\\\">\\n        <a href=\\\"/abs/2410.23123?context=cs\\\" rel=\\\"nofollow\\\">cs</a><br class=\\\"is-hidden-mobile\\\">\\n    </div>\\n  </div>\\n\\n    </div>\\n      <div class=\\\"extra-ref-cite\\\">\\n        <h3>References &amp; Citations</h3>\\n        <ul>\\n          <li><a  class=\\\"abs-button abs-button-small cite-ads\\\" href=\\\"https://ui.adsabs.harvard.edu/abs/arXiv:2410.23123\\\">NASA ADS</a></li><li><a  class=\\\"abs-button abs-button-small cite-google-scholar\\\" href=\\\"https://scholar.google.com/scholar_lookup?arxiv_id=2410.23123\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Google Scholar</a></li>\\n          <li><a  class=\\\"abs-button abs-button-small cite-semantic-scholar\\\" href=\\\"https://api.semanticscholar.org/arXiv:2410.23123\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Semantic Scholar</a></li>\\n        </ul>\\n        <div style=\\\"clear:both;\\\"></div>\\n      </div>\\n\\n<div class='extra-ref-cite'>\\n    <a id='bib-cite-css' hidden='true' href='/static/browse/0.3.4/css/cite.css'>a</a>\\n\\n    <span id='bib-cite-trigger' class=\\\"bib-cite-button abs-button\\\">export BibTeX citation</span>\\n    <span id='bib-cite-loading' hidden='true'>Loading...</span>\\n</div>\\n\\n<div id='bib-cite-modal' class='bib-modal' hidden='true'>\\n    <div class='bib-modal-content'>\\n        <div class='bib-modal-title'>\\n            <h2>BibTeX formatted citation</h2>\\n            <span class='bib-modal-close' >&times;</span>\\n        </div>\\n        <div>\\n            <textarea id='bib-cite-target' class=\\\"bib-citation-content\\\" aria-label=\\\"loading the citation\\\">loading...</textarea>\\n        </div>\\n        <div>\\n            <span>Data provided by: </span>\\n            <a id='bib-cite-source-api'></a>\\n        </div>\\n    </div>\\n</div><div class=\\\"bookmarks\\\">\\n  <div><h3>Bookmark</h3></div><a class=\\\"abs-button abs-button-grey abs-button-small\\\" href=\\\"http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2410.23123&amp;description=On Memorization of Large Language Models in Logical Reasoning\\\"\\n     title=\\\"Bookmark on BibSonomy\\\">\\n    <img src=\\\"/static/browse/0.3.4/images/icons/social/bibsonomy.png\\\"\\n         alt=\\\"BibSonomy logo\\\"/>\\n  </a>\\n  <a class=\\\"abs-button abs-button-grey abs-button-small\\\" href=\\\"https://reddit.com/submit?url=https://arxiv.org/abs/2410.23123&amp;title=On Memorization of Large Language Models in Logical Reasoning\\\"\\n     title=\\\"Bookmark on Reddit\\\">\\n    <img src=\\\"/static/browse/0.3.4/images/icons/social/reddit.png\\\"\\n         alt=\\\"Reddit logo\\\"/>\\n  </a>\\n</div>  </div>\\n  <!--end extra-services-->\\n<!-- LABS AREA -->\\n<div id=\\\"labstabs\\\">\\n  <div class=\\\"labstabs\\\"><input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"tabone\\\"checked=\\\"checked\\\">\\n    <label for=\\\"tabone\\\">Bibliographic Tools</label>\\n    <div class=\\\"tab labs-display-bib\\\">\\n      <h1>Bibliographic and Citation Tools</h1>\\n      <div class=\\\"toggle\\\">\\n        <div class=\\\"columns is-mobile lab-row\\\">\\n          <div class=\\\"column lab-switch\\\">\\n            <label class=\\\"switch\\\">\\n              <input id=\\\"bibex-toggle\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\"\\n                     data-script-url=\\\"/static/browse/0.3.4/bibex/bibex.js?20241202\\\">\\n              <span class=\\\"slider\\\"></span>\\n              <span class=\\\"is-sr-only\\\">Bibliographic Explorer Toggle</span>\\n            </label>\\n          </div>\\n          <div class=\\\"column lab-name\\\">\\n            <span id=\\\"label-for-bibex\\\">Bibliographic Explorer</span> <em>(<a href=\\\"https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer\\\">What is the Explorer?</a>)</em>\\n          </div>\\n        </div>\\n        <div class=\\\"columns is-mobile lab-row\\\">\\n          <div class=\\\"column lab-switch\\\">\\n            <label class=\\\"switch\\\">\\n              <input\\n                id=\\\"connectedpapers-toggle\\\"\\n                type=\\\"checkbox\\\"\\n                class=\\\"lab-toggle\\\"\\n                data-script-url=\\\"/static/browse/0.3.4/js/connectedpapers.js\\\"\\n                aria-labelledby=\\\"label-for-connected-papers\\\">\\n              <span class=\\\"slider\\\"></span>\\n              <span class=\\\"is-sr-only\\\">Connected Papers Toggle</span>\\n            </label>\\n          </div>\\n          <div class=\\\"column lab-name\\\">\\n            <span id=\\\"label-for-connected-papers\\\">Connected Papers</span> <em>(<a href=\\\"https://www.connectedpapers.com/about\\\" target=\\\"_blank\\\">What is Connected Papers?</a>)</em>\\n          </div>\\n        </div><div class=\\\"columns is-mobile lab-row\\\">\\n          <div class=\\\"column lab-switch\\\">\\n            <label class=\\\"switch\\\">\\n              <input\\n                id=\\\"litmaps-toggle\\\"\\n                type=\\\"checkbox\\\"\\n                class=\\\"lab-toggle\\\"\\n                data-script-url=\\\"/static/browse/0.3.4/js/litmaps.js?20210617\\\"\\n                aria-labelledby=\\\"label-for-litmaps\\\">\\n              <span class=\\\"slider\\\"></span>\\n              <span class=\\\"is-sr-only\\\">Litmaps Toggle</span>\\n            </label>\\n          </div>\\n          <div class=\\\"column lab-name\\\">\\n            <span id=\\\"label-for-litmaps\\\">Litmaps</span> <em>(<a href=\\\"https://www.litmaps.co/\\\" target=\\\"_blank\\\">What is Litmaps?</a>)</em>\\n          </div>\\n        </div>\\n        <div class=\\\"columns is-mobile lab-row\\\">\\n          <div class=\\\"column lab-switch\\\">\\n            <label class=\\\"switch\\\">\\n              <input\\n                id=\\\"scite-toggle\\\"\\n                type=\\\"checkbox\\\"\\n                class=\\\"lab-toggle\\\"\\n                data-script-url=\\\"/static/browse/0.3.4/js/scite.js?20210617\\\"\\n                aria-labelledby=\\\"label-for-scite\\\">\\n              <span class=\\\"slider\\\"></span>\\n              <span class=\\\"is-sr-only\\\">scite.ai Toggle</span>\\n            </label>\\n          </div>\\n          <div class=\\\"column lab-name\\\">\\n            <span id=\\\"label-for-scite\\\">scite Smart Citations</span> <em>(<a href=\\\"https://www.scite.ai/\\\" target=\\\"_blank\\\">What are Smart Citations?</a>)</em>\\n          </div>\\n        </div>\\n      </div>\\n        <div class=\\\"labs-content-placeholder labs-display\\\" style=\\\"display: none;\\\"></div>\\n        <div style=\\\"min-height: 15px\\\" id=\\\"connectedpapers-output\\\"></div>\\n        <div style=\\\"min-height: 15px\\\" id=\\\"litmaps-open-in\\\"></div>\\n        <div style=\\\"min-height: 15px\\\" id=\\\"scite-open-in\\\"></div>\\n    </div>\\n\\n\\n    <input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"tabtwo\\\">\\n    <label for=\\\"tabtwo\\\">Code, Data, Media</label>\\n    <div class=\\\"tab\\\">\\n      <h1>Code, Data and Media Associated with this Article</h1>\\n      <div class=\\\"toggle\\\">\\n        <div class=\\\"columns is-mobile lab-row\\\">\\n          <div class=\\\"column lab-switch\\\">\\n            <label class=\\\"switch\\\">\\n              <input\\n                id=\\\"alphaxiv-toggle\\\"\\n                data-script-url=\\\"/static/browse/0.3.4/js/alphaxiv.js\\\"\\n                type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-alphaxiv\\\">\\n              <span class=\\\"slider\\\"></span>\\n              <span class=\\\"is-sr-only\\\">alphaXiv Toggle</span>\\n            </label>\\n          </div>\\n          <div class=\\\"column lab-name\\\">\\n            <span id=\\\"label-for-alphaxiv\\\">alphaXiv</span> <em>(<a href=\\\"https://alphaxiv.org/\\\" target=\\\"_blank\\\">What is alphaXiv?</a>)</em>\\n          </div>\\n        </div>\\n\\n        <div class=\\\"columns is-mobile lab-row\\\">\\n          <div class=\\\"column lab-switch\\\">\\n            <label class=\\\"switch\\\">\\n              <input        \\n                id=\\\"catalyzex-toggle\\\"\\n                data-script-url=\\\"/static/browse/0.3.4/js/catalyzex.js\\\"\\n                type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-cx\\\">\\n              <span class=\\\"slider\\\"></span>\\n              <span class=\\\"is-sr-only\\\">Links to Code Toggle</span>\\n            </label>\\n          </div>\\n          <div class=\\\"column lab-name\\\">\\n            <span id=\\\"label-for-cx\\\">CatalyzeX Code Finder for Papers</span> <em>(<a href=\\\"https://www.catalyzex.com\\\" target=\\\"_blank\\\">What is CatalyzeX?</a>)</em>\\n          </div>\\n        </div>\\n\\n        <div class=\\\"columns is-mobile lab-row\\\">\\n          <div class=\\\"column lab-switch\\\">\\n            <label class=\\\"switch\\\">\\n              <input\\n                id=\\\"dagshub-toggle\\\"\\n                data-script-url=\\\"/static/browse/0.3.4/js/dagshub.js\\\"\\n                type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-dagshub\\\">\\n              <span class=\\\"slider\\\"></span>\\n              <span class=\\\"is-sr-only\\\">DagsHub Toggle</span>\\n            </label>\\n          </div>\\n          <div class=\\\"column lab-name\\\">\\n            <span id=\\\"label-for-dagshub\\\">DagsHub</span> <em>(<a href=\\\"https://dagshub.com/\\\" target=\\\"_blank\\\">What is DagsHub?</a>)</em>\\n          </div>\\n        </div>\\n  \\n        <div class=\\\"columns is-mobile lab-row\\\">\\n          <div class=\\\"column lab-switch\\\">\\n            <label class=\\\"switch\\\">\\n              <input\\n                id=\\\"gotitpub-toggle\\\"\\n                data-script-url=\\\"/static/browse/0.3.4/js/gotitpub.js\\\"\\n                type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-gotitpub\\\">\\n              <span class=\\\"slider\\\"></span>\\n              <span class=\\\"is-sr-only\\\">GotitPub Toggle</span>\\n            </label>\\n          </div>\\n          <div class=\\\"column lab-name\\\">\\n            <span id=\\\"label-for-gotitpub\\\">Gotit.pub</span> <em>(<a href=\\\"http://gotit.pub/faq\\\" target=\\\"_blank\\\">What is GotitPub?</a>)</em>\\n          </div>\\n        </div>\\n\\n        <div class=\\\"columns is-mobile lab-row\\\">\\n          <div class=\\\"column lab-switch\\\">\\n            <label class=\\\"switch\\\">\\n              <input\\n                id=\\\"huggingface-toggle\\\"\\n                data-script-url=\\\"/static/browse/0.3.4/js/huggingface.js\\\"\\n                type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-huggingface\\\">\\n              <span class=\\\"slider\\\"></span>\\n              <span class=\\\"is-sr-only\\\">Huggingface Toggle</span>\\n            </label>\\n          </div>\\n          <div class=\\\"column lab-name\\\">\\n            <span id=\\\"label-for-huggingface\\\">Hugging Face</span> <em>(<a href=\\\"https://huggingface.co/huggingface\\\" target=\\\"_blank\\\">What is Huggingface?</a>)</em>\\n          </div>\\n        </div>\\n\\n        <div class=\\\"columns is-mobile lab-row\\\">\\n          <div class=\\\"column lab-switch\\\">\\n            <label class=\\\"switch\\\">\\n              <input\\n                id=\\\"paperwithcode-toggle\\\"\\n                data-script-url=\\\"/static/browse/0.3.4/js/paperswithcode.js\\\"\\n                type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-pwc\\\">\\n              <span class=\\\"slider\\\"></span>\\n              <span class=\\\"is-sr-only\\\">Links to Code Toggle</span>\\n            </label>\\n          </div>\\n          <div class=\\\"column lab-name\\\">\\n            <span id=\\\"label-for-pwc\\\">Papers with Code</span> <em>(<a href=\\\"https://paperswithcode.com/\\\" target=\\\"_blank\\\">What is Papers with Code?</a>)</em>\\n          </div>\\n        </div>\\n\\n\\n        <div class=\\\"columns is-mobile lab-row\\\">\\n          <div class=\\\"column lab-switch\\\">\\n            <label class=\\\"switch\\\">\\n              <input\\n                id=\\\"sciencecast-toggle\\\"\\n                data-script-url=\\\"/static/browse/0.3.4/js/sciencecast.js\\\"\\n                type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-sciencecast\\\">\\n              <span class=\\\"slider\\\"></span>\\n              <span class=\\\"is-sr-only\\\">ScienceCast Toggle</span>\\n            </label>\\n          </div>\\n          <div class=\\\"column lab-name\\\">\\n            <span id=\\\"label-for-sciencecast\\\">ScienceCast</span> <em>(<a href=\\\"https://sciencecast.org/welcome\\\" target=\\\"_blank\\\">What is ScienceCast?</a>)</em>\\n          </div>\\n        </div>\\n      </div>\\n\\n      <div id=\\\"alphaxiv-output\\\" style=\\\"display:none\\\"></div>\\n      <div id=\\\"catalyzex-output\\\" style=\\\"display:none\\\"></div>\\n      <div id=\\\"dagshub-output\\\" style=\\\"display:none\\\"></div>\\n      <div id=\\\"gotitpub-output\\\" style=\\\"display:none\\\"></div>\\n      <div id=\\\"pwc-output\\\" style=\\\"display:none\\\"></div>\\n      <div id=\\\"pwc-data-output\\\" style=\\\"display:none\\\"></div>\\n      <div id=\\\"sciencecast-output\\\" style=\\\"display:none\\\"></div>\\n      <div id=\\\"huggingface-output\\\" style=\\\"display:none\\\"></div>\\n    </div>\\n\\n\\n      <input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"labstabs-demos-input\\\">\\n      <label for=\\\"labstabs-demos-input\\\" id=\\\"labstabs-demos-label\\\">Demos</label>\\n      <div class=\\\"tab\\\">\\n        <h1>Demos</h1>\\n        <div class=\\\"toggle\\\">\\n          <div class=\\\"columns is-mobile lab-row\\\">\\n            <div class=\\\"column lab-switch\\\">\\n              <label class=\\\"switch\\\">\\n                <input\\n                  id=\\\"replicate-toggle\\\"\\n                  data-script-url=\\\"/static/browse/0.3.4/js/replicate.js\\\"\\n                  type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-replicate\\\">\\n                <span class=\\\"slider\\\"></span>\\n                <span class=\\\"is-sr-only\\\">Replicate Toggle</span>\\n              </label>\\n            </div>\\n            <div class=\\\"column lab-name\\\">\\n              <span id=\\\"label-for-replicate\\\">Replicate</span> <em>(<a href=\\\"https://replicate.com/docs/arxiv/about\\\" target=\\\"_blank\\\">What is Replicate?</a>)</em>\\n            </div>\\n          </div>\\n          <div class=\\\"columns is-mobile lab-row\\\">\\n            <div class=\\\"column lab-switch\\\">\\n              <label class=\\\"switch\\\">\\n                <input\\n                  id=\\\"spaces-toggle\\\"\\n                  data-script-url=\\\"/static/browse/0.3.4/js/spaces.js\\\"\\n                  type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-spaces\\\">\\n                <span class=\\\"slider\\\"></span>\\n                <span class=\\\"is-sr-only\\\">Spaces Toggle</span>\\n              </label>\\n            </div>\\n            <div class=\\\"column lab-name\\\">\\n              <span id=\\\"label-for-spaces\\\">Hugging Face Spaces</span> <em>(<a href=\\\"https://huggingface.co/docs/hub/spaces\\\" target=\\\"_blank\\\">What is Spaces?</a>)</em>\\n            </div>\\n          </div>\\n          <div class=\\\"columns is-mobile lab-row\\\">\\n            <div class=\\\"column lab-switch\\\">\\n              <label class=\\\"switch\\\">\\n                <input\\n                  id=\\\"txyz-toggle\\\"\\n                  data-script-url=\\\"/static/browse/0.3.4/js/txyz.js\\\"\\n                  type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-txyz\\\">\\n                <span class=\\\"slider\\\"></span>\\n                <span class=\\\"is-sr-only\\\">Spaces Toggle</span>\\n              </label>\\n            </div>\\n            <div class=\\\"column lab-name\\\">\\n              <span id=\\\"label-for-txyz\\\">TXYZ.AI</span> <em>(<a href=\\\"https://txyz.ai\\\" target=\\\"_blank\\\">What is TXYZ.AI?</a>)</em>\\n            </div>\\n          </div>\\n        </div>\\n        <div id=\\\"replicate-output\\\"></div>\\n        <div id=\\\"spaces-output\\\"></div>\\n        <div id=\\\"txyz-output\\\"></div>\\n      </div>\\n      <input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"tabfour\\\">\\n      <label for=\\\"tabfour\\\">Related Papers</label>\\n      <div class=\\\"tab\\\">\\n        <h1>Recommenders and Search Tools</h1>\\n        <div class=\\\"toggle\\\">\\n          <div class=\\\"columns is-mobile lab-row\\\">\\n            <div class=\\\"column lab-switch\\\">\\n              <label class=\\\"switch\\\">\\n                <input id=\\\"influenceflower-toggle\\\"\\n                data-script-url=\\\"/static/browse/0.3.4/js/influenceflower.js\\\"\\n                type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-influenceflower\\\">\\n                <span class=\\\"slider\\\"></span>\\n                <span class=\\\"is-sr-only\\\">Link to Influence Flower</span>\\n              </label>\\n            </div>\\n            <div class=\\\"column lab-name\\\">\\n              <span id=\\\"label-for-influenceflower\\\">Influence Flower</span> <em>(<a href=\\\"https://influencemap.cmlab.dev/\\\" target=\\\"_blank\\\">What are Influence Flowers?</a>)</em>\\n            </div>\\n          </div>\\n          <div class=\\\"columns is-mobile lab-row\\\">\\n            <div class=\\\"column lab-switch\\\">\\n              <label class=\\\"switch\\\">\\n                <input id=\\\"core-recommender-toggle\\\" type=\\\"checkbox\\\" class=\\\"lab-toggle\\\" aria-labelledby=\\\"label-for-core\\\">\\n                <span class=\\\"slider\\\"></span>\\n                <span class=\\\"is-sr-only\\\">Core recommender toggle</span>\\n              </label>\\n            </div>\\n            <div class=\\\"column lab-name\\\">\\n              <span id=\\\"label-for-core\\\">CORE Recommender</span> <em>(<a href=\\\"https://core.ac.uk/services/recommender\\\">What is CORE?</a>)</em>\\n            </div>\\n          </div></div>\\n        <div id=\\\"influenceflower-output\\\"></div>\\n        <div id=\\\"influenceflower-output-graph\\\" style=\\\"display:none\\\">\\n          <ul class=\\\"flower-tabs\\\">\\n            <li class=\\\"active\\\"><a class=\\\"btn tab-btn\\\" onclick=\\\"openTab(event, 'tab-author')\\\">Author</a></li>\\n            <li><a class=\\\"btn tab-btn\\\" onclick=\\\"openTab(event, 'tab-venue')\\\">Venue</a></li>\\n            <li><a class=\\\"btn tab-btn\\\" onclick=\\\"openTab(event, 'tab-inst')\\\">Institution</a></li>\\n            <li><a class=\\\"btn tab-btn\\\" onclick=\\\"openTab(event, 'tab-topic')\\\">Topic</a></li>\\n          </ul>\\n          <div class=\\\"flower-tab-content\\\">\\n            <div class=\\\"tab-flower active\\\" id=\\\"tab-author\\\"><svg id=\\\"flower-graph-author\\\"></svg></div>\\n            <div class=\\\"tab-flower\\\" id=\\\"tab-venue\\\"><svg id=\\\"flower-graph-venue\\\"></svg></div>\\n            <div class=\\\"tab-flower\\\" id=\\\"tab-inst\\\"><svg id=\\\"flower-graph-inst\\\"></svg></div>\\n            <div class=\\\"tab-flower\\\" id=\\\"tab-topic\\\"><svg id=\\\"flower-graph-topic\\\"></svg></div>\\n          </div>\\n        </div>\\n        <div id=\\\"coreRecommenderOutput\\\"></div>\\n        <div id=\\\"iarxivOutput\\\"></div>\\n      </div>\\n\\n      <input type=\\\"radio\\\" name=\\\"tabs\\\" id=\\\"tabfive\\\">\\n      <label for=\\\"tabfive\\\">\\n        About arXivLabs\\n      </label>\\n      <div class=\\\"tab\\\">\\n        <div class=\\\"columns\\\">\\n          <div class=\\\"column\\\">\\n            <h1>arXivLabs: experimental projects with community collaborators</h1>\\n            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>\\n            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>\\n            <p>Have an idea for a project that will add value for arXiv's community? <a href=\\\"https://info.arxiv.org/labs/index.html\\\"><strong>Learn more about arXivLabs</strong></a>.</p>\\n          </div>\\n          <div class=\\\"column is-narrow is-full-mobile\\\">\\n            <p class=\\\"icon-labs\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" role=\\\"presentation\\\" viewBox=\\\"0 0 635.572 811\\\"><path d=\\\"M175.6 676v27h-27v-27zm-54 27v27h27v-27zm-27 27v27h27v-27zm396-54v27h-27v-27zm0 27v27h27v-27zm27 27v27h27v-27zm-27-414h27v27h-27zm27 0h27v-27h-27zm27-27h27v-27h-27zm-396 45h-27v-27h27zm-27-54h-27v27h27zm-27-27h-27v27h27z\\\"/><path d=\\\"M94.6 730v27h-27v-27zm477 0v27h-27v-27zm-27-495h27v27h-27zm-450 18h-27v-27h27zm477 9h27v27h-27zm-54 495h27v27h-27zm-423 0h27v27h-27zm-54-504h27v27h-27z\\\" fill=\\\"#666\\\"/><path d=\\\"M67.6 730v27h-27v-27zm54 54v27h-27v-27zm0-108v27h27v-27zm-27 27v27h27v-27zm-81 0v27h27v-27zm585 27v27h-27v-27zm-108-54v27h27v-27zm27 27v27h27v-27zm81 0v27h27v-27zm-54-495h27v27h-27zm-54 108h27v-27h-27zm27-27h27v-27h-27zm0-81h27v-27h-27zm-423 18h-27v-27h27zm54 54h-27v27h27zm-27-27h-27v27h27zm0-81h-27v27h27zm423 612v27h-27v-27zm81-522v27h-27v-27zm-585-9v27h-27v-27z\\\" fill=\\\"#999\\\"/><path d=\\\"M94.6 784v27h-27v-27zm-27-27v27h27v-27zm-27-54v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm-108 81v27h27v-27zm558 54v27h-27v-27zm-27-27v27h27v-27zm27-54v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm108 81v27h27v-27zm0-495h27v27h-27zm-27 27h27v-27h-27zm-54-27h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm81-108h27v-27h-27zm-504 45h-27v-27h27zm27-27h-27v27h27zm54-27h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm-81-108h-27v27h27z\\\" fill=\\\"#ccc\\\"/><path d=\\\"M598.6 665.1H41.5C-76.5 667 176 280.2 176 280.2h53a46.5 46.5 0 0162.8-56.3 29.2 29.2 0 1128.5 35.9h-1a46.5 46.5 0 01-1.5 20.3l142.5-.1s255.3 387 138.3 385.1zM291 181a29.3 29.3 0 10-29.2-29.3A29.3 29.3 0 00291 181zm65.4-66.8a22.4 22.4 0 10-22.5-22.4 22.4 22.4 0 0022.5 22.4z\\\" fill=\\\"#fc0\\\"/><path d=\\\"M245.5 172V10h153v162s324 495 198 495h-558c-126 0 207-495 207-495zm126 54h56m-13 72h56m-9 72h56m-20 72h56m-22 72h56m-29 72h56m-457-45c20.8 41.7 87.3 81 160.7 81 72.1 0 142.1-38.2 163.4-81\\\" fill=\\\"none\\\" stroke=\\\"#000\\\" stroke-miterlimit=\\\"10\\\" stroke-width=\\\"20\\\"/><path d=\\\"M273.3 421.7c0 31-9.8 56.3-21.9 56.3s-21.8-25.2-21.8-56.3 9.8-56.3 21.8-56.3 21.9 25.2 21.9 56.3zm114.4-56.3c-12 0-21.8 25.2-21.8 56.3s9.7 56.3 21.8 56.3 21.9-25.2 21.9-56.3-9.8-56.3-21.9-56.3zM150.1 526.6c-18.2 6.7-27.5 22.9-23.2 30.2s14.8-5.5 33-12.2 37.4-4.9 33-12.2-24.5-12.6-42.8-5.8zm296 5.8c-4.2 7.3 14.9 5.5 33.1 12.2s28.7 19.5 33 12.2-5-23.5-23.2-30.2-38.5-1.5-42.8 5.8z\\\"/></svg></p>\\n          </div>\\n        </div>\\n      </div>\\n\\n    </div>\\n</div>\\n<!-- END LABS AREA -->\\n  <div class=\\\"endorsers\\\">\\n    <a href=\\\"/auth/show-endorsers/2410.23123\\\" class=\\\"endorser-who\\\" rel=\\\"nofollow\\\">Which authors of this paper are endorsers?</a> |\\n    <a id=\\\"mathjax_toggle\\\" href=\\\"javascript:setMathjaxCookie()\\\">Disable MathJax</a> (<a href=\\\"https://info.arxiv.org/help/mathjax.html\\\">What is MathJax?</a>)\\n    <span class=\\\"help\\\" style=\\\"font-style: normal; float: right; margin-top: 0; margin-right: 1em;\\\"></span>\\n  </div>\\n  <script type=\\\"text/javascript\\\" language=\\\"javascript\\\">mathjaxToggle();</script>\\n</div>\\n      </div>\\n    </main>\\n\\n    <footer style=\\\"clear: both;\\\">\\n      <div class=\\\"columns is-desktop\\\" role=\\\"navigation\\\" aria-label=\\\"Secondary\\\" style=\\\"margin: -0.75em -0.75em 0.75em -0.75em\\\">\\n        <!-- Macro-Column 1 -->\\n        <div class=\\\"column\\\" style=\\\"padding: 0;\\\">\\n          <div class=\\\"columns\\\">\\n            <div class=\\\"column\\\">\\n              <ul style=\\\"list-style: none; line-height: 2;\\\">\\n                <li><a href=\\\"https://info.arxiv.org/about\\\">About</a></li>\\n                <li><a href=\\\"https://info.arxiv.org/help\\\">Help</a></li>\\n              </ul>\\n            </div>\\n            <div class=\\\"column\\\">\\n              <ul style=\\\"list-style: none; line-height: 2;\\\">\\n                <li>\\n                  <svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\" class=\\\"icon filter-black\\\" role=\\\"presentation\\\"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d=\\\"M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z\\\"/></svg>\\n                  <a href=\\\"https://info.arxiv.org/help/contact.html\\\"> Contact</a>\\n                </li>\\n                <li>\\n                  <svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\" class=\\\"icon filter-black\\\" role=\\\"presentation\\\"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d=\\\"M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z\\\"/></svg>\\n                  <a href=\\\"https://info.arxiv.org/help/subscribe\\\"> Subscribe</a>\\n                </li>\\n              </ul>\\n            </div>\\n          </div>\\n        </div>\\n        <!-- End Macro-Column 1 -->\\n        <!-- Macro-Column 2 -->\\n        <div class=\\\"column\\\" style=\\\"padding: 0;\\\">\\n          <div class=\\\"columns\\\">\\n            <div class=\\\"column\\\">\\n              <ul style=\\\"list-style: none; line-height: 2;\\\">\\n                <li><a href=\\\"https://info.arxiv.org/help/license/index.html\\\">Copyright</a></li>\\n                <li><a href=\\\"https://info.arxiv.org/help/policies/privacy_policy.html\\\">Privacy Policy</a></li>\\n              </ul>\\n            </div>\\n            <div class=\\\"column sorry-app-links\\\">\\n              <ul style=\\\"list-style: none; line-height: 2;\\\">\\n                <li><a href=\\\"https://info.arxiv.org/help/web_accessibility.html\\\">Web Accessibility Assistance</a></li>\\n                <li>\\n                  <p class=\\\"help\\\">\\n                    <a class=\\\"a11y-main-link\\\" href=\\\"https://status.arxiv.org\\\" target=\\\"_blank\\\">arXiv Operational Status <svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 256 512\\\" class=\\\"icon filter-dark_grey\\\" role=\\\"presentation\\\"><path d=\\\"M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z\\\"/></svg></a><br>\\n                    Get status notifications via\\n                    <a class=\\\"is-link\\\" href=\\\"https://subscribe.sorryapp.com/24846f03/email/new\\\" target=\\\"_blank\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 512 512\\\" class=\\\"icon filter-black\\\" role=\\\"presentation\\\"><path d=\\\"M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z\\\"/></svg>email</a>\\n                    or <a class=\\\"is-link\\\" href=\\\"https://subscribe.sorryapp.com/24846f03/slack/new\\\" target=\\\"_blank\\\"><svg xmlns=\\\"http://www.w3.org/2000/svg\\\" viewBox=\\\"0 0 448 512\\\" class=\\\"icon filter-black\\\" role=\\\"presentation\\\"><path d=\\\"M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z\\\"/></svg>slack</a>\\n                  </p>\\n                </li>\\n              </ul>\\n            </div>\\n          </div>\\n        </div> <!-- end MetaColumn 2 -->\\n        <!-- End Macro-Column 2 -->\\n      </div>\\n    </footer>\\n  </div>\\n\\n  <script src=\\\"/static/base/1.0.1/js/member_acknowledgement.js\\\"></script>\\n\\n</body>\\n\\n</html>\",\"annotations\":null}", "tool_call_id": "toolu_01Ws5Vzbfdd98bq7kZMEtMya"}, {"role": "assistant", "content": "Now let me search for more specific information about the datasets to get the exact details needed for the table:", "tool_calls": [{"id": "toolu_01XtowyAVtcbkZyrkDpXAtZm", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"BBH Big-Bench Hard dataset tasks number training set difficulty levels\"}"}}]}, {"role": "tool", "content": "Title: BIG-Bench Extra Hard\nLink: https://arxiv.org/html/2502.19187v1\nSnippet: We introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation.\nSitelinks: No sitelinks\n\nTitle: suzgunmirac/BIG-Bench-Hard\nLink: https://github.com/suzgunmirac/BIG-Bench-Hard\nSnippet: In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model ...\nSitelinks: No sitelinks\n\nTitle: LLM Benchmark Datasets for Training and Evaluation\nLink: https://medium.com/@sulbha.jindal/llm-benchmark-datasets-for-training-and-evaluation-1ec271129d7c\nSnippet: BIG-Bench, BIG-Bench Hard (BBH), BIG-Bench Extra Hard (BBEH): These are collections of challenging tasks aimed at pushing the boundaries of LLM ...\nSitelinks: No sitelinks\n\nTitle: Joschka/big_bench_hard \u00b7 Datasets at Hugging Face\nLink: https://huggingface.co/datasets/Joschka/big_bench_hard\nSnippet: BIG-Bench Hard, focuss on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model ...\nSitelinks: No sitelinks\n\nTitle: Top LLM Benchmarks Explained: MMLU, HellaSwag, BBH ...\nLink: https://www.confident-ai.com/blog/llm-benchmarks-mmlu-hellaswag-and-beyond\nSnippet: BIG-Bench Hard (Beyond the Imitation Game Benchmark)\u200b\u200b BIG-Bench Hard (BBH) selects 23 challenging tasks from the original BIG-Bench suite, which consisted of a ...\nSitelinks: No sitelinks\n\nTitle: \"Challenging BIG-Bench Tasks and Whether Chain-of- ...\nLink: https://www.reddit.com/r/mlscaling/comments/y6tzck/challenging_bigbench_tasks_and_whether/\nSnippet: A lot of the BBH tasks are also pretty narrow, and thus trivial to write a simple program for (e.g. navigate, word sorting). I'd be more ...\nSitelinks: No sitelinks\n\nTitle: Challenging BIG-Bench Tasks and Whether Chain-of- ...\nLink: https://aclanthology.org/2023.findings-acl.824.pdf\nSnippet: by M Suzgun \u00b7 2023 \u00b7 Cited by 1009 \u2014 In this paper, we curate BIG-Bench Hard (BBH), a subset of 23 particularly challeng- ing BIG-Bench tasks (27 subtasks) for which no prior ...\nSitelinks: No sitelinks\n\nTitle: [2210.09261] Challenging BIG-Bench Tasks and Whether ...\nLink: https://arxiv.org/abs/2210.09261\nSnippet: by M Suzgun \u00b7 2022 \u00b7 Cited by 1009 \u2014 In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior ...\nSitelinks: No sitelinks\n\nTitle: (PDF) BIG-Bench Extra Hard\nLink: https://www.researchgate.net/publication/389392420_BIG-Bench_Extra_Hard\nSnippet: BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We ...\nSitelinks: No sitelinks\n\nTitle: A collection of benchmarks and datasets for evaluating LLM.\nLink: https://github.com/leobeeson/llm_benchmarks\nSnippet: Big-Bench Hard (BBH). Description: A subset of BIG-Bench focusing on the most challenging tasks requiring multi-step reasoning. Purpose ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01XtowyAVtcbkZyrkDpXAtZm"}, {"role": "assistant", "content": null, "tool_calls": [{"id": "toolu_01SrHubZvVsZB2vqL7n34TGe", "type": "function", "function": {"name": "fetch-fetch_html", "arguments": "{\"url\": \"https://github.com/suzgunmirac/BIG-Bench-Hard\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"\\n\\n\\n\\n\\n\\n\\n<!DOCTYPE html>\\n<html\\n  lang=\\\"en\\\"\\n  \\n  data-color-mode=\\\"auto\\\" data-light-theme=\\\"light\\\" data-dark-theme=\\\"dark\\\"\\n  data-a11y-animated-images=\\\"system\\\" data-a11y-link-underlines=\\\"true\\\"\\n  \\n  >\\n\\n\\n\\n\\n  <head>\\n    <meta charset=\\\"utf-8\\\">\\n  <link rel=\\\"dns-prefetch\\\" href=\\\"https://github.githubassets.com\\\">\\n  <link rel=\\\"dns-prefetch\\\" href=\\\"https://avatars.githubusercontent.com\\\">\\n  <link rel=\\\"dns-prefetch\\\" href=\\\"https://github-cloud.s3.amazonaws.com\\\">\\n  <link rel=\\\"dns-prefetch\\\" href=\\\"https://user-images.githubusercontent.com/\\\">\\n  <link rel=\\\"preconnect\\\" href=\\\"https://github.githubassets.com\\\" crossorigin>\\n  <link rel=\\\"preconnect\\\" href=\\\"https://avatars.githubusercontent.com\\\">\\n\\n  \\n\\n  <link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/light-6448649c7147.css\\\" /><link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/light_high_contrast-42fc7e3b06b7.css\\\" /><link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/dark-d17b946fc2c5.css\\\" /><link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/dark_high_contrast-1b924088c83a.css\\\" /><link data-color-theme=\\\"light\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/light-6448649c7147.css\\\" /><link data-color-theme=\\\"light_high_contrast\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/light_high_contrast-42fc7e3b06b7.css\\\" /><link data-color-theme=\\\"light_colorblind\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/light_colorblind-44cfaf0c8f7b.css\\\" /><link data-color-theme=\\\"light_colorblind_high_contrast\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/light_colorblind_high_contrast-979217efd93e.css\\\" /><link data-color-theme=\\\"light_tritanopia\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/light_tritanopia-4d5383026bfa.css\\\" /><link data-color-theme=\\\"light_tritanopia_high_contrast\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/light_tritanopia_high_contrast-ff6ff8532348.css\\\" /><link data-color-theme=\\\"dark\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/dark-d17b946fc2c5.css\\\" /><link data-color-theme=\\\"dark_high_contrast\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/dark_high_contrast-1b924088c83a.css\\\" /><link data-color-theme=\\\"dark_colorblind\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/dark_colorblind-654786382462.css\\\" /><link data-color-theme=\\\"dark_colorblind_high_contrast\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/dark_colorblind_high_contrast-ecca008c6f6e.css\\\" /><link data-color-theme=\\\"dark_tritanopia\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/dark_tritanopia-fff376053989.css\\\" /><link data-color-theme=\\\"dark_tritanopia_high_contrast\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/dark_tritanopia_high_contrast-49adf52571e5.css\\\" /><link data-color-theme=\\\"dark_dimmed\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/dark_dimmed-66d97c13c98a.css\\\" /><link data-color-theme=\\\"dark_dimmed_high_contrast\\\" crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" data-href=\\\"https://github.githubassets.com/assets/dark_dimmed_high_contrast-c58f1d0432b9.css\\\" />\\n\\n  <style type=\\\"text/css\\\">\\n    :root {\\n      --tab-size-preference: 4;\\n    }\\n\\n    pre, code {\\n      tab-size: var(--tab-size-preference);\\n    }\\n  </style>\\n\\n    <link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/primer-primitives-dc7ca6859caf.css\\\" />\\n    <link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/primer-f96b923db733.css\\\" />\\n    <link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/global-2744ca59d025.css\\\" />\\n    <link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/github-efdc4f8b5091.css\\\" />\\n  <link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/repository-5d735668c600.css\\\" />\\n<link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/code-9c9b8dc61e74.css\\\" />\\n\\n  \\n\\n  <script type=\\\"application/json\\\" id=\\\"client-env\\\">{\\\"locale\\\":\\\"en\\\",\\\"featureFlags\\\":[\\\"alternate_user_config_repo\\\",\\\"api_insights_show_missing_data_banner\\\",\\\"attestations_filtering\\\",\\\"attestations_sorting\\\",\\\"billing_fixed_amount_metered_configured_discounts\\\",\\\"billing_show_top_100_users_usage_table\\\",\\\"client_version_header\\\",\\\"codespaces_prebuild_region_target_update\\\",\\\"contact_requests_implicit_opt_in\\\",\\\"contact_sales_locale_utm_medium\\\",\\\"contentful_lp_enterprise\\\",\\\"contentful_lp_footnotes\\\",\\\"copilot_agent_prefetch_tasks\\\",\\\"copilot_agent_task_caching\\\",\\\"copilot_agent_tasks_btn_code_nav\\\",\\\"copilot_agent_tasks_btn_code_view\\\",\\\"copilot_agent_tasks_btn_code_view_lines\\\",\\\"copilot_agent_tasks_btn_file_reference\\\",\\\"copilot_api_agentic_issue_marshal_yaml\\\",\\\"copilot_api_draft_issue_code_search\\\",\\\"copilot_chat_attach_multiple_images\\\",\\\"copilot_chat_file_redirect\\\",\\\"copilot_chat_group_notifications\\\",\\\"copilot_chat_reduce_quota_checks\\\",\\\"copilot_chat_search_bar_redirect\\\",\\\"copilot_chat_selection_attachments\\\",\\\"copilot_chat_vision_in_claude\\\",\\\"copilot_chat_vision_skip_thread_create\\\",\\\"copilot_coding_agent_diff_stats\\\",\\\"copilot_custom_copilots_feature_preview\\\",\\\"copilot_disable_generated_commit_description\\\",\\\"copilot_duplicate_thread\\\",\\\"copilot_features_raycast_logo\\\",\\\"copilot_free_to_paid_telem\\\",\\\"copilot_ftp_hyperspace_upgrade_prompt\\\",\\\"copilot_ftp_settings_upgrade\\\",\\\"copilot_ftp_upgrade_to_pro_from_models\\\",\\\"copilot_ftp_your_copilot_settings\\\",\\\"copilot_global_overlay_v2\\\",\\\"copilot_immersive_agent_branch_selection\\\",\\\"copilot_immersive_structured_model_picker\\\",\\\"copilot_loops_share_button\\\",\\\"copilot_no_floating_button\\\",\\\"copilot_read_shared_conversation\\\",\\\"copilot_show_copilot_sub_issues_button_on_issues_page\\\",\\\"copilot_spark_allow_empty_commit\\\",\\\"copilot_spark_progressive_error_handling\\\",\\\"copilot_spark_single_user_iteration\\\",\\\"copilot_spark_use_billing_headers\\\",\\\"copilot_spark_write_iteration_history_to_git\\\",\\\"copilot_stable_conversation_view\\\",\\\"copilot_workbench_agent_seed_tool\\\",\\\"copilot_workbench_cache\\\",\\\"copilot_workbench_connection_reload_banner\\\",\\\"copilot_workbench_preview_analytics\\\",\\\"copilot_workbench_ratelimit_fallback\\\",\\\"copilot_workbench_refresh_on_wsod\\\",\\\"copilot_workbench_synthetic_generation\\\",\\\"dashboard_public_preview\\\",\\\"direct_to_salesforce\\\",\\\"dotcom_chat_client_side_skills\\\",\\\"failbot_report_error_react_apps_on_page\\\",\\\"ghost_pilot_confidence_truncation_25\\\",\\\"ghost_pilot_confidence_truncation_40\\\",\\\"global_nav_reductive_user_menu\\\",\\\"global_search_multi_orgs\\\",\\\"hpc_improve_dom_insertion_observer\\\",\\\"hyperspace_diff_view\\\",\\\"inp_reduced_threshold\\\",\\\"insert_before_patch\\\",\\\"issue_fields_report_usage\\\",\\\"issues_react_blur_item_picker_on_close\\\",\\\"issues_react_bots_timeline_pagination\\\",\\\"issues_react_prohibit_title_fallback\\\",\\\"issues_react_remove_placeholders\\\",\\\"issues_sticky_sidebar\\\",\\\"lifecycle_label_name_updates\\\",\\\"link_contact_sales_swp_marketo\\\",\\\"marketing_pages_search_explore_provider\\\",\\\"mcp_registry_install\\\",\\\"memex_mwl_filter_field_delimiter\\\",\\\"migrate_toasts_to_banners_web_notifications\\\",\\\"new_traffic_page_banner\\\",\\\"override_pulse_legacy_url\\\",\\\"primer_react_segmented_control_tooltip\\\",\\\"primer_react_unified_portal_root\\\",\\\"record_sso_banner_metrics\\\",\\\"releases_update_ref_selector\\\",\\\"remove_child_patch\\\",\\\"repos_insights_remove_new_url\\\",\\\"sample_network_conn_type\\\",\\\"scheduled_reminders_updated_limits\\\",\\\"show_edit_on_default_option\\\",\\\"site_homepage_contentful\\\",\\\"site_msbuild_webgl_hero\\\",\\\"spark_commit_on_default_branch\\\",\\\"spark_show_data_access_on_publish\\\",\\\"spark_sync_repository_after_iteration\\\",\\\"viewscreen_sandbox\\\",\\\"webp_support\\\",\\\"workbench_default_sonnet4\\\",\\\"workbench_store_readonly\\\"],\\\"copilotApiOverrideUrl\\\":\\\"https://api.githubcopilot.com\\\"}</script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/high-contrast-cookie-f3788027bd8d.js\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/wp-runtime-e02e58e8fee7.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_oddbird_popover-polyfill_dist_popover-fn_js-468bf7cab607.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_stacktrace-parser_dist_stack-trace-parser_esm_js-node_modules_github_bro-2f4e04-280c10ec004d.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_arianotify-polyfill_ariaNotify-polyfill_js-node_modules_github_mi-c8eeba-690858154b11.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/environment-e36acb721009.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_primer_behaviors_dist_esm_index_mjs-7e8c9c5d642d.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_relative-time-element_dist_index_js-c98257dc79a7.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_selector-observer_dist_index_esm_js-7e4d99c9171d.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_auto-complete-element_dist_index_js-node_modules_github_catalyst_-0d7d60-9ac2b374b638.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_text-expander-element_dist_index_js-754f5b5e9e7e.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_remote-inp-b5f1d7-fd008e5636c7.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_markdown-toolbar-element_dist_index_js-d41270eb61be.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_primer_view-co-c63e9a-b1f4633cac7f.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/github-elements-e1b8d79d7efc.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/element-registry-0d3ad88220f7.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_index_js-node_modules_github_hydro-analytics-c-2a6094-2f3b5bfd7e3b.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_braintree_browser-detection_dist_browser-detection_js-node_modules_githu-bb80ec-4e90f1d1076c.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_lit-html_lit-html_js-06c8637a6071.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_morphdom_dist_morphdom-esm_js-node_modules_swc_helpers_esm__define_property_js-5c587dccb470.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_turbo_dist_turbo_es2017-esm_js-8eb9b2209bcd.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-893f9f-cd5d89ebdb50.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_quote-selection_dist_index_js-node_modules_github_session-resume_-590a09-91fdc8802bfa.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/packages_document-metadata_document-metadata_ts-packages_failbot_failbot_ts-b8e9eefcce78.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/packages_updatable-content_updatable-content_ts-a743e72edcf2.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_ajax-error_ts-app_assets_modules_github_behaviors_details-6493f1-5af0195d8874.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_task-list_ts-app_assets_modules_github_throttled-input_ts-047775-82bfe789bb2f.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/app_assets_modules_github_behaviors_commenting_edit_ts-app_assets_modules_github_behaviors_ht-83c235-6bc7a8849328.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/behaviors-2146aa3dcd3f.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_delegated-events_dist_index_js-node_modules_github_catalyst_lib_index_js-ef6d0f-641e2ed302b3.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/notifications-global-f5c28dea1388.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_index_js-node_modules_virtualized-list_es_inde-8b3def-5e84a4d714f0.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-970f7d-7c635f3a5fee.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/app_assets_modules_github_ref-selector_ts-98da180bcdc4.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/codespaces-d537a7dd382d.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_filter-input-element_dist_index_js-node_modules_github_remote-inp-3eebbd-154278e19722.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_decorators_js-node_modules_delegated-events_di-e161aa-34194327b80d.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_file-attachment-element_dist_index_js-node_modules_github_remote--abdaf7-e83eee3cdca9.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/repositories-1210f79a59a5.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_mini-throttle_dist_index_js-node_modules_github_catalyst_lib_inde-96937f-07580acb275f.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/code-menu-f0fbee92a5e1.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/primer-react-0af0c1e5dc77.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/react-core-9be5e3ed4ae2.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/react-lib-17ccbc80f53b.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/octicons-react-0f0d82031c98.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_emotion_is-prop-valid_dist_emotion-is-prop-valid_esm_js-node_modules_emo-1fff13-a67dacb6db80.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_tanstack_query-core_build_modern_mutation_js-node_modules_tanstack_query-9bf7e4-a1bacdef8ef2.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_tanstack_query-core_build_modern_queryClient_js-node_modules_tanstack_re-d87a32-ae1d44ebdc99.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_tanstack_query-core_build_modern_queryObserver_js-node_modules_tanstack_-defd52-362960c8d059.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_hydro-analytics-client_dist_analytics-client_js-node_modules_gith-320808-1c166cd40521.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/packages_paths_index_ts-541dcbefcfc4.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/packages_agent-sessions_utils_elapsed-time-util_ts-packages_agent-sessions_contexts_PullConte-004f94-40ebf871a235.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/packages_copilot-coding-agent-status_entry_ts-packages_fetch-headers_fetch-headers_ts-package-442e79-250816e78b79.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/copilot-coding-agent-status-b3c7a806b3f3.js\\\" defer=\\\"defer\\\"></script>\\n<link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/primer-react.1376d83c759fa3dd6e71.module.css\\\" />\\n<link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/copilot-coding-agent-status.18a276928d4b7c45824e.module.css\\\" />\\n  \\n  <script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_lit-labs_react_index_js-node_modules_primer_styled-react_dist_index_js-n-6066db-c8c65df23234.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/packages_notifications-subscriptions-menu_entry_ts-packages_promise-with-resolvers-polyfill_p-df0233-b36224de01b1.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/notifications-subscriptions-menu-369f323728b4.js\\\" defer=\\\"defer\\\"></script>\\n<link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/notifications-subscriptions-menu.44a4ce5e60cfd5c27b1a.module.css\\\" />\\n\\n  <link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/notifications-subscriptions-menu.44a4ce5e60cfd5c27b1a.module.css\\\" />\\n\\n\\n  <title>GitHub - suzgunmirac/BIG-Bench-Hard: Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them</title>\\n\\n\\n\\n  <meta name=\\\"route-pattern\\\" content=\\\"/:user_id/:repository\\\" data-turbo-transient>\\n  <meta name=\\\"route-controller\\\" content=\\\"files\\\" data-turbo-transient>\\n  <meta name=\\\"route-action\\\" content=\\\"disambiguate\\\" data-turbo-transient>\\n  <meta name=\\\"fetch-nonce\\\" content=\\\"v2:7a9ceba1-5824-e918-8cc7-44011ac6d5b6\\\">\\n\\n    \\n  <meta name=\\\"current-catalog-service-hash\\\" content=\\\"f3abb0cc802f3d7b95fc8762b94bdcb13bf39634c40c357301c4aa1d67a256fb\\\">\\n\\n\\n  <meta name=\\\"request-id\\\" content=\\\"C91E:2DC770:937B07:B4FDFB:68CFB10D\\\" data-pjax-transient=\\\"true\\\"/><meta name=\\\"html-safe-nonce\\\" content=\\\"8f6151b18ab7485c1a0b8dad6c27aca29df1ac3d8fe4542444a2962c0c1e9d73\\\" data-pjax-transient=\\\"true\\\"/><meta name=\\\"visitor-payload\\\" content=\\\"eyJyZWZlcnJlciI6IiIsInJlcXVlc3RfaWQiOiJDOTFFOjJEQzc3MDo5MzdCMDc6QjRGREZCOjY4Q0ZCMTBEIiwidmlzaXRvcl9pZCI6IjUzNzE1ODM2OTE4MzAwNDY5ODkiLCJyZWdpb25fZWRnZSI6InNvdXRoZWFzdGFzaWEiLCJyZWdpb25fcmVuZGVyIjoic291dGhlYXN0YXNpYSJ9\\\" data-pjax-transient=\\\"true\\\"/><meta name=\\\"visitor-hmac\\\" content=\\\"69f988967a2ebd3297b2dd9f2e93c7a27efb06053a04e0dee8f32437fe186de4\\\" data-pjax-transient=\\\"true\\\"/>\\n\\n\\n    <meta name=\\\"hovercard-subject-tag\\\" content=\\\"repository:552138582\\\" data-turbo-transient>\\n\\n\\n  <meta name=\\\"github-keyboard-shortcuts\\\" content=\\\"repository,copilot\\\" data-turbo-transient=\\\"true\\\" />\\n  \\n\\n  <meta name=\\\"selected-link\\\" value=\\\"repo_source\\\" data-turbo-transient>\\n  <link rel=\\\"assets\\\" href=\\\"https://github.githubassets.com/\\\">\\n\\n    <meta name=\\\"google-site-verification\\\" content=\\\"Apib7-x98H0j5cPqHWwSMm6dNU4GmODRoqxLiDzdx9I\\\">\\n\\n<meta name=\\\"octolytics-url\\\" content=\\\"https://collector.github.com/github/collect\\\" />\\n\\n  <meta name=\\\"analytics-location\\\" content=\\\"/&lt;user-name&gt;/&lt;repo-name&gt;\\\" data-turbo-transient=\\\"true\\\" />\\n\\n  \\n\\n\\n\\n\\n    <meta name=\\\"user-login\\\" content=\\\"\\\">\\n\\n  \\n\\n    <meta name=\\\"viewport\\\" content=\\\"width=device-width\\\">\\n\\n    \\n\\n      <meta name=\\\"description\\\" content=\\\"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them - suzgunmirac/BIG-Bench-Hard\\\">\\n\\n      <link rel=\\\"search\\\" type=\\\"application/opensearchdescription+xml\\\" href=\\\"/opensearch.xml\\\" title=\\\"GitHub\\\">\\n\\n    <link rel=\\\"fluid-icon\\\" href=\\\"https://github.com/fluidicon.png\\\" title=\\\"GitHub\\\">\\n    <meta property=\\\"fb:app_id\\\" content=\\\"1401488693436528\\\">\\n    <meta name=\\\"apple-itunes-app\\\" content=\\\"app-id=1477376905, app-argument=https://github.com/suzgunmirac/BIG-Bench-Hard\\\" />\\n\\n      <meta name=\\\"twitter:image\\\" content=\\\"https://opengraph.githubassets.com/8b291251080bbe686b05e0e0f6775d80926539e17caf68058749de71ebb229c5/suzgunmirac/BIG-Bench-Hard\\\" /><meta name=\\\"twitter:site\\\" content=\\\"@github\\\" /><meta name=\\\"twitter:card\\\" content=\\\"summary_large_image\\\" /><meta name=\\\"twitter:title\\\" content=\\\"GitHub - suzgunmirac/BIG-Bench-Hard: Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them\\\" /><meta name=\\\"twitter:description\\\" content=\\\"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them - suzgunmirac/BIG-Bench-Hard\\\" />\\n  <meta property=\\\"og:image\\\" content=\\\"https://opengraph.githubassets.com/8b291251080bbe686b05e0e0f6775d80926539e17caf68058749de71ebb229c5/suzgunmirac/BIG-Bench-Hard\\\" /><meta property=\\\"og:image:alt\\\" content=\\\"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them - suzgunmirac/BIG-Bench-Hard\\\" /><meta property=\\\"og:image:width\\\" content=\\\"1200\\\" /><meta property=\\\"og:image:height\\\" content=\\\"600\\\" /><meta property=\\\"og:site_name\\\" content=\\\"GitHub\\\" /><meta property=\\\"og:type\\\" content=\\\"object\\\" /><meta property=\\\"og:title\\\" content=\\\"GitHub - suzgunmirac/BIG-Bench-Hard: Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them\\\" /><meta property=\\\"og:url\\\" content=\\\"https://github.com/suzgunmirac/BIG-Bench-Hard\\\" /><meta property=\\\"og:description\\\" content=\\\"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them - suzgunmirac/BIG-Bench-Hard\\\" />\\n  \\n\\n\\n\\n\\n      <meta name=\\\"hostname\\\" content=\\\"github.com\\\">\\n\\n\\n\\n        <meta name=\\\"expected-hostname\\\" content=\\\"github.com\\\">\\n\\n\\n  <meta http-equiv=\\\"x-pjax-version\\\" content=\\\"e4a10edf7d20ccc0fa36600810f724475fbb16b42377d4e5ea6642ef7b79c657\\\" data-turbo-track=\\\"reload\\\">\\n  <meta http-equiv=\\\"x-pjax-csp-version\\\" content=\\\"c922ef32c4ab94f8b870c62883f3e41755ec705db76ec4efb0d343458f1e28c7\\\" data-turbo-track=\\\"reload\\\">\\n  <meta http-equiv=\\\"x-pjax-css-version\\\" content=\\\"0bc51a290919c52cc62b3d8b4eed96609edf264f742d0409c975553b0cdc84a8\\\" data-turbo-track=\\\"reload\\\">\\n  <meta http-equiv=\\\"x-pjax-js-version\\\" content=\\\"5fb0b72757a8af0550d564211ab2885da1b8e95014f6dc26acc4c863e1a2268f\\\" data-turbo-track=\\\"reload\\\">\\n\\n  <meta name=\\\"turbo-cache-control\\\" content=\\\"no-preview\\\" data-turbo-transient=\\\"\\\">\\n\\n      <meta data-hydrostats=\\\"publish\\\">\\n  <meta name=\\\"go-import\\\" content=\\\"github.com/suzgunmirac/BIG-Bench-Hard git https://github.com/suzgunmirac/BIG-Bench-Hard.git\\\">\\n\\n  <meta name=\\\"octolytics-dimension-user_id\\\" content=\\\"6288340\\\" /><meta name=\\\"octolytics-dimension-user_login\\\" content=\\\"suzgunmirac\\\" /><meta name=\\\"octolytics-dimension-repository_id\\\" content=\\\"552138582\\\" /><meta name=\\\"octolytics-dimension-repository_nwo\\\" content=\\\"suzgunmirac/BIG-Bench-Hard\\\" /><meta name=\\\"octolytics-dimension-repository_public\\\" content=\\\"true\\\" /><meta name=\\\"octolytics-dimension-repository_is_fork\\\" content=\\\"false\\\" /><meta name=\\\"octolytics-dimension-repository_network_root_id\\\" content=\\\"552138582\\\" /><meta name=\\\"octolytics-dimension-repository_network_root_nwo\\\" content=\\\"suzgunmirac/BIG-Bench-Hard\\\" />\\n\\n\\n\\n      <link rel=\\\"canonical\\\" href=\\\"https://github.com/suzgunmirac/BIG-Bench-Hard\\\" data-turbo-transient>\\n\\n\\n    <meta name=\\\"turbo-body-classes\\\" content=\\\"logged-out env-production page-responsive\\\">\\n\\n\\n  <meta name=\\\"browser-stats-url\\\" content=\\\"https://api.github.com/_private/browser/stats\\\">\\n\\n  <meta name=\\\"browser-errors-url\\\" content=\\\"https://api.github.com/_private/browser/errors\\\">\\n\\n  <meta name=\\\"release\\\" content=\\\"3a83c532047a42870ed81147b0dab76c3feb8943\\\">\\n  <meta name=\\\"ui-target\\\" content=\\\"canary-1\\\">\\n\\n  <link rel=\\\"mask-icon\\\" href=\\\"https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg\\\" color=\\\"#000000\\\">\\n  <link rel=\\\"alternate icon\\\" class=\\\"js-site-favicon\\\" type=\\\"image/png\\\" href=\\\"https://github.githubassets.com/favicons/favicon.png\\\">\\n  <link rel=\\\"icon\\\" class=\\\"js-site-favicon\\\" type=\\\"image/svg+xml\\\" href=\\\"https://github.githubassets.com/favicons/favicon.svg\\\" data-base-href=\\\"https://github.githubassets.com/favicons/favicon\\\">\\n\\n<meta name=\\\"theme-color\\\" content=\\\"#1e2327\\\">\\n<meta name=\\\"color-scheme\\\" content=\\\"light dark\\\" />\\n\\n\\n  <link rel=\\\"manifest\\\" href=\\\"/manifest.json\\\" crossOrigin=\\\"use-credentials\\\">\\n\\n  </head>\\n\\n  <body class=\\\"logged-out env-production page-responsive\\\" style=\\\"word-wrap: break-word;\\\">\\n    <div data-turbo-body class=\\\"logged-out env-production page-responsive\\\" style=\\\"word-wrap: break-word;\\\">\\n      \\n\\n\\n\\n    <div class=\\\"position-relative header-wrapper js-header-wrapper \\\">\\n      <a href=\\\"#start-of-content\\\" data-skip-target-assigned=\\\"false\\\" class=\\\"px-2 py-4 color-bg-accent-emphasis color-fg-on-emphasis show-on-focus js-skip-to-content\\\">Skip to content</a>\\n\\n      <span data-view-component=\\\"true\\\" class=\\\"progress-pjax-loader Progress position-fixed width-full\\\">\\n    <span style=\\\"width: 0%;\\\" data-view-component=\\\"true\\\" class=\\\"Progress-item progress-pjax-loader-bar left-0 top-0 color-bg-accent-emphasis\\\"></span>\\n</span>      \\n      \\n      <link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/keyboard-shortcuts-dialog.2de9c7d6456a311fce49.module.css\\\" />\\n\\n<react-partial\\n  partial-name=\\\"keyboard-shortcuts-dialog\\\"\\n  data-ssr=\\\"false\\\"\\n  data-attempted-ssr=\\\"false\\\"\\n  data-react-profiling=\\\"false\\\"\\n>\\n  \\n  <script type=\\\"application/json\\\" data-target=\\\"react-partial.embeddedData\\\">{\\\"props\\\":{\\\"docsUrl\\\":\\\"https://docs.github.com/get-started/accessibility/keyboard-shortcuts\\\"}}</script>\\n  <div data-target=\\\"react-partial.reactRoot\\\"></div>\\n</react-partial>\\n\\n\\n\\n\\n\\n      \\n\\n          \\n\\n              \\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_gsap_index_js-23c9606618ce.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/vendors-node_modules_github_remote-form_dist_index_js-node_modules_delegated-events_dist_inde-94fd67-04dd9f3c4754.js\\\" defer=\\\"defer\\\"></script>\\n<script crossorigin=\\\"anonymous\\\" type=\\\"application/javascript\\\" src=\\\"https://github.githubassets.com/assets/sessions-ac148cb2a2f3.js\\\" defer=\\\"defer\\\"></script>\\n\\n<header class=\\\"HeaderMktg header-logged-out js-details-container js-header Details f4 py-3\\\" role=\\\"banner\\\" data-is-top=\\\"true\\\" data-color-mode=light data-light-theme=light data-dark-theme=dark>\\n  <h2 class=\\\"sr-only\\\">Navigation Menu</h2>\\n\\n  <button type=\\\"button\\\" class=\\\"HeaderMktg-backdrop d-lg-none border-0 position-fixed top-0 left-0 width-full height-full js-details-target\\\" aria-label=\\\"Toggle navigation\\\">\\n    <span class=\\\"d-none\\\">Toggle navigation</span>\\n  </button>\\n\\n  <div class=\\\"d-flex flex-column flex-lg-row flex-items-center px-3 px-md-4 px-lg-5 height-full position-relative z-1\\\">\\n    <div class=\\\"d-flex flex-justify-between flex-items-center width-full width-lg-auto\\\">\\n      <div class=\\\"flex-1\\\">\\n        <button aria-label=\\\"Toggle navigation\\\" aria-expanded=\\\"false\\\" type=\\\"button\\\" data-view-component=\\\"true\\\" class=\\\"js-details-target js-nav-padding-recalculate js-header-menu-toggle Button--link Button--medium Button d-lg-none color-fg-inherit p-1\\\">  <span class=\\\"Button-content\\\">\\n    <span class=\\\"Button-label\\\"><div class=\\\"HeaderMenu-toggle-bar rounded my-1\\\"></div>\\n            <div class=\\\"HeaderMenu-toggle-bar rounded my-1\\\"></div>\\n            <div class=\\\"HeaderMenu-toggle-bar rounded my-1\\\"></div></span>\\n  </span>\\n</button>\\n      </div>\\n\\n      <a class=\\\"mr-lg-3 color-fg-inherit flex-order-2 js-prevent-focus-on-mobile-nav\\\"\\n        href=\\\"/\\\"\\n        aria-label=\\\"Homepage\\\"\\n        data-analytics-event=\\\"{&quot;category&quot;:&quot;Marketing nav&quot;,&quot;action&quot;:&quot;click to go to homepage&quot;,&quot;label&quot;:&quot;ref_page:Marketing;ref_cta:Logomark;ref_loc:Header&quot;}\\\">\\n        <svg height=\\\"32\\\" aria-hidden=\\\"true\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"32\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-mark-github\\\">\\n    <path d=\\\"M12 1C5.923 1 1 5.923 1 12c0 4.867 3.149 8.979 7.521 10.436.55.096.756-.233.756-.522 0-.262-.013-1.128-.013-2.049-2.764.509-3.479-.674-3.699-1.292-.124-.317-.66-1.293-1.127-1.554-.385-.207-.936-.715-.014-.729.866-.014 1.485.797 1.691 1.128.99 1.663 2.571 1.196 3.204.907.096-.715.385-1.196.701-1.471-2.448-.275-5.005-1.224-5.005-5.432 0-1.196.426-2.186 1.128-2.956-.111-.275-.496-1.402.11-2.915 0 0 .921-.288 3.024 1.128a10.193 10.193 0 0 1 2.75-.371c.936 0 1.871.123 2.75.371 2.104-1.43 3.025-1.128 3.025-1.128.605 1.513.221 2.64.111 2.915.701.77 1.127 1.747 1.127 2.956 0 4.222-2.571 5.157-5.019 5.432.399.344.743 1.004.743 2.035 0 1.471-.014 2.654-.014 3.025 0 .289.206.632.756.522C19.851 20.979 23 16.854 23 12c0-6.077-4.922-11-11-11Z\\\"></path>\\n</svg>\\n      </a>\\n\\n      <div class=\\\"d-flex flex-1 flex-order-2 text-right d-lg-none gap-2 flex-justify-end\\\">\\n          <a\\n            href=\\\"/login?return_to=https%3A%2F%2Fgithub.com%2Fsuzgunmirac%2FBIG-Bench-Hard\\\"\\n            class=\\\"HeaderMenu-link HeaderMenu-button d-inline-flex f5 no-underline border color-border-default rounded-2 px-2 py-1 color-fg-inherit js-prevent-focus-on-mobile-nav\\\"\\n            data-hydro-click=\\\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/suzgunmirac/BIG-Bench-Hard&quot;,&quot;user_id&quot;:null}}\\\" data-hydro-click-hmac=\\\"b00fa76ba133bc00e6c6d0e9b973f91a3cf234df5f3b96501eda7009ea006279\\\"\\n            data-analytics-event=\\\"{&quot;category&quot;:&quot;Marketing nav&quot;,&quot;action&quot;:&quot;click to Sign in&quot;,&quot;label&quot;:&quot;ref_page:Marketing;ref_cta:Sign in;ref_loc:Header&quot;}\\\"\\n          >\\n            Sign in\\n          </a>\\n              <div class=\\\"AppHeader-appearanceSettings\\\">\\n    <react-partial-anchor>\\n      <button data-target=\\\"react-partial-anchor.anchor\\\" id=\\\"icon-button-b94bdf6e-3dc8-4352-84a6-73cc5c585bb2\\\" aria-labelledby=\\\"tooltip-71a504bb-5331-4b8c-af7e-5d866f435be4\\\" type=\\\"button\\\" disabled=\\\"disabled\\\" data-view-component=\\\"true\\\" class=\\\"Button Button--iconOnly Button--invisible Button--medium AppHeader-button HeaderMenu-link border cursor-wait\\\">  <svg aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-sliders Button-visual\\\">\\n    <path d=\\\"M15 2.75a.75.75 0 0 1-.75.75h-4a.75.75 0 0 1 0-1.5h4a.75.75 0 0 1 .75.75Zm-8.5.75v1.25a.75.75 0 0 0 1.5 0v-4a.75.75 0 0 0-1.5 0V2H1.75a.75.75 0 0 0 0 1.5H6.5Zm1.25 5.25a.75.75 0 0 0 0-1.5h-6a.75.75 0 0 0 0 1.5h6ZM15 8a.75.75 0 0 1-.75.75H11.5V10a.75.75 0 1 1-1.5 0V6a.75.75 0 0 1 1.5 0v1.25h2.75A.75.75 0 0 1 15 8Zm-9 5.25v-2a.75.75 0 0 0-1.5 0v1.25H1.75a.75.75 0 0 0 0 1.5H4.5v1.25a.75.75 0 0 0 1.5 0v-2Zm9 0a.75.75 0 0 1-.75.75h-6a.75.75 0 0 1 0-1.5h6a.75.75 0 0 1 .75.75Z\\\"></path>\\n</svg>\\n</button><tool-tip id=\\\"tooltip-71a504bb-5331-4b8c-af7e-5d866f435be4\\\" for=\\\"icon-button-b94bdf6e-3dc8-4352-84a6-73cc5c585bb2\\\" popover=\\\"manual\\\" data-direction=\\\"s\\\" data-type=\\\"label\\\" data-view-component=\\\"true\\\" class=\\\"sr-only position-absolute\\\">Appearance settings</tool-tip>\\n\\n      <template data-target=\\\"react-partial-anchor.template\\\">\\n        <link crossorigin=\\\"anonymous\\\" media=\\\"all\\\" rel=\\\"stylesheet\\\" href=\\\"https://github.githubassets.com/assets/appearance-settings.6c63a6de228d6520804d.module.css\\\" />\\n\\n<react-partial\\n  partial-name=\\\"appearance-settings\\\"\\n  data-ssr=\\\"false\\\"\\n  data-attempted-ssr=\\\"false\\\"\\n  data-react-profiling=\\\"false\\\"\\n>\\n  \\n  <script type=\\\"application/json\\\" data-target=\\\"react-partial.embeddedData\\\">{\\\"props\\\":{}}</script>\\n  <div data-target=\\\"react-partial.reactRoot\\\"></div>\\n</react-partial>\\n\\n\\n      </template>\\n    </react-partial-anchor>\\n  </div>\\n\\n      </div>\\n    </div>\\n\\n\\n    <div class=\\\"HeaderMenu js-header-menu height-fit position-lg-relative d-lg-flex flex-column flex-auto top-0\\\">\\n      <div class=\\\"HeaderMenu-wrapper d-flex flex-column flex-self-start flex-lg-row flex-auto rounded rounded-lg-0\\\">\\n          <nav class=\\\"HeaderMenu-nav\\\" aria-label=\\\"Global\\\">\\n            <ul class=\\\"d-lg-flex list-style-none\\\">\\n                <li class=\\\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\\\">\\n      <button type=\\\"button\\\" class=\\\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\\\" aria-expanded=\\\"false\\\">\\n        Platform\\n        <svg opacity=\\\"0.5\\\" aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-chevron-down HeaderMenu-icon ml-1\\\">\\n    <path d=\\\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\\\"></path>\\n</svg>\\n      </button>\\n\\n      <div class=\\\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 pt-2 pt-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 dropdown-menu-wide\\\">\\n        <div class=\\\"d-lg-flex dropdown-menu-wide\\\">\\n            <div class=\\\"HeaderMenu-column px-lg-4\\\">\\n                <div class=\\\"\\\">\\n\\n                  <ul class=\\\"list-style-none f5\\\" >\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/features/copilot\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-copilot color-fg-subtle mr-3\\\">\\n    <path d=\\\"M23.922 16.992c-.861 1.495-5.859 5.023-11.922 5.023-6.063 0-11.061-3.528-11.922-5.023A.641.641 0 0 1 0 16.736v-2.869a.841.841 0 0 1 .053-.22c.372-.935 1.347-2.292 2.605-2.656.167-.429.414-1.055.644-1.517a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.499 1.132-3.368.397-.406.89-.717 1.474-.952 1.399-1.136 3.392-2.093 6.122-2.093 2.731 0 4.767.957 6.166 2.093.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086.23.462.477 1.088.644 1.517 1.258.364 2.233 1.721 2.605 2.656a.832.832 0 0 1 .053.22v2.869a.641.641 0 0 1-.078.256ZM12.172 11h-.344a4.323 4.323 0 0 1-.355.508C10.703 12.455 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a2.005 2.005 0 0 1-.085-.104L4 11.741v6.585c1.435.779 4.514 2.179 8 2.179 3.486 0 6.565-1.4 8-2.179v-6.585l-.098-.104s-.033.045-.085.104c-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.545-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.016.016Zm.641-2.935c.136 1.057.403 1.913.878 2.497.442.544 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.15.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.319-.862-2.824-1.025-1.487-.161-2.192.138-2.533.529-.269.307-.437.808-.438 1.578v.021c0 .265.021.562.063.893Zm-1.626 0c.042-.331.063-.628.063-.894v-.02c-.001-.77-.169-1.271-.438-1.578-.341-.391-1.046-.69-2.533-.529-1.505.163-2.347.537-2.824 1.025-.462.472-.705 1.179-.705 2.319 0 1.211.175 1.926.558 2.361.365.414 1.084.751 2.657.751 1.21 0 1.902-.394 2.344-.938.475-.584.742-1.44.878-2.497Z\\\"></path><path d=\\\"M14.5 14.25a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Zm-5 0a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          GitHub Copilot\\n\\n        </div>\\n\\n        Write better code with AI\\n      </div>\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_spark&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_spark_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/features/spark\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-sparkle-fill color-fg-subtle mr-3\\\">\\n    <path d=\\\"M11.296 1.924c.24-.656 1.168-.656 1.408 0l.717 1.958a11.25 11.25 0 0 0 6.697 6.697l1.958.717c.657.24.657 1.168 0 1.408l-1.958.717a11.25 11.25 0 0 0-6.697 6.697l-.717 1.958c-.24.657-1.168.657-1.408 0l-.717-1.958a11.25 11.25 0 0 0-6.697-6.697l-1.958-.717c-.656-.24-.656-1.168 0-1.408l1.958-.717a11.25 11.25 0 0 0 6.697-6.697l.717-1.958Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          GitHub Spark\\n\\n            <span class=\\\"HeaderMenu-label\\\">\\n              New\\n            </span>\\n        </div>\\n\\n        Build and deploy intelligent apps\\n      </div>\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/features/models\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-ai-model color-fg-subtle mr-3\\\">\\n    <path d=\\\"M19.375 8.5a3.25 3.25 0 1 1-3.163 4h-3a3.252 3.252 0 0 1-4.443 2.509L7.214 17.76a3.25 3.25 0 1 1-1.342-.674l1.672-2.957A3.238 3.238 0 0 1 6.75 12c0-.907.371-1.727.97-2.316L6.117 6.846A3.253 3.253 0 0 1 1.875 3.75a3.25 3.25 0 1 1 5.526 2.32l1.603 2.836A3.25 3.25 0 0 1 13.093 11h3.119a3.252 3.252 0 0 1 3.163-2.5ZM10 10.25a1.75 1.75 0 1 0-.001 3.499A1.75 1.75 0 0 0 10 10.25ZM5.125 2a1.75 1.75 0 1 0 0 3.5 1.75 1.75 0 0 0 0-3.5Zm12.5 9.75a1.75 1.75 0 1 0 3.5 0 1.75 1.75 0 0 0-3.5 0Zm-14.25 8.5a1.75 1.75 0 1 0 3.501-.001 1.75 1.75 0 0 0-3.501.001Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          GitHub Models\\n\\n            <span class=\\\"HeaderMenu-label\\\">\\n              New\\n            </span>\\n        </div>\\n\\n        Manage and compare prompts\\n      </div>\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/security/advanced-security\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-shield-check color-fg-subtle mr-3\\\">\\n    <path d=\\\"M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z\\\"></path><path d=\\\"m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          GitHub Advanced Security\\n\\n        </div>\\n\\n        Find and fix vulnerabilities\\n      </div>\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/features/actions\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-workflow color-fg-subtle mr-3\\\">\\n    <path d=\\\"M1 3a2 2 0 0 1 2-2h6.5a2 2 0 0 1 2 2v6.5a2 2 0 0 1-2 2H7v4.063C7 16.355 7.644 17 8.438 17H12.5v-2.5a2 2 0 0 1 2-2H21a2 2 0 0 1 2 2V21a2 2 0 0 1-2 2h-6.5a2 2 0 0 1-2-2v-2.5H8.437A2.939 2.939 0 0 1 5.5 15.562V11.5H3a2 2 0 0 1-2-2Zm2-.5a.5.5 0 0 0-.5.5v6.5a.5.5 0 0 0 .5.5h6.5a.5.5 0 0 0 .5-.5V3a.5.5 0 0 0-.5-.5ZM14.5 14a.5.5 0 0 0-.5.5V21a.5.5 0 0 0 .5.5H21a.5.5 0 0 0 .5-.5v-6.5a.5.5 0 0 0-.5-.5Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          Actions\\n\\n        </div>\\n\\n        Automate any workflow\\n      </div>\\n\\n    \\n</a></li>\\n\\n                  </ul>\\n                </div>\\n            </div>\\n            <div class=\\\"HeaderMenu-column px-lg-4 pb-3 pb-lg-0 border-lg-right\\\">\\n                <div class=\\\"border-bottom border-lg-bottom-0 pb-lg-0 pb-3\\\">\\n\\n                  <ul class=\\\"list-style-none f5\\\" >\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/features/codespaces\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-codespaces color-fg-subtle mr-3\\\">\\n    <path d=\\\"M3.5 3.75C3.5 2.784 4.284 2 5.25 2h13.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 18.75 13H5.25a1.75 1.75 0 0 1-1.75-1.75Zm-2 12c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v4a1.75 1.75 0 0 1-1.75 1.75H3.25a1.75 1.75 0 0 1-1.75-1.75ZM5.25 3.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h13.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Zm-2 12a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h17.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25Z\\\"></path><path d=\\\"M10 17.75a.75.75 0 0 1 .75-.75h6.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          Codespaces\\n\\n        </div>\\n\\n        Instant dev environments\\n      </div>\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/features/issues\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-issue-opened color-fg-subtle mr-3\\\">\\n    <path d=\\\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1ZM2.5 12a9.5 9.5 0 0 0 9.5 9.5 9.5 9.5 0 0 0 9.5-9.5A9.5 9.5 0 0 0 12 2.5 9.5 9.5 0 0 0 2.5 12Zm9.5 2a2 2 0 1 1-.001-3.999A2 2 0 0 1 12 14Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          Issues\\n\\n        </div>\\n\\n        Plan and track work\\n      </div>\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/features/code-review\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-code-review color-fg-subtle mr-3\\\">\\n    <path d=\\\"M10.3 6.74a.75.75 0 0 1-.04 1.06l-2.908 2.7 2.908 2.7a.75.75 0 1 1-1.02 1.1l-3.5-3.25a.75.75 0 0 1 0-1.1l3.5-3.25a.75.75 0 0 1 1.06.04Zm3.44 1.06a.75.75 0 1 1 1.02-1.1l3.5 3.25a.75.75 0 0 1 0 1.1l-3.5 3.25a.75.75 0 1 1-1.02-1.1l2.908-2.7-2.908-2.7Z\\\"></path><path d=\\\"M1.5 4.25c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v12.5a1.75 1.75 0 0 1-1.75 1.75h-9.69l-3.573 3.573A1.458 1.458 0 0 1 5 21.043V18.5H3.25a1.75 1.75 0 0 1-1.75-1.75ZM3.25 4a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h2.5a.75.75 0 0 1 .75.75v3.19l3.72-3.72a.749.749 0 0 1 .53-.22h10a.25.25 0 0 0 .25-.25V4.25a.25.25 0 0 0-.25-.25Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          Code Review\\n\\n        </div>\\n\\n        Manage code changes\\n      </div>\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/features/discussions\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-comment-discussion color-fg-subtle mr-3\\\">\\n    <path d=\\\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z\\\"></path><path d=\\\"M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          Discussions\\n\\n        </div>\\n\\n        Collaborate outside of code\\n      </div>\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/features/code-search\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-code-square color-fg-subtle mr-3\\\">\\n    <path d=\\\"M10.3 8.24a.75.75 0 0 1-.04 1.06L7.352 12l2.908 2.7a.75.75 0 1 1-1.02 1.1l-3.5-3.25a.75.75 0 0 1 0-1.1l3.5-3.25a.75.75 0 0 1 1.06.04Zm3.44 1.06a.75.75 0 1 1 1.02-1.1l3.5 3.25a.75.75 0 0 1 0 1.1l-3.5 3.25a.75.75 0 1 1-1.02-1.1l2.908-2.7-2.908-2.7Z\\\"></path><path d=\\\"M2 3.75C2 2.784 2.784 2 3.75 2h16.5c.966 0 1.75.784 1.75 1.75v16.5A1.75 1.75 0 0 1 20.25 22H3.75A1.75 1.75 0 0 1 2 20.25Zm1.75-.25a.25.25 0 0 0-.25.25v16.5c0 .138.112.25.25.25h16.5a.25.25 0 0 0 .25-.25V3.75a.25.25 0 0 0-.25-.25Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          Code Search\\n\\n        </div>\\n\\n        Find more, search less\\n      </div>\\n\\n    \\n</a></li>\\n\\n                  </ul>\\n                </div>\\n            </div>\\n            <div class=\\\"HeaderMenu-column px-lg-4\\\">\\n                <div class=\\\"border-bottom border-lg-bottom-0 pb-lg-0 mb-3 pb-3\\\">\\n\\n                      <span class=\\\"d-block h4 color-fg-default my-1\\\" id=\\\"platform-explore-heading\\\">Explore</span>\\n\\n                  <ul class=\\\"list-style-none f5\\\" aria-labelledby=\\\"platform-explore-heading\\\">\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;why_github&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;why_github_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/why-github\\\">\\n      Why GitHub\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary Link--external\\\" target=\\\"_blank\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;documentation&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;documentation_link_platform_navbar&quot;}\\\" href=\\\"https://docs.github.com\\\">\\n      Documentation\\n\\n    <svg aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\\\">\\n    <path d=\\\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\\\"></path>\\n</svg>\\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary Link--external\\\" target=\\\"_blank\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_skills&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_skills_link_platform_navbar&quot;}\\\" href=\\\"https://skills.github.com\\\">\\n      GitHub Skills\\n\\n    <svg aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\\\">\\n    <path d=\\\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\\\"></path>\\n</svg>\\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary Link--external\\\" target=\\\"_blank\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;blog&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;blog_link_platform_navbar&quot;}\\\" href=\\\"https://github.blog\\\">\\n      Blog\\n\\n    <svg aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\\\">\\n    <path d=\\\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\\\"></path>\\n</svg>\\n</a></li>\\n\\n                  </ul>\\n                </div>\\n                <div class=\\\"border-bottom border-lg-bottom-0 pb-lg-0 pb-3\\\">\\n\\n                      <span class=\\\"d-block h4 color-fg-default my-1\\\" id=\\\"platform-integrations-heading\\\">Integrations</span>\\n\\n                  <ul class=\\\"list-style-none f5\\\" aria-labelledby=\\\"platform-integrations-heading\\\">\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_marketplace&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_marketplace_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/marketplace\\\">\\n      GitHub Marketplace\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;mcp_registry&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;mcp_registry_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/mcp\\\">\\n      MCP Registry\\n\\n    \\n</a></li>\\n\\n                  </ul>\\n                </div>\\n            </div>\\n        </div>\\n\\n          <div class=\\\"HeaderMenu-trailing-link rounded-bottom-2 mt-lg-4 px-lg-4 py-4 py-lg-3 f5 text-semibold\\\">\\n            <a data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all_features&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_features_link_platform_navbar&quot;}\\\" href=\\\"https://github.com/features\\\">\\n              View all features\\n              <svg aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-chevron-right HeaderMenu-trailing-link-icon\\\">\\n    <path d=\\\"M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z\\\"></path>\\n</svg>\\n</a>          </div>\\n      </div>\\n</li>\\n\\n\\n                <li class=\\\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\\\">\\n      <button type=\\\"button\\\" class=\\\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\\\" aria-expanded=\\\"false\\\">\\n        Solutions\\n        <svg opacity=\\\"0.5\\\" aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-chevron-down HeaderMenu-icon ml-1\\\">\\n    <path d=\\\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\\\"></path>\\n</svg>\\n      </button>\\n\\n      <div class=\\\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 pt-2 pt-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 dropdown-menu-wide\\\">\\n        <div class=\\\"d-lg-flex dropdown-menu-wide\\\">\\n            <div class=\\\"HeaderMenu-column px-lg-4 pb-3 pb-lg-0 border-lg-right\\\">\\n                <div class=\\\"border-bottom border-lg-bottom-0 pb-lg-0 pb-3\\\">\\n\\n                      <span class=\\\"d-block h4 color-fg-default my-1\\\" id=\\\"solutions-by-company-size-heading\\\">By company size</span>\\n\\n                  <ul class=\\\"list-style-none f5\\\" aria-labelledby=\\\"solutions-by-company-size-heading\\\">\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprises&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprises_link_solutions_navbar&quot;}\\\" href=\\\"https://github.com/enterprise\\\">\\n      Enterprises\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;small_and_medium_teams&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;small_and_medium_teams_link_solutions_navbar&quot;}\\\" href=\\\"https://github.com/team\\\">\\n      Small and medium teams\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;startups&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;startups_link_solutions_navbar&quot;}\\\" href=\\\"https://github.com/enterprise/startups\\\">\\n      Startups\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;nonprofits&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;nonprofits_link_solutions_navbar&quot;}\\\" href=\\\"/solutions/industry/nonprofits\\\">\\n      Nonprofits\\n\\n    \\n</a></li>\\n\\n                  </ul>\\n                </div>\\n            </div>\\n            <div class=\\\"HeaderMenu-column px-lg-4 pb-3 pb-lg-0 border-lg-right\\\">\\n                <div class=\\\"border-bottom border-lg-bottom-0 pb-lg-0 pb-3\\\">\\n\\n                      <span class=\\\"d-block h4 color-fg-default my-1\\\" id=\\\"solutions-by-use-case-heading\\\">By use case</span>\\n\\n                  <ul class=\\\"list-style-none f5\\\" aria-labelledby=\\\"solutions-by-use-case-heading\\\">\\n                      \\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;devsecops&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;devsecops_link_solutions_navbar&quot;}\\\" href=\\\"/solutions/use-case/devsecops\\\">\\n      DevSecOps\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;devops&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;devops_link_solutions_navbar&quot;}\\\" href=\\\"/solutions/use-case/devops\\\">\\n      DevOps\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ci_cd&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ci_cd_link_solutions_navbar&quot;}\\\" href=\\\"/solutions/use-case/ci-cd\\\">\\n      CI/CD\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all_use_cases&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_use_cases_link_solutions_navbar&quot;}\\\" href=\\\"/solutions/use-case\\\">\\n      View all use cases\\n\\n    \\n</a></li>\\n\\n                  </ul>\\n                </div>\\n            </div>\\n            <div class=\\\"HeaderMenu-column px-lg-4\\\">\\n                <div class=\\\"border-bottom border-lg-bottom-0 pb-lg-0 pb-3\\\">\\n\\n                      <span class=\\\"d-block h4 color-fg-default my-1\\\" id=\\\"solutions-by-industry-heading\\\">By industry</span>\\n\\n                  <ul class=\\\"list-style-none f5\\\" aria-labelledby=\\\"solutions-by-industry-heading\\\">\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;healthcare&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;healthcare_link_solutions_navbar&quot;}\\\" href=\\\"/solutions/industry/healthcare\\\">\\n      Healthcare\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;financial_services&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;financial_services_link_solutions_navbar&quot;}\\\" href=\\\"/solutions/industry/financial-services\\\">\\n      Financial services\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;manufacturing&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;manufacturing_link_solutions_navbar&quot;}\\\" href=\\\"/solutions/industry/manufacturing\\\">\\n      Manufacturing\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;government&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;government_link_solutions_navbar&quot;}\\\" href=\\\"/solutions/industry/government\\\">\\n      Government\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all_industries&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_industries_link_solutions_navbar&quot;}\\\" href=\\\"/solutions/industry\\\">\\n      View all industries\\n\\n    \\n</a></li>\\n\\n                  </ul>\\n                </div>\\n            </div>\\n        </div>\\n\\n          <div class=\\\"HeaderMenu-trailing-link rounded-bottom-2 mt-lg-4 px-lg-4 py-4 py-lg-3 f5 text-semibold\\\">\\n            <a data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all_solutions&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_solutions_link_solutions_navbar&quot;}\\\" href=\\\"/solutions\\\">\\n              View all solutions\\n              <svg aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-chevron-right HeaderMenu-trailing-link-icon\\\">\\n    <path d=\\\"M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z\\\"></path>\\n</svg>\\n</a>          </div>\\n      </div>\\n</li>\\n\\n\\n                <li class=\\\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\\\">\\n      <button type=\\\"button\\\" class=\\\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\\\" aria-expanded=\\\"false\\\">\\n        Resources\\n        <svg opacity=\\\"0.5\\\" aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-chevron-down HeaderMenu-icon ml-1\\\">\\n    <path d=\\\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\\\"></path>\\n</svg>\\n      </button>\\n\\n      <div class=\\\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 pt-2 pt-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 pb-2 pb-lg-4 dropdown-menu-wide\\\">\\n        <div class=\\\"d-lg-flex dropdown-menu-wide\\\">\\n            <div class=\\\"HeaderMenu-column px-lg-4 pb-3 pb-lg-0 border-lg-right\\\">\\n                <div class=\\\"border-bottom border-lg-bottom-0 pb-lg-0 pb-3\\\">\\n\\n                      <span class=\\\"d-block h4 color-fg-default my-1\\\" id=\\\"resources-topics-heading\\\">Topics</span>\\n\\n                  <ul class=\\\"list-style-none f5\\\" aria-labelledby=\\\"resources-topics-heading\\\">\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ai&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ai_link_resources_navbar&quot;}\\\" href=\\\"/resources/articles/ai\\\">\\n      AI\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;devops&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;devops_link_resources_navbar&quot;}\\\" href=\\\"/resources/articles/devops\\\">\\n      DevOps\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;security&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;security_link_resources_navbar&quot;}\\\" href=\\\"/resources/articles/security\\\">\\n      Security\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;software_development&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;software_development_link_resources_navbar&quot;}\\\" href=\\\"/resources/articles/software-development\\\">\\n      Software Development\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_link_resources_navbar&quot;}\\\" href=\\\"/resources/articles\\\">\\n      View all\\n\\n    \\n</a></li>\\n\\n                  </ul>\\n                </div>\\n            </div>\\n            <div class=\\\"HeaderMenu-column px-lg-4\\\">\\n                <div class=\\\"border-bottom border-lg-bottom-0 pb-lg-0 border-bottom-0\\\">\\n\\n                      <span class=\\\"d-block h4 color-fg-default my-1\\\" id=\\\"resources-explore-heading\\\">Explore</span>\\n\\n                  <ul class=\\\"list-style-none f5\\\" aria-labelledby=\\\"resources-explore-heading\\\">\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary Link--external\\\" target=\\\"_blank\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}\\\" href=\\\"https://resources.github.com/learn/pathways\\\">\\n      Learning Pathways\\n\\n    <svg aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\\\">\\n    <path d=\\\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\\\"></path>\\n</svg>\\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}\\\" href=\\\"https://github.com/resources/events\\\">\\n      Events &amp; Webinars\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}\\\" href=\\\"https://github.com/resources/whitepapers\\\">\\n      Ebooks &amp; Whitepapers\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}\\\" href=\\\"https://github.com/customer-stories\\\">\\n      Customer Stories\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary Link--external\\\" target=\\\"_blank\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}\\\" href=\\\"https://partner.github.com\\\">\\n      Partners\\n\\n    <svg aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-link-external HeaderMenu-external-icon color-fg-subtle\\\">\\n    <path d=\\\"M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z\\\"></path>\\n</svg>\\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}\\\" href=\\\"https://github.com/solutions/executive-insights\\\">\\n      Executive Insights\\n\\n    \\n</a></li>\\n\\n                  </ul>\\n                </div>\\n            </div>\\n        </div>\\n\\n      </div>\\n</li>\\n\\n\\n                <li class=\\\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\\\">\\n      <button type=\\\"button\\\" class=\\\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\\\" aria-expanded=\\\"false\\\">\\n        Open Source\\n        <svg opacity=\\\"0.5\\\" aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-chevron-down HeaderMenu-icon ml-1\\\">\\n    <path d=\\\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\\\"></path>\\n</svg>\\n      </button>\\n\\n      <div class=\\\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 pt-2 pt-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 pb-2 pb-lg-4\\\">\\n        <div class=\\\"d-lg-flex dropdown-menu-wide\\\">\\n            <div class=\\\"HeaderMenu-column px-lg-4\\\">\\n                <div class=\\\"border-bottom mb-3 mb-lg-3 pb-3\\\">\\n\\n                  <ul class=\\\"list-style-none f5\\\" >\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}\\\" href=\\\"/sponsors\\\">\\n      \\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          GitHub Sponsors\\n\\n        </div>\\n\\n        Fund open source developers\\n      </div>\\n\\n    \\n</a></li>\\n\\n                  </ul>\\n                </div>\\n                <div class=\\\"border-bottom mb-3 mb-lg-3 pb-3\\\">\\n\\n                  <ul class=\\\"list-style-none f5\\\" >\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}\\\" href=\\\"https://github.com/readme\\\">\\n      \\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          The ReadME Project\\n\\n        </div>\\n\\n        GitHub community articles\\n      </div>\\n\\n    \\n</a></li>\\n\\n                  </ul>\\n                </div>\\n                <div class=\\\"border-bottom border-bottom-0\\\">\\n\\n                      <span class=\\\"d-block h4 color-fg-default my-1\\\" id=\\\"open-source-repositories-heading\\\">Repositories</span>\\n\\n                  <ul class=\\\"list-style-none f5\\\" aria-labelledby=\\\"open-source-repositories-heading\\\">\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;topics&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;topics_link_open_source_navbar&quot;}\\\" href=\\\"https://github.com/topics\\\">\\n      Topics\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;trending&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;trending_link_open_source_navbar&quot;}\\\" href=\\\"https://github.com/trending\\\">\\n      Trending\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;collections&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;collections_link_open_source_navbar&quot;}\\\" href=\\\"https://github.com/collections\\\">\\n      Collections\\n\\n    \\n</a></li>\\n\\n                  </ul>\\n                </div>\\n            </div>\\n        </div>\\n\\n      </div>\\n</li>\\n\\n\\n                <li class=\\\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\\\">\\n      <button type=\\\"button\\\" class=\\\"HeaderMenu-link border-0 width-full width-lg-auto px-0 px-lg-2 py-lg-2 no-wrap d-flex flex-items-center flex-justify-between js-details-target\\\" aria-expanded=\\\"false\\\">\\n        Enterprise\\n        <svg opacity=\\\"0.5\\\" aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-chevron-down HeaderMenu-icon ml-1\\\">\\n    <path d=\\\"M12.78 5.22a.749.749 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.06 0L3.22 6.28a.749.749 0 1 1 1.06-1.06L8 8.939l3.72-3.719a.749.749 0 0 1 1.06 0Z\\\"></path>\\n</svg>\\n      </button>\\n\\n      <div class=\\\"HeaderMenu-dropdown dropdown-menu rounded m-0 p-0 pt-2 pt-lg-4 position-relative position-lg-absolute left-0 left-lg-n3 pb-2 pb-lg-4\\\">\\n        <div class=\\\"d-lg-flex dropdown-menu-wide\\\">\\n            <div class=\\\"HeaderMenu-column px-lg-4\\\">\\n                <div class=\\\"border-bottom mb-3 mb-lg-3 pb-3\\\">\\n\\n                  <ul class=\\\"list-style-none f5\\\" >\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}\\\" href=\\\"/enterprise\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-stack color-fg-subtle mr-3\\\">\\n    <path d=\\\"M11.063 1.456a1.749 1.749 0 0 1 1.874 0l8.383 5.316a1.751 1.751 0 0 1 0 2.956l-8.383 5.316a1.749 1.749 0 0 1-1.874 0L2.68 9.728a1.751 1.751 0 0 1 0-2.956Zm1.071 1.267a.25.25 0 0 0-.268 0L3.483 8.039a.25.25 0 0 0 0 .422l8.383 5.316a.25.25 0 0 0 .268 0l8.383-5.316a.25.25 0 0 0 0-.422Z\\\"></path><path d=\\\"M1.867 12.324a.75.75 0 0 1 1.035-.232l8.964 5.685a.25.25 0 0 0 .268 0l8.964-5.685a.75.75 0 0 1 .804 1.267l-8.965 5.685a1.749 1.749 0 0 1-1.874 0l-8.965-5.685a.75.75 0 0 1-.231-1.035Z\\\"></path><path d=\\\"M1.867 16.324a.75.75 0 0 1 1.035-.232l8.964 5.685a.25.25 0 0 0 .268 0l8.964-5.685a.75.75 0 0 1 .804 1.267l-8.965 5.685a1.749 1.749 0 0 1-1.874 0l-8.965-5.685a.75.75 0 0 1-.231-1.035Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          Enterprise platform\\n\\n        </div>\\n\\n        AI-powered developer platform\\n      </div>\\n\\n    \\n</a></li>\\n\\n                  </ul>\\n                </div>\\n                <div class=\\\"border-bottom border-bottom-0\\\">\\n\\n                      <span class=\\\"d-block h4 color-fg-default my-1\\\" id=\\\"enterprise-available-add-ons-heading\\\">Available add-ons</span>\\n\\n                  <ul class=\\\"list-style-none f5\\\" aria-labelledby=\\\"enterprise-available-add-ons-heading\\\">\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_enterprise_navbar&quot;}\\\" href=\\\"https://github.com/security/advanced-security\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-shield-check color-fg-subtle mr-3\\\">\\n    <path d=\\\"M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z\\\"></path><path d=\\\"m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          GitHub Advanced Security\\n\\n        </div>\\n\\n        Enterprise-grade security features\\n      </div>\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description mb-lg-3\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;copilot_for_business&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;copilot_for_business_link_enterprise_navbar&quot;}\\\" href=\\\"/features/copilot/copilot-business\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-copilot color-fg-subtle mr-3\\\">\\n    <path d=\\\"M23.922 16.992c-.861 1.495-5.859 5.023-11.922 5.023-6.063 0-11.061-3.528-11.922-5.023A.641.641 0 0 1 0 16.736v-2.869a.841.841 0 0 1 .053-.22c.372-.935 1.347-2.292 2.605-2.656.167-.429.414-1.055.644-1.517a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.499 1.132-3.368.397-.406.89-.717 1.474-.952 1.399-1.136 3.392-2.093 6.122-2.093 2.731 0 4.767.957 6.166 2.093.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086.23.462.477 1.088.644 1.517 1.258.364 2.233 1.721 2.605 2.656a.832.832 0 0 1 .053.22v2.869a.641.641 0 0 1-.078.256ZM12.172 11h-.344a4.323 4.323 0 0 1-.355.508C10.703 12.455 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a2.005 2.005 0 0 1-.085-.104L4 11.741v6.585c1.435.779 4.514 2.179 8 2.179 3.486 0 6.565-1.4 8-2.179v-6.585l-.098-.104s-.033.045-.085.104c-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.545-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.016.016Zm.641-2.935c.136 1.057.403 1.913.878 2.497.442.544 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.15.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.319-.862-2.824-1.025-1.487-.161-2.192.138-2.533.529-.269.307-.437.808-.438 1.578v.021c0 .265.021.562.063.893Zm-1.626 0c.042-.331.063-.628.063-.894v-.02c-.001-.77-.169-1.271-.438-1.578-.341-.391-1.046-.69-2.533-.529-1.505.163-2.347.537-2.824 1.025-.462.472-.705 1.179-.705 2.319 0 1.211.175 1.926.558 2.361.365.414 1.084.751 2.657.751 1.21 0 1.902-.394 2.344-.938.475-.584.742-1.44.878-2.497Z\\\"></path><path d=\\\"M14.5 14.25a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Zm-5 0a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          Copilot for business\\n\\n        </div>\\n\\n        Enterprise-grade AI features\\n      </div>\\n\\n    \\n</a></li>\\n\\n                      <li>\\n  <a class=\\\"HeaderMenu-dropdown-link d-block no-underline position-relative py-2 Link--secondary d-flex flex-items-center Link--has-description\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;premium_support&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;premium_support_link_enterprise_navbar&quot;}\\\" href=\\\"/premium-support\\\">\\n      <svg aria-hidden=\\\"true\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" version=\\\"1.1\\\" width=\\\"24\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-comment-discussion color-fg-subtle mr-3\\\">\\n    <path d=\\\"M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z\\\"></path><path d=\\\"M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z\\\"></path>\\n</svg>\\n      <div>\\n        <div class=\\\"color-fg-default h4\\\">\\n          Premium Support\\n\\n        </div>\\n\\n        Enterprise-grade 24/7 support\\n      </div>\\n\\n    \\n</a></li>\\n\\n                  </ul>\\n                </div>\\n            </div>\\n        </div>\\n\\n      </div>\\n</li>\\n\\n\\n                <li class=\\\"HeaderMenu-item position-relative flex-wrap flex-justify-between flex-items-center d-block d-lg-flex flex-lg-nowrap flex-lg-items-center js-details-container js-header-menu-item\\\">\\n    <a class=\\\"HeaderMenu-link no-underline px-0 px-lg-2 py-3 py-lg-2 d-block d-lg-inline-block\\\" data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;platform&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;platform_link_global_navbar&quot;}\\\" href=\\\"https://github.com/pricing\\\">Pricing</a>\\n</li>\\n\\n            </ul>\\n          </nav>\\n\\n        <div class=\\\"d-flex flex-column flex-lg-row width-full flex-justify-end flex-lg-items-center text-center mt-3 mt-lg-0 text-lg-left ml-lg-3\\\">\\n                \\n\\n\\n<qbsearch-input class=\\\"search-input\\\" data-scope=\\\"repo:suzgunmirac/BIG-Bench-Hard\\\" data-custom-scopes-path=\\\"/search/custom_scopes\\\" data-delete-custom-scopes-csrf=\\\"WpunyeDh5l_CLwzuk4yA4J227B7nYhxKYzuFjYvwdYPvKWxUPMOgRaBG2dLiV4Oac0hfYYPpZbLCbNdhgo9EMQ\\\" data-max-custom-scopes=\\\"10\\\" data-header-redesign-enabled=\\\"false\\\" data-initial-value=\\\"\\\" data-blackbird-suggestions-path=\\\"/search/suggestions\\\" data-jump-to-suggestions-path=\\\"/_graphql/GetSuggestedNavigationDestinations\\\" data-current-repository=\\\"suzgunmirac/BIG-Bench-Hard\\\" data-current-org=\\\"\\\" data-current-owner=\\\"suzgunmirac\\\" data-logged-in=\\\"false\\\" data-copilot-chat-enabled=\\\"false\\\" data-nl-search-enabled=\\\"false\\\" data-retain-scroll-position=\\\"true\\\">\\n  <div\\n    class=\\\"search-input-container search-with-dialog position-relative d-flex flex-row flex-items-center mr-4 rounded\\\"\\n    data-action=\\\"click:qbsearch-input#searchInputContainerClicked\\\"\\n  >\\n      <button\\n        type=\\\"button\\\"\\n        class=\\\"header-search-button placeholder  input-button form-control d-flex flex-1 flex-self-stretch flex-items-center no-wrap width-full py-0 pl-2 pr-0 text-left border-0 box-shadow-none\\\"\\n        data-target=\\\"qbsearch-input.inputButton\\\"\\n        aria-label=\\\"Search or jump to\u2026\\\"\\n        aria-haspopup=\\\"dialog\\\"\\n        placeholder=\\\"Search or jump to...\\\"\\n        data-hotkey=s,/\\n        autocapitalize=\\\"off\\\"\\n        data-analytics-event=\\\"{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;searchbar&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;input&quot;,&quot;label&quot;:&quot;searchbar_input_global_navbar&quot;}\\\"\\n        data-action=\\\"click:qbsearch-input#handleExpand\\\"\\n      >\\n        <div class=\\\"mr-2 color-fg-muted\\\">\\n          <svg aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-search\\\">\\n    <path d=\\\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\\\"></path>\\n</svg>\\n        </div>\\n        <span class=\\\"flex-1\\\" data-target=\\\"qbsearch-input.inputButtonText\\\">Search or jump to...</span>\\n          <div class=\\\"d-flex\\\" data-target=\\\"qbsearch-input.hotkeyIndicator\\\">\\n            <svg xmlns=\\\"http://www.w3.org/2000/svg\\\" width=\\\"22\\\" height=\\\"20\\\" aria-hidden=\\\"true\\\" class=\\\"mr-1\\\"><path fill=\\\"none\\\" stroke=\\\"#979A9C\\\" opacity=\\\".4\\\" d=\\\"M3.5.5h12c1.7 0 3 1.3 3 3v13c0 1.7-1.3 3-3 3h-12c-1.7 0-3-1.3-3-3v-13c0-1.7 1.3-3 3-3z\\\"></path><path fill=\\\"#979A9C\\\" d=\\\"M11.8 6L8 15.1h-.9L10.8 6h1z\\\"></path></svg>\\n          </div>\\n      </button>\\n\\n    <input type=\\\"hidden\\\" name=\\\"type\\\" class=\\\"js-site-search-type-field\\\">\\n\\n    \\n<div class=\\\"Overlay--hidden \\\" data-modal-dialog-overlay>\\n  <modal-dialog data-action=\\\"close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose\\\" data-target=\\\"qbsearch-input.searchSuggestionsDialog\\\" role=\\\"dialog\\\" id=\\\"search-suggestions-dialog\\\" aria-modal=\\\"true\\\" aria-labelledby=\\\"search-suggestions-dialog-header\\\" data-view-component=\\\"true\\\" class=\\\"Overlay Overlay--width-large Overlay--height-auto\\\">\\n      <h1 id=\\\"search-suggestions-dialog-header\\\" class=\\\"sr-only\\\">Search code, repositories, users, issues, pull requests...</h1>\\n    <div class=\\\"Overlay-body Overlay-body--paddingNone\\\">\\n      \\n          <div data-view-component=\\\"true\\\">        <div class=\\\"search-suggestions position-fixed width-full color-shadow-large border color-fg-default color-bg-default overflow-hidden d-flex flex-column query-builder-container\\\"\\n          style=\\\"border-radius: 12px;\\\"\\n          data-target=\\\"qbsearch-input.queryBuilderContainer\\\"\\n          hidden\\n        >\\n          <!-- '\\\"` --><!-- </textarea></xmp> --></option></form><form id=\\\"query-builder-test-form\\\" action=\\\"\\\" accept-charset=\\\"UTF-8\\\" method=\\\"get\\\">\\n  <query-builder data-target=\\\"qbsearch-input.queryBuilder\\\" id=\\\"query-builder-query-builder-test\\\" data-filter-key=\\\":\\\" data-view-component=\\\"true\\\" class=\\\"QueryBuilder search-query-builder\\\">\\n    <div class=\\\"FormControl FormControl--fullWidth\\\">\\n      <label id=\\\"query-builder-test-label\\\" for=\\\"query-builder-test\\\" class=\\\"FormControl-label sr-only\\\">\\n        Search\\n      </label>\\n      <div\\n        class=\\\"QueryBuilder-StyledInput width-fit \\\"\\n        data-target=\\\"query-builder.styledInput\\\"\\n      >\\n          <span id=\\\"query-builder-test-leadingvisual-wrap\\\" class=\\\"FormControl-input-leadingVisualWrap QueryBuilder-leadingVisualWrap\\\">\\n            <svg aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-search FormControl-input-leadingVisual\\\">\\n    <path d=\\\"M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z\\\"></path>\\n</svg>\\n          </span>\\n        <div data-target=\\\"query-builder.styledInputContainer\\\" class=\\\"QueryBuilder-StyledInputContainer\\\">\\n          <div\\n            aria-hidden=\\\"true\\\"\\n            class=\\\"QueryBuilder-StyledInputContent\\\"\\n            data-target=\\\"query-builder.styledInputContent\\\"\\n          ></div>\\n          <div class=\\\"QueryBuilder-InputWrapper\\\">\\n            <div aria-hidden=\\\"true\\\" class=\\\"QueryBuilder-Sizer\\\" data-target=\\\"query-builder.sizer\\\"></div>\\n            <input id=\\\"query-builder-test\\\" name=\\\"query-builder-test\\\" value=\\\"\\\" autocomplete=\\\"off\\\" type=\\\"text\\\" role=\\\"combobox\\\" spellcheck=\\\"false\\\" aria-expanded=\\\"false\\\" aria-describedby=\\\"validation-e8259a86-8adc-493c-961d-235b0242f352\\\" data-target=\\\"query-builder.input\\\" data-action=\\\"\\n          input:query-builder#inputChange\\n          blur:query-builder#inputBlur\\n          keydown:query-builder#inputKeydown\\n          focus:query-builder#inputFocus\\n        \\\" data-view-component=\\\"true\\\" class=\\\"FormControl-input QueryBuilder-Input FormControl-medium\\\" />\\n          </div>\\n        </div>\\n          <span class=\\\"sr-only\\\" id=\\\"query-builder-test-clear\\\">Clear</span>\\n          <button role=\\\"button\\\" id=\\\"query-builder-test-clear-button\\\" aria-labelledby=\\\"query-builder-test-clear query-builder-test-label\\\" data-target=\\\"query-builder.clearButton\\\" data-action=\\\"\\n                click:query-builder#clear\\n                focus:query-builder#clearButtonFocus\\n                blur:query-builder#clearButtonBlur\\n              \\\" variant=\\\"small\\\" hidden=\\\"hidden\\\" type=\\\"button\\\" data-view-component=\\\"true\\\" class=\\\"Button Button--iconOnly Button--invisible Button--medium mr-1 px-2 py-0 d-flex flex-items-center rounded-1 color-fg-muted\\\">  <svg aria-hidden=\\\"true\\\" height=\\\"16\\\" viewBox=\\\"0 0 16 16\\\" version=\\\"1.1\\\" width=\\\"16\\\" data-view-component=\\\"true\\\" class=\\\"octicon octicon-x-circle-fill Button-visual\\\">\\n    <path d=\\\"M2.343 13.657A8 8 0 1 1 13.658 2.343 8 8 0 0 1 2.343 13.657ZM6.03 4.97a.751.751 0 0 0-1.042.018.751.751 0 0 0-.018 1.042L6.94 8 4.97 9.97a.749.749 0 0 0 .326 1.275.749.749 0 0 0 .734-.215L8 9.06l1.97 1.97a.749.749 0 0 0 1.275-.326.749.749 0 0 0-.215-.734L9.06 8l1.97-1.97a.749.749 0 0 0-.326-1.275.749.749 0 0 0-.734.215L8 6.94Z\\\"></path>\\n</svg>\\n</button>\\n\\n      </div>\\n      <template id=\\\"search-icon\\\">\\n  <svg aria-hidden=\\\"true\\\" height=\\\"16\\\" view ...\n\n(The output of the tool call (shortuuid identifier: dTAQpDnaWbbcqo3VRUyRQT) is too long! Only the first 100000 characters are shown here. The original output length is 276603 characters. The full output has been saved to the file /workspace/dumps/workspace/.overlong_tool_outputs/dTAQpDnaWbbcqo3VRUyRQT.json. Please check this file carefully, as it may be very long!)", "tool_call_id": "toolu_01SrHubZvVsZB2vqL7n34TGe"}, {"role": "assistant", "content": "Let me search for more specific information about the datasets:", "tool_calls": [{"id": "toolu_01KJ51yd6G16yQhoM3TnChwP", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"Zebra Logic\\\" dataset tasks training set difficulty levels\"}"}}]}, {"role": "tool", "content": "Title: 1 Introduction\nLink: https://arxiv.org/html/2505.19641v3\nSnippet: Dataset, Tasks, Trainable, Adjustable Difficulty. BBH (Suzgun et al., 2022), 23, \u2717, \u2717. Zebra Logic (Lin et al., 2024), 1, \u2717, \u2713. KOR-Bench (Ma et ...\nSitelinks: No sitelinks\n\nTitle: Examining Stanford's ZebraLogic Study: AI's Struggles with ...\nLink: https://podscan.fm/podcasts/new-paradigm-ai-research-summaries/episodes/examining-stanfords-zebralogic-study-ais-struggles-with-complex-logical-reasoning\nSnippet: Thank you for joining me in this exploration of Zebra Logic and the fascinating challenges of logical reasoning in AI. Until next time, this has been New ...\nSitelinks: No sitelinks\n\nTitle: ZebraLogic: Benchmarking the Logical Reasoning Ability ...\nLink: https://huggingface.co/blog/yuchenlin/zebra-logic\nSnippet: To evaluate the logical reasoning abilities of LLMs, we have created a benchmark named ZebraLogic. Each example is a Logic Grid Puzzle, also known as a Zebra ...\nSitelinks: No sitelinks\n\nTitle: SynLogic: Synthesizing Verifiable Reasoning Data at Scale ...\nLink: https://arxiv.org/html/2505.19641v1\nSnippet: Dataset, Tasks, Trainable, Adjustable Difficulty. BBH (Suzgun et al., 2022), 23, \u2717, \u2717. Zebra Logic (Lin et al., 2024), 1, \u2717, \u2713. KOR-Bench (Ma et ...\nSitelinks: No sitelinks\n\nTitle: Scaling Agentic Intelligence in Enterprise AI.\nLink: https://www.linkedin.com/pulse/scaling-agentic-intelligence-enterprise-ai-jitender-malik-r04cc\nSnippet: Here's a recent paper from Zebra Logic(ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning), published earlier this year, focused on ...\nSitelinks: No sitelinks\n\nTitle: e2-research#1: Entering the Next Phase of Cognitive Reasoning\nLink: https://embraceable.ai/en/ankuendigung/e2-research1-entering-the-next-phase-of-cognitive-reasoning/\nSnippet: The tasks combine ... This combination of logic and semantics is less common in the training data ... Zebra-Logic Matrix Complexity: https://github.com ...\nSitelinks: No sitelinks\n\nTitle: CodeIO: Condensing Reasoning Patterns via Code Input- ...\nLink: https://openreview.net/forum?id=feIaF6vYFl&noteId=egU1nZravP\nSnippet: Zebra-Logic, Kor-Bench, Live-Bench, AVG. Baseline (Instruction Tuning) ... The paper reports on work on generating training data for reasoning ...\nSitelinks: No sitelinks\n\nTitle: Reinforcement Learning from Human Feedback\nLink: https://rlhfbook.com/book.pdf\nSnippet: like BigBenchHard, Zebra Logic, AIME, etc. The superficial alignment hypothesis is wrong for the same reason that people who think. RLHF and post-training ...\nSitelinks: No sitelinks\n\nTitle: Simple Bench (from AI Explained YouTuber) really ...\nLink: https://www.reddit.com/r/LocalLLaMA/comments/1ezks7m/simple_bench_from_ai_explained_youtuber_really/\nSnippet: A thinking fast, thinking slow training data set. ... zebra logic are really high quality leaderboards so surely your not ...\nSitelinks: No sitelinks\n\nTitle: Papers Explained 414: Out-of-distribution Math Problems ...\nLink: https://ritvik19.medium.com/papers-explained-414-out-of-distribution-math-problems-evaluation-with-3-generalization-axes-ac4abe71a794\nSnippet: The performance boost from RL is not uniform across all domains; geometry shows smaller gains compared to Zebra Logic. Press enter or click to ...\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01KJ51yd6G16yQhoM3TnChwP"}, {"role": "assistant", "content": "Perfect! I found a table that shows the exact information needed. Let me search for more information about KOR-Bench and BBEH:", "tool_calls": [{"id": "toolu_01JC2QQ8zLMrhw8cdufY7uaa", "type": "function", "function": {"name": "local-web_search", "arguments": "{\"query\": \"\\\"KOR-Bench\\\" \\\"broader categorization\\\" tasks number\"}"}}]}, {"role": "tool", "content": "Title: 1 Introduction\nLink: https://arxiv.org/html/2505.19641v3\nSnippet: The number of tasks of KOR-Bench is based on the broader categorization in the paper. \u201cTrainable\u201d indicates whether the dataset provides ...\nSitelinks: No sitelinks\n\nTitle: SynLogic: Synthesizing Verifiable Reasoning Data at Scale ...\nLink: https://arxiv.org/html/2505.19641v1\nSnippet: The number of tasks of KOR-Bench is based on the broader categorization in the paper. \u201cTrainable\u201d indicates whether the dataset provides ...\nSitelinks: No sitelinks\n\nTitle: (PDF) SynLogic: Synthesizing Verifiable Reasoning Data ...\nLink: https://www.researchgate.net/publication/392105561_SynLogic_Synthesizing_Verifiable_Reasoning_Data_at_Scale_for_Learning_Logical_Reasoning_and_Beyond\nSnippet: ... broader categorization in the paper. \u201cTrainable ... Kor-bench: Benchmarking language models on knowledge-orthogonal reasoning tasks.\nSitelinks: No sitelinks\n", "tool_call_id": "toolu_01JC2QQ8zLMrhw8cdufY7uaa"}, {"role": "assistant", "content": null, "tool_calls": [{"id": "toolu_017v4YxpQUXByqcTYwUPt1mb", "type": "function", "function": {"name": "fetch-fetch_html", "arguments": "{\"url\": \"https://arxiv.org/html/2505.19641v3\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"<!DOCTYPE html>\\n<html lang=\\\"en\\\">\\n<head>\\n<meta content=\\\"text/html; charset=utf-8\\\" http-equiv=\\\"content-type\\\"/>\\n<title>1 Introduction</title>\\n<!--Generated on Wed May 28 16:04:48 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->\\n<meta content=\\\"width=device-width, initial-scale=1, shrink-to-fit=no\\\" name=\\\"viewport\\\"/>\\n<link href=\\\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\\\" rel=\\\"stylesheet\\\" type=\\\"text/css\\\"/>\\n<link href=\\\"/static/browse/0.3.4/css/ar5iv.0.7.9.min.css\\\" rel=\\\"stylesheet\\\" type=\\\"text/css\\\"/>\\n<link href=\\\"/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css\\\" rel=\\\"stylesheet\\\" type=\\\"text/css\\\"/>\\n<link href=\\\"/static/browse/0.3.4/css/latexml_styles.css\\\" rel=\\\"stylesheet\\\" type=\\\"text/css\\\"/>\\n<script src=\\\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\\\"></script>\\n<script src=\\\"https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js\\\"></script>\\n<script src=\\\"/static/browse/0.3.4/js/addons_new.js\\\"></script>\\n<script src=\\\"/static/browse/0.3.4/js/feedbackOverlay.js\\\"></script>\\n<base href=\\\"/html/2505.19641v3/\\\"/></head>\\n<body>\\n<nav class=\\\"ltx_page_navbar\\\">\\n<nav class=\\\"ltx_TOC\\\">\\n<ol class=\\\"ltx_toclist\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_section\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S1\\\" title=\\\"\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">1 </span>Introduction</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_section\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S2\\\" title=\\\"\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">2 </span><span class=\\\"ltx_text ltx_font_smallcaps\\\">SynLogic</span>: Synthesizing Logical Reasoning Data at Scale</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_section\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S2.SS1\\\" title=\\\"In 2 SynLogic: Synthesizing Logical Reasoning Data at Scale\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">2.1 </span>Background</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S2.SS2\\\" title=\\\"In 2 SynLogic: Synthesizing Logical Reasoning Data at Scale\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">2.2 </span>The Data Synthesis Framework</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S2.SS2.SSS1\\\" title=\\\"In 2.2 The Data Synthesis Framework \u2023 2 SynLogic: Synthesizing Logical Reasoning Data at Scale\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">2.2.1 </span>Risk of Data Contamination</span></a></li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S2.SS3\\\" title=\\\"In 2 SynLogic: Synthesizing Logical Reasoning Data at Scale\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">2.3 </span>The <span class=\\\"ltx_text ltx_font_smallcaps\\\">SynLogic</span> Datasets</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S2.SS3.SSS1\\\" title=\\\"In 2.3 The SynLogic Datasets \u2023 2 SynLogic: Synthesizing Logical Reasoning Data at Scale\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">2.3.1 </span>Difficulty Analysis</span></a></li>\\n</ol>\\n</li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_section\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S3\\\" title=\\\"\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">3 </span>Reinforcement Learning on <span class=\\\"ltx_text ltx_font_smallcaps\\\">SynLogic</span></span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_section\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S3.SS1\\\" title=\\\"In 3 Reinforcement Learning on SynLogic\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">3.1 </span>Setup Details</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S3.SS1.SSS1\\\" title=\\\"In 3.1 Setup Details \u2023 3 Reinforcement Learning on SynLogic\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">3.1.1 </span>Training Template</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S3.SS1.SSS2\\\" title=\\\"In 3.1 Setup Details \u2023 3 Reinforcement Learning on SynLogic\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">3.1.2 </span>Reward Design</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S3.SS1.SSS3\\\" title=\\\"In 3.1 Setup Details \u2023 3 Reinforcement Learning on SynLogic\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">3.1.3 </span>Training Details</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S3.SS1.SSS4\\\" title=\\\"In 3.1 Setup Details \u2023 3 Reinforcement Learning on SynLogic\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">3.1.4 </span>Evaluation Details</span></a></li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S3.SS2\\\" title=\\\"In 3 Reinforcement Learning on SynLogic\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">3.2 </span>Results</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S3.SS2.SSS1\\\" title=\\\"In 3.2 Results \u2023 3 Reinforcement Learning on SynLogic\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">3.2.1 </span>Substantial Improvements in Logical Reasoning</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S3.SS2.SSS2\\\" title=\\\"In 3.2 Results \u2023 3 Reinforcement Learning on SynLogic\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">3.2.2 </span>Generalization to Mathematical Domains</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S3.SS2.SSS3\\\" title=\\\"In 3.2 Results \u2023 3 Reinforcement Learning on SynLogic\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">3.2.3 </span>Increased Chain-of-Thought Length</span></a></li>\\n</ol>\\n</li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_section\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S4\\\" title=\\\"\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">4 </span>Scaling RL Training with Diverse Verifiable Reasoning Data</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_section\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S4.SS1\\\" title=\\\"In 4 Scaling RL Training with Diverse Verifiable Reasoning Data\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">4.1 </span>Setup Details</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S4.SS2\\\" title=\\\"In 4 Scaling RL Training with Diverse Verifiable Reasoning Data\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">4.2 </span>Mixing <span class=\\\"ltx_text ltx_font_smallcaps\\\">SynLogic</span> with Math or Code Data: A Pilot Ablation Study</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S4.SS2.SSS1\\\" title=\\\"In 4.2 Mixing SynLogic with Math or Code Data: A Pilot Ablation Study \u2023 4 Scaling RL Training with Diverse Verifiable Reasoning Data\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">4.2.1 </span>Mixed Training with Math</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S4.SS2.SSS2\\\" title=\\\"In 4.2 Mixing SynLogic with Math or Code Data: A Pilot Ablation Study \u2023 4 Scaling RL Training with Diverse Verifiable Reasoning Data\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">4.2.2 </span>Mixed Training with Code</span></a></li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S4.SS3\\\" title=\\\"In 4 Scaling RL Training with Diverse Verifiable Reasoning Data\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">4.3 </span>32B Zero-RL Training with Diverse Reasoning Data</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S4.SS3.SSS1\\\" title=\\\"In 4.3 32B Zero-RL Training with Diverse Reasoning Data \u2023 4 Scaling RL Training with Diverse Verifiable Reasoning Data\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">4.3.1 </span>Results</span></a></li>\\n</ol>\\n</li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_section\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S5\\\" title=\\\"\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">5 </span>Conclusion</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_appendix\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A1\\\" title=\\\"\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">A </span>Comprehensive Overview of <span class=\\\"ltx_text ltx_font_smallcaps\\\">SynLogic</span></span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_appendix\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A1.SS1\\\" title=\\\"In Appendix A Comprehensive Overview of SynLogic\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">A.1 </span>Task Composition and Sources</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A1.SS2\\\" title=\\\"In Appendix A Comprehensive Overview of SynLogic\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">A.2 </span> <span class=\\\"ltx_text ltx_font_smallcaps\\\">SynLogic</span>-Hard and <span class=\\\"ltx_text ltx_font_smallcaps\\\">SynLogic</span>-Easy</span></a></li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_appendix\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A2\\\" title=\\\"\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">B </span>Training and Evaluation Details</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_appendix\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A2.SS1\\\" title=\\\"In Appendix B Training and Evaluation Details\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">B.1 </span>Training</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A2.SS1.SSS1\\\" title=\\\"In B.1 Training \u2023 Appendix B Training and Evaluation Details\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">B.1.1 </span>Training Template</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A2.SS1.SSS2\\\" title=\\\"In B.1 Training \u2023 Appendix B Training and Evaluation Details\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">B.1.2 </span>Training hyper-parameters</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A2.SS1.SSS3\\\" title=\\\"In B.1 Training \u2023 Appendix B Training and Evaluation Details\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">B.1.3 </span>Training Dynamics Analysis</span></a></li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A2.SS2\\\" title=\\\"In Appendix B Training and Evaluation Details\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">B.2 </span>Evaluation</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A2.SS2.SSS1\\\" title=\\\"In B.2 Evaluation \u2023 Appendix B Training and Evaluation Details\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">B.2.1 </span>Performance Analysis of Mixed Training with Math</span></a></li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A2.SS3\\\" title=\\\"In Appendix B Training and Evaluation Details\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">B.3 </span>Performance Analysis of Mixed Training with Coding</span></a></li>\\n</ol>\\n</li>\\n</ol></nav>\\n</nav>\\n<div class=\\\"ltx_page_main\\\">\\n<div class=\\\"ltx_page_content\\\">\\n<article class=\\\"ltx_document ltx_authors_1line\\\">\\n<div class=\\\"ltx_para\\\" id=\\\"p1\\\">\\n<span class=\\\"ltx_ERROR undefined\\\" id=\\\"p1.1\\\">\\\\reportnumber</span>\\n</div>\\n<div class=\\\"ltx_abstract\\\">\\n<h6 class=\\\"ltx_title ltx_title_abstract\\\">Abstract</h6>\\n<p class=\\\"ltx_p\\\" id=\\\"20.1\\\">Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL.\\nWe hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"20.1.1\\\">SynLogic</span>, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"20.1.2\\\">SynLogic</span> approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards.\\nIn our experiments, we validate the effectiveness of RL training on the <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"20.1.3\\\">SynLogic</span> dataset based on 7B and 32B models. <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"20.1.4\\\">SynLogic</span> leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"20.1.5\\\">SynLogic</span> data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks.\\nThese findings position <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"20.1.6\\\">SynLogic</span> as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"20.1.7\\\">SynLogic</span> dataset at <a class=\\\"ltx_ref ltx_url ltx_font_typewriter\\\" href=\\\"https://github.com/MiniMax-AI/SynLogic\\\" title=\\\"\\\">https://github.com/MiniMax-AI/SynLogic</a>.</p>\\n</div>\\n<div class=\\\"ltx_abstract\\\">\\n<h6 class=\\\"ltx_title ltx_title_abstract\\\">Abstract</h6>\\n<p class=\\\"ltx_p\\\" id=\\\"21.1\\\"><span class=\\\"ltx_text\\\" id=\\\"21.1.1\\\" lang=\\\"en\\\">Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL.\\nWe hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"21.1.1.1\\\">SynLogic</span>, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"21.1.1.2\\\">SynLogic</span> approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards.\\nIn our experiments, we validate the effectiveness of RL training on the <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"21.1.1.3\\\">SynLogic</span> dataset based on 7B and 32B models. <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"21.1.1.4\\\">SynLogic</span> leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"21.1.1.5\\\">SynLogic</span> data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks.\\nThese findings position <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"21.1.1.6\\\">SynLogic</span> as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"21.1.1.7\\\">SynLogic</span> dataset at <a class=\\\"ltx_ref ltx_url ltx_font_typewriter\\\" href=\\\"https://github.com/MiniMax-AI/SynLogic\\\" title=\\\"\\\">https://github.com/MiniMax-AI/SynLogic</a>.</span></p>\\n</div>\\n<figure class=\\\"ltx_figure\\\" id=\\\"id1\\\" lang=\\\"en\\\"><img alt=\\\"[Uncaptioned image]\\\" class=\\\"ltx_graphics ltx_img_landscape\\\" height=\\\"178\\\" id=\\\"id1.g1\\\" src=\\\"x1.png\\\" width=\\\"830\\\"/>\\n</figure>\\n<section class=\\\"ltx_section\\\" id=\\\"S1\\\" lang=\\\"en\\\">\\n<h2 class=\\\"ltx_title ltx_title_section\\\">\\n<span class=\\\"ltx_tag ltx_tag_section\\\">1 </span>Introduction</h2>\\n<figure class=\\\"ltx_figure\\\" id=\\\"S1.F1\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_centering ltx_img_landscape\\\" height=\\\"301\\\" id=\\\"S1.F1.g1\\\" src=\\\"x2.png\\\" width=\\\"747\\\"/>\\n<figcaption class=\\\"ltx_caption ltx_centering\\\"><span class=\\\"ltx_tag ltx_tag_figure\\\">Figure 1: </span>The framework of logic data synthesis. The process begins with the selection of suitable tasks and the identification of key parameters that control task difficulty. Next, logic instances are generated with appropriate difficulty control (e.g., setting the grid size of Sudoku to 7). These instances are subsequently formalized into natural language instructions. Each task is paired with a task-specific verifier to check the correctness of responses. This framework enables the systematic synthesis of high-quality logic data, covering a wide range of difficulty levels and 35 task types.</figcaption>\\n</figure>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.p1.1\\\">The success of Deepseek R1\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(DeepSeek-AI et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib3\\\" title=\\\"\\\">2025</a>)</cite> and OpenAI-o1\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Jaech et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib7\\\" title=\\\"\\\">2024</a>)</cite> demonstrates the great potential of post-training in advancing strong reasoning capabilities. These works reveal that the core methodology behind these advancements is reinforcement learning with verifiable rewards (RLVR), inspiring numerous replication efforts focused on RL training. However, most of these works have concentrated on the mathematics and coding domains, primarily because it is straightforward to design binary reward rules in these areas\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Zeng et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib24\\\" title=\\\"\\\">2025b</a>, <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib23\\\" title=\\\"\\\">a</a>; Yu et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib22\\\" title=\\\"\\\">2025</a>; Hu et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib6\\\" title=\\\"\\\">2025</a>; Zhang et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib25\\\" title=\\\"\\\">2025</a>)</cite>.\\nTo foster more general and comprehensive reasoning abilities, it is essential to utilize diverse tasks and examples with verifiable rewards.\\nIn this work, we concentrate on logical reasoning as a promising domain for this objective, hypothesizing that logical reasoning serves as a fundamental building block for developing general reasoning skills.\\nAlthough prior work has explored RL training in the context of logic tasks\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Xie et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib20\\\" title=\\\"\\\">2025b</a>; Pan et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib14\\\" title=\\\"\\\">2025</a>)</cite>, these efforts have typically focused on a single task, leaving the potential of broader and more diverse synthetic logic datasets largely underexplored.</p>\\n</div>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.p2\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.p2.1\\\">Synthetic logic data presents distinct advantages and challenges. Its synthetic nature allows for unlimited data generation with controllable difficulty levels, enabling the creation of increasingly challenging samples. Additionally, the intrinsic properties of some logic tasks like Sudoku often require trial and backtracking in the reasoning process, which closely relates to the \u201caha moments\u201d\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(DeepSeek-AI et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib3\\\" title=\\\"\\\">2025</a>)</cite> in problem-solving. Therefore, the primary advantages of synthetic logic data for RLVR lie in its scalability and inherent characteristics that align well with complex reasoning processes. The main challenge, however, is the complexity of generating and designing specific rules for different logic tasks, as tasks like Game of 24 and Sudoku each require distinct verifiers.</p>\\n</div>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.p3\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.p3.1\\\">Recent works primarily focus on logic evaluation\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Suzgun et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib18\\\" title=\\\"\\\">2022</a>; Ma et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib13\\\" title=\\\"\\\">2024</a>; Kazemi et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib9\\\" title=\\\"\\\">2025</a>)</cite>, but lack high-quality accessible logical reasoning training data. In this work, to address the gap in comprehensive logic tasks, we present <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S1.p3.1.1\\\">SynLogic</span>: a logical reasoning data synthesis framework and a comprehensive synthetic logic dataset containing 35 tasks, including typical logical tasks such as Sudoku, Game of 24, and Cipher. For each task, we develop task-specific generation code paired with a corresponding rule-based verifier, allowing for fine-grained difficulty control through adjustable generation hyperparameters.</p>\\n</div>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.p4\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.p4.1\\\">To validate the effectiveness of reinforcement learning on the <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S1.p4.1.1\\\">SynLogic</span> data, we run RL training on it with the GRPO algorithm\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Shao et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib17\\\" title=\\\"\\\">2024</a>)</cite> and implement binary outcome rewards determined by each task\u2019s verification rules. By adapting recent GRPO training techniques introduced in DAPO\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Yu et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib22\\\" title=\\\"\\\">2025</a>)</cite>, we successfully train Qwen2.5 Base models\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Yang et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib21\\\" title=\\\"\\\">2024</a>)</cite> on the <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S1.p4.1.2\\\">SynLogic</span> data in a zero RL training setting, achieving progressively longer COT responses and observing the emergence of reflection behaviors.\\nStarting from Qwen2.5-7B-Base and Qwen2.5-32B-Base foundations, our models achieve over 8 absolute percentage points improvement on the logic benchmark KOR-Bench\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Ma et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib13\\\" title=\\\"\\\">2024</a>)</cite> compared to their instruction models. Notably, our 32B model surpasses DeepSeek-R1-Distill-Qwen-32B on BBEH\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Kazemi et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib9\\\" title=\\\"\\\">2025</a>)</cite> tasks by 5 absolute points, establishing <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S1.p4.1.3\\\">SynLogic</span> as the state-of-the-art open-source dataset for logical reasoning to date. Additionally, both models demonstrate strong generalization to unseen mathematics domains.</p>\\n</div>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.p5\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.p5.1\\\">Furthermore, we explore mixing the <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S1.p5.1.1\\\">SynLogic</span> data with mathematics or coding data for RL training. Surprisingly, conducting the mixed training on Qwen2.5-7B-Base model\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Yang et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib21\\\" title=\\\"\\\">2024</a>)</cite>, incorporating <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S1.p5.1.2\\\">SynLogic</span> data improves training efficiency for developing mathematical and coding skills. For mathematics, mixed training maintains similar mathematics performance under the same number of training steps, which consume fewer math training samples.\\nSimultaneously, mixed training achieves much higher performance on logic tasks.\\nA similar trend is observed when mixing <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S1.p5.1.3\\\">SynLogic</span> with coding data, further demonstrating the complementary benefits of logical reasoning training. Finally, we conduct large-scale mixed training on the Qwen2.5-32B-Base model to enhance the capability of Zero-RL training. Our mixed training achieves superior performance on multiple benchmarks compared to the DeepSeek-R1-Zero-Qwen-32B model, consistently outperforming or matching it on BBEH\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Kazemi et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib9\\\" title=\\\"\\\">2025</a>)</cite>, KOR-Bench\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Ma et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib13\\\" title=\\\"\\\">2024</a>)</cite>, LiveCodeBench\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Jain et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib8\\\" title=\\\"\\\">2025</a>)</cite>, and GPQA-Diamond\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Rein et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib15\\\" title=\\\"\\\">2024</a>)</cite>, validating the generalization benefits provided by the inclusion of logical reasoning data.</p>\\n</div>\\n</section>\\n<section class=\\\"ltx_section\\\" id=\\\"S2\\\" lang=\\\"en\\\">\\n<h2 class=\\\"ltx_title ltx_title_section\\\">\\n<span class=\\\"ltx_tag ltx_tag_section\\\">2 </span><span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.1.1\\\">SynLogic</span>: Synthesizing Logical Reasoning Data at Scale</h2>\\n<figure class=\\\"ltx_table\\\" id=\\\"S2.T1\\\">\\n<figcaption class=\\\"ltx_caption\\\"><span class=\\\"ltx_tag ltx_tag_table\\\">Table 1: </span>Comparison of <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.T1.3.1\\\">SynLogic</span> with existing synthetic logic datasets. <sup class=\\\"ltx_sup\\\" id=\\\"S2.T1.4.2\\\">*</sup>The number of tasks of KOR-Bench is based on the broader categorization in the paper. \u201cTrainable\u201d indicates whether the dataset provides training data.</figcaption>\\n<table class=\\\"ltx_tabular ltx_centering ltx_align_middle\\\" id=\\\"S2.T1.5\\\">\\n<tr class=\\\"ltx_tr\\\" id=\\\"S2.T1.5.1\\\">\\n<td class=\\\"ltx_td ltx_align_left ltx_border_tt\\\" id=\\\"S2.T1.5.1.1\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.1.1.1\\\" style=\\\"font-size:90%;\\\">Dataset</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_tt\\\" id=\\\"S2.T1.5.1.2\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.1.2.1\\\" style=\\\"font-size:90%;\\\">Tasks</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_tt\\\" id=\\\"S2.T1.5.1.3\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.1.3.1\\\" style=\\\"font-size:90%;\\\">Trainable</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_tt\\\" id=\\\"S2.T1.5.1.4\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.1.4.1\\\" style=\\\"font-size:90%;\\\">Adjustable Difficulty</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S2.T1.5.2\\\">\\n<td class=\\\"ltx_td ltx_align_left ltx_border_t\\\" id=\\\"S2.T1.5.2.1\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\">\\n<span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.2.1.1\\\" style=\\\"font-size:90%;\\\">BBH\u00a0</span><cite class=\\\"ltx_cite ltx_citemacro_citep\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.2.1.2.1\\\" style=\\\"font-size:90%;\\\">(</span>Suzgun et\u00a0al.<span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.2.1.3.2.1.1\\\" style=\\\"font-size:90%;\\\">, </span><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib18\\\" title=\\\"\\\">2022</a><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.2.1.4.3\\\" style=\\\"font-size:90%;\\\">)</span></cite>\\n</td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S2.T1.5.2.2\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.2.2.1\\\" style=\\\"font-size:90%;\\\">23</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S2.T1.5.2.3\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.2.3.1\\\" style=\\\"font-size:90%;color:#FF0000;\\\">\u2717</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S2.T1.5.2.4\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.2.4.1\\\" style=\\\"font-size:90%;color:#FF0000;\\\">\u2717</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S2.T1.5.3\\\">\\n<td class=\\\"ltx_td ltx_align_left\\\" id=\\\"S2.T1.5.3.1\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\">\\n<span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.3.1.1\\\" style=\\\"font-size:90%;\\\">Zebra Logic\u00a0</span><cite class=\\\"ltx_cite ltx_citemacro_citep\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.3.1.2.1\\\" style=\\\"font-size:90%;\\\">(</span>Lin et\u00a0al.<span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.3.1.3.2.1.1\\\" style=\\\"font-size:90%;\\\">, </span><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib12\\\" title=\\\"\\\">2024</a><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.3.1.4.3\\\" style=\\\"font-size:90%;\\\">)</span></cite>\\n</td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S2.T1.5.3.2\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.3.2.1\\\" style=\\\"font-size:90%;\\\">1</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S2.T1.5.3.3\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.3.3.1\\\" style=\\\"font-size:90%;color:#FF0000;\\\">\u2717</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S2.T1.5.3.4\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.3.4.1\\\" style=\\\"font-size:90%;color:#006400;\\\">\u2713</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S2.T1.5.4\\\">\\n<td class=\\\"ltx_td ltx_align_left\\\" id=\\\"S2.T1.5.4.1\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\">\\n<span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.4.1.1\\\" style=\\\"font-size:90%;\\\">KOR-Bench\u00a0</span><cite class=\\\"ltx_cite ltx_citemacro_citep\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.4.1.2.1\\\" style=\\\"font-size:90%;\\\">(</span>Ma et\u00a0al.<span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.4.1.3.2.1.1\\\" style=\\\"font-size:90%;\\\">, </span><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib13\\\" title=\\\"\\\">2024</a><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.4.1.4.3\\\" style=\\\"font-size:90%;\\\">)</span></cite>\\n</td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S2.T1.5.4.2\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\">\\n<span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.4.2.1\\\" style=\\\"font-size:90%;\\\">5</span><sup class=\\\"ltx_sup\\\" id=\\\"S2.T1.5.4.2.2\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.4.2.2.1\\\" style=\\\"font-size:90%;\\\">*</span></sup>\\n</td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S2.T1.5.4.3\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.4.3.1\\\" style=\\\"font-size:90%;color:#FF0000;\\\">\u2717</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S2.T1.5.4.4\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.4.4.1\\\" style=\\\"font-size:90%;color:#FF0000;\\\">\u2717</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S2.T1.5.5\\\">\\n<td class=\\\"ltx_td ltx_align_left\\\" id=\\\"S2.T1.5.5.1\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\">\\n<span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.5.1.1\\\" style=\\\"font-size:90%;\\\">K&amp;K\u00a0</span><cite class=\\\"ltx_cite ltx_citemacro_citep\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.5.1.2.1\\\" style=\\\"font-size:90%;\\\">(</span>Xie et\u00a0al.<span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.5.1.3.2.1.1\\\" style=\\\"font-size:90%;\\\">, </span><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib19\\\" title=\\\"\\\">2025a</a><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.5.1.4.3\\\" style=\\\"font-size:90%;\\\">)</span></cite>\\n</td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S2.T1.5.5.2\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.5.2.1\\\" style=\\\"font-size:90%;\\\">1</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S2.T1.5.5.3\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.5.3.1\\\" style=\\\"font-size:90%;color:#006400;\\\">\u2713</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S2.T1.5.5.4\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.5.4.1\\\" style=\\\"font-size:90%;color:#006400;\\\">\u2713</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S2.T1.5.6\\\">\\n<td class=\\\"ltx_td ltx_align_left\\\" id=\\\"S2.T1.5.6.1\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\">\\n<span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.6.1.1\\\" style=\\\"font-size:90%;\\\">BBEH\u00a0</span><cite class=\\\"ltx_cite ltx_citemacro_citep\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.6.1.2.1\\\" style=\\\"font-size:90%;\\\">(</span>Kazemi et\u00a0al.<span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.6.1.3.2.1.1\\\" style=\\\"font-size:90%;\\\">, </span><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib9\\\" title=\\\"\\\">2025</a><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.6.1.4.3\\\" style=\\\"font-size:90%;\\\">)</span></cite>\\n</td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S2.T1.5.6.2\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.6.2.1\\\" style=\\\"font-size:90%;\\\">23</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S2.T1.5.6.3\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.6.3.1\\\" style=\\\"font-size:90%;color:#FF0000;\\\">\u2717</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S2.T1.5.6.4\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.6.4.1\\\" style=\\\"font-size:90%;color:#FF0000;\\\">\u2717</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S2.T1.5.7\\\">\\n<td class=\\\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\\\" id=\\\"S2.T1.5.7.1\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.T1.5.7.1.1\\\" style=\\\"font-size:90%;\\\">SynLogic</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\\\" id=\\\"S2.T1.5.7.2\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.7.2.1\\\" style=\\\"font-size:90%;\\\">35</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\\\" id=\\\"S2.T1.5.7.3\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.7.3.1\\\" style=\\\"font-size:90%;color:#006400;\\\">\u2713</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\\\" id=\\\"S2.T1.5.7.4\\\" style=\\\"padding-left:4.0pt;padding-right:4.0pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S2.T1.5.7.4.1\\\" style=\\\"font-size:90%;color:#006400;\\\">\u2713</span></td>\\n</tr>\\n</table>\\n</figure>\\n<section class=\\\"ltx_subsection\\\" id=\\\"S2.SS1\\\">\\n<h3 class=\\\"ltx_title ltx_title_subsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsection\\\">2.1 </span>Background</h3>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.SS1.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.SS1.p1.1\\\">Logical reasoning has long been a crucial indicator of model intelligence\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(ai2, <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib1\\\" title=\\\"\\\">2019</a>; Suzgun et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib18\\\" title=\\\"\\\">2022</a>)</cite>, valued for both its significance and synthetic accessibility. With the advancement of reasoning capabilities in Large Language Models (LLMs), researchers have developed increasingly challenging benchmarks to evaluate logical reasoning abilities\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Kazemi et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib9\\\" title=\\\"\\\">2025</a>; Ma et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib13\\\" title=\\\"\\\">2024</a>)</cite>. However, as illustrated in Table\u00a0<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S2.T1\\\" title=\\\"Tab. 1 \u2023 2 SynLogic: Synthesizing Logical Reasoning Data at Scale\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">1</span></a>, existing benchmarks either lack training support or are limited to a small number of tasks. Synthetic logic data serves as an important source of verifiable data and offers straightforward control over task difficulty, presenting the potential for developing scalable stronger models by training on it. Consequently, comprehensive synthetic logic datasets are essential for developing general strong reasoning models\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Seed et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib16\\\" title=\\\"\\\">2025</a>)</cite>.</p>\\n</div>\\n</section>\\n<section class=\\\"ltx_subsection\\\" id=\\\"S2.SS2\\\">\\n<h3 class=\\\"ltx_title ltx_title_subsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsection\\\">2.2 </span>The Data Synthesis Framework</h3>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.SS2.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.SS2.p1.1\\\">To synthesize large-scale, diverse synthetic data, we develop a comprehensive data synthesis framework encompassing 35 tasks. While the benchmarks in Table\u00a0<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S2.T1\\\" title=\\\"Tab. 1 \u2023 2 SynLogic: Synthesizing Logical Reasoning Data at Scale\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">1</span></a> include a wide variety of tasks, a significant challenge we faced is that nearly all evaluation benchmarks do not open-source their data generation methods. This hinders us from building training data for these logic tasks directly. Therefore, we develop these tasks independently, building the <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.SS2.p1.1.1\\\">SynLogic</span> framework, illustrated in Figure\u00a0<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S1.F1\\\" title=\\\"Fig. 1 \u2023 1 Introduction\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">1</span></a>. The framework consists of the following key components:</p>\\n</div>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.SS2.p2\\\">\\n<ol class=\\\"ltx_enumerate\\\" id=\\\"S2.I1\\\">\\n<li class=\\\"ltx_item\\\" id=\\\"S2.I1.i1\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">1.</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.I1.i1.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.I1.i1.p1.1\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S2.I1.i1.p1.1.1\\\">Task Selection</span>\\nWe select a diverse set of logic tasks that require non-trivial reasoning, drawing from two carefully curated categories of data sources: (1) widely recognized puzzle problems from logic communities, such as the game of 24, Sudoku, and cryptarithms. Many of these puzzles have been previously highlighted in works like\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Kurtic et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib10\\\" title=\\\"\\\">2024</a>; Li et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib11\\\" title=\\\"\\\">2024</a>; Ma et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib13\\\" title=\\\"\\\">2024</a>)</cite>. (2) Logic tasks featured in established evaluation benchmarks, including BBH\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Suzgun et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib18\\\" title=\\\"\\\">2022</a>)</cite> and BBEH\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Kazemi et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib9\\\" title=\\\"\\\">2025</a>)</cite>. Detailed descriptions and sources for all 35 tasks can be found in the Appendix\u00a0<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A1.SS1\\\" title=\\\"A.1 Task Composition and Sources \u2023 Appendix A Comprehensive Overview of SynLogic\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">A.1</span></a>.</p>\\n</div>\\n</li>\\n<li class=\\\"ltx_item\\\" id=\\\"S2.I1.i2\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">2.</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.I1.i2.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.I1.i2.p1.1\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S2.I1.i2.p1.1.1\\\">Parameter Identification</span>\\nFor each task, we identify key parameters that control difficulty (e.g., grid size in Sudoku, or missing numbers in Math Path). These parameters form the basis for scalable and adjustable difficulty data synthesis.</p>\\n</div>\\n</li>\\n<li class=\\\"ltx_item\\\" id=\\\"S2.I1.i3\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">3.</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.I1.i3.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.I1.i3.p1.1\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S2.I1.i3.p1.1.1\\\">Logic Instance Generation</span>\\nWe formalize the task-specific rules into code by manually implementing rule-based logic generators for each task. These generators are designed to encode the specific constraints and rules of the logic problems, ensuring that the generated instances adhere to the intended task structure (e.g., enforcing the unique digits rule in Sudoku). This rule-based approach allows us to efficiently produce large quantities of data and to cover a broad spectrum of difficulty levels by adjusting the difficulty related parameters. All generated instances undergo automated checks for correctness and solvability.</p>\\n</div>\\n</li>\\n<li class=\\\"ltx_item\\\" id=\\\"S2.I1.i4\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">4.</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.I1.i4.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.I1.i4.p1.1\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S2.I1.i4.p1.1.1\\\">Appropriate Difficulty Control</span>\\nTo ensure that the generated data is both challenging and learnable, we carefully adjust difficulty-related parameters during data generation. We use strong reasoning models, DeepSeek R1\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(DeepSeek-AI et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib3\\\" title=\\\"\\\">2025</a>)</cite> and OpenAI-o3-mini to set an upper bound on difficulty: the highest difficulty parameters for which R1 or o3-mini can solve samples with a pass@10 greater than zero, representing the limit of these models\u2019 solvability. This approach prevents the inclusion of instances that are too difficult.\\nSimilarly, we use chat models to determine the lower bound of difficulty: the lowest difficulty parameters for which the models achieve a pass rate between 0 and 0.5.\\nThis dual-bound approach ensures that the dataset includes a balanced range of samples, maintaining an appropriate level of complexity and learnability.</p>\\n</div>\\n</li>\\n<li class=\\\"ltx_item\\\" id=\\\"S2.I1.i5\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">5.</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.I1.i5.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.I1.i5.p1.1\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S2.I1.i5.p1.1.1\\\">Prompt Formalization</span>\\nTo facilitate training and evaluation with LLMs, we convert abstract logic instances into natural language prompts using task-specific prompt templates. This step ensures that each instance is accessible to both humans and language models.</p>\\n</div>\\n</li>\\n<li class=\\\"ltx_item\\\" id=\\\"S2.I1.i6\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">6.</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.I1.i6.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.I1.i6.p1.1\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S2.I1.i6.p1.1.1\\\">Verification Suite</span>\\nFor every task, we implement a dedicated verifier that can automatically check the correctness of model outputs, supporting both training supervision and automatic evaluation.</p>\\n</div>\\n</li>\\n</ol>\\n</div>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.SS2.p3\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.SS2.p3.1\\\">A key innovation in our approach is the development of customized difficulty control mechanisms for each task type. Unlike existing benchmarks that often provide fixed-difficulty evaluation data, our system allows precise calibration of problem complexity through task-specific parameters, such as grid size in Sudoku. This difficulty-tuning capability enables the creation of different difficulty level data, presenting the potential of progressively challenging training curricula. We overcame significant challenges in implementing these controls, as many evaluation benchmarks do not open-source their data generation methods.\\nAt last, most tasks in <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.SS2.p3.1.1\\\">SynLogic</span> are designed with: (1) a data generation code capable of producing varied instances, (2) a corresponding verification rule for evaluating solution correctness, and (3) configurable difficulty parameters to enable controlled difficulty of generated data. We independently develop and generate data for 33 tasks in our dataset, while only the data of 2 tasks (Zebra Puzzle\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Lin et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib12\\\" title=\\\"\\\">2024</a>)</cite> and ARC-AGI\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Chollet, <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib2\\\" title=\\\"\\\">2019</a>)</cite>) are directly adopted from existing open source resources.</p>\\n</div>\\n<section class=\\\"ltx_subsubsection\\\" id=\\\"S2.SS2.SSS1\\\">\\n<h4 class=\\\"ltx_title ltx_title_subsubsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsubsection\\\">2.2.1 </span>Risk of Data Contamination</h4>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.SS2.SSS1.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.SS2.SSS1.p1.1\\\">Although several tasks overlap between our selected tasks and current benchmarks, such as KOR-Bench and BBEH, the synthetic nature of our data, combined with the large synthesis space, makes the probability of generating data identical to benchmark test samples very low \u2013 we have verified that there are no identical samples between our generated datasets and the benchmark test sets.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\\\"ltx_subsection\\\" id=\\\"S2.SS3\\\">\\n<h3 class=\\\"ltx_title ltx_title_subsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsection\\\">2.3 </span>The <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.SS3.1.1\\\">SynLogic</span> Datasets</h3>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.SS3.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.SS3.p1.1\\\">We synthesized our dataset with controlled difficulty parameters for each task, carefully balancing challenge and learnability to ensure the success of our subsequent experiments \u00a7<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S3\\\" title=\\\"3 Reinforcement Learning on SynLogic\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">3</span></a>. To accommodate different model capacities, we developed two distinct versions of our dataset: <span class=\\\"ltx_text ltx_font_bold ltx_font_smallcaps\\\" id=\\\"S2.SS3.p1.1.1\\\">SynLogic<span class=\\\"ltx_text ltx_font_upright\\\" id=\\\"S2.SS3.p1.1.1.1\\\">-Hard</span></span> for Qwen2.5-32B training and <span class=\\\"ltx_text ltx_font_bold ltx_font_smallcaps\\\" id=\\\"S2.SS3.p1.1.2\\\">SynLogic<span class=\\\"ltx_text ltx_font_upright\\\" id=\\\"S2.SS3.p1.1.2.1\\\">-Easy</span></span> for Qwen2.5-7B training. <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.SS3.p1.1.3\\\">SynLogic</span>-Hard presents more complex challenges with its broader difficulty level for each tasks with difficulty upper bound described in \u00a7\u00a0<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S2.SS2\\\" title=\\\"2.2 The Data Synthesis Framework \u2023 2 SynLogic: Synthesizing Logical Reasoning Data at Scale\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">2.2</span></a>. For <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.SS3.p1.1.4\\\">SynLogic</span>-Easy, we systematically lower difficulty parameters across all tasks to create easier versions. Despite these adjustments, eight tasks still remain beyond the learning capacity of the 7B model with zero training accuracy after RL, thus we removed them from this easy version, where the details about the removed tasks are provided in Appendix\u00a0<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A1.SS2\\\" title=\\\"A.2 SynLogic-Hard and SynLogic-Easy \u2023 Appendix A Comprehensive Overview of SynLogic\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">A.2</span></a>.\\nFinally, we synthesized 33k <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.SS3.p1.1.5\\\">SynLogic</span>-Hard samples and 16k <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.SS3.p1.1.6\\\">SynLogic</span>-Easy samples used in subsequent experiments for training, along with 10 validation samples per task, separately for the Easy and Hard validation splits.</p>\\n</div>\\n<section class=\\\"ltx_subsubsection\\\" id=\\\"S2.SS3.SSS1\\\">\\n<h4 class=\\\"ltx_title ltx_title_subsubsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsubsection\\\">2.3.1 </span>Difficulty Analysis</h4>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.SS3.SSS1.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.SS3.SSS1.p1.1\\\">To assess the difficulty of the synthetic data, we conduct an evaluation on the validation splits, assessing model performance using both avg@8 (average pass rate with eight attempts) and pass@8 (success within eight attempts) metrics. The results, illustrated in Figure \u00a0<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S2.F2\\\" title=\\\"Fig. 2 \u2023 2.3.1 Difficulty Analysis \u2023 2.3 The SynLogic Datasets \u2023 2 SynLogic: Synthesizing Logical Reasoning Data at Scale\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">2</span></a>, confirm the appropriate difficulty levels for each model scale, demonstrating that our datasets provide suitable training challenges across different model capacities.</p>\\n</div>\\n<figure class=\\\"ltx_figure\\\" id=\\\"S2.F2\\\">\\n<div class=\\\"ltx_flex_figure\\\">\\n<div class=\\\"ltx_flex_cell ltx_flex_size_2\\\">\\n<figure class=\\\"ltx_figure ltx_figure_panel ltx_align_center\\\" id=\\\"S2.F2.sf1\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_img_landscape\\\" height=\\\"168\\\" id=\\\"S2.F2.sf1.g1\\\" src=\\\"x3.png\\\" width=\\\"226\\\"/>\\n<figcaption class=\\\"ltx_caption\\\"><span class=\\\"ltx_tag ltx_tag_figure\\\">(a) </span>7B models on <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.F2.sf1.2.1\\\">SynLogic</span>-Easy</figcaption>\\n</figure>\\n</div>\\n<div class=\\\"ltx_flex_cell ltx_flex_size_2\\\">\\n<figure class=\\\"ltx_figure ltx_figure_panel ltx_align_center\\\" id=\\\"S2.F2.sf2\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_img_landscape\\\" height=\\\"168\\\" id=\\\"S2.F2.sf2.g1\\\" src=\\\"x4.png\\\" width=\\\"226\\\"/>\\n<figcaption class=\\\"ltx_caption\\\"><span class=\\\"ltx_tag ltx_tag_figure\\\">(b) </span>32B models on <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.F2.sf2.2.1\\\">SynLogic</span>-Hard</figcaption>\\n</figure>\\n</div>\\n</div>\\n<figcaption class=\\\"ltx_caption ltx_centering\\\"><span class=\\\"ltx_tag ltx_tag_figure\\\">Figure 2: </span>Evaluation of task difficulty across our dataset versions. (a) Shows the performance of 7B-scale models on the <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.F2.3.1\\\">SynLogic</span>-Easy dataset, while (b) demonstrates the performance of 32B-scale models on the more challenging <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S2.F2.4.2\\\">SynLogic</span>-Hard dataset. Results are measured using avg@8 (average pass rate with eight attempts) and pass@8 (success within eight attempts) metrics, illustrating the appropriate difficulty control for each model scale.</figcaption>\\n</figure>\\n</section>\\n</section>\\n</section>\\n<section class=\\\"ltx_section\\\" id=\\\"S3\\\" lang=\\\"en\\\">\\n<h2 class=\\\"ltx_title ltx_title_section\\\">\\n<span class=\\\"ltx_tag ltx_tag_section\\\">3 </span>Reinforcement Learning on <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.1.1\\\">SynLogic</span> </h2>\\n<figure class=\\\"ltx_table\\\" id=\\\"S3.T2\\\">\\n<figcaption class=\\\"ltx_caption\\\"><span class=\\\"ltx_tag ltx_tag_table\\\">Table 2: </span>Evaluation accuracy (%) of 7B and 32B models across logic benchmarks (<span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.T2.6.1\\\">SynLogic</span>-Val, KOR-Bench, BBH, BBEH) and mathematical benchmarks (AIME 2024, MATH 500, AMC 2023). For 7B models, <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.T2.7.2\\\">SynLogic</span>-Val refers to <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.T2.8.3\\\">SynLogic</span>-Easy, while for 32B models, it refers to <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.T2.9.4\\\">SynLogic</span>-Hard as described in \u00a7\u00a0<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S2.SS3\\\" title=\\\"2.3 The SynLogic Datasets \u2023 2 SynLogic: Synthesizing Logical Reasoning Data at Scale\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">2.3</span></a>. All evaluations were conducted under zero-shot conditions, with <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.T2.10.5\\\">SynLogic</span>-Val and AIME 2024 reported as avg@8 to reduce variance.</figcaption>\\n<br class=\\\"ltx_break\\\"/>\\n<table class=\\\"ltx_tabular ltx_centering ltx_align_middle\\\" id=\\\"S3.T2.11\\\">\\n<tr class=\\\"ltx_tr\\\" id=\\\"S3.T2.11.1\\\">\\n<td class=\\\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\\\" id=\\\"S3.T2.11.1.1\\\" rowspan=\\\"2\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.1.1.1\\\" style=\\\"font-size:90%;\\\">Model</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\\\" colspan=\\\"4\\\" id=\\\"S3.T2.11.1.2\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.1.2.1\\\" style=\\\"font-size:90%;\\\">Logic Benchmarks</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_tt\\\" colspan=\\\"3\\\" id=\\\"S3.T2.11.1.3\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.1.3.1\\\" style=\\\"font-size:90%;\\\">Mathematical Benchmarks</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S3.T2.11.2\\\">\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.2.1\\\" style=\\\"padding:0.9pt 2.8pt;\\\">\\n<span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.T2.11.2.1.1\\\" style=\\\"font-size:90%;\\\">SynLogic</span><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.2.1.2\\\" style=\\\"font-size:90%;\\\">-Val</span>\\n</td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.2.2\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.2.2.1\\\" style=\\\"font-size:90%;\\\">KOR-Bench</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.2.3\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.2.3.1\\\" style=\\\"font-size:90%;\\\">BBH</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_r\\\" id=\\\"S3.T2.11.2.4\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.2.4.1\\\" style=\\\"font-size:90%;\\\">BBEH</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.2.5\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.2.5.1\\\" style=\\\"font-size:90%;\\\">AIME 2024</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.2.6\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.2.6.1\\\" style=\\\"font-size:90%;\\\">MATH 500</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.2.7\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.2.7.1\\\" style=\\\"font-size:90%;\\\">AMC 2023</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S3.T2.11.3\\\">\\n<td class=\\\"ltx_td ltx_align_left ltx_border_r ltx_border_t\\\" id=\\\"S3.T2.11.3.1\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.3.1.1\\\" style=\\\"font-size:90%;\\\">Qwen2.5-7B-Base</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S3.T2.11.3.2\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.3.2.1\\\" style=\\\"font-size:90%;\\\">2.8</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S3.T2.11.3.3\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.3.3.1\\\" style=\\\"font-size:90%;\\\">11.6</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S3.T2.11.3.4\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.3.4.1\\\" style=\\\"font-size:90%;\\\">45.2</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_r ltx_border_t\\\" id=\\\"S3.T2.11.3.5\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.3.5.1\\\" style=\\\"font-size:90%;\\\">3.8</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S3.T2.11.3.6\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.3.6.1\\\" style=\\\"font-size:90%;\\\">0.3</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S3.T2.11.3.7\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.3.7.1\\\" style=\\\"font-size:90%;\\\">64.6</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S3.T2.11.3.8\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.3.8.1\\\" style=\\\"font-size:90%;\\\">30.0</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S3.T2.11.4\\\">\\n<td class=\\\"ltx_td ltx_align_left ltx_border_r\\\" id=\\\"S3.T2.11.4.1\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.4.1.1\\\" style=\\\"font-size:90%;\\\">Qwen2.5-7B-Instruct</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.4.2\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.4.2.1\\\" style=\\\"font-size:90%;\\\">9.0</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.4.3\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.4.3.1\\\" style=\\\"font-size:90%;\\\">38.6</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.4.4\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.4.4.1\\\" style=\\\"font-size:90%;\\\">62.7</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_r\\\" id=\\\"S3.T2.11.4.5\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.4.5.1\\\" style=\\\"font-size:90%;\\\">12.4</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.4.6\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.4.6.1\\\" style=\\\"font-size:90%;\\\">6.3</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.4.7\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.4.7.1\\\" style=\\\"font-size:90%;\\\">76.4</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.4.8\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.4.8.1\\\" style=\\\"font-size:90%;\\\">52.5</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S3.T2.11.5\\\" style=\\\"background-color:#F5F5F5;\\\">\\n<td class=\\\"ltx_td ltx_align_left ltx_border_r\\\" id=\\\"S3.T2.11.5.1\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.T2.11.5.1.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">SynLogic<span class=\\\"ltx_text ltx_font_upright\\\" id=\\\"S3.T2.11.5.1.1.1\\\">-7B</span></span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.5.2\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.5.2.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">44.4</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.5.3\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.5.3.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">48.1</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.5.4\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.5.4.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">66.5</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_r\\\" id=\\\"S3.T2.11.5.5\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.5.5.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">8.0</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.5.6\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.5.6.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">10.0</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.5.7\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.5.7.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">71.8</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.5.8\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.5.8.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">55.0</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S3.T2.11.6\\\">\\n<td class=\\\"ltx_td ltx_align_left ltx_border_r ltx_border_t\\\" id=\\\"S3.T2.11.6.1\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.6.1.1\\\" style=\\\"font-size:90%;\\\">Qwen2.5-32B-Base</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S3.T2.11.6.2\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.6.2.1\\\" style=\\\"font-size:90%;\\\">1.6</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S3.T2.11.6.3\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.6.3.1\\\" style=\\\"font-size:90%;\\\">10.9</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S3.T2.11.6.4\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.6.4.1\\\" style=\\\"font-size:90%;\\\">58.4</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_r ltx_border_t\\\" id=\\\"S3.T2.11.6.5\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.6.5.1\\\" style=\\\"font-size:90%;\\\">3.3</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S3.T2.11.6.6\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.6.6.1\\\" style=\\\"font-size:90%;\\\">4.5</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S3.T2.11.6.7\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.6.7.1\\\" style=\\\"font-size:90%;\\\">68.6</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_t\\\" id=\\\"S3.T2.11.6.8\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.6.8.1\\\" style=\\\"font-size:90%;\\\">45.0</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S3.T2.11.7\\\">\\n<td class=\\\"ltx_td ltx_align_left ltx_border_r\\\" id=\\\"S3.T2.11.7.1\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.7.1.1\\\" style=\\\"font-size:90%;\\\">Qwen2.5-32B-Instruct</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.7.2\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.7.2.1\\\" style=\\\"font-size:90%;\\\">12.0</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.7.3\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.7.3.1\\\" style=\\\"font-size:90%;\\\">54.7</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.7.4\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.7.4.1\\\" style=\\\"font-size:90%;\\\">84.5</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_r\\\" id=\\\"S3.T2.11.7.5\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.7.5.1\\\" style=\\\"font-size:90%;\\\">17.5</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.7.6\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.7.6.1\\\" style=\\\"font-size:90%;\\\">10.0</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.7.7\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.7.7.1\\\" style=\\\"font-size:90%;\\\">82.2</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.7.8\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.7.8.1\\\" style=\\\"font-size:90%;\\\">57.5</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S3.T2.11.8\\\">\\n<td class=\\\"ltx_td ltx_align_left ltx_border_r\\\" id=\\\"S3.T2.11.8.1\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.8.1.1\\\" style=\\\"font-size:90%;\\\">R1-Distill-Qwen-32B</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.8.2\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.8.2.1\\\" style=\\\"font-size:90%;\\\">33.0</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.8.3\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.8.3.1\\\" style=\\\"font-size:90%;\\\">66.6</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.8.4\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.8.4.1\\\" style=\\\"font-size:90%;\\\">88.3</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_r\\\" id=\\\"S3.T2.11.8.5\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.8.5.1\\\" style=\\\"font-size:90%;\\\">19.2</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.8.6\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.8.6.1\\\" style=\\\"font-size:90%;\\\">72.6</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.8.7\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.8.7.1\\\" style=\\\"font-size:90%;\\\">94.3</span></td>\\n<td class=\\\"ltx_td ltx_align_center\\\" id=\\\"S3.T2.11.8.8\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.8.8.1\\\" style=\\\"font-size:90%;\\\">85.0</span></td>\\n</tr>\\n<tr class=\\\"ltx_tr\\\" id=\\\"S3.T2.11.9\\\" style=\\\"background-color:#F5F5F5;\\\">\\n<td class=\\\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\\\" id=\\\"S3.T2.11.9.1\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.T2.11.9.1.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">SynLogic<span class=\\\"ltx_text ltx_font_upright\\\" id=\\\"S3.T2.11.9.1.1.1\\\">-32B</span></span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_bb\\\" id=\\\"S3.T2.11.9.2\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.9.2.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">52.9</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_bb\\\" id=\\\"S3.T2.11.9.3\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.9.3.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">62.2</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_bb\\\" id=\\\"S3.T2.11.9.4\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.9.4.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">85.8</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\\\" id=\\\"S3.T2.11.9.5\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text ltx_font_bold\\\" id=\\\"S3.T2.11.9.5.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">25.5</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_bb\\\" id=\\\"S3.T2.11.9.6\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.9.6.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">19.6</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_bb\\\" id=\\\"S3.T2.11.9.7\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.9.7.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">82.0</span></td>\\n<td class=\\\"ltx_td ltx_align_center ltx_border_bb\\\" id=\\\"S3.T2.11.9.8\\\" style=\\\"padding:0.9pt 2.8pt;\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.T2.11.9.8.1\\\" style=\\\"font-size:90%;background-color:#F5F5F5;\\\">57.5</span></td>\\n</tr>\\n</table>\\n</figure>\\n<div class=\\\"ltx_para\\\" id=\\\"S3.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S3.p1.1\\\">Reinforcement learning with verifiable rewards (RLVR) has emerged as a highly effective approach for enhancing reasoning capabilities in large language models\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Zeng et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib24\\\" title=\\\"\\\">2025b</a>; Yu et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib22\\\" title=\\\"\\\">2025</a>; Hu et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib6\\\" title=\\\"\\\">2025</a>; Zhang et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib25\\\" title=\\\"\\\">2025</a>)</cite>. Building on these advances, our experimental framework also focuses on applying reinforcement learning techniques to the <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.p1.1.1\\\">SynLogic</span> dataset, leveraging the verifiable nature of logical reasoning tasks. In this section, we validate the effectiveness of reinforcement learning training on the <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.p1.1.2\\\">SynLogic</span> dataset using Qwen2.5-7B-Base and Qwen2.5-32B-Base models.</p>\\n</div>\\n<section class=\\\"ltx_subsection\\\" id=\\\"S3.SS1\\\">\\n<h3 class=\\\"ltx_title ltx_title_subsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsection\\\">3.1 </span>Setup Details</h3>\\n<section class=\\\"ltx_subsubsection\\\" id=\\\"S3.SS1.SSS1\\\">\\n<h4 class=\\\"ltx_title ltx_title_subsubsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsubsection\\\">3.1.1 </span>Training Template</h4>\\n<div class=\\\"ltx_para\\\" id=\\\"S3.SS1.SSS1.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S3.SS1.SSS1.p1.1\\\">Following the DAPO training prompt\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Yu et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib22\\\" title=\\\"\\\">2025</a>)</cite>, we modify and design the training prompt template for logic training as shown in Figure\u00a0<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S3.F3\\\" title=\\\"Fig. 3 \u2023 3.1.1 Training Template \u2023 3.1 Setup Details \u2023 3 Reinforcement Learning on SynLogic\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">3</span></a>:</p>\\n</div>\\n<figure class=\\\"ltx_figure\\\" id=\\\"S3.F3\\\"><svg class=\\\"ltx_picture ltx_centering\\\" height=\\\"160.03\\\" id=\\\"S3.F3.pic1\\\" overflow=\\\"visible\\\" version=\\\"1.1\\\" width=\\\"600\\\"><g fill=\\\"#000000\\\" stroke=\\\"#000000\\\" stroke-width=\\\"0.4pt\\\" transform=\\\"translate(0,160.03) matrix(1 0 0 -1 0 0)\\\"><g fill=\\\"#004600\\\" fill-opacity=\\\"1.0\\\"><path d=\\\"M 0 17.13 L 0 142.89 C 0 152.36 7.67 160.03 17.13 160.03 L 582.87 160.03 C 592.33 160.03 600 152.36 600 142.89 L 600 17.13 C 600 7.67 592.33 0 582.87 0 L 17.13 0 C 7.67 0 0 7.67 0 17.13 Z\\\" style=\\\"stroke:none\\\"></path></g><g fill=\\\"#E6EFE6\\\" fill-opacity=\\\"1.0\\\"><path d=\\\"M 1.38 17.13 L 1.38 137.09 L 598.62 137.09 L 598.62 17.13 C 598.62 8.43 591.57 1.38 582.87 1.38 L 17.13 1.38 C 8.43 1.38 1.38 8.43 1.38 17.13 Z\\\" style=\\\"stroke:none\\\"></path></g><g fill-opacity=\\\"1.0\\\" transform=\\\"matrix(1.0 0.0 0.0 1.0 21.07 142.41)\\\"><foreignobject color=\\\"#FFFFFF\\\" height=\\\"12.3\\\" overflow=\\\"visible\\\" transform=\\\"matrix(1 0 0 -1 0 16.6)\\\" width=\\\"557.86\\\">\\n<span class=\\\"ltx_inline-block ltx_minipage ltx_align_bottom\\\" id=\\\"S3.F3.pic1.1.1.1.1.1\\\" style=\\\"width:403.2pt;\\\">\\n<span class=\\\"ltx_p\\\" id=\\\"S3.F3.pic1.1.1.1.1.1.1\\\"><span class=\\\"ltx_text ltx_font_bold ltx_font_smallcaps\\\" id=\\\"S3.F3.pic1.1.1.1.1.1.1.1\\\">SynLogic<span class=\\\"ltx_text ltx_font_upright\\\" id=\\\"S3.F3.pic1.1.1.1.1.1.1.1.1\\\"> Training Prompt Template</span></span></span>\\n</span></foreignobject></g><g fill-opacity=\\\"1.0\\\" transform=\\\"matrix(1.0 0.0 0.0 1.0 21.07 13.19)\\\"><foreignobject color=\\\"#000000\\\" height=\\\"112.08\\\" overflow=\\\"visible\\\" transform=\\\"matrix(1 0 0 -1 0 16.6)\\\" width=\\\"557.86\\\">\\n<span class=\\\"ltx_inline-block ltx_minipage ltx_align_bottom\\\" id=\\\"S3.F3.pic1.2.2.2.1.1\\\" style=\\\"width:403.2pt;\\\">\\n<span class=\\\"ltx_p\\\" id=\\\"S3.F3.pic1.2.2.2.1.1.1\\\"><span class=\\\"ltx_text\\\" id=\\\"S3.F3.pic1.2.2.2.1.1.1.1\\\" style=\\\"font-size:90%;\\\">Solve the following problem step by step. First, think about the reasoning process in the mind and then provide the answer. The reasoning process is enclosed within &lt;think&gt; &lt;/think&gt; and the final answer is enclosed within &lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;/think&gt; &lt;answer&gt; answer here&lt;/answer&gt;.\\\\n\\\\nSolve the following problem step by step. First, think about the reasoning process in the mind and then provide the answer. The reasoning process is enclosed within &lt;think&gt; &lt;/think&gt; and the final answer is enclosed within &lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;/think&gt; &lt;answer&gt; answer here&lt;/answer&gt;.</span></span>\\n</span></foreignobject></g></g></svg>\\n<figcaption class=\\\"ltx_caption ltx_centering\\\"><span class=\\\"ltx_tag ltx_tag_figure\\\">Figure 3: </span>The prompt template used for training models on <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.F3.2.1\\\">SynLogic</span> data.</figcaption>\\n</figure>\\n</section>\\n<section class=\\\"ltx_subsubsection\\\" id=\\\"S3.SS1.SSS2\\\">\\n<h4 class=\\\"ltx_title ltx_title_subsubsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsubsection\\\">3.1.2 </span>Reward Design</h4>\\n<div class=\\\"ltx_para\\\" id=\\\"S3.SS1.SSS2.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S3.SS1.SSS2.p1.1\\\">Our reward function employs a binary scoring mechanism that evaluates both format adherence and answer correctness. Specifically, we assign a reward of 1 only when a model-generated response satisfies two criteria: (1) it correctly follows the designated format by including both <span class=\\\"ltx_text ltx_font_typewriter\\\" id=\\\"S3.SS1.SSS2.p1.1.1\\\">&lt;think&gt;</span> <span class=\\\"ltx_text ltx_font_typewriter\\\" id=\\\"S3.SS1.SSS2.p1.1.2\\\">&lt;/think&gt;</span> and <span class=\\\"ltx_text ltx_font_typewriter\\\" id=\\\"S3.SS1.SSS2.p1.1.3\\\">&lt;answer&gt;</span> <span class=\\\"ltx_text ltx_font_typewriter\\\" id=\\\"S3.SS1.SSS2.p1.1.4\\\">&lt;/answer&gt;</span> tags, and (2) the final answer provided is correct. Responses that either deviate from the required format or contain incorrect answers receive a reward of 0.</p>\\n</div>\\n<div class=\\\"ltx_para\\\" id=\\\"S3.SS1.SSS2.p2\\\">\\n<table class=\\\"ltx_equation ltx_eqn_table\\\" id=\\\"S3.E1\\\">\\n<tbody><tr class=\\\"ltx_equation ltx_eqn_row ltx_align_baseline\\\">\\n<td class=\\\"ltx_eqn_cell ltx_eqn_center_padleft\\\"></td>\\n<td class=\\\"ltx_eqn_cell ltx_align_center\\\"><math alttext=\\\"R(o_{i})=\\\\begin{cases}1,&amp;\\\\text{if }\\\\text{format}(o_{i})=\\\\text{True }\\\\wedge%\\n\\\\text{ correct}(o_{i})=\\\\text{True}\\\\\\\\\\n0,&amp;\\\\text{otherwise}\\\\end{cases}\\\" class=\\\"ltx_Math\\\" display=\\\"block\\\" id=\\\"S3.E1.m1.5\\\"><semantics id=\\\"S3.E1.m1.5a\\\"><mrow id=\\\"S3.E1.m1.5.5\\\" xref=\\\"S3.E1.m1.5.5.cmml\\\"><mrow id=\\\"S3.E1.m1.5.5.1\\\" xref=\\\"S3.E1.m1.5.5.1.cmml\\\"><mi id=\\\"S3.E1.m1.5.5.1.3\\\" xref=\\\"S3.E1.m1.5.5.1.3.cmml\\\">R</mi><mo id=\\\"S3.E1.m1.5.5.1.2\\\" xref=\\\"S3.E1.m1.5.5.1.2.cmml\\\">\u2062</mo><mrow id=\\\"S3.E1.m1.5.5.1.1.1\\\" xref=\\\"S3.E1.m1.5.5.1.1.1.1.cmml\\\"><mo id=\\\"S3.E1.m1.5.5.1.1.1.2\\\" stretchy=\\\"false\\\" xref=\\\"S3.E1.m1.5.5.1.1.1.1.cmml\\\">(</mo><msub id=\\\"S3.E1.m1.5.5.1.1.1.1\\\" xref=\\\"S3.E1.m1.5.5.1.1.1.1.cmml\\\"><mi id=\\\"S3.E1.m1.5.5.1.1.1.1.2\\\" xref=\\\"S3.E1.m1.5.5.1.1.1.1.2.cmml\\\">o</mi><mi id=\\\"S3.E1.m1.5.5.1.1.1.1.3\\\" xref=\\\"S3.E1.m1.5.5.1.1.1.1.3.cmml\\\">i</mi></msub><mo id=\\\"S3.E1.m1.5.5.1.1.1.3\\\" stretchy=\\\"false\\\" xref=\\\"S3.E1.m1.5.5.1.1.1.1.cmml\\\">)</mo></mrow></mrow><mo id=\\\"S3.E1.m1.5.5.2\\\" xref=\\\"S3.E1.m1.5.5.2.cmml\\\">=</mo><mrow id=\\\"S3.E1.m1.4.4\\\" xref=\\\"S3.E1.m1.5.5.3.1.cmml\\\"><mo id=\\\"S3.E1.m1.4.4.5\\\" xref=\\\"S3.E1.m1.5.5.3.1.1.cmml\\\">{</mo><mtable columnspacing=\\\"5pt\\\" displaystyle=\\\"true\\\" id=\\\"S3.E1.m1.4.4.4\\\" rowspacing=\\\"0pt\\\" xref=\\\"S3.E1.m1.5.5.3.1.cmml\\\"><mtr id=\\\"S3.E1.m1.4.4.4a\\\" xref=\\\"S3.E1.m1.5.5.3.1.cmml\\\"><mtd class=\\\"ltx_align_left\\\" columnalign=\\\"left\\\" id=\\\"S3.E1.m1.4.4.4b\\\" xref=\\\"S3.E1.m1.5.5.3.1.cmml\\\"><mrow id=\\\"S3.E1.m1.1.1.1.1.1.1.3\\\" xref=\\\"S3.E1.m1.5.5.3.1.cmml\\\"><mn id=\\\"S3.E1.m1.1.1.1.1.1.1.1\\\" xref=\\\"S3.E1.m1.1.1.1.1.1.1.1.cmml\\\">1</mn><mo id=\\\"S3.E1.m1.1.1.1.1.1.1.3.1\\\" xref=\\\"S3.E1.m1.5.5.3.1.cmml\\\">,</mo></mrow></mtd><mtd class=\\\"ltx_align_left\\\" columnalign=\\\"left\\\" id=\\\"S3.E1.m1.4.4.4c\\\" xref=\\\"S3.E1.m1.5.5.3.1.cmml\\\"><mrow id=\\\"S3.E1.m1.2.2.2.2.2.1\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.cmml\\\"><mrow id=\\\"S3.E1.m1.2.2.2.2.2.1.1\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.cmml\\\"><mrow id=\\\"S3.E1.m1.2.2.2.2.2.1.1.3\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.3c.cmml\\\"><mtext id=\\\"S3.E1.m1.2.2.2.2.2.1.1.3a\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.3c.cmml\\\">if\u00a0</mtext><mtext id=\\\"S3.E1.m1.2.2.2.2.2.1.1.3b\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.3c.cmml\\\">format</mtext></mrow><mo id=\\\"S3.E1.m1.2.2.2.2.2.1.1.2\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.2.cmml\\\">\u2062</mo><mrow id=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.cmml\\\"><mo id=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.2\\\" stretchy=\\\"false\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.cmml\\\">(</mo><msub id=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.cmml\\\"><mi id=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.2\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.2.cmml\\\">o</mi><mi id=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.3\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.3.cmml\\\">i</mi></msub><mo id=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.3\\\" stretchy=\\\"false\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.cmml\\\">)</mo></mrow></mrow><mo id=\\\"S3.E1.m1.2.2.2.2.2.1.4\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.4.cmml\\\">=</mo><mrow id=\\\"S3.E1.m1.2.2.2.2.2.1.2\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.cmml\\\"><mtext id=\\\"S3.E1.m1.2.2.2.2.2.1.2.3\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.3a.cmml\\\">True\u00a0</mtext><mo id=\\\"S3.E1.m1.2.2.2.2.2.1.2.2\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.2.cmml\\\">\u2227</mo><mrow id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.cmml\\\"><mtext id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.3\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.3a.cmml\\\">\u00a0correct</mtext><mo id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.2\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.2.cmml\\\">\u2062</mo><mrow id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.cmml\\\"><mo id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.2\\\" stretchy=\\\"false\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.cmml\\\">(</mo><msub id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.cmml\\\"><mi id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.2\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.2.cmml\\\">o</mi><mi id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.3\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.3.cmml\\\">i</mi></msub><mo id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.3\\\" stretchy=\\\"false\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.cmml\\\">)</mo></mrow></mrow></mrow><mo id=\\\"S3.E1.m1.2.2.2.2.2.1.5\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.5.cmml\\\">=</mo><mtext id=\\\"S3.E1.m1.2.2.2.2.2.1.6\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.6a.cmml\\\">True</mtext></mrow></mtd></mtr><mtr id=\\\"S3.E1.m1.4.4.4d\\\" xref=\\\"S3.E1.m1.5.5.3.1.cmml\\\"><mtd class=\\\"ltx_align_left\\\" columnalign=\\\"left\\\" id=\\\"S3.E1.m1.4.4.4e\\\" xref=\\\"S3.E1.m1.5.5.3.1.cmml\\\"><mrow id=\\\"S3.E1.m1.3.3.3.3.1.1.3\\\" xref=\\\"S3.E1.m1.5.5.3.1.cmml\\\"><mn id=\\\"S3.E1.m1.3.3.3.3.1.1.1\\\" xref=\\\"S3.E1.m1.3.3.3.3.1.1.1.cmml\\\">0</mn><mo id=\\\"S3.E1.m1.3.3.3.3.1.1.3.1\\\" xref=\\\"S3.E1.m1.5.5.3.1.cmml\\\">,</mo></mrow></mtd><mtd class=\\\"ltx_align_left\\\" columnalign=\\\"left\\\" id=\\\"S3.E1.m1.4.4.4f\\\" xref=\\\"S3.E1.m1.5.5.3.1.cmml\\\"><mtext id=\\\"S3.E1.m1.4.4.4.4.2.1\\\" xref=\\\"S3.E1.m1.4.4.4.4.2.1a.cmml\\\">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S3.E1.m1.5b\\\"><apply id=\\\"S3.E1.m1.5.5.cmml\\\" xref=\\\"S3.E1.m1.5.5\\\"><eq id=\\\"S3.E1.m1.5.5.2.cmml\\\" xref=\\\"S3.E1.m1.5.5.2\\\"></eq><apply id=\\\"S3.E1.m1.5.5.1.cmml\\\" xref=\\\"S3.E1.m1.5.5.1\\\"><times id=\\\"S3.E1.m1.5.5.1.2.cmml\\\" xref=\\\"S3.E1.m1.5.5.1.2\\\"></times><ci id=\\\"S3.E1.m1.5.5.1.3.cmml\\\" xref=\\\"S3.E1.m1.5.5.1.3\\\">\ud835\udc45</ci><apply id=\\\"S3.E1.m1.5.5.1.1.1.1.cmml\\\" xref=\\\"S3.E1.m1.5.5.1.1.1\\\"><csymbol cd=\\\"ambiguous\\\" id=\\\"S3.E1.m1.5.5.1.1.1.1.1.cmml\\\" xref=\\\"S3.E1.m1.5.5.1.1.1\\\">subscript</csymbol><ci id=\\\"S3.E1.m1.5.5.1.1.1.1.2.cmml\\\" xref=\\\"S3.E1.m1.5.5.1.1.1.1.2\\\">\ud835\udc5c</ci><ci id=\\\"S3.E1.m1.5.5.1.1.1.1.3.cmml\\\" xref=\\\"S3.E1.m1.5.5.1.1.1.1.3\\\">\ud835\udc56</ci></apply></apply><apply id=\\\"S3.E1.m1.5.5.3.1.cmml\\\" xref=\\\"S3.E1.m1.4.4\\\"><csymbol cd=\\\"latexml\\\" id=\\\"S3.E1.m1.5.5.3.1.1.cmml\\\" xref=\\\"S3.E1.m1.4.4.5\\\">cases</csymbol><cn id=\\\"S3.E1.m1.1.1.1.1.1.1.1.cmml\\\" type=\\\"integer\\\" xref=\\\"S3.E1.m1.1.1.1.1.1.1.1\\\">1</cn><apply id=\\\"S3.E1.m1.2.2.2.2.2.1.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1\\\"><and id=\\\"S3.E1.m1.2.2.2.2.2.1a.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1\\\"></and><apply id=\\\"S3.E1.m1.2.2.2.2.2.1b.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1\\\"><eq id=\\\"S3.E1.m1.2.2.2.2.2.1.4.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.4\\\"></eq><apply id=\\\"S3.E1.m1.2.2.2.2.2.1.1.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1\\\"><times id=\\\"S3.E1.m1.2.2.2.2.2.1.1.2.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.2\\\"></times><ci id=\\\"S3.E1.m1.2.2.2.2.2.1.1.3c.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.3\\\"><mrow id=\\\"S3.E1.m1.2.2.2.2.2.1.1.3.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.3\\\"><mtext id=\\\"S3.E1.m1.2.2.2.2.2.1.1.3a.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.3\\\">if\u00a0</mtext><mtext id=\\\"S3.E1.m1.2.2.2.2.2.1.1.3b.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.3\\\">format</mtext></mrow></ci><apply id=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1\\\"><csymbol cd=\\\"ambiguous\\\" id=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1\\\">subscript</csymbol><ci id=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.2.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.2\\\">\ud835\udc5c</ci><ci id=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.3.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.1.1.1.1.3\\\">\ud835\udc56</ci></apply></apply><apply id=\\\"S3.E1.m1.2.2.2.2.2.1.2.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2\\\"><and id=\\\"S3.E1.m1.2.2.2.2.2.1.2.2.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.2\\\"></and><ci id=\\\"S3.E1.m1.2.2.2.2.2.1.2.3a.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.3\\\"><mtext id=\\\"S3.E1.m1.2.2.2.2.2.1.2.3.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.3\\\">True\u00a0</mtext></ci><apply id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1\\\"><times id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.2.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.2\\\"></times><ci id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.3a.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.3\\\"><mtext id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.3.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.3\\\">\u00a0correct</mtext></ci><apply id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1\\\"><csymbol cd=\\\"ambiguous\\\" id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.1.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1\\\">subscript</csymbol><ci id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.2.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.2\\\">\ud835\udc5c</ci><ci id=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.3.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.2.1.1.1.1.3\\\">\ud835\udc56</ci></apply></apply></apply></apply><apply id=\\\"S3.E1.m1.2.2.2.2.2.1c.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1\\\"><eq id=\\\"S3.E1.m1.2.2.2.2.2.1.5.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.5\\\"></eq><share href=\\\"https://arxiv.org/html/2505.19641v3#S3.E1.m1.2.2.2.2.2.1.2.cmml\\\" id=\\\"S3.E1.m1.2.2.2.2.2.1d.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1\\\"></share><ci id=\\\"S3.E1.m1.2.2.2.2.2.1.6a.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.6\\\"><mtext id=\\\"S3.E1.m1.2.2.2.2.2.1.6.cmml\\\" xref=\\\"S3.E1.m1.2.2.2.2.2.1.6\\\">True</mtext></ci></apply></apply><cn id=\\\"S3.E1.m1.3.3.3.3.1.1.1.cmml\\\" type=\\\"integer\\\" xref=\\\"S3.E1.m1.3.3.3.3.1.1.1\\\">0</cn><ci id=\\\"S3.E1.m1.4.4.4.4.2.1a.cmml\\\" xref=\\\"S3.E1.m1.4.4.4.4.2.1\\\"><mtext id=\\\"S3.E1.m1.4.4.4.4.2.1.cmml\\\" xref=\\\"S3.E1.m1.4.4.4.4.2.1\\\">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S3.E1.m1.5c\\\">R(o_{i})=\\\\begin{cases}1,&amp;\\\\text{if }\\\\text{format}(o_{i})=\\\\text{True }\\\\wedge%\\n\\\\text{ correct}(o_{i})=\\\\text{True}\\\\\\\\\\n0,&amp;\\\\text{otherwise}\\\\end{cases}</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S3.E1.m1.5d\\\">italic_R ( italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = { start_ROW start_CELL 1 , end_CELL start_CELL if roman_format ( italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = True \u2227 correct ( italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = True end_CELL end_ROW start_ROW start_CELL 0 , end_CELL start_CELL otherwise end_CELL end_ROW</annotation></semantics></math></td>\\n<td class=\\\"ltx_eqn_cell ltx_eqn_center_padright\\\"></td>\\n<td class=\\\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\\\" rowspan=\\\"1\\\"><span class=\\\"ltx_tag ltx_tag_equation ltx_align_right\\\">(1)</span></td>\\n</tr></tbody>\\n</table>\\n</div>\\n<div class=\\\"ltx_para\\\" id=\\\"S3.SS1.SSS2.p3\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S3.SS1.SSS2.p3.3\\\">where <math alttext=\\\"\\\\text{format}(o_{i})\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S3.SS1.SSS2.p3.1.m1.1\\\"><semantics id=\\\"S3.SS1.SSS2.p3.1.m1.1a\\\"><mrow id=\\\"S3.SS1.SSS2.p3.1.m1.1.1\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.cmml\\\"><mtext id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.3\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.3a.cmml\\\">format</mtext><mo id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.2\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.2.cmml\\\">\u2062</mo><mrow id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.cmml\\\"><mo id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.2\\\" stretchy=\\\"false\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.cmml\\\">(</mo><msub id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.cmml\\\"><mi id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.2\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.2.cmml\\\">o</mi><mi id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.3\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.3.cmml\\\">i</mi></msub><mo id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.3\\\" stretchy=\\\"false\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.cmml\\\">)</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S3.SS1.SSS2.p3.1.m1.1b\\\"><apply id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1\\\"><times id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.2.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.2\\\"></times><ci id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.3a.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.3\\\"><mtext id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.3.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.3\\\">format</mtext></ci><apply id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1\\\"><csymbol cd=\\\"ambiguous\\\" id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.1.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1\\\">subscript</csymbol><ci id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.2.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.2\\\">\ud835\udc5c</ci><ci id=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.3.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.1.m1.1.1.1.1.1.3\\\">\ud835\udc56</ci></apply></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S3.SS1.SSS2.p3.1.m1.1c\\\">\\\\text{format}(o_{i})</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S3.SS1.SSS2.p3.1.m1.1d\\\">format ( italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> evaluates whether response <math alttext=\\\"o_{i}\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S3.SS1.SSS2.p3.2.m2.1\\\"><semantics id=\\\"S3.SS1.SSS2.p3.2.m2.1a\\\"><msub id=\\\"S3.SS1.SSS2.p3.2.m2.1.1\\\" xref=\\\"S3.SS1.SSS2.p3.2.m2.1.1.cmml\\\"><mi id=\\\"S3.SS1.SSS2.p3.2.m2.1.1.2\\\" xref=\\\"S3.SS1.SSS2.p3.2.m2.1.1.2.cmml\\\">o</mi><mi id=\\\"S3.SS1.SSS2.p3.2.m2.1.1.3\\\" xref=\\\"S3.SS1.SSS2.p3.2.m2.1.1.3.cmml\\\">i</mi></msub><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S3.SS1.SSS2.p3.2.m2.1b\\\"><apply id=\\\"S3.SS1.SSS2.p3.2.m2.1.1.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.2.m2.1.1\\\"><csymbol cd=\\\"ambiguous\\\" id=\\\"S3.SS1.SSS2.p3.2.m2.1.1.1.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.2.m2.1.1\\\">subscript</csymbol><ci id=\\\"S3.SS1.SSS2.p3.2.m2.1.1.2.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.2.m2.1.1.2\\\">\ud835\udc5c</ci><ci id=\\\"S3.SS1.SSS2.p3.2.m2.1.1.3.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.2.m2.1.1.3\\\">\ud835\udc56</ci></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S3.SS1.SSS2.p3.2.m2.1c\\\">o_{i}</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S3.SS1.SSS2.p3.2.m2.1d\\\">italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> includes both the required <span class=\\\"ltx_text ltx_font_typewriter\\\" id=\\\"S3.SS1.SSS2.p3.3.1\\\">&lt;think&gt;</span> <span class=\\\"ltx_text ltx_font_typewriter\\\" id=\\\"S3.SS1.SSS2.p3.3.2\\\">&lt;/think&gt;</span> and <span class=\\\"ltx_text ltx_font_typewriter\\\" id=\\\"S3.SS1.SSS2.p3.3.3\\\">&lt;answer&gt;</span> <span class=\\\"ltx_text ltx_font_typewriter\\\" id=\\\"S3.SS1.SSS2.p3.3.4\\\">&lt;/answer&gt;</span> tags, and <math alttext=\\\"\\\\text{correct}(o_{i})\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S3.SS1.SSS2.p3.3.m3.1\\\"><semantics id=\\\"S3.SS1.SSS2.p3.3.m3.1a\\\"><mrow id=\\\"S3.SS1.SSS2.p3.3.m3.1.1\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.cmml\\\"><mtext id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.3\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.3a.cmml\\\">correct</mtext><mo id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.2\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.2.cmml\\\">\u2062</mo><mrow id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.cmml\\\"><mo id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.2\\\" stretchy=\\\"false\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.cmml\\\">(</mo><msub id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.cmml\\\"><mi id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.2\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.2.cmml\\\">o</mi><mi id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.3\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.3.cmml\\\">i</mi></msub><mo id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.3\\\" stretchy=\\\"false\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.cmml\\\">)</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S3.SS1.SSS2.p3.3.m3.1b\\\"><apply id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1\\\"><times id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.2.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.2\\\"></times><ci id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.3a.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.3\\\"><mtext id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.3.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.3\\\">correct</mtext></ci><apply id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1\\\"><csymbol cd=\\\"ambiguous\\\" id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.1.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1\\\">subscript</csymbol><ci id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.2.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.2\\\">\ud835\udc5c</ci><ci id=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.3.cmml\\\" xref=\\\"S3.SS1.SSS2.p3.3.m3.1.1.1.1.1.3\\\">\ud835\udc56</ci></apply></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S3.SS1.SSS2.p3.3.m3.1c\\\">\\\\text{correct}(o_{i})</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S3.SS1.SSS2.p3.3.m3.1d\\\">correct ( italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> determines whether the answer provided is correct verified by its task\u2019s verification rule.</p>\\n</div>\\n</section>\\n<section class=\\\"ltx_subsubsection\\\" id=\\\"S3.SS1.SSS3\\\">\\n<h4 class=\\\"ltx_title ltx_title_subsubsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsubsection\\\">3.1.3 </span>Training Details</h4>\\n<div class=\\\"ltx_para\\\" id=\\\"S3.SS1.SSS3.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S3.SS1.SSS3.p1.1\\\">For our experiments, we synthesized approximately 16k <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.SS1.SSS3.p1.1.1\\\">SynLogic</span>-Easy and 33k <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.SS1.SSS3.p1.1.2\\\">SynLogic</span>-Hard instances to train the Qwen2.5-7B-Base and Qwen2.5-32B-Base models with DAPO, respectively, as described in \u00a7<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S2.SS3\\\" title=\\\"2.3 The SynLogic Datasets \u2023 2 SynLogic: Synthesizing Logical Reasoning Data at Scale\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">2.3</span></a>. During training, we employed a prompt batch size of 128, generated 16 rollouts per prompt, and set maximum rollout lengths of 16,384 tokens for the 7B model and 28,672 tokens for the 32B model. We configured the clip high parameter <math alttext=\\\"\\\\epsilon_{\\\\text{high}}\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S3.SS1.SSS3.p1.1.m1.1\\\"><semantics id=\\\"S3.SS1.SSS3.p1.1.m1.1a\\\"><msub id=\\\"S3.SS1.SSS3.p1.1.m1.1.1\\\" xref=\\\"S3.SS1.SSS3.p1.1.m1.1.1.cmml\\\"><mi id=\\\"S3.SS1.SSS3.p1.1.m1.1.1.2\\\" xref=\\\"S3.SS1.SSS3.p1.1.m1.1.1.2.cmml\\\">\u03f5</mi><mtext id=\\\"S3.SS1.SSS3.p1.1.m1.1.1.3\\\" xref=\\\"S3.SS1.SSS3.p1.1.m1.1.1.3a.cmml\\\">high</mtext></msub><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S3.SS1.SSS3.p1.1.m1.1b\\\"><apply id=\\\"S3.SS1.SSS3.p1.1.m1.1.1.cmml\\\" xref=\\\"S3.SS1.SSS3.p1.1.m1.1.1\\\"><csymbol cd=\\\"ambiguous\\\" id=\\\"S3.SS1.SSS3.p1.1.m1.1.1.1.cmml\\\" xref=\\\"S3.SS1.SSS3.p1.1.m1.1.1\\\">subscript</csymbol><ci id=\\\"S3.SS1.SSS3.p1.1.m1.1.1.2.cmml\\\" xref=\\\"S3.SS1.SSS3.p1.1.m1.1.1.2\\\">italic-\u03f5</ci><ci id=\\\"S3.SS1.SSS3.p1.1.m1.1.1.3a.cmml\\\" xref=\\\"S3.SS1.SSS3.p1.1.m1.1.1.3\\\"><mtext id=\\\"S3.SS1.SSS3.p1.1.m1.1.1.3.cmml\\\" mathsize=\\\"70%\\\" xref=\\\"S3.SS1.SSS3.p1.1.m1.1.1.3\\\">high</mtext></ci></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S3.SS1.SSS3.p1.1.m1.1c\\\">\\\\epsilon_{\\\\text{high}}</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S3.SS1.SSS3.p1.1.m1.1d\\\">italic_\u03f5 start_POSTSUBSCRIPT high end_POSTSUBSCRIPT</annotation></semantics></math> at 0.28. Additional training hyperparameters and implementation details are provided in Appendix\u00a0<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#A2.SS1.SSS2\\\" title=\\\"B.1.2 Training hyper-parameters \u2023 B.1 Training \u2023 Appendix B Training and Evaluation Details\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">B.1.2</span></a>.</p>\\n</div>\\n</section>\\n<section class=\\\"ltx_subsubsection\\\" id=\\\"S3.SS1.SSS4\\\">\\n<h4 class=\\\"ltx_title ltx_title_subsubsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsubsection\\\">3.1.4 </span>Evaluation Details</h4>\\n<div class=\\\"ltx_para\\\" id=\\\"S3.SS1.SSS4.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S3.SS1.SSS4.p1.1\\\">Our evaluation strategy encompasses two distinct benchmark categories. For assessing logical reasoning capabilities, we employ the validation splits of <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.SS1.SSS4.p1.1.1\\\">SynLogic</span> alongside established benchmarks including Knowledge-Orthogonal Reasoning (KOR-Bench)\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Ma et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib13\\\" title=\\\"\\\">2024</a>)</cite>, BBH\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Suzgun et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib18\\\" title=\\\"\\\">2022</a>)</cite>, and the substantially more challenging BBEH\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Kazemi et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib9\\\" title=\\\"\\\">2025</a>)</cite>. To investigate cross-domain generalization effects, we incorporate mathematics evaluations on MATH 500\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Hendrycks et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#bib.bib5\\\" title=\\\"\\\">2021</a>)</cite>, AMC 2023, and AIME 2024. All evaluations are conducted in a zero-shot setting, with avg@8 metrics computed for AIME 2024 and <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.SS1.SSS4.p1.1.2\\\">SynLogic</span>-Val to mitigate variance.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\\\"ltx_subsection\\\" id=\\\"S3.SS2\\\">\\n<h3 class=\\\"ltx_title ltx_title_subsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsection\\\">3.2 </span>Results</h3>\\n<figure class=\\\"ltx_figure\\\" id=\\\"S3.F4\\\">\\n<div class=\\\"ltx_flex_figure\\\">\\n<div class=\\\"ltx_flex_cell ltx_flex_size_2\\\">\\n<figure class=\\\"ltx_figure ltx_figure_panel ltx_align_center\\\" id=\\\"S3.F4.sf1\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_img_landscape\\\" height=\\\"177\\\" id=\\\"S3.F4.sf1.g1\\\" src=\\\"x5.png\\\" width=\\\"239\\\"/>\\n<figcaption class=\\\"ltx_caption\\\"><span class=\\\"ltx_tag ltx_tag_figure\\\">(a) </span>Avg Length and Reflection of 7B Training.</figcaption>\\n</figure>\\n</div>\\n<div class=\\\"ltx_flex_cell ltx_flex_size_2\\\">\\n<figure class=\\\"ltx_figure ltx_figure_panel ltx_align_center\\\" id=\\\"S3.F4.sf2\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_img_landscape\\\" height=\\\"177\\\" id=\\\"S3.F4.sf2.g1\\\" src=\\\"x6.png\\\" width=\\\"239\\\"/>\\n<figcaption class=\\\"ltx_caption\\\"><span class=\\\"ltx_tag ltx_tag_figure\\\">(b) </span>Avg Length and Reflection of 32B Training.</figcaption>\\n</figure>\\n</div>\\n</div>\\n<figcaption class=\\\"ltx_caption ltx_centering\\\"><span class=\\\"ltx_tag ltx_tag_figure\\\">Figure 4: </span>Response length and reflection ratio across the 7B and 32B training process on the training dataset. The reflection ratio represents the proportion of generated responses containing at least one reflection phrase (including \u201crecheck\u201d, \u201crethink\u201d, \u201ctry again\u201d, \u201clet\u2019s correct it\u201d, \u201cre-evaluate\u201d, \u201ccheck again\u201d, \u201cthink again\u201d). </figcaption>\\n</figure>\\n<section class=\\\"ltx_subsubsection\\\" id=\\\"S3.SS2.SSS1\\\">\\n<h4 class=\\\"ltx_title ltx_title_subsubsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsubsection\\\">3.2.1 </span>Substantial Improvements in Logical Reasoning</h4>\\n<div class=\\\"ltx_para\\\" id=\\\"S3.SS2.SSS1.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S3.SS2.SSS1.p1.1\\\">The evaluation results presented in Table\u00a0<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2505.19641v3#S3.T2\\\" title=\\\"Tab. 2 \u2023 3 Reinforcement Learning on SynLogic\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">2</span></a> demonstrate significant improvements across logical reasoning tasks.\\nBeyond the notable gains on <span class=\\\"ltx_text ltx_font_smallcaps\\\" id=\\\"S3.SS2.SSS1.p1.1.1\\\">SynLogic</span>\u2019s validation split, our models demonstrate enhanced performance across multiple logical benchmarks, leading state-of-the-art results among open-source datasets. Our 7B model achieves 48.1% on KOR-Bench\u00a0<cite class=\\\"ltx_cite l ...\n\n(The output of the tool call (shortuuid identifier: 5mBXU5E9MwNeciw9Gfwubf) is too long! Only the first 100000 characters are shown here. The original output length is 196036 characters. The full output has been saved to the file /workspace/dumps/workspace/.overlong_tool_outputs/5mBXU5E9MwNeciw9Gfwubf.json. Please check this file carefully, as it may be very long!)", "tool_call_id": "toolu_017v4YxpQUXByqcTYwUPt1mb"}, {"role": "assistant", "content": "Perfect! I found the exact table I need. Now let me check the K&K paper status again:", "tool_calls": [{"id": "toolu_012X7LcaK9bWkB5aA5m4oQR7", "type": "function", "function": {"name": "arxiv_local-download_paper", "arguments": "{\"paper_id\": \"2410.23123\", \"check_status\": true}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"{\\\"status\\\": \\\"converting\\\", \\\"started_at\\\": \\\"2025-09-21T08:01:45.550828\\\", \\\"completed_at\\\": null, \\\"error\\\": null, \\\"message\\\": \\\"Paper conversion converting\\\"}\",\"annotations\":null}", "tool_call_id": "toolu_012X7LcaK9bWkB5aA5m4oQR7"}, {"role": "assistant", "content": "Based on all the information I've gathered, I now have enough details to create the table. From the SynLogic paper, I found a perfect table that shows the exact information needed. Let me create the LaTeX table:", "tool_calls": [{"id": "toolu_01NgoQhfwtCMiMknWN32fdTL", "type": "function", "function": {"name": "filesystem-write_file", "arguments": "{\"path\": \"/workspace/dumps/workspace/datasets.tex\", \"content\": \"\\\\begin{table}[!ht]\\n    \\\\begin{center}\\n    \\\\begin{tabular}{lccc}\\n        \\\\toprule\\n        Dataset & Tasks & Trainable & Adjustable Difficulty\\\\\\\\\\n        \\\\midrule\\n        BBH & 23 & \\\\ding{55} & \\\\ding{55}\\\\\\\\\\n        Zebra Logic & 1 & \\\\ding{55} & \\\\ding{51}\\\\\\\\\\n        KOR-Bench & 5 & \\\\ding{55} & \\\\ding{55}\\\\\\\\\\n        K\\\\&K & 1 & \\\\ding{51} & \\\\ding{51}\\\\\\\\\\n        BBEH & 23 & \\\\ding{55} & \\\\ding{55}\\\\\\\\\\n        \\\\bottomrule\\n    \\\\end{tabular}\\n  \\\\end{center}\\n\\\\end{table}\"}"}}]}, {"role": "tool", "content": "{\"type\":\"text\",\"text\":\"Successfully wrote to /workspace/dumps/workspace/datasets.tex\",\"annotations\":null}", "tool_call_id": "toolu_01NgoQhfwtCMiMknWN32fdTL"}, {"role": "assistant", "content": "Perfect! I have successfully created the LaTeX table file `datasets.tex` in the workspace. The table contains the information for all five datasets as requested:\n\n1. **BBH (Big-Bench Hard)**: 23 tasks, not trainable (no training set), no adjustable difficulty\n2. **Zebra Logic**: 1 task, not trainable, has adjustable difficulty  \n3. **KOR-Bench**: 5 tasks (based on broader categorization as specified), not trainable, no adjustable difficulty\n4. **K&K (Knights and Knaves)**: 1 task, trainable (includes training set), has adjustable difficulty\n5. **BBEH (Big-Bench Extra Hard)**: 23 tasks, not trainable, no adjustable difficulty\n\nThe table uses:\n- `\\ding{51}` (\u2713) for \"yes\" \n- `\\ding{55}` (\u2717) for \"no\"\n- Standard LaTeX table formatting with `\\toprule`, `\\midrule`, and `\\bottomrule`\n- The exact format requested with four columns: Dataset, Tasks, Trainable, and Adjustable Difficulty\n\nThe file contains only the table content without any comments or other content, as requested.", "tool_calls": [{"id": "toolu_01U3wzCLvfEJxajLENKovzFK", "type": "function", "function": {"name": "local-claim_done", "arguments": ""}}]}, {"role": "tool", "content": "you have claimed the task is done!", "tool_call_id": "toolu_01U3wzCLvfEJxajLENKovzFK"}], "key_stats": {"interaction_turns": 1, "tool_calls": 17, "agent_llm_requests": 18, "total_tokens": 1115287, "input_tokens": 1113407, "output_tokens": 1880, "total_turns": 18, "total_messages": 46, "truncations": 0, "user_input_turns": 1, "assistant_turns": 17}, "agent_cost": {"total_cost": 3.3684, "total_input_tokens": 1113407, "total_output_tokens": 1880, "total_requests": 18}, "user_cost": {"total_cost": 0.0, "total_input_tokens": 0, "total_output_tokens": 0, "total_requests": 0}, "resumed": false, "session_id": "task_finalpool-logical-datasets-collection_session", "history_file": "/workspace/dumps/conversation_history/task_finalpool-logical-datasets-collection_session_history.jsonl", "pass": true}