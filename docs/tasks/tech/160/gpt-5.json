{
    "pass": true,
    "messages": [
        {
            "role": "user",
            "content": "Hello! I am a PhD student engaged in deep learning research. Recently, my collaborators and I have proposed a logical reasoning dataset and are writing a paper to introduce this new dataset to the community. To demonstrate the differences between our work and previous work, I need you to help me summarize a latex table and save it in the workspace with name as `datasets.tex`, the tex file should only contain table content (no commented lines), without any other content. The table needs to include four columns: Dataset (dataset name), Tasks (number of tasks included in the dataset), Trainable (whether it includes a training set, filled with \\ding{55} or \\ding{51}), and Adjustable Difficulty (whether it includes different difficulty levels, filled with \\ding{55} or \\ding{51}). The table format is as follows:\n\n```tex\n\\begin{table}[!ht]\n    \\begin{center}\n    \\begin{tabular}{lccc}\n        \\toprule\n        Dataset & Tasks & Trainable & Adjustable Difficulty\\\\\n        \\midrule\n        % content\n        \\bottomrule\n    \\end{tabular}\n  \\end{center}\n\\end{table}\n```\n\nThe names of the datasets we need to include are (from top to bottom):\n- BBH\n- Zebra Logic\n- KOR-Bench (# of tasks should based the broader categorization in the paper.)\n- K&K (https://arxiv.org/abs/2410.23123)\n- BBEH"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_vhPMk2g5oyx3ssLKAKAzPmk3",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\":\"Zebra Logic dataset logical reasoning benchmark zebra logic puzzles dataset \\\"Zebra Logic\\\"\", \"num_results\": 10}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: ZebraLogic: Benchmarking the Logical Reasoning Ability ...\nLink: https://huggingface.co/blog/yuchenlin/zebra-logic\nSnippet: To evaluate the logical reasoning abilities of LLMs, we have created a benchmark named ZebraLogic. Each example is a Logic Grid Puzzle, also known as a Zebra ...\nSitelinks: No sitelinks\n\nTitle: Zebra Logic Bench - a Hugging Face Space by allenai\nLink: https://huggingface.co/spaces/allenai/ZebraLogic\nSnippet: This application displays a leaderboard for model evaluations, allowing users to filter models by mode and explore specific model predictions on zebra ...\nSitelinks: No sitelinks\n\nTitle: Logic.py: Bridging the Gap between LLMs and Constraint ...\nLink: https://arxiv.org/html/2502.15776v1\nSnippet: The validation function containing the constraints derived from the Zebra Logic clues is equally converted to a C representation, and embedded ...\nSitelinks: No sitelinks\n\nTitle: Bill Yuchen Lin\nLink: https://x.com/billyuchenlin/status/1814254565128335705\nSnippet: Introducing ZebraLogic, designed for evaluating LLMs with Logic Puzzles. Quick summary: Each example is a Zebra Puzzle (a type of Logic Grid Puzzles).\nSitelinks: No sitelinks\n\nTitle: e2-research#1: Entering the Next Phase of Cognitive Reasoning\nLink: https://embraceable.ai/en/ankuendigung/e2-research1-entering-the-next-phase-of-cognitive-reasoning/\nSnippet: To evaluate primarily logical performance, we use the established benchmark ZebraLogicBench from AllenAI (Ai2) & Stanford University, which investigates the ...\nSitelinks: No sitelinks\n\nTitle: LLMs reasoning // knowledge representation | by noailabs\nLink: https://noailabs.medium.com/llms-reasoning-knowledge-representation-aec5916695c2\nSnippet: To evaluate the logical reasoning abilities of LLMs, we have created a benchmark named ZebraLogic. Each example is a Logic Grid Puzzle, also ...\nSitelinks: No sitelinks\n\nTitle: 1 Introduction\nLink: https://arxiv.org/html/2505.19641v4\nSnippet: In this work, to address the gap in comprehensive logic tasks, we present SynLogic: a logical reasoning data synthesis framework and a ...\nSitelinks: No sitelinks\n\nTitle: Highlights by Bill Yuchen Lin (@billyuchenlin) ...\nLink: https://mobile.x.com/billyuchenlin/highlights\nSnippet: ) \u2b07\ufe0f 1\ufe0f\u20e3 It ranks 1st on GSM8K! 2\ufe0f\u20e3 Its logical reasoning ability on ZebraLogic is quite similar to Sonnet 3.5, and much better than the others. (note that ...\nSitelinks: No sitelinks\n\nTitle: LLMs Project 1 Presentation | PDF | Logic | Cognitive Science\nLink: https://www.scribd.com/document/873976944/LLMs-Project-1-Presentation\nSnippet: Reviews prompting advanced reasoning puzzles into logic strategies (C-o-T ... with logic-heavy puzzles repeat errors like Zebra logic puzzles. Proposes ...\nSitelinks: No sitelinks\n\nTitle: LOGICEVOLVE: ADVANCING LOGICAL REASONING TO\nLink: https://openreview.net/pdf/6089285418432b41c8b84bbe037604a3cb0a91b1.pdf\nSnippet: This appendix presents the complete meta-data for all logical-reasoning puzzles. ... Logic ... zebralogic : The core of the Zebra Puzzle is to use deductive ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "call_vhPMk2g5oyx3ssLKAKAzPmk3"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_asY2eNm5FYSydhvYEUSRsF3g",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\":\"ZebraLogic dataset number of puzzles how many examples ZebraLogic benchmark size\", \"num_results\": 10}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: ZebraLogic: Benchmarking the Logical Reasoning Ability of ...\nLink: https://huggingface.co/blog/yuchenlin/zebra-logic\nSnippet: We programmatically created 1,000 such puzzles, with sizes ranging from 2x2 to 6x6, and there are 40 puzzles for each size. We test large ...\nSitelinks: No sitelinks\n\nTitle: ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\nLink: https://arxiv.org/html/2502.01100v1\nSnippet: We create the ZebraLogic dataset, a benchmark of 1,000 logic grid puzzles spanning multiple complexity levels, designed to evaluate LLMs' ...\nSitelinks: No sitelinks\n\nTitle: ZebraLogic: A Logical Reasoning AI Benchmark Designed ... - i-genie\nLink: https://i-genie.co.uk/zebralogic-a-logical-reasoning-ai-benchmark-designed-for-evaluating-llms-with-logic-puzzles/\nSnippet: The ZebraLogic benchmark comprises 1,000 programmatically generated puzzles, ranging from 2\u00d72 to 6\u00d76 in size, with 40 puzzles per size category. Large ...\nSitelinks: No sitelinks\n\nTitle: [PDF] Probing the Limits of Reasoning Abilities in Large Language Models\nLink: https://aclanthology.org/2025.acl-long.664.pdf\nSnippet: It includes 4,250 newly generated puzzles in 12 for- mal difficulty classes and 42 sizes. While existing datasets control difficulty mainly based on the grid.\nSitelinks: No sitelinks\n\nTitle: ZebraLogic Benchmark - AI Performance for Builders - airank.dev\nLink: https://airank.dev/benchmarks/zebralogic\nSnippet: ZebraLogic is a comprehensive logical reasoning benchmark featuring 1,000 logic grid puzzles (Zebra puzzles) ranging from 2x2 to 6x6 complexity levels.\nSitelinks: No sitelinks\n\nTitle: A Logical Reasoning AI Benchmark Designed for Evaluating LLMs ...\nLink: https://app.daily.dev/posts/zebralogic-a-logical-reasoning-ai-benchmark-designed-for-evaluating-llms-with-logic-puzzles-2eszu8nkq\nSnippet: ZebraLogic is a benchmark designed to assess the logical reasoning skills of large language models (LLMs) using Logic Grid Puzzles.\nSitelinks: No sitelinks\n\nTitle: Logic.py: Bridging the Gap between LLMs and Constraint Solvers\nLink: https://arxiv.org/html/2502.15776v1\nSnippet: (2024) . ZebraLogicBench is a dataset of 1000 Logic Grid Puzzles, also referred to as Zebra Puzzles. These puzzles consist of a series of clues ...\nSitelinks: No sitelinks\n\nTitle: ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\nLink: https://www.researchgate.net/publication/388658069_ZebraLogic_On_the_Scaling_Limits_of_LLMs_for_Logical_Reasoning\nSnippet: 1). Dataset Statistics. The dataset consists of 1,000 puzzles. where the size of the search space varies signi\ufb01cantly. The. puzzles are based ...\nSitelinks: No sitelinks\n\nTitle: Large language models struggle with logic puzzles - Facebook\nLink: https://www.facebook.com/groups/DeepNetGroup/posts/2240236876369182/\nSnippet: This synthetic dataset included 22,000 preference pairs, split between 17,000 prompts from the WildChat corpus and 5,000 mathematical queries.\nSitelinks: No sitelinks\n\nTitle: ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning\nLink: https://openreview.net/forum?id=sTAJ9QyA6l\nSnippet: We created ZebraLogic, a set of 1,000 logic grid puzzles, to test how well these models handle pure logical reasoning, similar to solving a ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "call_asY2eNm5FYSydhvYEUSRsF3g"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_TFtH4jV8eXSVufsLPFSFcZim",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\":\"KOR-Bench broader categorization tasks KOR-Bench paper tasks categories KOR Bench dataset reasoning\", \"num_results\": 10}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: KOR-Bench\nLink: https://kor-bench.github.io/\nSnippet: KOR-Bench contains five categories, each containing 25 manually defined rules that are suitably modified to ensure that they do not appear in common pre- ...\nSitelinks: No sitelinks\n\nTitle: [2410.06526] KOR-Bench: Benchmarking Language Models ... - arXiv\nLink: https://arxiv.org/abs/2410.06526\nSnippet: We propose the Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench), encompassing five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual.\nSitelinks: No sitelinks\n\nTitle: [PDF] kor-bench: benchmarking language models - ICLR Proceedings\nLink: https://proceedings.iclr.cc/paper_files/paper/2025/file/c6f5851a0d9cb435ed8b50e87bd6a257-Paper-Conference.pdf\nSnippet: KOR-Bench contains five categories, each containing 25 manually defined rules that are suitably modified to ensure that they do not appear in common pre- ...\nSitelinks: No sitelinks\n\nTitle: KOR-Bench: Benchmarking Language Models on Knowledge ...\nLink: https://huggingface.co/papers/2410.06526\nSnippet: KOR-Bench evaluates models' reasoning capabilities by minimizing domain-specific knowledge impact across five task categories, revealing ...\nSitelinks: No sitelinks\n\nTitle: KOR-Bench for Evaluating Intrinsic Reasoning Abilities of Models\nLink: https://www.2077ai.com/blog/kor-bench\nSnippet: The knowledge orthogonality of the KOR-Bench dataset ensures that evaluation tasks are independent of pre-trained knowledge, requiring models to rely on their ...\nSitelinks: No sitelinks\n\nTitle: KOR-Bench: Benchmarking Language Models on Knowledge ...\nLink: https://consensus.app/papers/korbench-benchmarking-language-models-on-wang-liu/ff4255f8a0795b1aa0f863c94a958b34/\nSnippet: Abstract. In this paper, we introduce Knowledge-Orthogonal Reasoning (KOR), a concept aimed at minimizing reliance on domain-specific ...\nSitelinks: No sitelinks\n\nTitle: KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation\nLink: https://www.researchgate.net/publication/391911820_KORGym_A_Dynamic_Game_Platform_for_LLM_Reasoning_Evaluation\nSnippet: Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and ...\nSitelinks: No sitelinks\n\nTitle: [PDF] IFDECORATOR: Wrapping Instruction Following Reinforcement ...\nLink: https://arxiv.org/pdf/2508.04632\nSnippet: These include general reasoning (KOR-. Bench), math calculation (Math-500), and multi-turn con- versations (MT-Bench). The GA metric covers ...\nSitelinks: No sitelinks\n\nTitle: [Revue de papier] KOR-Bench: Benchmarking Language Models on ...\nLink: https://www.themoonlight.io/fr/review/kor-bench-benchmarking-language-models-on-knowledge-orthogonal-reasoning-tasks\nSnippet: KOR-BENCH Framework: KOR-Bench is structured around five main task categories, each focusing on different types of reasoning: Operation Reasoning: Involves ...\nSitelinks: No sitelinks\n\nTitle: [PDF] KRX Bench: Automating Financial Benchmark Creation via Large ...\nLink: https://aclanthology.org/2024.finnlp-1.2.pdf\nSnippet: To demonstrate the effectiveness of the pipeline, we create KRX-Bench-POC, a benchmark assessing the knowledge of LLMs in real-world companies.\nSitelinks: No sitelinks\n",
            "tool_call_id": "call_TFtH4jV8eXSVufsLPFSFcZim"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_QNdvNOVxtplQnjm8RlfmWiTQ",
                    "type": "function",
                    "function": {
                        "name": "fetch-fetch_json",
                        "arguments": "{\"url\":\"https://arxiv.org/abs/2410.23123\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Unexpected token '<', \\\"<!DOCTYPE \\\"... is not valid JSON\",\"annotations\":null}",
            "tool_call_id": "call_QNdvNOVxtplQnjm8RlfmWiTQ"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_GgjnqcqcidVHyzw5l02n0Dy5",
                    "type": "function",
                    "function": {
                        "name": "fetch-fetch_markdown",
                        "arguments": "{\"url\":\"https://arxiv.org/abs/2410.23123\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"\\\\[2410.23123\\\\] On Memorization of Large Language Models in Logical Reasoning                              \\n\\n[Skip to main content](#content)\\n\\n[![Cornell University](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\\n\\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors. [Donate](https://info.arxiv.org/about/donate.html)\\n\\n[](/IgnoreMe)\\n\\n[![arxiv logo](/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](/) \\\\> [cs](/list/cs/recent) \\\\> arXiv:2410.23123\\n\\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\\n\\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\\n\\n Search\\n\\n[![arXiv logo](/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\\n\\n  [![Cornell University Logo](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\\n\\nopen search\\n\\n   GO\\n\\nopen navigation menu\\n\\nquick links\\n-----------\\n\\n*   [Login](https://arxiv.org/login)\\n*   [Help Pages](https://info.arxiv.org/help)\\n*   [About](https://info.arxiv.org/about)\\n\\nComputer Science > Computation and Language\\n===========================================\\n\\n**arXiv:2410.23123** (cs)\\n\\n\\\\[Submitted on 30 Oct 2024 ([v1](https://arxiv.org/abs/2410.23123v1)), last revised 4 Mar 2025 (this version, v2)\\\\]\\n\\nTitle:On Memorization of Large Language Models in Logical Reasoning\\n===================================================================\\n\\nAuthors:[Chulin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+C), [Yangsibo Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+Y), [Chiyuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+C), [Da Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+D), [Xinyun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+X), [Bill Yuchen Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin,+B+Y), [Bo Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+B), [Badih Ghazi](https://arxiv.org/search/cs?searchtype=author&query=Ghazi,+B), [Ravi Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar,+R)\\n\\nView a PDF of the paper titled On Memorization of Large Language Models in Logical Reasoning, by Chulin Xie and 8 other authors\\n\\n[View PDF](/pdf/2410.23123) [HTML (experimental)](https://arxiv.org/html/2410.23123v2)\\n\\n> Abstract:Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&K) puzzles. We find that LLMs could interpolate and memorize the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet they struggle with slight variations of these puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. Through in-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers, we establish that LLMs develop reasoning skills on K&K puzzles alongside memorization. Finally, our analysis based on a per-sample memorization score sheds light on how LLMs switch between reasoning and memorization when solving logical puzzles. Our code and data are available at [this https URL](https://memkklogic.github.io).\\n\\nSubjects:\\n\\nComputation and Language (cs.CL)\\n\\nCite as:\\n\\n[arXiv:2410.23123](https://arxiv.org/abs/2410.23123) \\\\[cs.CL\\\\]\\n\\n\u00a0\\n\\n(or [arXiv:2410.23123v2](https://arxiv.org/abs/2410.23123v2) \\\\[cs.CL\\\\] for this version)\\n\\n\u00a0\\n\\n[https://doi.org/10.48550/arXiv.2410.23123](https://doi.org/10.48550/arXiv.2410.23123)\\n\\nFocus to learn more\\n\\narXiv-issued DOI via DataCite\\n\\nSubmission history\\n------------------\\n\\nFrom: Chulin Xie \\\\[[view email](/show-email/d1541218/2410.23123)\\\\]  \\n**[\\\\[v1\\\\]](/abs/2410.23123v1)** Wed, 30 Oct 2024 15:31:54 UTC (8,530 KB)  \\n**\\\\[v2\\\\]** Tue, 4 Mar 2025 06:22:40 UTC (9,623 KB)  \\n\\nFull-text links:\\n\\nAccess Paper:\\n-------------\\n\\nView a PDF of the paper titled On Memorization of Large Language Models in Logical Reasoning, by Chulin Xie and 8 other authors\\n\\n*   [View PDF](/pdf/2410.23123)\\n*   [HTML (experimental)](https://arxiv.org/html/2410.23123v2)\\n*   [TeX Source](/src/2410.23123)\\n*   [Other Formats](/format/2410.23123)\\n\\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ \\\"Rights to this article\\\")\\n\\nCurrent browse context:\\n\\ncs.CL\\n\\n[<\u00a0prev](/prevnext?id=2410.23123&function=prev&context=cs.CL \\\"previous in cs.CL (accesskey p)\\\") \u00a0 | \u00a0 [next\u00a0>](/prevnext?id=2410.23123&function=next&context=cs.CL \\\"next in cs.CL (accesskey n)\\\")  \\n\\n[new](/list/cs.CL/new) | [recent](/list/cs.CL/recent) | [2024-10](/list/cs.CL/2024-10)\\n\\nChange to browse by:\\n\\n[cs](/abs/2410.23123?context=cs)  \\n\\n### References & Citations\\n\\n*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.23123)\\n*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.23123)\\n*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.23123)\\n\\nexport BibTeX citation Loading...\\n\\nBibTeX formatted citation\\n-------------------------\\n\\n\u00d7\\n\\nloading...\\n\\nData provided by:\\n\\n### Bookmark\\n\\n [![BibSonomy logo](/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2410.23123&description=On Memorization of Large Language Models in Logical Reasoning \\\"Bookmark on BibSonomy\\\")[![Reddit logo](/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2410.23123&title=On Memorization of Large Language Models in Logical Reasoning \\\"Bookmark on Reddit\\\")\\n\\n Bibliographic Tools\\n\\nBibliographic and Citation Tools\\n================================\\n\\n Bibliographic Explorer Toggle\\n\\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\\n\\n Connected Papers Toggle\\n\\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\\n\\n Litmaps Toggle\\n\\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\\n\\n scite.ai Toggle\\n\\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\\n\\n Code, Data, Media\\n\\nCode, Data and Media Associated with this Article\\n=================================================\\n\\n alphaXiv Toggle\\n\\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\\n\\n Links to Code Toggle\\n\\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com))_\\n\\n DagsHub Toggle\\n\\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\\n\\n GotitPub Toggle\\n\\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\\n\\n Huggingface Toggle\\n\\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\\n\\n Links to Code Toggle\\n\\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\\n\\n ScienceCast Toggle\\n\\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\\n\\n Demos\\n\\nDemos\\n=====\\n\\n Replicate Toggle\\n\\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\\n\\n Spaces Toggle\\n\\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\\n\\n Spaces Toggle\\n\\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai))_\\n\\n Related Papers\\n\\nRecommenders and Search Tools\\n=============================\\n\\n Link to Influence Flower\\n\\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\\n\\n Core recommender toggle\\n\\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\\n\\n*   Author\\n*   Venue\\n*   Institution\\n*   Topic\\n\\n About arXivLabs\\n\\narXivLabs: experimental projects with community collaborators\\n=============================================================\\n\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\\n\\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\n\\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\\n\\n[Which authors of this paper are endorsers?](/auth/show-endorsers/2410.23123) | [Disable MathJax](javascript:setMathjaxCookie\\\\(\\\\)) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html))\\n\\nmathjaxToggle();\\n\\n*   [About](https://info.arxiv.org/about)\\n*   [Help](https://info.arxiv.org/help)\\n\\n*   contact arXivClick here to contact arXiv [Contact](https://info.arxiv.org/help/contact.html)\\n*   subscribe to arXiv mailingsClick here to subscribe [Subscribe](https://info.arxiv.org/help/subscribe)\\n\\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\\n\\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\\n*   [arXiv Operational Status](https://status.arxiv.org)  \\n    Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\",\"annotations\":null}",
            "tool_call_id": "call_GgjnqcqcidVHyzw5l02n0Dy5"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_u9kygqL0PsEiF4o32RNQOEDl",
                    "type": "function",
                    "function": {
                        "name": "fetch-fetch_html",
                        "arguments": "{\"url\":\"https://arxiv.org/html/2410.23123v2\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"<!DOCTYPE html>\\n<html lang=\\\"en\\\">\\n<head>\\n<meta content=\\\"text/html; charset=utf-8\\\" http-equiv=\\\"content-type\\\"/>\\n<title>On Memorization of Large Language Models in Logical Reasoning</title>\\n<!--Generated on Tue Mar  4 06:22:58 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->\\n<meta content=\\\"width=device-width, initial-scale=1, shrink-to-fit=no\\\" name=\\\"viewport\\\"/>\\n<link href=\\\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\\\" rel=\\\"stylesheet\\\" type=\\\"text/css\\\"/>\\n<link href=\\\"/static/browse/0.3.4/css/ar5iv.0.7.9.min.css\\\" rel=\\\"stylesheet\\\" type=\\\"text/css\\\"/>\\n<link href=\\\"/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css\\\" rel=\\\"stylesheet\\\" type=\\\"text/css\\\"/>\\n<link href=\\\"/static/browse/0.3.4/css/latexml_styles.css\\\" rel=\\\"stylesheet\\\" type=\\\"text/css\\\"/>\\n<script src=\\\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\\\"></script>\\n<script src=\\\"https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js\\\"></script>\\n<script src=\\\"/static/browse/0.3.4/js/addons_new.js\\\"></script>\\n<script src=\\\"/static/browse/0.3.4/js/feedbackOverlay.js\\\"></script>\\n<meta content=\\\"Machine Learning,  ICML\\\" lang=\\\"en\\\" name=\\\"keywords\\\"/>\\n<base href=\\\"/html/2410.23123v2/\\\"/></head>\\n<body>\\n<nav class=\\\"ltx_page_navbar\\\">\\n<nav class=\\\"ltx_TOC\\\">\\n<ol class=\\\"ltx_toclist\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_section\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S1\\\" title=\\\"In On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">1 </span>Introduction</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_section\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S2\\\" title=\\\"In On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">2 </span>Measuring Memorization in Reasoning</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_section\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S2.SS1\\\" title=\\\"In 2 Measuring Memorization in Reasoning \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">2.1 </span>Memorization Metrics for Reasoning Tasks</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S2.SS2\\\" title=\\\"In 2 Measuring Memorization in Reasoning \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">2.2 </span>Knights and Knaves Reasoning Benchmark</span></a></li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_section\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S3\\\" title=\\\"In On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">3 </span>Quantifying Memorization in Reasoning</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_section\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S4\\\" title=\\\"In On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">4 </span>Learn to Reason by Fine-tuning With Answers Only</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_section\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S4.SS1\\\" title=\\\"In 4 Learn to Reason by Fine-tuning With Answers Only \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">4.1 </span>Reasoning capabilities of Direct FT-ed model</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S4.SS2\\\" title=\\\"In 4 Learn to Reason by Fine-tuning With Answers Only \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">4.2 </span>Probing Direct FTed models</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S4.SS3\\\" title=\\\"In 4 Learn to Reason by Fine-tuning With Answers Only \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">4.3 </span>Direct FT with Wrong Answers</span></a></li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_section\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S5\\\" title=\\\"In On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">5 </span>Learn to Reason by Fine-tuning with CoTs</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_section\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S6\\\" title=\\\"In On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">6 </span>Distinguish Memorization from Reasoning</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_section\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S7\\\" title=\\\"In On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">7 </span>Related Work</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_section\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S8\\\" title=\\\"In On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">8 </span>Conclusion</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_appendix\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A1\\\" title=\\\"In On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">A </span>Discussion and Future Work</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_appendix\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A2\\\" title=\\\"In On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">B </span>Extended Related Work</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_appendix\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A3\\\" title=\\\"In On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">C </span>Details on K&amp;K Benchmark</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_appendix\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A3.SS1\\\" title=\\\"In Appendix C Details on K&amp;K Benchmark \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">C.1 </span>The Abstract Representation</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A3.SS2\\\" title=\\\"In Appendix C Details on K&amp;K Benchmark \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">C.2 </span>The Abstract Puzzle Module: Generator</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A3.SS3\\\" title=\\\"In Appendix C Details on K&amp;K Benchmark \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">C.3 </span>The Abstract Puzzle Module: Solver and Reasoner</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A3.SS4\\\" title=\\\"In Appendix C Details on K&amp;K Benchmark \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">C.4 </span>The Abstract Puzzle Module: Perturber</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A3.SS5\\\" title=\\\"In Appendix C Details on K&amp;K Benchmark \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">C.5 </span>The Natural Language Module: NL-Generator</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A3.SS6\\\" title=\\\"In Appendix C Details on K&amp;K Benchmark \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">C.6 </span>The Natural Language Module: NL-Reasoner</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A3.SS7\\\" title=\\\"In Appendix C Details on K&amp;K Benchmark \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">C.7 </span>The Natural Language Module: NL-Perturber</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A3.SS8\\\" title=\\\"In Appendix C Details on K&amp;K Benchmark \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">C.8 </span>Dataset Generation</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A3.SS8.SSS0.Px1\\\" title=\\\"In C.8 Dataset Generation \u2023 Appendix C Details on K&amp;K Benchmark \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">K&amp;K dataset</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A3.SS8.SSS0.Px2\\\" title=\\\"In C.8 Dataset Generation \u2023 Appendix C Details on K&amp;K Benchmark \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Local perturbation</span></a></li>\\n</ol>\\n</li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_appendix\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A4\\\" title=\\\"In On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">D </span>Experimental Setups</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_appendix\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A4.SS1\\\" title=\\\"In Appendix D Experimental Setups \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">D.1 </span>Models</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A4.SS2\\\" title=\\\"In Appendix D Experimental Setups \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">D.2 </span>Experimental Details</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A4.SS2.SSS1\\\" title=\\\"In D.2 Experimental Details \u2023 Appendix D Experimental Setups \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">D.2.1 </span>Evaluation</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A4.SS2.SSS2\\\" title=\\\"In D.2 Experimental Details \u2023 Appendix D Experimental Setups \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">D.2.2 </span>Fine-tuning</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsubsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A4.SS2.SSS2.Px1\\\" title=\\\"In D.2.2 Fine-tuning \u2023 D.2 Experimental Details \u2023 Appendix D Experimental Setups \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Training instance</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A4.SS2.SSS2.Px2\\\" title=\\\"In D.2.2 Fine-tuning \u2023 D.2 Experimental Details \u2023 Appendix D Experimental Setups \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Training loss</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A4.SS2.SSS2.Px3\\\" title=\\\"In D.2.2 Fine-tuning \u2023 D.2 Experimental Details \u2023 Appendix D Experimental Setups \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Training hyperparameters</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A4.SS2.SSS2.Px4\\\" title=\\\"In D.2.2 Fine-tuning \u2023 D.2 Experimental Details \u2023 Appendix D Experimental Setups \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Reported Training accuracy</span></a></li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A4.SS2.SSS3\\\" title=\\\"In D.2 Experimental Details \u2023 Appendix D Experimental Setups \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">D.2.3 </span>Probing</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A4.SS2.SSS4\\\" title=\\\"In D.2 Experimental Details \u2023 Appendix D Experimental Setups \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">D.2.4 </span>Distinguishing Memorization from Reasoning</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A4.SS2.SSS5\\\" title=\\\"In D.2 Experimental Details \u2023 Appendix D Experimental Setups \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">D.2.5 </span>Computation Resources</span></a></li>\\n</ol>\\n</li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_appendix\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5\\\" title=\\\"In On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">E </span>Additional Experimental Results</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_appendix\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS1\\\" title=\\\"In Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">E.1 </span>Benchmark Performance of Off-the-shelf Models</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS1.SSS0.Px1\\\" title=\\\"In E.1 Benchmark Performance of Off-the-shelf Models \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Off-the-shelf models</span></a></li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS2\\\" title=\\\"In Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">E.2 </span>Memorization Measurement</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS2.SSS0.Px1\\\" title=\\\"In E.2 Memorization Measurement \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Fine-tuned models</span></a></li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS3\\\" title=\\\"In Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">E.3 </span>Evaluation on Reasoning Capability</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS3.SSS1\\\" title=\\\"In E.3 Evaluation on Reasoning Capability \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">E.3.1 </span>Llama3-8B</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsubsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS3.SSS1.Px1\\\" title=\\\"In E.3.1 Llama3-8B \u2023 E.3 Evaluation on Reasoning Capability \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Accuracy over epochs</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS3.SSS1.Px2\\\" title=\\\"In E.3.1 Llama3-8B \u2023 E.3 Evaluation on Reasoning Capability \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Transferability</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS3.SSS1.Px3\\\" title=\\\"In E.3.1 Llama3-8B \u2023 E.3 Evaluation on Reasoning Capability \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Fine-tuning on 10<math alttext=\\\"k\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\"><semantics><mi>k</mi><annotation-xml encoding=\\\"MathML-Content\\\"><ci>\ud835\udc58</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\">k</annotation><annotation encoding=\\\"application/x-llamapun\\\">italic_k</annotation></semantics></math> 8-people K&amp;K samples</span></a></li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS3.SSS2\\\" title=\\\"In E.3 Evaluation on Reasoning Capability \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">E.3.2 </span>GPT4o-mini</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsubsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS3.SSS2.Px1\\\" title=\\\"In E.3.2 GPT4o-mini \u2023 E.3 Evaluation on Reasoning Capability \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Accuracy over epochs</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS3.SSS2.Px2\\\" title=\\\"In E.3.2 GPT4o-mini \u2023 E.3 Evaluation on Reasoning Capability \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Transferability</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS3.SSS2.Px3\\\" title=\\\"In E.3.2 GPT4o-mini \u2023 E.3 Evaluation on Reasoning Capability \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Fine-tuning on 10<math alttext=\\\"k\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\"><semantics><mi>k</mi><annotation-xml encoding=\\\"MathML-Content\\\"><ci>\ud835\udc58</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\">k</annotation><annotation encoding=\\\"application/x-llamapun\\\">italic_k</annotation></semantics></math> 8-people K&amp;K samples</span></a></li>\\n</ol>\\n</li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS4\\\" title=\\\"In Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">E.4 </span>Direct FT with Wrong Answers</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS4.SSS1\\\" title=\\\"In E.4 Direct FT with Wrong Answers \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">E.4.1 </span>Llama3-8B</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsubsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS4.SSS2\\\" title=\\\"In E.4 Direct FT with Wrong Answers \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">E.4.2 </span>GPT4o-mini</span></a></li>\\n</ol>\\n</li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS5\\\" title=\\\"In Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">E.5 </span>Probing</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_subsection\\\">\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS6\\\" title=\\\"In Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\"><span class=\\\"ltx_tag ltx_tag_ref\\\">E.6 </span>Distinguishing Memorization from Reasoning</span></a>\\n<ol class=\\\"ltx_toclist ltx_toclist_subsection\\\">\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS6.SSS0.Px1\\\" title=\\\"In E.6 Distinguishing Memorization from Reasoning \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Puzzle-based indicators</span></a></li>\\n<li class=\\\"ltx_tocentry ltx_tocentry_paragraph\\\"><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.SS6.SSS0.Px2\\\" title=\\\"In E.6 Distinguishing Memorization from Reasoning \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_title\\\">Model-based indicators</span></a></li>\\n</ol>\\n</li>\\n</ol>\\n</li>\\n</ol></nav>\\n</nav>\\n<div class=\\\"ltx_page_main\\\">\\n<div class=\\\"ltx_page_content\\\">\\n<article class=\\\"ltx_document ltx_pruned_first\\\">\\n<h1 class=\\\"ltx_title ltx_title_document\\\">On Memorization of Large Language Models in Logical Reasoning</h1>\\n<div class=\\\"ltx_authors\\\">\\n<span class=\\\"ltx_creator ltx_role_author\\\">\\n<span class=\\\"ltx_personname\\\">Chulin Xie\\n</span></span>\\n<span class=\\\"ltx_author_before\\\">\u2003\u2003</span><span class=\\\"ltx_creator ltx_role_author\\\">\\n<span class=\\\"ltx_personname\\\">Yangsibo Huang\\n</span></span>\\n<span class=\\\"ltx_author_before\\\">\u2003\u2003</span><span class=\\\"ltx_creator ltx_role_author\\\">\\n<span class=\\\"ltx_personname\\\">Chiyuan Zhang\\n</span></span>\\n<span class=\\\"ltx_author_before\\\">\u2003\u2003</span><span class=\\\"ltx_creator ltx_role_author\\\">\\n<span class=\\\"ltx_personname\\\">Da Yu\\n</span></span>\\n<span class=\\\"ltx_author_before\\\">\u2003\u2003</span><span class=\\\"ltx_creator ltx_role_author\\\">\\n<span class=\\\"ltx_personname\\\">Xinyun Chen\\n</span></span>\\n<span class=\\\"ltx_author_before\\\">\u2003\u2003</span><span class=\\\"ltx_creator ltx_role_author\\\">\\n<span class=\\\"ltx_personname\\\">Bill Yuchen Lin\\n</span></span>\\n<span class=\\\"ltx_author_before\\\">\u2003\u2003</span><span class=\\\"ltx_creator ltx_role_author\\\">\\n<span class=\\\"ltx_personname\\\">Bo Li\\n</span></span>\\n<span class=\\\"ltx_author_before\\\">\u2003\u2003</span><span class=\\\"ltx_creator ltx_role_author\\\">\\n<span class=\\\"ltx_personname\\\">Badih Ghazi\\n</span></span>\\n<span class=\\\"ltx_author_before\\\">\u2003\u2003</span><span class=\\\"ltx_creator ltx_role_author\\\">\\n<span class=\\\"ltx_personname\\\">Ravi Kumar\\n</span></span>\\n</div>\\n<div class=\\\"ltx_abstract\\\">\\n<h6 class=\\\"ltx_title ltx_title_abstract\\\">Abstract</h6>\\n<p class=\\\"ltx_p\\\" id=\\\"id1.id1\\\">Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs\u2019 reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&amp;K) puzzles. We find that LLMs could interpolate and memorize the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet they struggle with slight variations of these puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. Through in-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers, we establish that LLMs develop reasoning skills on K&amp;K puzzles alongside memorization.\\nFinally, our analysis based on a per-sample memorization score sheds light on how LLMs switch between reasoning and memorization when solving logical puzzles.</p>\\n</div>\\n<div class=\\\"ltx_keywords\\\">Machine Learning, ICML\\n</div>\\n<div class=\\\"ltx_para\\\" id=\\\"p2\\\">\\n<br class=\\\"ltx_break\\\"/>\\n</div>\\n<section class=\\\"ltx_section\\\" id=\\\"S1\\\">\\n<h2 class=\\\"ltx_title ltx_title_section\\\">\\n<span class=\\\"ltx_tag ltx_tag_section\\\">1 </span>Introduction</h2>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.p1.1\\\">Modern Large Language Models (LLMs) show impressive reasoning capabilities\\nthat allow them to solve a wide range of challenging problems including commonsense reasoning and mathematical reasoning.\\nIn the meantime, LLMs also make mistakes on some of the most basic problems, such as comparing which number is bigger\u201413.11 or 13.8\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Lin, <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib35\\\" title=\\\"\\\">2024</a>)</cite>, and counting the number of sisters that Alice\u2019s brother has\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Nezhurina et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib46\\\" title=\\\"\\\">2024</a>)</cite>.\\nThis contrast is puzzling when it comes to understanding how exactly LLMs solve reasoning tasks. This question is important both scientifically and practically: understanding how LLMs reason could shed light on their learning and generalization behaviors. It is also crucial for real-world applications where robust reasoning is required due to safety\\nand trustworthiness concerns\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Wang et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib66\\\" title=\\\"\\\">2023a</a>; Wallace et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib64\\\" title=\\\"\\\">2024</a>; Lee et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib33\\\" title=\\\"\\\">2024</a>; Wei et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib70\\\" title=\\\"\\\">2024a</a>)</cite>.</p>\\n</div>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.p2\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.p2.1\\\">One hypothesis is that LLMs could be relying on <em class=\\\"ltx_emph ltx_font_italic\\\" id=\\\"S1.p2.1.1\\\">memorization</em> when solving those reasoning tasks, especially when measured by popular benchmarks that could be accidentally leaked into various massive internet-crawled pre-training datasets. Previous work\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Tirumala et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib63\\\" title=\\\"\\\">2022</a>; Carlini et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib9\\\" title=\\\"\\\">2023</a>)</cite> show that LLMs could indeed memorize the training data, which may lead to potential privacy\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Carlini et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib8\\\" title=\\\"\\\">2021</a>)</cite> or copyright\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Karamolegkou et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib30\\\" title=\\\"\\\">2023</a>; Wei et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib71\\\" title=\\\"\\\">2024b</a>)</cite> concerns. Additional evidences of potential memorization come from extensive studies on data contamination in LLMs\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Magar &amp; Schwartz, <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib40\\\" title=\\\"\\\">2022</a>; Balloccu et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib2\\\" title=\\\"\\\">2024</a>; Shi et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib60\\\" title=\\\"\\\">2024</a>; Xu et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib76\\\" title=\\\"\\\">2024</a>; Oren et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib48\\\" title=\\\"\\\">2024</a>)</cite>. To mitigate the issue of benchmark saturation potentially due to memorization, some papers focus on designing dynamic benchmarks\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Roberts et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib56\\\" title=\\\"\\\">2023</a>; Zhu et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib85\\\" title=\\\"\\\">2024</a>; Srivastava et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib62\\\" title=\\\"\\\">2024</a>; Jain et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib26\\\" title=\\\"\\\">2024</a>; Wu et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib74\\\" title=\\\"\\\">2024a</a>)</cite> or alternative evaluation protocols\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Zeng et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib81\\\" title=\\\"\\\">2023</a>; Zhang et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib82\\\" title=\\\"\\\">2024</a>; Xu et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib76\\\" title=\\\"\\\">2024</a>; Srivastava et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib62\\\" title=\\\"\\\">2024</a>)</cite>.</p>\\n</div>\\n<figure class=\\\"ltx_figure\\\" id=\\\"S1.F1\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_centering ltx_img_landscape\\\" height=\\\"252\\\" id=\\\"S1.F1.g1\\\" src=\\\"x1.png\\\" width=\\\"789\\\"/>\\n<figcaption class=\\\"ltx_caption ltx_centering\\\"><span class=\\\"ltx_tag ltx_tag_figure\\\">Figure 1: </span>\\n<span class=\\\"ltx_text\\\" id=\\\"S1.F1.2.1\\\" style=\\\"font-size:90%;\\\">Illustration of the definition of Local Inconsistency based Memorization Score, <math alttext=\\\"\\\\mathsf{LiMem}\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S1.F1.2.1.m1.1\\\"><semantics id=\\\"S1.F1.2.1.m1.1b\\\"><mi id=\\\"S1.F1.2.1.m1.1.1\\\" xref=\\\"S1.F1.2.1.m1.1.1.cmml\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</mi><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S1.F1.2.1.m1.1c\\\"><ci id=\\\"S1.F1.2.1.m1.1.1.cmml\\\" xref=\\\"S1.F1.2.1.m1.1.1\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S1.F1.2.1.m1.1d\\\">\\\\mathsf{LiMem}</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S1.F1.2.1.m1.1e\\\">sansserif_LiMem</annotation></semantics></math>. High level of memorization occurs when the model shows high accuracy in solving some problems but fails to consistently solve those problems under local perturbations that require similar underlying reasoning principles.\\n</span></figcaption>\\n</figure>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.p3\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.p3.1\\\">In this paper, we take a direct approach to quantify the memorization behaviors of LLMs in reasoning tasks within a controlled setting. Specifically, we seek to understand: (i) whether LLMs rely on memorization to solve reasoning tasks, and (ii) whether memorization is only detrimental to learning to reason. Both questions are inspired by human behavior. For instance, when a student works hard on the preparation material for an exam, the preparation could help them get familiarized with the problems, and their ability to solve new problems could usually improve with enough exercises. However, without genuinely understanding the principles, they might fail when the same problem is slightly changed despite doing well on prepared problems.\\nOur metric of memorization <math alttext=\\\"\\\\mathsf{LiMem}\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S1.p3.1.m1.1\\\"><semantics id=\\\"S1.p3.1.m1.1a\\\"><mi id=\\\"S1.p3.1.m1.1.1\\\" xref=\\\"S1.p3.1.m1.1.1.cmml\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</mi><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S1.p3.1.m1.1b\\\"><ci id=\\\"S1.p3.1.m1.1.1.cmml\\\" xref=\\\"S1.p3.1.m1.1.1\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S1.p3.1.m1.1c\\\">\\\\mathsf{LiMem}</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S1.p3.1.m1.1d\\\">sansserif_LiMem</annotation></semantics></math>, illustrated in <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S1.F1\\\" title=\\\"In 1 Introduction \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">Fig.</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">1</span></a>, is based on this intuition: an LLM shows a high level of memorization when it solves reasoning problems with high accuracy but struggles to consistently solve those problems under local perturbations requiring similar mathematical principles (i.e., low consistency).\\nWe note that a similar perturbation (mostly at language-level) idea has been used in previous work, especially in detecting contamination\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Golchin &amp; Surdeanu, <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib19\\\" title=\\\"\\\">2023</a>; Yang et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib77\\\" title=\\\"\\\">2023</a>; Xu et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib76\\\" title=\\\"\\\">2024</a>)</cite>.\\nHowever, given our focus on understanding memorization in logical reasoning tasks, we further consider problem-level perturbation that slightly changes the mathematical structure of a puzzle, in addition to language-level perturbations.\\nTo facilitate our study, we propose a new logical reasoning benchmark that supports automatic problem-level perturbation.\\nWith this tool, we evaluate the reasoning power of 17 off-the-shelf LLMs. We then fine-tune Llama3-8B and GPT4o-mini to quantify their memorization in reasoning tasks, and reveal interesting interplay: while models indeed tend to memorize many training logical puzzles, they also develop reasoning capabilities during fine-tuning (even directly on question-answer pairs without reasoning steps), and the reasoning performance improves when memorizing more training puzzles.</p>\\n</div>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.p4\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.p4.1\\\">In the following, we\\nsummarize our key contributions:</p>\\n<ul class=\\\"ltx_itemize\\\" id=\\\"S1.I1\\\">\\n<li class=\\\"ltx_item\\\" id=\\\"S1.I1.i1\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">\u2022</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.I1.i1.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.I1.i1.p1.1\\\">To quantify memorization in reasoning tasks, we define a memorization score based on the notions of performance inconsistency under local perturbation, inspired by human behavior (<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S2.SS1\\\" title=\\\"2.1 Memorization Metrics for Reasoning Tasks \u2023 2 Measuring Memorization in Reasoning \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">\u00a7</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">2.1</span></a>).</p>\\n</div>\\n</li>\\n<li class=\\\"ltx_item\\\" id=\\\"S1.I1.i2\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">\u2022</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.I1.i2.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.I1.i2.p1.1\\\">To facilitate the measurement, we propose a new logical reasoning benchmark based on the <em class=\\\"ltx_emph ltx_font_italic\\\" id=\\\"S1.I1.i2.p1.1.1\\\">Knights and Knaves</em>\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(K&amp;K, Smullyan, <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib61\\\" title=\\\"\\\">1978</a>; Johnson-Laird &amp; Byrne, <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib29\\\" title=\\\"\\\">1990</a>)</cite> puzzles, that can generate new puzzles at different difficulty levels, locally perturb existing puzzles, and automatically synthesize detailed reasoning steps to solve a given puzzle (<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S2.SS2\\\" title=\\\"2.2 Knights and Knaves Reasoning Benchmark \u2023 2 Measuring Memorization in Reasoning \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">\u00a7</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">2.2</span></a>).</p>\\n</div>\\n</li>\\n<li class=\\\"ltx_item\\\" id=\\\"S1.I1.i3\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">\u2022</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.I1.i3.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.I1.i3.p1.1\\\">We show that K&amp;K puzzles are challenging, and only the most advanced LLMs could solve them well.\\nMoreover, our analysis suggests those models exhibit some level of memorization (<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S3\\\" title=\\\"3 Quantifying Memorization in Reasoning \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">\u00a7</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">3</span></a>).</p>\\n</div>\\n</li>\\n<li class=\\\"ltx_item\\\" id=\\\"S1.I1.i4\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">\u2022</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.I1.i4.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.I1.i4.p1.1\\\">By fine-tuning on K&amp;K samples, we confirm that modern LLMs are capable of memorizing a large collection of puzzles, and reach high memorization score when interpolating\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(i.e., fitting, Belkin et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib5\\\" title=\\\"\\\">2018</a>)</cite> the training set. We observe that the models\u2019 generalization accuracies continue to improve as memorization grows (<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S4\\\" title=\\\"4 Learn to Reason by Fine-tuning With Answers Only \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">\u00a7</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">4</span></a>).</p>\\n</div>\\n</li>\\n<li class=\\\"ltx_item\\\" id=\\\"S1.I1.i5\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">\u2022</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.I1.i5.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.I1.i5.p1.1\\\">We design various in-depth analyses (<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S4.SS1\\\" title=\\\"4.1 Reasoning capabilities of Direct FT-ed model \u2023 4 Learn to Reason by Fine-tuning With Answers Only \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">\u00a7</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">4.1</span></a><math alttext=\\\"\\\\sim\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S1.I1.i5.p1.1.m1.1\\\"><semantics id=\\\"S1.I1.i5.p1.1.m1.1a\\\"><mo id=\\\"S1.I1.i5.p1.1.m1.1.1\\\" xref=\\\"S1.I1.i5.p1.1.m1.1.1.cmml\\\">\u223c</mo><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S1.I1.i5.p1.1.m1.1b\\\"><csymbol cd=\\\"latexml\\\" id=\\\"S1.I1.i5.p1.1.m1.1.1.cmml\\\" xref=\\\"S1.I1.i5.p1.1.m1.1.1\\\">similar-to</csymbol></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S1.I1.i5.p1.1.m1.1c\\\">\\\\sim</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S1.I1.i5.p1.1.m1.1d\\\">\u223c</annotation></semantics></math><a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S4.SS3\\\" title=\\\"4.3 Direct FT with Wrong Answers \u2023 4 Learn to Reason by Fine-tuning With Answers Only \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">\u00a7</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">4.3</span></a>) to verify that LLMs developed improved reasoning capabilities (i.e., generalization)\\nafter fine-tuning even with only question-answer pairs, via local perturbation tests, cross difficulty-level transferability, fine-tuning with wrong answers,\\nand model internal probing.</p>\\n</div>\\n</li>\\n<li class=\\\"ltx_item\\\" id=\\\"S1.I1.i6\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">\u2022</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.I1.i6.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.I1.i6.p1.1\\\">We show that fine-tuning with detailed reasoning steps can further boost the generalization on K&amp;K puzzles, even when fine-tuned with wrong reasoning steps (<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S5\\\" title=\\\"5 Learn to Reason by Fine-tuning with CoTs \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">\u00a7</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">5</span></a>).</p>\\n</div>\\n</li>\\n<li class=\\\"ltx_item\\\" id=\\\"S1.I1.i7\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">\u2022</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S1.I1.i7.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S1.I1.i7.p1.1\\\">To analyze the interplay between memorization and reasoning, we measure per-sample memorization and study how LLMs switch between memorization and reasoning to solve a puzzle (<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S6\\\" title=\\\"6 Distinguish Memorization from Reasoning \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">\u00a7</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">6</span></a>).</p>\\n</div>\\n</li>\\n</ul>\\n</div>\\n</section>\\n<section class=\\\"ltx_section\\\" id=\\\"S2\\\">\\n<h2 class=\\\"ltx_title ltx_title_section\\\">\\n<span class=\\\"ltx_tag ltx_tag_section\\\">2 </span>Measuring Memorization in Reasoning</h2>\\n<section class=\\\"ltx_subsection\\\" id=\\\"S2.SS1\\\">\\n<h3 class=\\\"ltx_title ltx_title_subsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsection\\\">2.1 </span>Memorization Metrics for Reasoning Tasks</h3>\\n<figure class=\\\"ltx_figure\\\" id=\\\"S2.F3\\\">\\n<div class=\\\"ltx_flex_figure\\\">\\n<div class=\\\"ltx_flex_cell ltx_flex_size_2\\\">\\n<figure class=\\\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top\\\" id=\\\"S2.F3.7\\\" style=\\\"width:273.2pt;\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_img_landscape\\\" height=\\\"339\\\" id=\\\"S2.F3.1.g1\\\" src=\\\"x2.png\\\" width=\\\"830\\\"/>\\n<figcaption class=\\\"ltx_caption ltx_centering\\\"><span class=\\\"ltx_tag ltx_tag_figure\\\">Figure 2: </span>\\n<span class=\\\"ltx_text\\\" id=\\\"S2.F3.7.6.3\\\" style=\\\"font-size:90%;\\\">K&amp;K data generation framework employs abstract module and natural language module to generate question-answer pair and synthetic CoTs for each K&amp;K puzzle, based on the problem specification: number of persons (<math alttext=\\\"N\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.F3.5.4.1.m1.1\\\"><semantics id=\\\"S2.F3.5.4.1.m1.1b\\\"><mi id=\\\"S2.F3.5.4.1.m1.1.1\\\" xref=\\\"S2.F3.5.4.1.m1.1.1.cmml\\\">N</mi><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.F3.5.4.1.m1.1c\\\"><ci id=\\\"S2.F3.5.4.1.m1.1.1.cmml\\\" xref=\\\"S2.F3.5.4.1.m1.1.1\\\">\ud835\udc41</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.F3.5.4.1.m1.1d\\\">N</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.F3.5.4.1.m1.1e\\\">italic_N</annotation></semantics></math>), tree width (<math alttext=\\\"W\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.F3.6.5.2.m2.1\\\"><semantics id=\\\"S2.F3.6.5.2.m2.1b\\\"><mi id=\\\"S2.F3.6.5.2.m2.1.1\\\" xref=\\\"S2.F3.6.5.2.m2.1.1.cmml\\\">W</mi><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.F3.6.5.2.m2.1c\\\"><ci id=\\\"S2.F3.6.5.2.m2.1.1.cmml\\\" xref=\\\"S2.F3.6.5.2.m2.1.1\\\">\ud835\udc4a</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.F3.6.5.2.m2.1d\\\">W</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.F3.6.5.2.m2.1e\\\">italic_W</annotation></semantics></math>), and depth (<math alttext=\\\"D\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.F3.7.6.3.m3.1\\\"><semantics id=\\\"S2.F3.7.6.3.m3.1b\\\"><mi id=\\\"S2.F3.7.6.3.m3.1.1\\\" xref=\\\"S2.F3.7.6.3.m3.1.1.cmml\\\">D</mi><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.F3.7.6.3.m3.1c\\\"><ci id=\\\"S2.F3.7.6.3.m3.1.1.cmml\\\" xref=\\\"S2.F3.7.6.3.m3.1.1\\\">\ud835\udc37</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.F3.7.6.3.m3.1d\\\">D</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.F3.7.6.3.m3.1e\\\">italic_D</annotation></semantics></math>).\\nPerturbers in these modules can alter the math structure and language description, respectively, and recompute the question-answer pair.</span></figcaption>\\n</figure>\\n</div>\\n<div class=\\\"ltx_flex_cell ltx_flex_size_2\\\">\\n<figure class=\\\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top\\\" id=\\\"S2.F3.8\\\" style=\\\"width:151.8pt;\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_img_landscape\\\" height=\\\"550\\\" id=\\\"S2.F3.8.g1\\\" src=\\\"x3.png\\\" width=\\\"831\\\"/>\\n<figcaption class=\\\"ltx_caption ltx_centering\\\"><span class=\\\"ltx_tag ltx_tag_figure\\\">Figure 3: </span>\\n<span class=\\\"ltx_text\\\" id=\\\"S2.F3.8.2.1\\\" style=\\\"font-size:90%;\\\">Test acc of off-the-shelf LLMs under 0-shot direct prompting drops with increasing puzzle complexity. For reference, OpenAI o1 model with test-time compute achieves an acc of 0.86 on 8-ppl task and 0.67 on 18-ppl task.\\n\\n</span></figcaption>\\n</figure>\\n</div>\\n</div>\\n</figure>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.SS1.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.SS1.p1.1\\\">Memorization of LLMs has been studied in various contexts such as privacy\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Carlini et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib9\\\" title=\\\"\\\">2023</a>)</cite>, copyright \u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Carlini et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib8\\\" title=\\\"\\\">2021</a>; Karamolegkou et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib30\\\" title=\\\"\\\">2023</a>; Wei et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib71\\\" title=\\\"\\\">2024b</a>; He et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib22\\\" title=\\\"\\\">2024</a>)</cite>, and solving knowledge intensive tasks\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Hartmann et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib21\\\" title=\\\"\\\">2023</a>)</cite>. In this paper, we are specifically interested in measuring the level of memorization when solving reasoning tasks, by borrowing intuition from human behavior.\\nFor example, when preparing for an exam, a student may not be able to fully digest the underlying principles due to various reasons or constraints. But when (luckily) facing the same problem the student had prepared for, they would still be able to solve it. A key characteristic of this type of memorization is: (A) high accuracy on observed problems and (B) low accuracy when the problem is slightly changed.\\nBased on this intuition, for a dataset <math alttext=\\\"\\\\mathcal{D}\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS1.p1.1.m1.1\\\"><semantics id=\\\"S2.SS1.p1.1.m1.1a\\\"><mi class=\\\"ltx_font_mathcaligraphic\\\" id=\\\"S2.SS1.p1.1.m1.1.1\\\" xref=\\\"S2.SS1.p1.1.m1.1.1.cmml\\\">\ud835\udc9f</mi><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS1.p1.1.m1.1b\\\"><ci id=\\\"S2.SS1.p1.1.m1.1.1.cmml\\\" xref=\\\"S2.SS1.p1.1.m1.1.1\\\">\ud835\udc9f</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.SS1.p1.1.m1.1c\\\">\\\\mathcal{D}</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.SS1.p1.1.m1.1d\\\">caligraphic_D</annotation></semantics></math> of reasoning puzzles, we combine the following two quantities to measure memorization:</p>\\n<ol class=\\\"ltx_enumerate\\\" id=\\\"S2.I1\\\">\\n<li class=\\\"ltx_item\\\" id=\\\"S2.I1.i1\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">1.</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.I1.i1.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.I1.i1.p1.6\\\">For (A), we measure the accuracy of a target model <math alttext=\\\"f\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.I1.i1.p1.1.m1.1\\\"><semantics id=\\\"S2.I1.i1.p1.1.m1.1a\\\"><mi id=\\\"S2.I1.i1.p1.1.m1.1.1\\\" xref=\\\"S2.I1.i1.p1.1.m1.1.1.cmml\\\">f</mi><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.I1.i1.p1.1.m1.1b\\\"><ci id=\\\"S2.I1.i1.p1.1.m1.1.1.cmml\\\" xref=\\\"S2.I1.i1.p1.1.m1.1.1\\\">\ud835\udc53</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.I1.i1.p1.1.m1.1c\\\">f</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.I1.i1.p1.1.m1.1d\\\">italic_f</annotation></semantics></math> on <math alttext=\\\"\\\\mathcal{D}\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.I1.i1.p1.2.m2.1\\\"><semantics id=\\\"S2.I1.i1.p1.2.m2.1a\\\"><mi class=\\\"ltx_font_mathcaligraphic\\\" id=\\\"S2.I1.i1.p1.2.m2.1.1\\\" xref=\\\"S2.I1.i1.p1.2.m2.1.1.cmml\\\">\ud835\udc9f</mi><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.I1.i1.p1.2.m2.1b\\\"><ci id=\\\"S2.I1.i1.p1.2.m2.1.1.cmml\\\" xref=\\\"S2.I1.i1.p1.2.m2.1.1\\\">\ud835\udc9f</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.I1.i1.p1.2.m2.1c\\\">\\\\mathcal{D}</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.I1.i1.p1.2.m2.1d\\\">caligraphic_D</annotation></semantics></math>, denoted as <math alttext=\\\"\\\\mathsf{Acc}(f;\\\\mathcal{D})\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.I1.i1.p1.3.m3.2\\\"><semantics id=\\\"S2.I1.i1.p1.3.m3.2a\\\"><mrow id=\\\"S2.I1.i1.p1.3.m3.2.3\\\" xref=\\\"S2.I1.i1.p1.3.m3.2.3.cmml\\\"><mi id=\\\"S2.I1.i1.p1.3.m3.2.3.2\\\" xref=\\\"S2.I1.i1.p1.3.m3.2.3.2.cmml\\\">\ud835\udda0\ud835\uddbc\ud835\uddbc</mi><mo id=\\\"S2.I1.i1.p1.3.m3.2.3.1\\\" xref=\\\"S2.I1.i1.p1.3.m3.2.3.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.I1.i1.p1.3.m3.2.3.3.2\\\" xref=\\\"S2.I1.i1.p1.3.m3.2.3.3.1.cmml\\\"><mo id=\\\"S2.I1.i1.p1.3.m3.2.3.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.I1.i1.p1.3.m3.2.3.3.1.cmml\\\">(</mo><mi id=\\\"S2.I1.i1.p1.3.m3.1.1\\\" xref=\\\"S2.I1.i1.p1.3.m3.1.1.cmml\\\">f</mi><mo id=\\\"S2.I1.i1.p1.3.m3.2.3.3.2.2\\\" xref=\\\"S2.I1.i1.p1.3.m3.2.3.3.1.cmml\\\">;</mo><mi class=\\\"ltx_font_mathcaligraphic\\\" id=\\\"S2.I1.i1.p1.3.m3.2.2\\\" xref=\\\"S2.I1.i1.p1.3.m3.2.2.cmml\\\">\ud835\udc9f</mi><mo id=\\\"S2.I1.i1.p1.3.m3.2.3.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.I1.i1.p1.3.m3.2.3.3.1.cmml\\\">)</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.I1.i1.p1.3.m3.2b\\\"><apply id=\\\"S2.I1.i1.p1.3.m3.2.3.cmml\\\" xref=\\\"S2.I1.i1.p1.3.m3.2.3\\\"><times id=\\\"S2.I1.i1.p1.3.m3.2.3.1.cmml\\\" xref=\\\"S2.I1.i1.p1.3.m3.2.3.1\\\"></times><ci id=\\\"S2.I1.i1.p1.3.m3.2.3.2.cmml\\\" xref=\\\"S2.I1.i1.p1.3.m3.2.3.2\\\">\ud835\udda0\ud835\uddbc\ud835\uddbc</ci><list id=\\\"S2.I1.i1.p1.3.m3.2.3.3.1.cmml\\\" xref=\\\"S2.I1.i1.p1.3.m3.2.3.3.2\\\"><ci id=\\\"S2.I1.i1.p1.3.m3.1.1.cmml\\\" xref=\\\"S2.I1.i1.p1.3.m3.1.1\\\">\ud835\udc53</ci><ci id=\\\"S2.I1.i1.p1.3.m3.2.2.cmml\\\" xref=\\\"S2.I1.i1.p1.3.m3.2.2\\\">\ud835\udc9f</ci></list></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.I1.i1.p1.3.m3.2c\\\">\\\\mathsf{Acc}(f;\\\\mathcal{D})</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.I1.i1.p1.3.m3.2d\\\">sansserif_Acc ( italic_f ; caligraphic_D )</annotation></semantics></math>. We are especially interested in measuring on the set of <em class=\\\"ltx_emph ltx_font_italic\\\" id=\\\"S2.I1.i1.p1.6.1\\\">observed puzzles</em>, i.e., the training set, <math alttext=\\\"\\\\mathsf{Acc}(f;\\\\mathsf{Tr})\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.I1.i1.p1.4.m4.2\\\"><semantics id=\\\"S2.I1.i1.p1.4.m4.2a\\\"><mrow id=\\\"S2.I1.i1.p1.4.m4.2.3\\\" xref=\\\"S2.I1.i1.p1.4.m4.2.3.cmml\\\"><mi id=\\\"S2.I1.i1.p1.4.m4.2.3.2\\\" xref=\\\"S2.I1.i1.p1.4.m4.2.3.2.cmml\\\">\ud835\udda0\ud835\uddbc\ud835\uddbc</mi><mo id=\\\"S2.I1.i1.p1.4.m4.2.3.1\\\" xref=\\\"S2.I1.i1.p1.4.m4.2.3.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.I1.i1.p1.4.m4.2.3.3.2\\\" xref=\\\"S2.I1.i1.p1.4.m4.2.3.3.1.cmml\\\"><mo id=\\\"S2.I1.i1.p1.4.m4.2.3.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.I1.i1.p1.4.m4.2.3.3.1.cmml\\\">(</mo><mi id=\\\"S2.I1.i1.p1.4.m4.1.1\\\" xref=\\\"S2.I1.i1.p1.4.m4.1.1.cmml\\\">f</mi><mo id=\\\"S2.I1.i1.p1.4.m4.2.3.3.2.2\\\" xref=\\\"S2.I1.i1.p1.4.m4.2.3.3.1.cmml\\\">;</mo><mi id=\\\"S2.I1.i1.p1.4.m4.2.2\\\" xref=\\\"S2.I1.i1.p1.4.m4.2.2.cmml\\\">\ud835\uddb3\ud835\uddcb</mi><mo id=\\\"S2.I1.i1.p1.4.m4.2.3.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.I1.i1.p1.4.m4.2.3.3.1.cmml\\\">)</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.I1.i1.p1.4.m4.2b\\\"><apply id=\\\"S2.I1.i1.p1.4.m4.2.3.cmml\\\" xref=\\\"S2.I1.i1.p1.4.m4.2.3\\\"><times id=\\\"S2.I1.i1.p1.4.m4.2.3.1.cmml\\\" xref=\\\"S2.I1.i1.p1.4.m4.2.3.1\\\"></times><ci id=\\\"S2.I1.i1.p1.4.m4.2.3.2.cmml\\\" xref=\\\"S2.I1.i1.p1.4.m4.2.3.2\\\">\ud835\udda0\ud835\uddbc\ud835\uddbc</ci><list id=\\\"S2.I1.i1.p1.4.m4.2.3.3.1.cmml\\\" xref=\\\"S2.I1.i1.p1.4.m4.2.3.3.2\\\"><ci id=\\\"S2.I1.i1.p1.4.m4.1.1.cmml\\\" xref=\\\"S2.I1.i1.p1.4.m4.1.1\\\">\ud835\udc53</ci><ci id=\\\"S2.I1.i1.p1.4.m4.2.2.cmml\\\" xref=\\\"S2.I1.i1.p1.4.m4.2.2\\\">\ud835\uddb3\ud835\uddcb</ci></list></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.I1.i1.p1.4.m4.2c\\\">\\\\mathsf{Acc}(f;\\\\mathsf{Tr})</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.I1.i1.p1.4.m4.2d\\\">sansserif_Acc ( italic_f ; sansserif_Tr )</annotation></semantics></math>. We say <math alttext=\\\"f\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.I1.i1.p1.5.m5.1\\\"><semantics id=\\\"S2.I1.i1.p1.5.m5.1a\\\"><mi id=\\\"S2.I1.i1.p1.5.m5.1.1\\\" xref=\\\"S2.I1.i1.p1.5.m5.1.1.cmml\\\">f</mi><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.I1.i1.p1.5.m5.1b\\\"><ci id=\\\"S2.I1.i1.p1.5.m5.1.1.cmml\\\" xref=\\\"S2.I1.i1.p1.5.m5.1.1\\\">\ud835\udc53</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.I1.i1.p1.5.m5.1c\\\">f</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.I1.i1.p1.5.m5.1d\\\">italic_f</annotation></semantics></math> <span class=\\\"ltx_text ltx_font_italic\\\" id=\\\"S2.I1.i1.p1.6.2\\\">interpolates</span>\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Belkin et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib5\\\" title=\\\"\\\">2018</a>; Muthukumar et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib45\\\" title=\\\"\\\">2020</a>; Belkin, <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib4\\\" title=\\\"\\\">2021</a>; Bartlett et\u00a0al., <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib3\\\" title=\\\"\\\">2021</a>)</cite> the training puzzles if <math alttext=\\\"\\\\mathsf{Acc}(f;\\\\mathsf{Tr})\\\\approx 100\\\\%\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.I1.i1.p1.6.m6.2\\\"><semantics id=\\\"S2.I1.i1.p1.6.m6.2a\\\"><mrow id=\\\"S2.I1.i1.p1.6.m6.2.3\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.cmml\\\"><mrow id=\\\"S2.I1.i1.p1.6.m6.2.3.2\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.2.cmml\\\"><mi id=\\\"S2.I1.i1.p1.6.m6.2.3.2.2\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.2.2.cmml\\\">\ud835\udda0\ud835\uddbc\ud835\uddbc</mi><mo id=\\\"S2.I1.i1.p1.6.m6.2.3.2.1\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.2.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.I1.i1.p1.6.m6.2.3.2.3.2\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.2.3.1.cmml\\\"><mo id=\\\"S2.I1.i1.p1.6.m6.2.3.2.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.2.3.1.cmml\\\">(</mo><mi id=\\\"S2.I1.i1.p1.6.m6.1.1\\\" xref=\\\"S2.I1.i1.p1.6.m6.1.1.cmml\\\">f</mi><mo id=\\\"S2.I1.i1.p1.6.m6.2.3.2.3.2.2\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.2.3.1.cmml\\\">;</mo><mi id=\\\"S2.I1.i1.p1.6.m6.2.2\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.2.cmml\\\">\ud835\uddb3\ud835\uddcb</mi><mo id=\\\"S2.I1.i1.p1.6.m6.2.3.2.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.2.3.1.cmml\\\">)</mo></mrow></mrow><mo id=\\\"S2.I1.i1.p1.6.m6.2.3.1\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.1.cmml\\\">\u2248</mo><mrow id=\\\"S2.I1.i1.p1.6.m6.2.3.3\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.3.cmml\\\"><mn id=\\\"S2.I1.i1.p1.6.m6.2.3.3.2\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.3.2.cmml\\\">100</mn><mo id=\\\"S2.I1.i1.p1.6.m6.2.3.3.1\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.3.1.cmml\\\">%</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.I1.i1.p1.6.m6.2b\\\"><apply id=\\\"S2.I1.i1.p1.6.m6.2.3.cmml\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3\\\"><approx id=\\\"S2.I1.i1.p1.6.m6.2.3.1.cmml\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.1\\\"></approx><apply id=\\\"S2.I1.i1.p1.6.m6.2.3.2.cmml\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.2\\\"><times id=\\\"S2.I1.i1.p1.6.m6.2.3.2.1.cmml\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.2.1\\\"></times><ci id=\\\"S2.I1.i1.p1.6.m6.2.3.2.2.cmml\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.2.2\\\">\ud835\udda0\ud835\uddbc\ud835\uddbc</ci><list id=\\\"S2.I1.i1.p1.6.m6.2.3.2.3.1.cmml\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.2.3.2\\\"><ci id=\\\"S2.I1.i1.p1.6.m6.1.1.cmml\\\" xref=\\\"S2.I1.i1.p1.6.m6.1.1\\\">\ud835\udc53</ci><ci id=\\\"S2.I1.i1.p1.6.m6.2.2.cmml\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.2\\\">\ud835\uddb3\ud835\uddcb</ci></list></apply><apply id=\\\"S2.I1.i1.p1.6.m6.2.3.3.cmml\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.3\\\"><csymbol cd=\\\"latexml\\\" id=\\\"S2.I1.i1.p1.6.m6.2.3.3.1.cmml\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.3.1\\\">percent</csymbol><cn id=\\\"S2.I1.i1.p1.6.m6.2.3.3.2.cmml\\\" type=\\\"integer\\\" xref=\\\"S2.I1.i1.p1.6.m6.2.3.3.2\\\">100</cn></apply></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.I1.i1.p1.6.m6.2c\\\">\\\\mathsf{Acc}(f;\\\\mathsf{Tr})\\\\approx 100\\\\%</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.I1.i1.p1.6.m6.2d\\\">sansserif_Acc ( italic_f ; sansserif_Tr ) \u2248 100 %</annotation></semantics></math>.</p>\\n</div>\\n</li>\\n<li class=\\\"ltx_item\\\" id=\\\"S2.I1.i2\\\" style=\\\"list-style-type:none;\\\">\\n<span class=\\\"ltx_tag ltx_tag_item\\\">2.</span>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.I1.i2.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.I1.i2.p1.1\\\">For (B), we measure a <em class=\\\"ltx_emph ltx_font_italic\\\" id=\\\"S2.I1.i2.p1.1.1\\\">consistency ratio</em> <math alttext=\\\"\\\\mathsf{CR}(f;\\\\mathcal{D})\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.I1.i2.p1.1.m1.2\\\"><semantics id=\\\"S2.I1.i2.p1.1.m1.2a\\\"><mrow id=\\\"S2.I1.i2.p1.1.m1.2.3\\\" xref=\\\"S2.I1.i2.p1.1.m1.2.3.cmml\\\"><mi id=\\\"S2.I1.i2.p1.1.m1.2.3.2\\\" xref=\\\"S2.I1.i2.p1.1.m1.2.3.2.cmml\\\">\ud835\udda2\ud835\uddb1</mi><mo id=\\\"S2.I1.i2.p1.1.m1.2.3.1\\\" xref=\\\"S2.I1.i2.p1.1.m1.2.3.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.I1.i2.p1.1.m1.2.3.3.2\\\" xref=\\\"S2.I1.i2.p1.1.m1.2.3.3.1.cmml\\\"><mo id=\\\"S2.I1.i2.p1.1.m1.2.3.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.I1.i2.p1.1.m1.2.3.3.1.cmml\\\">(</mo><mi id=\\\"S2.I1.i2.p1.1.m1.1.1\\\" xref=\\\"S2.I1.i2.p1.1.m1.1.1.cmml\\\">f</mi><mo id=\\\"S2.I1.i2.p1.1.m1.2.3.3.2.2\\\" xref=\\\"S2.I1.i2.p1.1.m1.2.3.3.1.cmml\\\">;</mo><mi class=\\\"ltx_font_mathcaligraphic\\\" id=\\\"S2.I1.i2.p1.1.m1.2.2\\\" xref=\\\"S2.I1.i2.p1.1.m1.2.2.cmml\\\">\ud835\udc9f</mi><mo id=\\\"S2.I1.i2.p1.1.m1.2.3.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.I1.i2.p1.1.m1.2.3.3.1.cmml\\\">)</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.I1.i2.p1.1.m1.2b\\\"><apply id=\\\"S2.I1.i2.p1.1.m1.2.3.cmml\\\" xref=\\\"S2.I1.i2.p1.1.m1.2.3\\\"><times id=\\\"S2.I1.i2.p1.1.m1.2.3.1.cmml\\\" xref=\\\"S2.I1.i2.p1.1.m1.2.3.1\\\"></times><ci id=\\\"S2.I1.i2.p1.1.m1.2.3.2.cmml\\\" xref=\\\"S2.I1.i2.p1.1.m1.2.3.2\\\">\ud835\udda2\ud835\uddb1</ci><list id=\\\"S2.I1.i2.p1.1.m1.2.3.3.1.cmml\\\" xref=\\\"S2.I1.i2.p1.1.m1.2.3.3.2\\\"><ci id=\\\"S2.I1.i2.p1.1.m1.1.1.cmml\\\" xref=\\\"S2.I1.i2.p1.1.m1.1.1\\\">\ud835\udc53</ci><ci id=\\\"S2.I1.i2.p1.1.m1.2.2.cmml\\\" xref=\\\"S2.I1.i2.p1.1.m1.2.2\\\">\ud835\udc9f</ci></list></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.I1.i2.p1.1.m1.2c\\\">\\\\mathsf{CR}(f;\\\\mathcal{D})</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.I1.i2.p1.1.m1.2d\\\">sansserif_CR ( italic_f ; caligraphic_D )</annotation></semantics></math> between the number of <em class=\\\"ltx_emph ltx_font_italic\\\" id=\\\"S2.I1.i2.p1.1.2\\\">consistently solved puzzles</em> after some <em class=\\\"ltx_emph ltx_font_italic\\\" id=\\\"S2.I1.i2.p1.1.3\\\">local perturbations</em>,\\nand the number of solved puzzles (without perturbation). We are interested in local perturbations that make minimal changes to the puzzle and maintain the same underlying principle for solving it, and a similar difficulty level (to be specified in <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S2.SS2\\\" title=\\\"2.2 Knights and Knaves Reasoning Benchmark \u2023 2 Measuring Memorization in Reasoning \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">\u00a7</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">2.2</span></a>).</p>\\n</div>\\n</li>\\n</ol>\\n<p class=\\\"ltx_p\\\" id=\\\"S2.SS1.p1.2\\\">We combine the two factors to define a <span class=\\\"ltx_text ltx_font_italic\\\" id=\\\"S2.SS1.p1.2.1\\\">Local Inconsistency-based Memorization Score</span> <math alttext=\\\"\\\\mathsf{LiMem}(f;\\\\mathcal{D})\\\\in[0,1]\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS1.p1.2.m1.4\\\"><semantics id=\\\"S2.SS1.p1.2.m1.4a\\\"><mrow id=\\\"S2.SS1.p1.2.m1.4.5\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.cmml\\\"><mrow id=\\\"S2.SS1.p1.2.m1.4.5.2\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.2.cmml\\\"><mi id=\\\"S2.SS1.p1.2.m1.4.5.2.2\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.2.2.cmml\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</mi><mo id=\\\"S2.SS1.p1.2.m1.4.5.2.1\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.2.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.SS1.p1.2.m1.4.5.2.3.2\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.2.3.1.cmml\\\"><mo id=\\\"S2.SS1.p1.2.m1.4.5.2.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.2.3.1.cmml\\\">(</mo><mi id=\\\"S2.SS1.p1.2.m1.1.1\\\" xref=\\\"S2.SS1.p1.2.m1.1.1.cmml\\\">f</mi><mo id=\\\"S2.SS1.p1.2.m1.4.5.2.3.2.2\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.2.3.1.cmml\\\">;</mo><mi class=\\\"ltx_font_mathcaligraphic\\\" id=\\\"S2.SS1.p1.2.m1.2.2\\\" xref=\\\"S2.SS1.p1.2.m1.2.2.cmml\\\">\ud835\udc9f</mi><mo id=\\\"S2.SS1.p1.2.m1.4.5.2.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.2.3.1.cmml\\\">)</mo></mrow></mrow><mo id=\\\"S2.SS1.p1.2.m1.4.5.1\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.1.cmml\\\">\u2208</mo><mrow id=\\\"S2.SS1.p1.2.m1.4.5.3.2\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.3.1.cmml\\\"><mo id=\\\"S2.SS1.p1.2.m1.4.5.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.3.1.cmml\\\">[</mo><mn id=\\\"S2.SS1.p1.2.m1.3.3\\\" xref=\\\"S2.SS1.p1.2.m1.3.3.cmml\\\">0</mn><mo id=\\\"S2.SS1.p1.2.m1.4.5.3.2.2\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.3.1.cmml\\\">,</mo><mn id=\\\"S2.SS1.p1.2.m1.4.4\\\" xref=\\\"S2.SS1.p1.2.m1.4.4.cmml\\\">1</mn><mo id=\\\"S2.SS1.p1.2.m1.4.5.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.3.1.cmml\\\">]</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS1.p1.2.m1.4b\\\"><apply id=\\\"S2.SS1.p1.2.m1.4.5.cmml\\\" xref=\\\"S2.SS1.p1.2.m1.4.5\\\"><in id=\\\"S2.SS1.p1.2.m1.4.5.1.cmml\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.1\\\"></in><apply id=\\\"S2.SS1.p1.2.m1.4.5.2.cmml\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.2\\\"><times id=\\\"S2.SS1.p1.2.m1.4.5.2.1.cmml\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.2.1\\\"></times><ci id=\\\"S2.SS1.p1.2.m1.4.5.2.2.cmml\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.2.2\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</ci><list id=\\\"S2.SS1.p1.2.m1.4.5.2.3.1.cmml\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.2.3.2\\\"><ci id=\\\"S2.SS1.p1.2.m1.1.1.cmml\\\" xref=\\\"S2.SS1.p1.2.m1.1.1\\\">\ud835\udc53</ci><ci id=\\\"S2.SS1.p1.2.m1.2.2.cmml\\\" xref=\\\"S2.SS1.p1.2.m1.2.2\\\">\ud835\udc9f</ci></list></apply><interval closure=\\\"closed\\\" id=\\\"S2.SS1.p1.2.m1.4.5.3.1.cmml\\\" xref=\\\"S2.SS1.p1.2.m1.4.5.3.2\\\"><cn id=\\\"S2.SS1.p1.2.m1.3.3.cmml\\\" type=\\\"integer\\\" xref=\\\"S2.SS1.p1.2.m1.3.3\\\">0</cn><cn id=\\\"S2.SS1.p1.2.m1.4.4.cmml\\\" type=\\\"integer\\\" xref=\\\"S2.SS1.p1.2.m1.4.4\\\">1</cn></interval></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.SS1.p1.2.m1.4c\\\">\\\\mathsf{LiMem}(f;\\\\mathcal{D})\\\\in[0,1]</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.SS1.p1.2.m1.4d\\\">sansserif_LiMem ( italic_f ; caligraphic_D ) \u2208 [ 0 , 1 ]</annotation></semantics></math>:</p>\\n<table class=\\\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\\\" id=\\\"A5.EGx1\\\">\\n<tbody id=\\\"S2.E1\\\"><tr class=\\\"ltx_equation ltx_eqn_row ltx_align_baseline\\\">\\n<td class=\\\"ltx_eqn_cell ltx_eqn_center_padleft\\\"></td>\\n<td class=\\\"ltx_td ltx_align_right ltx_eqn_cell\\\"><math alttext=\\\"\\\\displaystyle\\\\mathsf{LiMem}(f;\\\\mathcal{D})\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.E1.m1.2\\\"><semantics id=\\\"S2.E1.m1.2a\\\"><mrow id=\\\"S2.E1.m1.2.3\\\" xref=\\\"S2.E1.m1.2.3.cmml\\\"><mi id=\\\"S2.E1.m1.2.3.2\\\" xref=\\\"S2.E1.m1.2.3.2.cmml\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</mi><mo id=\\\"S2.E1.m1.2.3.1\\\" xref=\\\"S2.E1.m1.2.3.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.E1.m1.2.3.3.2\\\" xref=\\\"S2.E1.m1.2.3.3.1.cmml\\\"><mo id=\\\"S2.E1.m1.2.3.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.E1.m1.2.3.3.1.cmml\\\">(</mo><mi id=\\\"S2.E1.m1.1.1\\\" xref=\\\"S2.E1.m1.1.1.cmml\\\">f</mi><mo id=\\\"S2.E1.m1.2.3.3.2.2\\\" xref=\\\"S2.E1.m1.2.3.3.1.cmml\\\">;</mo><mi class=\\\"ltx_font_mathcaligraphic\\\" id=\\\"S2.E1.m1.2.2\\\" xref=\\\"S2.E1.m1.2.2.cmml\\\">\ud835\udc9f</mi><mo id=\\\"S2.E1.m1.2.3.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.E1.m1.2.3.3.1.cmml\\\">)</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.E1.m1.2b\\\"><apply id=\\\"S2.E1.m1.2.3.cmml\\\" xref=\\\"S2.E1.m1.2.3\\\"><times id=\\\"S2.E1.m1.2.3.1.cmml\\\" xref=\\\"S2.E1.m1.2.3.1\\\"></times><ci id=\\\"S2.E1.m1.2.3.2.cmml\\\" xref=\\\"S2.E1.m1.2.3.2\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</ci><list id=\\\"S2.E1.m1.2.3.3.1.cmml\\\" xref=\\\"S2.E1.m1.2.3.3.2\\\"><ci id=\\\"S2.E1.m1.1.1.cmml\\\" xref=\\\"S2.E1.m1.1.1\\\">\ud835\udc53</ci><ci id=\\\"S2.E1.m1.2.2.cmml\\\" xref=\\\"S2.E1.m1.2.2\\\">\ud835\udc9f</ci></list></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.E1.m1.2c\\\">\\\\displaystyle\\\\mathsf{LiMem}(f;\\\\mathcal{D})</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.E1.m1.2d\\\">sansserif_LiMem ( italic_f ; caligraphic_D )</annotation></semantics></math></td>\\n<td class=\\\"ltx_td ltx_align_left ltx_eqn_cell\\\"><math alttext=\\\"\\\\displaystyle=\\\\mathsf{Acc}(f;\\\\mathcal{D})\\\\cdot(1-\\\\mathsf{CR}(f;\\\\mathcal{D}))\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.E1.m2.5\\\"><semantics id=\\\"S2.E1.m2.5a\\\"><mrow id=\\\"S2.E1.m2.5.5\\\" xref=\\\"S2.E1.m2.5.5.cmml\\\"><mi id=\\\"S2.E1.m2.5.5.3\\\" xref=\\\"S2.E1.m2.5.5.3.cmml\\\"></mi><mo id=\\\"S2.E1.m2.5.5.2\\\" xref=\\\"S2.E1.m2.5.5.2.cmml\\\">=</mo><mrow id=\\\"S2.E1.m2.5.5.1\\\" xref=\\\"S2.E1.m2.5.5.1.cmml\\\"><mrow id=\\\"S2.E1.m2.5.5.1.3\\\" xref=\\\"S2.E1.m2.5.5.1.3.cmml\\\"><mi id=\\\"S2.E1.m2.5.5.1.3.2\\\" xref=\\\"S2.E1.m2.5.5.1.3.2.cmml\\\">\ud835\udda0\ud835\uddbc\ud835\uddbc</mi><mo id=\\\"S2.E1.m2.5.5.1.3.1\\\" xref=\\\"S2.E1.m2.5.5.1.3.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.E1.m2.5.5.1.3.3.2\\\" xref=\\\"S2.E1.m2.5.5.1.3.3.1.cmml\\\"><mo id=\\\"S2.E1.m2.5.5.1.3.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.E1.m2.5.5.1.3.3.1.cmml\\\">(</mo><mi id=\\\"S2.E1.m2.1.1\\\" xref=\\\"S2.E1.m2.1.1.cmml\\\">f</mi><mo id=\\\"S2.E1.m2.5.5.1.3.3.2.2\\\" xref=\\\"S2.E1.m2.5.5.1.3.3.1.cmml\\\">;</mo><mi class=\\\"ltx_font_mathcaligraphic\\\" id=\\\"S2.E1.m2.2.2\\\" xref=\\\"S2.E1.m2.2.2.cmml\\\">\ud835\udc9f</mi><mo id=\\\"S2.E1.m2.5.5.1.3.3.2.3\\\" rspace=\\\"0.055em\\\" stretchy=\\\"false\\\" xref=\\\"S2.E1.m2.5.5.1.3.3.1.cmml\\\">)</mo></mrow></mrow><mo id=\\\"S2.E1.m2.5.5.1.2\\\" rspace=\\\"0.222em\\\" xref=\\\"S2.E1.m2.5.5.1.2.cmml\\\">\u22c5</mo><mrow id=\\\"S2.E1.m2.5.5.1.1.1\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.cmml\\\"><mo id=\\\"S2.E1.m2.5.5.1.1.1.2\\\" stretchy=\\\"false\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.cmml\\\">(</mo><mrow id=\\\"S2.E1.m2.5.5.1.1.1.1\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.cmml\\\"><mn id=\\\"S2.E1.m2.5.5.1.1.1.1.2\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.2.cmml\\\">1</mn><mo id=\\\"S2.E1.m2.5.5.1.1.1.1.1\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.1.cmml\\\">\u2212</mo><mrow id=\\\"S2.E1.m2.5.5.1.1.1.1.3\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.3.cmml\\\"><mi id=\\\"S2.E1.m2.5.5.1.1.1.1.3.2\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.3.2.cmml\\\">\ud835\udda2\ud835\uddb1</mi><mo id=\\\"S2.E1.m2.5.5.1.1.1.1.3.1\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.3.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.E1.m2.5.5.1.1.1.1.3.3.2\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.3.3.1.cmml\\\"><mo id=\\\"S2.E1.m2.5.5.1.1.1.1.3.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.3.3.1.cmml\\\">(</mo><mi id=\\\"S2.E1.m2.3.3\\\" xref=\\\"S2.E1.m2.3.3.cmml\\\">f</mi><mo id=\\\"S2.E1.m2.5.5.1.1.1.1.3.3.2.2\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.3.3.1.cmml\\\">;</mo><mi class=\\\"ltx_font_mathcaligraphic\\\" id=\\\"S2.E1.m2.4.4\\\" xref=\\\"S2.E1.m2.4.4.cmml\\\">\ud835\udc9f</mi><mo id=\\\"S2.E1.m2.5.5.1.1.1.1.3.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.3.3.1.cmml\\\">)</mo></mrow></mrow></mrow><mo id=\\\"S2.E1.m2.5.5.1.1.1.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.cmml\\\">)</mo></mrow></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.E1.m2.5b\\\"><apply id=\\\"S2.E1.m2.5.5.cmml\\\" xref=\\\"S2.E1.m2.5.5\\\"><eq id=\\\"S2.E1.m2.5.5.2.cmml\\\" xref=\\\"S2.E1.m2.5.5.2\\\"></eq><csymbol cd=\\\"latexml\\\" id=\\\"S2.E1.m2.5.5.3.cmml\\\" xref=\\\"S2.E1.m2.5.5.3\\\">absent</csymbol><apply id=\\\"S2.E1.m2.5.5.1.cmml\\\" xref=\\\"S2.E1.m2.5.5.1\\\"><ci id=\\\"S2.E1.m2.5.5.1.2.cmml\\\" xref=\\\"S2.E1.m2.5.5.1.2\\\">\u22c5</ci><apply id=\\\"S2.E1.m2.5.5.1.3.cmml\\\" xref=\\\"S2.E1.m2.5.5.1.3\\\"><times id=\\\"S2.E1.m2.5.5.1.3.1.cmml\\\" xref=\\\"S2.E1.m2.5.5.1.3.1\\\"></times><ci id=\\\"S2.E1.m2.5.5.1.3.2.cmml\\\" xref=\\\"S2.E1.m2.5.5.1.3.2\\\">\ud835\udda0\ud835\uddbc\ud835\uddbc</ci><list id=\\\"S2.E1.m2.5.5.1.3.3.1.cmml\\\" xref=\\\"S2.E1.m2.5.5.1.3.3.2\\\"><ci id=\\\"S2.E1.m2.1.1.cmml\\\" xref=\\\"S2.E1.m2.1.1\\\">\ud835\udc53</ci><ci id=\\\"S2.E1.m2.2.2.cmml\\\" xref=\\\"S2.E1.m2.2.2\\\">\ud835\udc9f</ci></list></apply><apply id=\\\"S2.E1.m2.5.5.1.1.1.1.cmml\\\" xref=\\\"S2.E1.m2.5.5.1.1.1\\\"><minus id=\\\"S2.E1.m2.5.5.1.1.1.1.1.cmml\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.1\\\"></minus><cn id=\\\"S2.E1.m2.5.5.1.1.1.1.2.cmml\\\" type=\\\"integer\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.2\\\">1</cn><apply id=\\\"S2.E1.m2.5.5.1.1.1.1.3.cmml\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.3\\\"><times id=\\\"S2.E1.m2.5.5.1.1.1.1.3.1.cmml\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.3.1\\\"></times><ci id=\\\"S2.E1.m2.5.5.1.1.1.1.3.2.cmml\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.3.2\\\">\ud835\udda2\ud835\uddb1</ci><list id=\\\"S2.E1.m2.5.5.1.1.1.1.3.3.1.cmml\\\" xref=\\\"S2.E1.m2.5.5.1.1.1.1.3.3.2\\\"><ci id=\\\"S2.E1.m2.3.3.cmml\\\" xref=\\\"S2.E1.m2.3.3\\\">\ud835\udc53</ci><ci id=\\\"S2.E1.m2.4.4.cmml\\\" xref=\\\"S2.E1.m2.4.4\\\">\ud835\udc9f</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.E1.m2.5c\\\">\\\\displaystyle=\\\\mathsf{Acc}(f;\\\\mathcal{D})\\\\cdot(1-\\\\mathsf{CR}(f;\\\\mathcal{D}))</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.E1.m2.5d\\\">= sansserif_Acc ( italic_f ; caligraphic_D ) \u22c5 ( 1 - sansserif_CR ( italic_f ; caligraphic_D ) )</annotation></semantics></math></td>\\n<td class=\\\"ltx_eqn_cell ltx_eqn_center_padright\\\"></td>\\n<td class=\\\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\\\" rowspan=\\\"1\\\"><span class=\\\"ltx_tag ltx_tag_equation ltx_align_right\\\">(1)</span></td>\\n</tr></tbody>\\n<tbody id=\\\"S2.Ex1\\\"><tr class=\\\"ltx_equation ltx_eqn_row ltx_align_baseline\\\">\\n<td class=\\\"ltx_eqn_cell ltx_eqn_center_padleft\\\"></td>\\n<td class=\\\"ltx_td ltx_eqn_cell\\\"></td>\\n<td class=\\\"ltx_td ltx_align_left ltx_eqn_cell\\\"><math alttext=\\\"\\\\displaystyle=\\\\frac{\\\\texttt{\\\\#Correct}-\\\\texttt{\\\\#Consistently\\\\_Correct}}{%\\n\\\\texttt{\\\\#Total}}.\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.Ex1.m1.1\\\"><semantics id=\\\"S2.Ex1.m1.1a\\\"><mrow id=\\\"S2.Ex1.m1.1.1.1\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.cmml\\\"><mrow id=\\\"S2.Ex1.m1.1.1.1.1\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.cmml\\\"><mi id=\\\"S2.Ex1.m1.1.1.1.1.2\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.2.cmml\\\"></mi><mo id=\\\"S2.Ex1.m1.1.1.1.1.1\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.1.cmml\\\">=</mo><mstyle displaystyle=\\\"true\\\" id=\\\"S2.Ex1.m1.1.1.1.1.3\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.cmml\\\"><mfrac id=\\\"S2.Ex1.m1.1.1.1.1.3a\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.cmml\\\"><mrow id=\\\"S2.Ex1.m1.1.1.1.1.3.2\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.2.cmml\\\"><mtext class=\\\"ltx_mathvariant_monospace\\\" id=\\\"S2.Ex1.m1.1.1.1.1.3.2.2\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.2.2a.cmml\\\">#Correct</mtext><mo id=\\\"S2.Ex1.m1.1.1.1.1.3.2.1\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.2.1.cmml\\\">\u2212</mo><mtext class=\\\"ltx_mathvariant_monospace\\\" id=\\\"S2.Ex1.m1.1.1.1.1.3.2.3\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.2.3a.cmml\\\">#Consistently_Correct</mtext></mrow><mtext class=\\\"ltx_mathvariant_monospace\\\" id=\\\"S2.Ex1.m1.1.1.1.1.3.3\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.3a.cmml\\\">#Total</mtext></mfrac></mstyle></mrow><mo id=\\\"S2.Ex1.m1.1.1.1.2\\\" lspace=\\\"0em\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.cmml\\\">.</mo></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.Ex1.m1.1b\\\"><apply id=\\\"S2.Ex1.m1.1.1.1.1.cmml\\\" xref=\\\"S2.Ex1.m1.1.1.1\\\"><eq id=\\\"S2.Ex1.m1.1.1.1.1.1.cmml\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.1\\\"></eq><csymbol cd=\\\"latexml\\\" id=\\\"S2.Ex1.m1.1.1.1.1.2.cmml\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.2\\\">absent</csymbol><apply id=\\\"S2.Ex1.m1.1.1.1.1.3.cmml\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3\\\"><divide id=\\\"S2.Ex1.m1.1.1.1.1.3.1.cmml\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3\\\"></divide><apply id=\\\"S2.Ex1.m1.1.1.1.1.3.2.cmml\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.2\\\"><minus id=\\\"S2.Ex1.m1.1.1.1.1.3.2.1.cmml\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.2.1\\\"></minus><ci id=\\\"S2.Ex1.m1.1.1.1.1.3.2.2a.cmml\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.2.2\\\"><mtext class=\\\"ltx_mathvariant_monospace\\\" id=\\\"S2.Ex1.m1.1.1.1.1.3.2.2.cmml\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.2.2\\\">#Correct</mtext></ci><ci id=\\\"S2.Ex1.m1.1.1.1.1.3.2.3a.cmml\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.2.3\\\"><mtext class=\\\"ltx_mathvariant_monospace\\\" id=\\\"S2.Ex1.m1.1.1.1.1.3.2.3.cmml\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.2.3\\\">#Consistently_Correct</mtext></ci></apply><ci id=\\\"S2.Ex1.m1.1.1.1.1.3.3a.cmml\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.3\\\"><mtext class=\\\"ltx_mathvariant_monospace\\\" id=\\\"S2.Ex1.m1.1.1.1.1.3.3.cmml\\\" xref=\\\"S2.Ex1.m1.1.1.1.1.3.3\\\">#Total</mtext></ci></apply></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.Ex1.m1.1c\\\">\\\\displaystyle=\\\\frac{\\\\texttt{\\\\#Correct}-\\\\texttt{\\\\#Consistently\\\\_Correct}}{%\\n\\\\texttt{\\\\#Total}}.</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.Ex1.m1.1d\\\">= divide start_ARG #Correct - #Consistently_Correct end_ARG start_ARG #Total end_ARG .</annotation></semantics></math></td>\\n<td class=\\\"ltx_eqn_cell ltx_eqn_center_padright\\\"></td>\\n</tr></tbody>\\n</table>\\n<p class=\\\"ltx_p\\\" id=\\\"S2.SS1.p1.8\\\">When there is no ambiguity, we call it the memorization score. A larger score provides stronger evidence of memorization (i.e., a larger proportion of memorized examples in the given dataset).\\nSpecifically, a high <math alttext=\\\"\\\\mathsf{LiMem}(f;\\\\mathsf{Tr})\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS1.p1.3.m1.2\\\"><semantics id=\\\"S2.SS1.p1.3.m1.2a\\\"><mrow id=\\\"S2.SS1.p1.3.m1.2.3\\\" xref=\\\"S2.SS1.p1.3.m1.2.3.cmml\\\"><mi id=\\\"S2.SS1.p1.3.m1.2.3.2\\\" xref=\\\"S2.SS1.p1.3.m1.2.3.2.cmml\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</mi><mo id=\\\"S2.SS1.p1.3.m1.2.3.1\\\" xref=\\\"S2.SS1.p1.3.m1.2.3.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.SS1.p1.3.m1.2.3.3.2\\\" xref=\\\"S2.SS1.p1.3.m1.2.3.3.1.cmml\\\"><mo id=\\\"S2.SS1.p1.3.m1.2.3.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.3.m1.2.3.3.1.cmml\\\">(</mo><mi id=\\\"S2.SS1.p1.3.m1.1.1\\\" xref=\\\"S2.SS1.p1.3.m1.1.1.cmml\\\">f</mi><mo id=\\\"S2.SS1.p1.3.m1.2.3.3.2.2\\\" xref=\\\"S2.SS1.p1.3.m1.2.3.3.1.cmml\\\">;</mo><mi id=\\\"S2.SS1.p1.3.m1.2.2\\\" xref=\\\"S2.SS1.p1.3.m1.2.2.cmml\\\">\ud835\uddb3\ud835\uddcb</mi><mo id=\\\"S2.SS1.p1.3.m1.2.3.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.3.m1.2.3.3.1.cmml\\\">)</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS1.p1.3.m1.2b\\\"><apply id=\\\"S2.SS1.p1.3.m1.2.3.cmml\\\" xref=\\\"S2.SS1.p1.3.m1.2.3\\\"><times id=\\\"S2.SS1.p1.3.m1.2.3.1.cmml\\\" xref=\\\"S2.SS1.p1.3.m1.2.3.1\\\"></times><ci id=\\\"S2.SS1.p1.3.m1.2.3.2.cmml\\\" xref=\\\"S2.SS1.p1.3.m1.2.3.2\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</ci><list id=\\\"S2.SS1.p1.3.m1.2.3.3.1.cmml\\\" xref=\\\"S2.SS1.p1.3.m1.2.3.3.2\\\"><ci id=\\\"S2.SS1.p1.3.m1.1.1.cmml\\\" xref=\\\"S2.SS1.p1.3.m1.1.1\\\">\ud835\udc53</ci><ci id=\\\"S2.SS1.p1.3.m1.2.2.cmml\\\" xref=\\\"S2.SS1.p1.3.m1.2.2\\\">\ud835\uddb3\ud835\uddcb</ci></list></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.SS1.p1.3.m1.2c\\\">\\\\mathsf{LiMem}(f;\\\\mathsf{Tr})</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.SS1.p1.3.m1.2d\\\">sansserif_LiMem ( italic_f ; sansserif_Tr )</annotation></semantics></math> matches the characteristic behavior of human memorizing observed puzzles, and in this case we say <math alttext=\\\"f\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS1.p1.4.m2.1\\\"><semantics id=\\\"S2.SS1.p1.4.m2.1a\\\"><mi id=\\\"S2.SS1.p1.4.m2.1.1\\\" xref=\\\"S2.SS1.p1.4.m2.1.1.cmml\\\">f</mi><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS1.p1.4.m2.1b\\\"><ci id=\\\"S2.SS1.p1.4.m2.1.1.cmml\\\" xref=\\\"S2.SS1.p1.4.m2.1.1\\\">\ud835\udc53</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.SS1.p1.4.m2.1c\\\">f</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.SS1.p1.4.m2.1d\\\">italic_f</annotation></semantics></math> <span class=\\\"ltx_text ltx_font_italic\\\" id=\\\"S2.SS1.p1.8.1\\\">memorized</span> the training puzzles.\\nNote that the <math alttext=\\\"\\\\mathsf{Acc}(f;\\\\mathcal{D})\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS1.p1.5.m3.2\\\"><semantics id=\\\"S2.SS1.p1.5.m3.2a\\\"><mrow id=\\\"S2.SS1.p1.5.m3.2.3\\\" xref=\\\"S2.SS1.p1.5.m3.2.3.cmml\\\"><mi id=\\\"S2.SS1.p1.5.m3.2.3.2\\\" xref=\\\"S2.SS1.p1.5.m3.2.3.2.cmml\\\">\ud835\udda0\ud835\uddbc\ud835\uddbc</mi><mo id=\\\"S2.SS1.p1.5.m3.2.3.1\\\" xref=\\\"S2.SS1.p1.5.m3.2.3.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.SS1.p1.5.m3.2.3.3.2\\\" xref=\\\"S2.SS1.p1.5.m3.2.3.3.1.cmml\\\"><mo id=\\\"S2.SS1.p1.5.m3.2.3.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.5.m3.2.3.3.1.cmml\\\">(</mo><mi id=\\\"S2.SS1.p1.5.m3.1.1\\\" xref=\\\"S2.SS1.p1.5.m3.1.1.cmml\\\">f</mi><mo id=\\\"S2.SS1.p1.5.m3.2.3.3.2.2\\\" xref=\\\"S2.SS1.p1.5.m3.2.3.3.1.cmml\\\">;</mo><mi class=\\\"ltx_font_mathcaligraphic\\\" id=\\\"S2.SS1.p1.5.m3.2.2\\\" xref=\\\"S2.SS1.p1.5.m3.2.2.cmml\\\">\ud835\udc9f</mi><mo id=\\\"S2.SS1.p1.5.m3.2.3.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.5.m3.2.3.3.1.cmml\\\">)</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS1.p1.5.m3.2b\\\"><apply id=\\\"S2.SS1.p1.5.m3.2.3.cmml\\\" xref=\\\"S2.SS1.p1.5.m3.2.3\\\"><times id=\\\"S2.SS1.p1.5.m3.2.3.1.cmml\\\" xref=\\\"S2.SS1.p1.5.m3.2.3.1\\\"></times><ci id=\\\"S2.SS1.p1.5.m3.2.3.2.cmml\\\" xref=\\\"S2.SS1.p1.5.m3.2.3.2\\\">\ud835\udda0\ud835\uddbc\ud835\uddbc</ci><list id=\\\"S2.SS1.p1.5.m3.2.3.3.1.cmml\\\" xref=\\\"S2.SS1.p1.5.m3.2.3.3.2\\\"><ci id=\\\"S2.SS1.p1.5.m3.1.1.cmml\\\" xref=\\\"S2.SS1.p1.5.m3.1.1\\\">\ud835\udc53</ci><ci id=\\\"S2.SS1.p1.5.m3.2.2.cmml\\\" xref=\\\"S2.SS1.p1.5.m3.2.2\\\">\ud835\udc9f</ci></list></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.SS1.p1.5.m3.2c\\\">\\\\mathsf{Acc}(f;\\\\mathcal{D})</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.SS1.p1.5.m3.2d\\\">sansserif_Acc ( italic_f ; caligraphic_D )</annotation></semantics></math> factor is necessary, as there can be three types of behaviors: (i) solving by memorization, (ii) solving by reasoning, (iii) not solving (e.g., random guessing). A high <math alttext=\\\"\\\\mathsf{LiMem}(f;\\\\mathcal{D})\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS1.p1.6.m4.2\\\"><semantics id=\\\"S2.SS1.p1.6.m4.2a\\\"><mrow id=\\\"S2.SS1.p1.6.m4.2.3\\\" xref=\\\"S2.SS1.p1.6.m4.2.3.cmml\\\"><mi id=\\\"S2.SS1.p1.6.m4.2.3.2\\\" xref=\\\"S2.SS1.p1.6.m4.2.3.2.cmml\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</mi><mo id=\\\"S2.SS1.p1.6.m4.2.3.1\\\" xref=\\\"S2.SS1.p1.6.m4.2.3.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.SS1.p1.6.m4.2.3.3.2\\\" xref=\\\"S2.SS1.p1.6.m4.2.3.3.1.cmml\\\"><mo id=\\\"S2.SS1.p1.6.m4.2.3.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.6.m4.2.3.3.1.cmml\\\">(</mo><mi id=\\\"S2.SS1.p1.6.m4.1.1\\\" xref=\\\"S2.SS1.p1.6.m4.1.1.cmml\\\">f</mi><mo id=\\\"S2.SS1.p1.6.m4.2.3.3.2.2\\\" xref=\\\"S2.SS1.p1.6.m4.2.3.3.1.cmml\\\">;</mo><mi class=\\\"ltx_font_mathcaligraphic\\\" id=\\\"S2.SS1.p1.6.m4.2.2\\\" xref=\\\"S2.SS1.p1.6.m4.2.2.cmml\\\">\ud835\udc9f</mi><mo id=\\\"S2.SS1.p1.6.m4.2.3.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.6.m4.2.3.3.1.cmml\\\">)</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS1.p1.6.m4.2b\\\"><apply id=\\\"S2.SS1.p1.6.m4.2.3.cmml\\\" xref=\\\"S2.SS1.p1.6.m4.2.3\\\"><times id=\\\"S2.SS1.p1.6.m4.2.3.1.cmml\\\" xref=\\\"S2.SS1.p1.6.m4.2.3.1\\\"></times><ci id=\\\"S2.SS1.p1.6.m4.2.3.2.cmml\\\" xref=\\\"S2.SS1.p1.6.m4.2.3.2\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</ci><list id=\\\"S2.SS1.p1.6.m4.2.3.3.1.cmml\\\" xref=\\\"S2.SS1.p1.6.m4.2.3.3.2\\\"><ci id=\\\"S2.SS1.p1.6.m4.1.1.cmml\\\" xref=\\\"S2.SS1.p1.6.m4.1.1\\\">\ud835\udc53</ci><ci id=\\\"S2.SS1.p1.6.m4.2.2.cmml\\\" xref=\\\"S2.SS1.p1.6.m4.2.2\\\">\ud835\udc9f</ci></list></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.SS1.p1.6.m4.2c\\\">\\\\mathsf{LiMem}(f;\\\\mathcal{D})</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.SS1.p1.6.m4.2d\\\">sansserif_LiMem ( italic_f ; caligraphic_D )</annotation></semantics></math> indicates (i), but a low <math alttext=\\\"\\\\mathsf{LiMem}(f;\\\\mathcal{D})\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS1.p1.7.m5.2\\\"><semantics id=\\\"S2.SS1.p1.7.m5.2a\\\"><mrow id=\\\"S2.SS1.p1.7.m5.2.3\\\" xref=\\\"S2.SS1.p1.7.m5.2.3.cmml\\\"><mi id=\\\"S2.SS1.p1.7.m5.2.3.2\\\" xref=\\\"S2.SS1.p1.7.m5.2.3.2.cmml\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</mi><mo id=\\\"S2.SS1.p1.7.m5.2.3.1\\\" xref=\\\"S2.SS1.p1.7.m5.2.3.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.SS1.p1.7.m5.2.3.3.2\\\" xref=\\\"S2.SS1.p1.7.m5.2.3.3.1.cmml\\\"><mo id=\\\"S2.SS1.p1.7.m5.2.3.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.7.m5.2.3.3.1.cmml\\\">(</mo><mi id=\\\"S2.SS1.p1.7.m5.1.1\\\" xref=\\\"S2.SS1.p1.7.m5.1.1.cmml\\\">f</mi><mo id=\\\"S2.SS1.p1.7.m5.2.3.3.2.2\\\" xref=\\\"S2.SS1.p1.7.m5.2.3.3.1.cmml\\\">;</mo><mi class=\\\"ltx_font_mathcaligraphic\\\" id=\\\"S2.SS1.p1.7.m5.2.2\\\" xref=\\\"S2.SS1.p1.7.m5.2.2.cmml\\\">\ud835\udc9f</mi><mo id=\\\"S2.SS1.p1.7.m5.2.3.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.7.m5.2.3.3.1.cmml\\\">)</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS1.p1.7.m5.2b\\\"><apply id=\\\"S2.SS1.p1.7.m5.2.3.cmml\\\" xref=\\\"S2.SS1.p1.7.m5.2.3\\\"><times id=\\\"S2.SS1.p1.7.m5.2.3.1.cmml\\\" xref=\\\"S2.SS1.p1.7.m5.2.3.1\\\"></times><ci id=\\\"S2.SS1.p1.7.m5.2.3.2.cmml\\\" xref=\\\"S2.SS1.p1.7.m5.2.3.2\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</ci><list id=\\\"S2.SS1.p1.7.m5.2.3.3.1.cmml\\\" xref=\\\"S2.SS1.p1.7.m5.2.3.3.2\\\"><ci id=\\\"S2.SS1.p1.7.m5.1.1.cmml\\\" xref=\\\"S2.SS1.p1.7.m5.1.1\\\">\ud835\udc53</ci><ci id=\\\"S2.SS1.p1.7.m5.2.2.cmml\\\" xref=\\\"S2.SS1.p1.7.m5.2.2\\\">\ud835\udc9f</ci></list></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.SS1.p1.7.m5.2c\\\">\\\\mathsf{LiMem}(f;\\\\mathcal{D})</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.SS1.p1.7.m5.2d\\\">sansserif_LiMem ( italic_f ; caligraphic_D )</annotation></semantics></math> would only indicate (ii) if we separately check that <math alttext=\\\"\\\\mathsf{Acc}(f;\\\\mathcal{D})\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS1.p1.8.m6.2\\\"><semantics id=\\\"S2.SS1.p1.8.m6.2a\\\"><mrow id=\\\"S2.SS1.p1.8.m6.2.3\\\" xref=\\\"S2.SS1.p1.8.m6.2.3.cmml\\\"><mi id=\\\"S2.SS1.p1.8.m6.2.3.2\\\" xref=\\\"S2.SS1.p1.8.m6.2.3.2.cmml\\\">\ud835\udda0\ud835\uddbc\ud835\uddbc</mi><mo id=\\\"S2.SS1.p1.8.m6.2.3.1\\\" xref=\\\"S2.SS1.p1.8.m6.2.3.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.SS1.p1.8.m6.2.3.3.2\\\" xref=\\\"S2.SS1.p1.8.m6.2.3.3.1.cmml\\\"><mo id=\\\"S2.SS1.p1.8.m6.2.3.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.8.m6.2.3.3.1.cmml\\\">(</mo><mi id=\\\"S2.SS1.p1.8.m6.1.1\\\" xref=\\\"S2.SS1.p1.8.m6.1.1.cmml\\\">f</mi><mo id=\\\"S2.SS1.p1.8.m6.2.3.3.2.2\\\" xref=\\\"S2.SS1.p1.8.m6.2.3.3.1.cmml\\\">;</mo><mi class=\\\"ltx_font_mathcaligraphic\\\" id=\\\"S2.SS1.p1.8.m6.2.2\\\" xref=\\\"S2.SS1.p1.8.m6.2.2.cmml\\\">\ud835\udc9f</mi><mo id=\\\"S2.SS1.p1.8.m6.2.3.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p1.8.m6.2.3.3.1.cmml\\\">)</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS1.p1.8.m6.2b\\\"><apply id=\\\"S2.SS1.p1.8.m6.2.3.cmml\\\" xref=\\\"S2.SS1.p1.8.m6.2.3\\\"><times id=\\\"S2.SS1.p1.8.m6.2.3.1.cmml\\\" xref=\\\"S2.SS1.p1.8.m6.2.3.1\\\"></times><ci id=\\\"S2.SS1.p1.8.m6.2.3.2.cmml\\\" xref=\\\"S2.SS1.p1.8.m6.2.3.2\\\">\ud835\udda0\ud835\uddbc\ud835\uddbc</ci><list id=\\\"S2.SS1.p1.8.m6.2.3.3.1.cmml\\\" xref=\\\"S2.SS1.p1.8.m6.2.3.3.2\\\"><ci id=\\\"S2.SS1.p1.8.m6.1.1.cmml\\\" xref=\\\"S2.SS1.p1.8.m6.1.1\\\">\ud835\udc53</ci><ci id=\\\"S2.SS1.p1.8.m6.2.2.cmml\\\" xref=\\\"S2.SS1.p1.8.m6.2.2\\\">\ud835\udc9f</ci></list></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.SS1.p1.8.m6.2c\\\">\\\\mathsf{Acc}(f;\\\\mathcal{D})</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.SS1.p1.8.m6.2d\\\">sansserif_Acc ( italic_f ; caligraphic_D )</annotation></semantics></math> is high.</p>\\n</div>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.SS1.p2\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.SS1.p2.1\\\">To effectively measure the memorization score <math alttext=\\\"\\\\mathsf{LiMem}(f;\\\\mathcal{D})\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS1.p2.1.m1.2\\\"><semantics id=\\\"S2.SS1.p2.1.m1.2a\\\"><mrow id=\\\"S2.SS1.p2.1.m1.2.3\\\" xref=\\\"S2.SS1.p2.1.m1.2.3.cmml\\\"><mi id=\\\"S2.SS1.p2.1.m1.2.3.2\\\" xref=\\\"S2.SS1.p2.1.m1.2.3.2.cmml\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</mi><mo id=\\\"S2.SS1.p2.1.m1.2.3.1\\\" xref=\\\"S2.SS1.p2.1.m1.2.3.1.cmml\\\">\u2062</mo><mrow id=\\\"S2.SS1.p2.1.m1.2.3.3.2\\\" xref=\\\"S2.SS1.p2.1.m1.2.3.3.1.cmml\\\"><mo id=\\\"S2.SS1.p2.1.m1.2.3.3.2.1\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p2.1.m1.2.3.3.1.cmml\\\">(</mo><mi id=\\\"S2.SS1.p2.1.m1.1.1\\\" xref=\\\"S2.SS1.p2.1.m1.1.1.cmml\\\">f</mi><mo id=\\\"S2.SS1.p2.1.m1.2.3.3.2.2\\\" xref=\\\"S2.SS1.p2.1.m1.2.3.3.1.cmml\\\">;</mo><mi class=\\\"ltx_font_mathcaligraphic\\\" id=\\\"S2.SS1.p2.1.m1.2.2\\\" xref=\\\"S2.SS1.p2.1.m1.2.2.cmml\\\">\ud835\udc9f</mi><mo id=\\\"S2.SS1.p2.1.m1.2.3.3.2.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS1.p2.1.m1.2.3.3.1.cmml\\\">)</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS1.p2.1.m1.2b\\\"><apply id=\\\"S2.SS1.p2.1.m1.2.3.cmml\\\" xref=\\\"S2.SS1.p2.1.m1.2.3\\\"><times id=\\\"S2.SS1.p2.1.m1.2.3.1.cmml\\\" xref=\\\"S2.SS1.p2.1.m1.2.3.1\\\"></times><ci id=\\\"S2.SS1.p2.1.m1.2.3.2.cmml\\\" xref=\\\"S2.SS1.p2.1.m1.2.3.2\\\">\ud835\uddab\ud835\uddc2\ud835\uddac\ud835\uddbe\ud835\uddc6</ci><list id=\\\"S2.SS1.p2.1.m1.2.3.3.1.cmml\\\" xref=\\\"S2.SS1.p2.1.m1.2.3.3.2\\\"><ci id=\\\"S2.SS1.p2.1.m1.1.1.cmml\\\" xref=\\\"S2.SS1.p2.1.m1.1.1\\\">\ud835\udc53</ci><ci id=\\\"S2.SS1.p2.1.m1.2.2.cmml\\\" xref=\\\"S2.SS1.p2.1.m1.2.2\\\">\ud835\udc9f</ci></list></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.SS1.p2.1.m1.2c\\\">\\\\mathsf{LiMem}(f;\\\\mathcal{D})</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.SS1.p2.1.m1.2d\\\">sansserif_LiMem ( italic_f ; caligraphic_D )</annotation></semantics></math>, we need a principled way to (1) locally perturb the puzzle while maintaining its difficulty level; (2) compute the new correct answer after perturbation. Towards this goal, we design and implement a functional dataset based on the Knights and Knaves puzzles\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(Smullyan, <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib61\\\" title=\\\"\\\">1978</a>; Johnson-Laird &amp; Byrne, <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib29\\\" title=\\\"\\\">1990</a>)</cite>.</p>\\n</div>\\n<figure class=\\\"ltx_figure\\\" id=\\\"S2.F5\\\">\\n<div class=\\\"ltx_flex_figure\\\">\\n<div class=\\\"ltx_flex_cell ltx_flex_size_2\\\">\\n<figure class=\\\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top\\\" id=\\\"S2.F5.6\\\" style=\\\"width:160.4pt;\\\">\\n<div class=\\\"ltx_flex_figure\\\">\\n<div class=\\\"ltx_flex_cell ltx_flex_size_1\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_figure_panel ltx_img_square\\\" height=\\\"324\\\" id=\\\"S2.F5.1.g1\\\" src=\\\"x4.png\\\" width=\\\"374\\\"/></div>\\n<div class=\\\"ltx_flex_break\\\"></div>\\n<div class=\\\"ltx_flex_cell ltx_flex_size_1\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_square\\\" height=\\\"324\\\" id=\\\"S2.F5.2.g2\\\" src=\\\"x5.png\\\" width=\\\"374\\\"/></div>\\n<div class=\\\"ltx_flex_break\\\"></div>\\n<div class=\\\"ltx_flex_cell ltx_flex_size_1\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_square\\\" height=\\\"315\\\" id=\\\"S2.F5.3.g3\\\" src=\\\"x6.png\\\" width=\\\"374\\\"/></div>\\n<div class=\\\"ltx_flex_break\\\"></div>\\n<div class=\\\"ltx_flex_cell ltx_flex_size_1\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_square\\\" height=\\\"334\\\" id=\\\"S2.F5.4.g4\\\" src=\\\"x7.png\\\" width=\\\"374\\\"/></div>\\n</div>\\n<figcaption class=\\\"ltx_caption ltx_centering\\\"><span class=\\\"ltx_tag ltx_tag_figure\\\">Figure 4: </span><span class=\\\"ltx_text\\\" id=\\\"S2.F5.6.2.1\\\" style=\\\"font-size:90%;\\\">Train &amp; test accuracy increases over the epochs. FTed LLMs can achieve interpolation (<math alttext=\\\"\\\\approx 100\\\\%\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.F5.6.2.1.m1.1\\\"><semantics id=\\\"S2.F5.6.2.1.m1.1b\\\"><mrow id=\\\"S2.F5.6.2.1.m1.1.1\\\" xref=\\\"S2.F5.6.2.1.m1.1.1.cmml\\\"><mi id=\\\"S2.F5.6.2.1.m1.1.1.2\\\" xref=\\\"S2.F5.6.2.1.m1.1.1.2.cmml\\\"></mi><mo id=\\\"S2.F5.6.2.1.m1.1.1.1\\\" xref=\\\"S2.F5.6.2.1.m1.1.1.1.cmml\\\">\u2248</mo><mrow id=\\\"S2.F5.6.2.1.m1.1.1.3\\\" xref=\\\"S2.F5.6.2.1.m1.1.1.3.cmml\\\"><mn id=\\\"S2.F5.6.2.1.m1.1.1.3.2\\\" xref=\\\"S2.F5.6.2.1.m1.1.1.3.2.cmml\\\">100</mn><mo id=\\\"S2.F5.6.2.1.m1.1.1.3.1\\\" xref=\\\"S2.F5.6.2.1.m1.1.1.3.1.cmml\\\">%</mo></mrow></mrow><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.F5.6.2.1.m1.1c\\\"><apply id=\\\"S2.F5.6.2.1.m1.1.1.cmml\\\" xref=\\\"S2.F5.6.2.1.m1.1.1\\\"><approx id=\\\"S2.F5.6.2.1.m1.1.1.1.cmml\\\" xref=\\\"S2.F5.6.2.1.m1.1.1.1\\\"></approx><csymbol cd=\\\"latexml\\\" id=\\\"S2.F5.6.2.1.m1.1.1.2.cmml\\\" xref=\\\"S2.F5.6.2.1.m1.1.1.2\\\">absent</csymbol><apply id=\\\"S2.F5.6.2.1.m1.1.1.3.cmml\\\" xref=\\\"S2.F5.6.2.1.m1.1.1.3\\\"><csymbol cd=\\\"latexml\\\" id=\\\"S2.F5.6.2.1.m1.1.1.3.1.cmml\\\" xref=\\\"S2.F5.6.2.1.m1.1.1.3.1\\\">percent</csymbol><cn id=\\\"S2.F5.6.2.1.m1.1.1.3.2.cmml\\\" type=\\\"integer\\\" xref=\\\"S2.F5.6.2.1.m1.1.1.3.2\\\">100</cn></apply></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.F5.6.2.1.m1.1d\\\">\\\\approx 100\\\\%</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.F5.6.2.1.m1.1e\\\">\u2248 100 %</annotation></semantics></math> train accuracy) for easy tasks, e.g., 3/5-ppl puzzles. Llama3-8B struggles with CoT FT on K&amp;K tasks, likely due to limited model capacity.\\n</span></figcaption>\\n</figure>\\n</div>\\n<div class=\\\"ltx_flex_cell ltx_flex_size_2\\\">\\n<figure class=\\\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top\\\" id=\\\"S2.F5.9\\\" style=\\\"width:260.2pt;\\\">\\n<div class=\\\"ltx_flex_figure\\\">\\n<div class=\\\"ltx_flex_cell ltx_flex_size_1\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_figure_panel ltx_img_landscape\\\" height=\\\"137\\\" id=\\\"S2.F5.7.g1\\\" src=\\\"x8.png\\\" width=\\\"830\\\"/></div>\\n<div class=\\\"ltx_flex_break\\\"></div>\\n<div class=\\\"ltx_flex_cell ltx_flex_size_1\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\\\" height=\\\"137\\\" id=\\\"S2.F5.8.g2\\\" src=\\\"x9.png\\\" width=\\\"830\\\"/></div>\\n<div class=\\\"ltx_flex_break\\\"></div>\\n<div class=\\\"ltx_flex_cell ltx_flex_size_1\\\"><img alt=\\\"Refer to caption\\\" class=\\\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\\\" height=\\\"156\\\" id=\\\"S2.F5.9.g3\\\" src=\\\"x10.png\\\" width=\\\"830\\\"/></div>\\n</div>\\n<figcaption class=\\\"ltx_caption ltx_centering\\\"><span class=\\\"ltx_tag ltx_tag_figure\\\">Figure 5: </span><span class=\\\"ltx_text\\\" id=\\\"S2.F5.9.2.1\\\" style=\\\"font-size:90%;\\\">Fine-tuned LLMs generally exhibit\\nboth higher clean accuracy (x-axis) &amp; inconsistency ratio under perturbations (y-axis) on the train set than test set, resulting in a higher memorization score (color spectrum).\\nLLMs show stronger memorization under math-level perturbations (statement/leaf) than language level.\\nWe separately report memorization score in <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.F23.3\\\" title=\\\"In Fine-tuned models \u2023 E.2 Memorization Measurement \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">Fig.</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">23</span></a> and consistency ratio in <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.F24.5\\\" title=\\\"In Fine-tuned models \u2023 E.2 Memorization Measurement \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">Fig.</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">24</span></a>, and results under combined math &amp; language-level perturbations in <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#A5.F25.1\\\" title=\\\"In Fine-tuned models \u2023 E.2 Memorization Measurement \u2023 Appendix E Additional Experimental Results \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">Fig.</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">25</span></a>.\\n\\n</span></figcaption>\\n</figure>\\n</div>\\n</div>\\n</figure>\\n</section>\\n<section class=\\\"ltx_subsection\\\" id=\\\"S2.SS2\\\">\\n<h3 class=\\\"ltx_title ltx_title_subsection\\\">\\n<span class=\\\"ltx_tag ltx_tag_subsection\\\">2.2 </span>Knights and Knaves Reasoning Benchmark</h3>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.SS2.p1\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.SS2.p1.3\\\"><em class=\\\"ltx_emph ltx_font_italic\\\" id=\\\"S2.SS2.p1.3.1\\\">Knights and Knaves (K&amp;K)</em> is a type of logical puzzle, where the goal is to infer each character <math alttext=\\\"i\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS2.p1.1.m1.1\\\"><semantics id=\\\"S2.SS2.p1.1.m1.1a\\\"><mi id=\\\"S2.SS2.p1.1.m1.1.1\\\" xref=\\\"S2.SS2.p1.1.m1.1.1.cmml\\\">i</mi><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS2.p1.1.m1.1b\\\"><ci id=\\\"S2.SS2.p1.1.m1.1.1.cmml\\\" xref=\\\"S2.SS2.p1.1.m1.1.1\\\">\ud835\udc56</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.SS2.p1.1.m1.1c\\\">i</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.SS2.p1.1.m1.1d\\\">italic_i</annotation></semantics></math>\u2019s truthfulness <math alttext=\\\"B_{i}\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS2.p1.2.m2.1\\\"><semantics id=\\\"S2.SS2.p1.2.m2.1a\\\"><msub id=\\\"S2.SS2.p1.2.m2.1.1\\\" xref=\\\"S2.SS2.p1.2.m2.1.1.cmml\\\"><mi id=\\\"S2.SS2.p1.2.m2.1.1.2\\\" xref=\\\"S2.SS2.p1.2.m2.1.1.2.cmml\\\">B</mi><mi id=\\\"S2.SS2.p1.2.m2.1.1.3\\\" xref=\\\"S2.SS2.p1.2.m2.1.1.3.cmml\\\">i</mi></msub><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS2.p1.2.m2.1b\\\"><apply id=\\\"S2.SS2.p1.2.m2.1.1.cmml\\\" xref=\\\"S2.SS2.p1.2.m2.1.1\\\"><csymbol cd=\\\"ambiguous\\\" id=\\\"S2.SS2.p1.2.m2.1.1.1.cmml\\\" xref=\\\"S2.SS2.p1.2.m2.1.1\\\">subscript</csymbol><ci id=\\\"S2.SS2.p1.2.m2.1.1.2.cmml\\\" xref=\\\"S2.SS2.p1.2.m2.1.1.2\\\">\ud835\udc35</ci><ci id=\\\"S2.SS2.p1.2.m2.1.1.3.cmml\\\" xref=\\\"S2.SS2.p1.2.m2.1.1.3\\\">\ud835\udc56</ci></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.SS2.p1.2.m2.1c\\\">B_{i}</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.SS2.p1.2.m2.1d\\\">italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> (Boolean value) by judging the logical consistency of the statements <math alttext=\\\"S_{i}\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS2.p1.3.m3.1\\\"><semantics id=\\\"S2.SS2.p1.3.m3.1a\\\"><msub id=\\\"S2.SS2.p1.3.m3.1.1\\\" xref=\\\"S2.SS2.p1.3.m3.1.1.cmml\\\"><mi id=\\\"S2.SS2.p1.3.m3.1.1.2\\\" xref=\\\"S2.SS2.p1.3.m3.1.1.2.cmml\\\">S</mi><mi id=\\\"S2.SS2.p1.3.m3.1.1.3\\\" xref=\\\"S2.SS2.p1.3.m3.1.1.3.cmml\\\">i</mi></msub><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS2.p1.3.m3.1b\\\"><apply id=\\\"S2.SS2.p1.3.m3.1.1.cmml\\\" xref=\\\"S2.SS2.p1.3.m3.1.1\\\"><csymbol cd=\\\"ambiguous\\\" id=\\\"S2.SS2.p1.3.m3.1.1.1.cmml\\\" xref=\\\"S2.SS2.p1.3.m3.1.1\\\">subscript</csymbol><ci id=\\\"S2.SS2.p1.3.m3.1.1.2.cmml\\\" xref=\\\"S2.SS2.p1.3.m3.1.1.2\\\">\ud835\udc46</ci><ci id=\\\"S2.SS2.p1.3.m3.1.1.3.cmml\\\" xref=\\\"S2.SS2.p1.3.m3.1.1.3\\\">\ud835\udc56</ci></apply></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.SS2.p1.3.m3.1c\\\">S_{i}</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.SS2.p1.3.m3.1d\\\">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> they made.\\n<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#S2.F3\\\" title=\\\"In 2.1 Memorization Metrics for Reasoning Tasks \u2023 2 Measuring Memorization in Reasoning \u2023 On Memorization of Large Language Models in Logical Reasoning\\\"><span class=\\\"ltx_text ltx_ref_tag\\\">Fig.</span>\u00a0<span class=\\\"ltx_text ltx_ref_tag\\\">3</span></a> shows an example.</p>\\n</div>\\n<div class=\\\"ltx_para\\\" id=\\\"S2.SS2.p2\\\">\\n<p class=\\\"ltx_p\\\" id=\\\"S2.SS2.p2.6\\\">The principle underlying K&amp;K is the Boolean satisfiability problem (SAT)\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(<a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib7\\\" title=\\\"\\\">Boolean satisfiability problem, </a>)</cite>. SAT was the first problem proven to be NP-complete and many well-known problems can be translated into SAT, such as hardware and software verification and theorem proving\u00a0<cite class=\\\"ltx_cite ltx_citemacro_citep\\\">(SAT solver, <a class=\\\"ltx_ref\\\" href=\\\"https://arxiv.org/html/2410.23123v2#bib.bib59\\\" title=\\\"\\\">2024</a>)</cite>. Hence, the performance of a model on SAT (i.e., K&amp;K puzzles) can be important indicative of its reasoning capabilities.\\nSpecifically, consider a K&amp;K puzzle involving <math alttext=\\\"N\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS2.p2.1.m1.1\\\"><semantics id=\\\"S2.SS2.p2.1.m1.1a\\\"><mi id=\\\"S2.SS2.p2.1.m1.1.1\\\" xref=\\\"S2.SS2.p2.1.m1.1.1.cmml\\\">N</mi><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS2.p2.1.m1.1b\\\"><ci id=\\\"S2.SS2.p2.1.m1.1.1.cmml\\\" xref=\\\"S2.SS2.p2.1.m1.1.1\\\">\ud835\udc41</ci></annotation-xml><annotation encoding=\\\"application/x-tex\\\" id=\\\"S2.SS2.p2.1.m1.1c\\\">N</annotation><annotation encoding=\\\"application/x-llamapun\\\" id=\\\"S2.SS2.p2.1.m1.1d\\\">italic_N</annotation></semantics></math> people, and a possible Boolean value assignments to <math alttext=\\\"\\\\{B_{i}\\\\}_{i=1}^{N}\\\" class=\\\"ltx_Math\\\" display=\\\"inline\\\" id=\\\"S2.SS2.p2.2.m2.1\\\"><semantics id=\\\"S2.SS2.p2.2.m2.1a\\\"><msubsup id=\\\"S2.SS2.p2.2.m2.1.1\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.cmml\\\"><mrow id=\\\"S2.SS2.p2.2.m2.1.1.1.1.1\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.1.2.cmml\\\"><mo id=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.2\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.1.2.cmml\\\">{</mo><msub id=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1.cmml\\\"><mi id=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1.2\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1.2.cmml\\\">B</mi><mi id=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1.3\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1.3.cmml\\\">i</mi></msub><mo id=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.3\\\" stretchy=\\\"false\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.1.2.cmml\\\">}</mo></mrow><mrow id=\\\"S2.SS2.p2.2.m2.1.1.1.3\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.3.cmml\\\"><mi id=\\\"S2.SS2.p2.2.m2.1.1.1.3.2\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.3.2.cmml\\\">i</mi><mo id=\\\"S2.SS2.p2.2.m2.1.1.1.3.1\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.3.1.cmml\\\">=</mo><mn id=\\\"S2.SS2.p2.2.m2.1.1.1.3.3\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.3.3.cmml\\\">1</mn></mrow><mi id=\\\"S2.SS2.p2.2.m2.1.1.3\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.3.cmml\\\">N</mi></msubsup><annotation-xml encoding=\\\"MathML-Content\\\" id=\\\"S2.SS2.p2.2.m2.1b\\\"><apply id=\\\"S2.SS2.p2.2.m2.1.1.cmml\\\" xref=\\\"S2.SS2.p2.2.m2.1.1\\\"><csymbol cd=\\\"ambiguous\\\" id=\\\"S2.SS2.p2.2.m2.1.1.2.cmml\\\" xref=\\\"S2.SS2.p2.2.m2.1.1\\\">superscript</csymbol><apply id=\\\"S2.SS2.p2.2.m2.1.1.1.cmml\\\" xref=\\\"S2.SS2.p2.2.m2.1.1\\\"><csymbol cd=\\\"ambiguous\\\" id=\\\"S2.SS2.p2.2.m2.1.1.1.2.cmml\\\" xref=\\\"S2.SS2.p2.2.m2.1.1\\\">subscript</csymbol><set id=\\\"S2.SS2.p2.2.m2.1.1.1.1.2.cmml\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.1.1\\\"><apply id=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1.cmml\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1\\\"><csymbol cd=\\\"ambiguous\\\" id=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1.1.cmml\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1\\\">subscript</csymbol><ci id=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1.2.cmml\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1.2\\\">\ud835\udc35</ci><ci id=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1.3.cmml\\\" xref=\\\"S2.SS2.p2.2.m2.1.1.1.1.1.1.3\\\">\ud835\udc56</ci></apply></set><apply id=\\\"S2.SS2.p2.2.m2.1.1.1.3 ...\n\n(The output of the tool call (shortuuid identifier: 8BuGBrSdJGs9y82WvMKpyb) is too long! Only the first 100000 characters are shown here. The original output length is 531036 characters. The full output has been saved to the file /workspace/dumps/workspace/.overlong_tool_outputs/8BuGBrSdJGs9y82WvMKpyb.json. Please check this file carefully, as it may be very long!)",
            "tool_call_id": "call_u9kygqL0PsEiF4o32RNQOEDl"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_i2kUQVDcJLzJ6nHrYpAPhNNs",
                    "type": "function",
                    "function": {
                        "name": "local-web_search",
                        "arguments": "{\"query\":\"BBEH dataset logical reasoning BBEH benchmark\", \"num_results\": 10}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Title: google-deepmind/bbeh - GitHub\nLink: https://github.com/google-deepmind/bbeh\nSnippet: A new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning ...\nSitelinks: No sitelinks\n\nTitle: BIG-Bench Extra Hard - arXiv\nLink: https://arxiv.org/html/2502.19187v1\nSnippet: We introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation.\nSitelinks: No sitelinks\n\nTitle: [2502.19187] BIG-Bench Extra Hard - arXiv\nLink: https://arxiv.org/abs/2502.19187\nSnippet: We introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation.\nSitelinks: No sitelinks\n\nTitle: Paper page - BIG-Bench Extra Hard - Hugging Face\nLink: https://huggingface.co/papers/2502.19187\nSnippet: We introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation.\nSitelinks: No sitelinks\n\nTitle: (PDF) BIG-Bench Extra Hard - ResearchGate\nLink: https://www.researchgate.net/publication/389392420_BIG-Bench_Extra_Hard\nSnippet: To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning ...\nSitelinks: No sitelinks\n\nTitle: [Literature Review] BIG-Bench Extra Hard - Moonlight\nLink: https://www.themoonlight.io/en/review/big-bench-extra-hard\nSnippet: BBEH replaces each of the 23 tasks from the BBH dataset with novel tasks which maintain a similar reasoning domain but significantly increase in ...\nSitelinks: No sitelinks\n\nTitle: 90: Why AI's Reasoning Tests Keep Failing Us - Ksenia Se - Medium\nLink: https://kseniase.medium.com/90-why-ais-reasoning-tests-keep-failing-us-8c82df4fe4f1\nSnippet: In response, researchers have introduced BIG-Bench Extra Hard (BBEH), a new benchmark designed to push AI reasoning to its limits. But if ...\nSitelinks: No sitelinks\n\nTitle: Sweet spots for reasoning models - by Kendra Vant - Data Runs Deep\nLink: https://kendravant.substack.com/p/sweet-spots-for-reasoning-models\nSnippet: For me, the BBEH paper highlights beautifully why benchmarks saturate, why evaluations are hard and why both are vital. I hope I've prompted you ...\nSitelinks: No sitelinks\n\nTitle: Daily Papers - Hugging Face\nLink: https://huggingface.co/papers?q=benchmark%20suite%20BBH\nSnippet: Improvements include gains of up to 30 points on logical reasoning benchmarks ... BenchHub aggregates and automatically classifies benchmark datasets from ...\nSitelinks: No sitelinks\n\nTitle: LLM Benchmark Datasets for Training and Evaluation | by Sulbha Jain\nLink: https://medium.com/@sulbha.jindal/llm-benchmark-datasets-for-training-and-evaluation-1ec271129d7c\nSnippet: BIG-Bench, BIG-Bench Hard (BBH), BIG-Bench Extra Hard (BBEH): These are collections of challenging tasks aimed at pushing the boundaries of LLM ...\nSitelinks: No sitelinks\n",
            "tool_call_id": "call_i2kUQVDcJLzJ6nHrYpAPhNNs"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_IYK9uaJXCXufFASrHla3GDGy",
                    "type": "function",
                    "function": {
                        "name": "filesystem-write_file",
                        "arguments": "{\"path\":\"/workspace/dumps/workspace/datasets.tex\",\"content\":\"\\\\begin{table}[!ht]\\n    \\\\begin{center}\\n    \\\\begin{tabular}{lccc}\\n        \\\\toprule\\n        Dataset & Tasks & Trainable & Adjustable Difficulty\\\\\\\\\\n        \\\\midrule\\n        BBH & 23 & \\\\ding{55} & \\\\ding{55}\\\\\\\\\\n        Zebra Logic & 1 & \\\\ding{55} & \\\\ding{51}\\\\\\\\\\n        KOR-Bench & 5 & \\\\ding{55} & \\\\ding{55}\\\\\\\\\\n        K\\\\&K & 1 & \\\\ding{51} & \\\\ding{51}\\\\\\\\\\n        BBEH & 23 & \\\\ding{55} & \\\\ding{55}\\\\\\\\\\n        \\\\bottomrule\\n    \\\\end{tabular}\\n  \\\\end{center}\\n\\\\end{table}\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Successfully wrote to /workspace/dumps/workspace/datasets.tex\",\"annotations\":null}",
            "tool_call_id": "call_IYK9uaJXCXufFASrHla3GDGy"
        },
        {
            "role": "assistant",
            "content": "I\u2019ve created datasets.tex in the workspace with the requested LaTeX table.\n\nNotes on entries:\n- BBH: 23 tasks (the \u201chard\u201d subset of BIG-Bench), evaluation-only, no difficulty tiers.\n- Zebra Logic: single task family (logic grid puzzles), evaluation benchmark with multiple puzzle sizes (adjustable difficulty).\n- KOR-Bench: 5 task categories (Operation, Logic, Cipher, Puzzle, Counterfactual) as broader categorization; evaluation benchmark without train split or explicit difficulty tiers.\n- K&K (arXiv:2410.23123): single task family (Knights & Knaves), includes generated training data and supports multiple difficulty levels; thus trainable and adjustable difficulty.\n- BBEH: 23 tasks (one novel replacement per BBH task), evaluation-only, no difficulty tiers.\n\nFile path: /workspace/dumps/workspace/datasets.tex"
        }
    ]
}