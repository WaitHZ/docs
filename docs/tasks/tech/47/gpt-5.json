{
    "pass": true,
    "messages": [
        {
            "role": "user",
            "content": "I'm trying to reproduce the experiments from a deep learning paper (paper.pdf). Unfortunately, the paper does not provide an official implementation code repository. To work more efficiently, please help me find the unofficial implementation repository with the most stars on GitHub and update the existing `result.json` with the information."
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_slEIBiTx4mHRWo2y8F4mCaex",
                    "type": "function",
                    "function": {
                        "name": "filesystem-list_directory",
                        "arguments": "{\"path\":\"/workspace/dumps/workspace\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"[DIR] .pdf_tools_tempfiles\\n[FILE] paper.pdf\\n[FILE] result.json\",\"annotations\":null}",
            "tool_call_id": "call_slEIBiTx4mHRWo2y8F4mCaex"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_MgAJYSOHQIbubKf6qUtdhIct",
                    "type": "function",
                    "function": {
                        "name": "filesystem-read_file",
                        "arguments": "{\"path\":\"/workspace/dumps/workspace/result.json\",\"tail\":200,\"head\":200}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Error: Cannot specify both head and tail parameters simultaneously\",\"annotations\":null}",
            "tool_call_id": "call_MgAJYSOHQIbubKf6qUtdhIct"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_TVX9PbBq6hAp8o7ADXeOTW7a",
                    "type": "function",
                    "function": {
                        "name": "filesystem-read_file",
                        "arguments": "{\"path\":\"/workspace/dumps/workspace/result.json\",\"tail\":2000,\"head\":0}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"{\\n    \\\"URL\\\": \\\"\\\"\\n}\",\"annotations\":null}",
            "tool_call_id": "call_TVX9PbBq6hAp8o7ADXeOTW7a"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_iHzSvmN6Sj15IT4c5KzEpbF0",
                    "type": "function",
                    "function": {
                        "name": "pdf-tools-get_pdf_info",
                        "arguments": "{\"pdf_file_path\":\"/workspace/dumps/workspace/paper.pdf\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"PDF file information:\\nTotal pages: 14\\nTitle: \\nAuthor: \\nCreator: LaTeX with hyperref\\nCreation date: Unknown\\n\",\"annotations\":null}",
            "tool_call_id": "call_iHzSvmN6Sj15IT4c5KzEpbF0"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_fxOwp4PhVZXJGVQZXCzr2DQV",
                    "type": "function",
                    "function": {
                        "name": "pdf-tools-read_pdf_pages",
                        "arguments": "{\"pdf_file_path\":\"/workspace/dumps/workspace/paper.pdf\",\"start_page\":1,\"end_page\":2}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"PDF total pages: 14\\nExtracted page range: 1-2\\n\\n=== Page 1 ===\\nMixture-of-Depths: Dynamically allocating\\ncompute in transformer-based language\\nmodels\\nDavid Raposo1*, Sam Ritter1, Blake Richards1,2, Timothy Lillicrap1, Peter Conway Humphreys1and Adam\\nSantoro1*\\n1Google DeepMind,2McGill University & Mila,*Equal Contribution\\nTransformer-based language models spread FLOPs uniformly across input sequences. In this work we\\ndemonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute ) to specific\\npositions in a sequence, optimising the allocation along the sequence for different layers across the\\nmodel depth. Our method enforces a total compute budget by capping the number of tokens ( \ud835\udc58) that\\ncan participate in the self-attention and MLP computations at a given layer. The tokens to be processed\\nare determined by the network using a top- \ud835\udc58routing mechanism. Since \ud835\udc58is defined a priori, this\\nsimple procedure uses a static computation graph with known tensor sizes, unlike other conditional\\ncomputation techniques. Nevertheless, since the identities of the \ud835\udc58tokens are fluid, this method can\\nexpend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure\\nis entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do\\nmodels trained in this way learn to dynamically allocate compute, they do so efficiently. These models\\nmatch baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of\\nthe FLOPs per forward pass, and can be upwards of 50% faster to step during post-training sampling.\\n1. Introduction\\nNotallproblemsrequirethesameamountoftimeorefforttosolve. Analogously, inlanguagemodeling\\nnot all tokens and sequences require the same time or effort to accurately make a prediction. And\\nyet, transformer models expend the same amount of compute per token in a forward pass. Ideally,\\ntransformers would use smaller total compute budgets by not spending compute unnecessarily.\\nConditional computation is a technique that tries to reduce total compute by expending it only\\nwhen needed (Bengio et al., 2016; Bengio, 2013; Bengio et al., 2013). Various algorithms offer\\nsolutions to when and how much compute should be used (Ainslie et al., 2023; Bapna et al., 2020;\\nFedus et al., 2022). However, general formulations of this challenging problem may not work well with\\nexisting hardware constraints since they tend to introduce dynamic computation graphs (Dehghani\\net al., 2018; Graves, 2016). The most promising conditional computation methods may instead be\\nthose that are harmonious with our current hardware stack, which prioritizes static computation\\ngraphs, and known tensor sizes that are selected to maximize hardware utilization.\\nHere we consider the problem of language modeling using a staticcompute budget that can be\\nmade less than that used by a vanilla transformer. The network must learn how to dynamically allocate\\nthe available compute by making decisions per-token, in each layer, about where to spend compute\\nfrom the available budget. In our implementation total compute is user defined and unchanging\\nprior to training, rather than being a function of the network\u2019s on-the-fly decisions. Thus, hardware\\nefficiency gains\u2014such as reduced memory footprint, or reduced FLOPs per forward pass\u2014can be\\nanticipated and exploited ahead of time. As we will show, these gains can be had without sacrificing\\noverall performance.\\nCorresponding author(s): draposo@google.com, adamsantoro@google.com\\n\u00a92024 Google DeepMind. All rights reservedarXiv:2404.02258v1  [cs.LG]  2 Apr 2024\\n\\n=== Page 2 ===\\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\\nWe leverage an approach akin to Mixture of Experts (MoE) transformers, in which dynamic\\ntoken-level routing decisions are made across the network depth. Departing from MoE, we choose\\nto either apply a computation to a token (as would be the case for a standard transformer), or pass\\nit through a residual connection (remaining unchanged and saving compute). Also in contrast to\\nMoE, we apply this routing to both forward MLPs and multi-head attention. Since this therefore\\nalso impacts the keys and queries we process, the routing makes decisions not only about which\\ntokens to update, but also which tokens are made available to attend to. We refer to this strategy\\nas Mixture-of-Depths (MoD) to emphasize how individual tokens pass through different numbers of\\nlayers, or blocks, through the depth of the transformer (see figure 1).\\nThe MoD technique also allows one to trade-off performance with speed. On the one hand, one\\ncan train an MoD transformer that improves upon vanilla transformers by as much as 1.5%on the\\nfinal log probability training objective for equivalent training FLOPs (isoFLOP), and while taking an\\nequivalent amount of wall-clock time to train. On the other hand, one can train an MoD transformer\\nthat achieves training loss parity with an isoFLOP optimal vanilla transformer, but which uses a\\nfraction of the FLOPs (upwards of 50%) per forward pass, and hence is faster to step. Together, these\\nresults imply that MoD transformers learn to route intelligently (i.e., skipping computations that are\\nunnecessary) since they can achieve equal or better log probabilities per sequence despite a smaller\\nFLOP footprint per forward pass.\\n2. Background\\nThe transformer architecture has become the workhorse of a revolution in practical artificial in-\\ntelligence, bringing unprecedented capabilities at the cost of expensive training runs and serving\\nprocedures. This has spurred tremendous interest in making transformer architectures more effi-\\ncient (Gupta and Agrawal, 2021; Tay et al., 2020). One of the promising approaches is conditional\\ncomputation , whereby learned mechanisms determine when and how to expend computation. This\\nterminology was introduced by Bengio (2013), and the concept was explored further over the next\\nseveral years (Bengio et al., 2016, 2013; Cho and Bengio, 2014; Graves, 2016; Jernite et al., 2017;\\nWang et al., 2017).\\nA wide variety of recent work has developed conditional computation methods for transformers.\\nSome of this work focuses on \\\"early exiting\\\", that is, learning to decide when to end computation on\\na given token, allowing the token to skip any remaining transformer layers after the exit decision\\nis made (Elbayad et al., 2019; Liu et al., 2021; Schuster et al., 2022). In MoD, unlike in early-exit\\nmethods, a token can skip middle layers, then be updated via self-attention with tokens that that\\nhave gone through all the middle layers. We speculate that this might be a useful property.\\nOther work has developed methods for iterating transformer layers with shared weights for\\nan adaptive number of steps (Dehghani et al., 2018; Simoulin and Crabb\u00e9, 2021). Bolya et al.\\n(2023) developed a method for choosing tokens to merge when running inference on a trained vision\\ntransformer which notably requires no learning. Lei et al. (2023) make use of conditional computation\\nin a fine tuning setting by building on adapter approaches (He et al., 2021) to learn to skip blocks of\\nfrozen pre-trained weights in favor of running only a small fine-tuned adapter.\\nCoLT5 (Ainslie et al., 2023) uses conditional routing to select whether a given token will pass\\nthrough a heavy or light pathway for each feedforward layer. Further, they use the same routing\\nmechanism to select whether a token will attend to all other tokens or to a select few, as in Guo et al.\\n(2022). Like MoD, CoLT5 uses soft top-k for making routing decisions. However, CoLT5 focuses on\\na encoder-decoder setting, and thus does need to contend with the problem of efficient sequential\\ndecoding given the non-causal nature of the top-k operation. In contrast, our current work with\\n2\",\"annotations\":null}",
            "tool_call_id": "call_fxOwp4PhVZXJGVQZXCzr2DQV"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_2uCkZYV2xk1LvAao7VAvd4bT",
                    "type": "function",
                    "function": {
                        "name": "github-search_repositories",
                        "arguments": "{\"minimal_output\": true, \"page\": 1, \"perPage\": 50, \"query\": \"Mixture-of-Depths in:name,description,readme\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"{\\\"total_count\\\":733,\\\"incomplete_results\\\":false,\\\"items\\\":[{\\\"id\\\":785664688,\\\"name\\\":\\\"Mixture-of-depths\\\",\\\"full_name\\\":\\\"astramind-ai/Mixture-of-depths\\\",\\\"description\\\":\\\"Unofficial implementation for the paper \\\\\\\"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\\\\\\\"\\\",\\\"html_url\\\":\\\"https://github.com/astramind-ai/Mixture-of-depths\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":173,\\\"forks_count\\\":8,\\\"open_issues_count\\\":5,\\\"updated_at\\\":\\\"2025-10-11T08:37:33Z\\\",\\\"created_at\\\":\\\"2024-04-12T11:07:48Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":782269816,\\\"name\\\":\\\"Mixture-of-Depths\\\",\\\"full_name\\\":\\\"kyegomez/Mixture-of-Depths\\\",\\\"description\\\":\\\"Implementation of the paper: \\\\\\\"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\\\\\\\"\\\",\\\"html_url\\\":\\\"https://github.com/kyegomez/Mixture-of-Depths\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":107,\\\"forks_count\\\":7,\\\"open_issues_count\\\":8,\\\"updated_at\\\":\\\"2025-10-11T08:37:35Z\\\",\\\"created_at\\\":\\\"2024-04-05T00:51:20Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":1002668918,\\\"name\\\":\\\"mixture_of_recursions\\\",\\\"full_name\\\":\\\"raymin0223/mixture_of_recursions\\\",\\\"description\\\":\\\"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation (NeurIPS 2025)\\\",\\\"html_url\\\":\\\"https://github.com/raymin0223/mixture_of_recursions\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":481,\\\"forks_count\\\":70,\\\"open_issues_count\\\":1,\\\"updated_at\\\":\\\"2025-10-15T08:47:44Z\\\",\\\"created_at\\\":\\\"2025-06-16T00:20:03Z\\\",\\\"topics\\\":[\\\"adaptive-computation\\\",\\\"early-exiting\\\",\\\"kv-cache\\\",\\\"llm\\\",\\\"recursive-transformers\\\",\\\"router\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":783744120,\\\"name\\\":\\\"mergoo\\\",\\\"full_name\\\":\\\"Leeroo-AI/mergoo\\\",\\\"description\\\":\\\"A library for easily merging multiple LLM experts, and efficiently train the merged LLM.\\\",\\\"html_url\\\":\\\"https://github.com/Leeroo-AI/mergoo\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":493,\\\"forks_count\\\":35,\\\"open_issues_count\\\":11,\\\"updated_at\\\":\\\"2025-10-14T21:34:15Z\\\",\\\"created_at\\\":\\\"2024-04-08T13:40:41Z\\\",\\\"topics\\\":[\\\"artificial-intelligence\\\",\\\"fine-tuning\\\",\\\"generative-ai\\\",\\\"large-language-models\\\",\\\"llm\\\",\\\"lora\\\",\\\"merge\\\",\\\"mixture-of-adapters\\\",\\\"mixture-of-experts\\\",\\\"multi-model\\\",\\\"nlp\\\",\\\"open-source\\\",\\\"transformers\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":1009063652,\\\"name\\\":\\\"Awesome-Efficient-Arch\\\",\\\"full_name\\\":\\\"weigao266/Awesome-Efficient-Arch\\\",\\\"description\\\":\\\"Speed Always Wins: A Survey on Efficient Architectures for Large Language Models\\\",\\\"html_url\\\":\\\"https://github.com/weigao266/Awesome-Efficient-Arch\\\",\\\"stargazers_count\\\":343,\\\"forks_count\\\":30,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-10-16T13:25:01Z\\\",\\\"created_at\\\":\\\"2025-06-26T14:15:07Z\\\",\\\"topics\\\":[\\\"diffusion-llm\\\",\\\"efficient-architectures\\\",\\\"linear-attention\\\",\\\"linear-models\\\",\\\"linear-rnn\\\",\\\"mamba\\\",\\\"mixture-of-experts\\\",\\\"moe\\\",\\\"sparse-attention\\\",\\\"state-space-model\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":782847289,\\\"name\\\":\\\"mixture-of-depths\\\",\\\"full_name\\\":\\\"sramshetty/mixture-of-depths\\\",\\\"description\\\":\\\"An unofficial implementation of \\\\\\\"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\\\\\\\"\\\",\\\"html_url\\\":\\\"https://github.com/sramshetty/mixture-of-depths\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":36,\\\"forks_count\\\":3,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-09-05T01:19:59Z\\\",\\\"created_at\\\":\\\"2024-04-06T07:51:03Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":317358544,\\\"name\\\":\\\"GaMMA\\\",\\\"full_name\\\":\\\"AI4EPS/GaMMA\\\",\\\"description\\\":\\\"GaMMA: Earthquake Phase Association using a Bayesian Gaussian Mixture Model\\\",\\\"html_url\\\":\\\"https://github.com/AI4EPS/GaMMA\\\",\\\"language\\\":\\\"Jupyter Notebook\\\",\\\"stargazers_count\\\":88,\\\"forks_count\\\":35,\\\"open_issues_count\\\":15,\\\"updated_at\\\":\\\"2025-09-22T02:52:20Z\\\",\\\"created_at\\\":\\\"2020-11-30T22:10:02Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":786911662,\\\"name\\\":\\\"A-Survey-on-Mixture-of-Experts-in-LLMs\\\",\\\"full_name\\\":\\\"withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs\\\",\\\"description\\\":\\\"[TKDE'25] The official GitHub page for the survey paper \\\\\\\"A Survey on Mixture of Experts in Large Language Models\\\\\\\".\\\",\\\"html_url\\\":\\\"https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs\\\",\\\"stargazers_count\\\":432,\\\"forks_count\\\":22,\\\"open_issues_count\\\":5,\\\"updated_at\\\":\\\"2025-10-15T12:38:51Z\\\",\\\"created_at\\\":\\\"2024-04-15T14:35:32Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":786779390,\\\"name\\\":\\\"infini-transformer\\\",\\\"full_name\\\":\\\"dingo-actual/infini-transformer\\\",\\\"description\\\":\\\"PyTorch implementation of Infini-Transformer from \\\\\\\"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\\\\\\\" (https://arxiv.org/abs/2404.07143)\\\",\\\"html_url\\\":\\\"https://github.com/dingo-actual/infini-transformer\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":291,\\\"forks_count\\\":23,\\\"open_issues_count\\\":2,\\\"updated_at\\\":\\\"2025-10-04T19:51:39Z\\\",\\\"created_at\\\":\\\"2024-04-15T09:37:36Z\\\",\\\"topics\\\":[\\\"attention-mechanism\\\",\\\"deep-learning\\\",\\\"infini-attention\\\",\\\"long-context\\\",\\\"mixture-of-depths\\\",\\\"pytorch\\\",\\\"transformers\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":119560854,\\\"name\\\":\\\"Trimix-analyzer\\\",\\\"full_name\\\":\\\"captainigloo/Trimix-analyzer\\\",\\\"description\\\":\\\"Trimix-analyser\\\",\\\"html_url\\\":\\\"https://github.com/captainigloo/Trimix-analyzer\\\",\\\"language\\\":\\\"C++\\\",\\\"stargazers_count\\\":21,\\\"forks_count\\\":11,\\\"open_issues_count\\\":4,\\\"updated_at\\\":\\\"2025-04-23T02:41:31Z\\\",\\\"created_at\\\":\\\"2018-01-30T16:13:01Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":333108833,\\\"name\\\":\\\"SATC\\\",\\\"full_name\\\":\\\"popgenDK/SATC\\\",\\\"description\\\":\\\"Sex assignment through coverage\\\",\\\"html_url\\\":\\\"https://github.com/popgenDK/SATC\\\",\\\"language\\\":\\\"R\\\",\\\"stargazers_count\\\":20,\\\"forks_count\\\":10,\\\"open_issues_count\\\":11,\\\"updated_at\\\":\\\"2025-08-29T15:50:56Z\\\",\\\"created_at\\\":\\\"2021-01-26T14:23:33Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":803133604,\\\"name\\\":\\\"Mixture-of-Depths\\\",\\\"full_name\\\":\\\"Mixture-AI/Mixture-of-Depths\\\",\\\"description\\\":\\\"Google DeepMind: Mixture of Depths Unofficial Implementation.\\\",\\\"html_url\\\":\\\"https://github.com/Mixture-AI/Mixture-of-Depths\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":12,\\\"forks_count\\\":3,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-08-18T02:56:53Z\\\",\\\"created_at\\\":\\\"2024-05-20T06:14:28Z\\\",\\\"topics\\\":[\\\"conditional-computation\\\",\\\"llm\\\",\\\"mixture-of-depths\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":629520282,\\\"name\\\":\\\"Efficient-LLMs-Survey\\\",\\\"full_name\\\":\\\"AIoT-MLSys-Lab/Efficient-LLMs-Survey\\\",\\\"description\\\":\\\"[TMLR 2024] Efficient Large Language Models: A Survey\\\",\\\"html_url\\\":\\\"https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey\\\",\\\"stargazers_count\\\":1220,\\\"forks_count\\\":97,\\\"open_issues_count\\\":2,\\\"updated_at\\\":\\\"2025-10-09T08:07:18Z\\\",\\\"created_at\\\":\\\"2023-04-18T13:32:01Z\\\",\\\"topics\\\":[\\\"efficient-deep-learning\\\",\\\"generative-ai\\\",\\\"large-language-models\\\",\\\"machine-learning-systems\\\",\\\"survey\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":351354962,\\\"name\\\":\\\"CAPTRA\\\",\\\"full_name\\\":\\\"HalfSummer11/CAPTRA\\\",\\\"description\\\":\\\"[ICCV 2021, Oral] Official PyTorch implementation of CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds\\\",\\\"html_url\\\":\\\"https://github.com/HalfSummer11/CAPTRA\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":123,\\\"forks_count\\\":21,\\\"open_issues_count\\\":1,\\\"updated_at\\\":\\\"2025-10-16T06:22:30Z\\\",\\\"created_at\\\":\\\"2021-03-25T08:04:37Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":874565778,\\\"name\\\":\\\"Router-Tuning-Mixture-of-Depths\\\",\\\"full_name\\\":\\\"CASE-Lab-UMD/Router-Tuning-Mixture-of-Depths\\\",\\\"description\\\":\\\"The open-source Mixture of Depths code and the official implementation of the paper \\\\\\\"Router-Tuning: A Simple and Effective Approach for Enabling Dynamic Depth in Transformers. (EMNLP 2025)\\\\\\\"\\\",\\\"html_url\\\":\\\"https://github.com/CASE-Lab-UMD/Router-Tuning-Mixture-of-Depths\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":16,\\\"forks_count\\\":3,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-10-01T23:31:40Z\\\",\\\"created_at\\\":\\\"2024-10-18T04:11:52Z\\\",\\\"topics\\\":[\\\"deep-learning\\\",\\\"large-language-models\\\",\\\"machine-learning\\\",\\\"mixture-of-depths\\\",\\\"neural-network\\\",\\\"parameter-efficient-fine-tuning\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":564867527,\\\"name\\\":\\\"SHIKE\\\",\\\"full_name\\\":\\\"jinyan-06/SHIKE\\\",\\\"description\\\":\\\"Long-Tailed Visual Recognition via Self-Heterogeneous Integration with Knowledge Excavation (CVPR 2023)\\\",\\\"html_url\\\":\\\"https://github.com/jinyan-06/SHIKE\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":39,\\\"forks_count\\\":5,\\\"open_issues_count\\\":5,\\\"updated_at\\\":\\\"2025-09-06T05:52:27Z\\\",\\\"created_at\\\":\\\"2022-11-11T17:32:28Z\\\",\\\"topics\\\":[\\\"cvpr2023\\\",\\\"imbalanced-learning\\\",\\\"long-tailed-learning\\\",\\\"long-tailed-recognition\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":646410686,\\\"name\\\":\\\"LLaMA-Factory\\\",\\\"full_name\\\":\\\"hiyouga/LLaMA-Factory\\\",\\\"description\\\":\\\"Unified Efficient Fine-Tuning of 100+ LLMs \\\\u0026 VLMs (ACL 2024)\\\",\\\"html_url\\\":\\\"https://github.com/hiyouga/LLaMA-Factory\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":60290,\\\"forks_count\\\":7306,\\\"open_issues_count\\\":708,\\\"updated_at\\\":\\\"2025-10-16T13:33:08Z\\\",\\\"created_at\\\":\\\"2023-05-28T10:09:12Z\\\",\\\"topics\\\":[\\\"agent\\\",\\\"ai\\\",\\\"deepseek\\\",\\\"fine-tuning\\\",\\\"gemma\\\",\\\"gpt\\\",\\\"instruction-tuning\\\",\\\"large-language-models\\\",\\\"llama\\\",\\\"llama3\\\",\\\"llm\\\",\\\"lora\\\",\\\"moe\\\",\\\"nlp\\\",\\\"peft\\\",\\\"qlora\\\",\\\"quantization\\\",\\\"qwen\\\",\\\"rlhf\\\",\\\"transformers\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":853227707,\\\"name\\\":\\\"Awesome-Token-Compress\\\",\\\"full_name\\\":\\\"daixiangzi/Awesome-Token-Compress\\\",\\\"description\\\":\\\"A paper list of some recent  works about Token Compress for Vit and VLM\\\",\\\"html_url\\\":\\\"https://github.com/daixiangzi/Awesome-Token-Compress\\\",\\\"stargazers_count\\\":693,\\\"forks_count\\\":30,\\\"open_issues_count\\\":1,\\\"updated_at\\\":\\\"2025-10-16T08:40:18Z\\\",\\\"created_at\\\":\\\"2024-09-06T08:35:07Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":897319488,\\\"name\\\":\\\"p-MoD\\\",\\\"full_name\\\":\\\"MCG-NJU/p-MoD\\\",\\\"description\\\":\\\"[ICCV 2025] p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay\\\",\\\"html_url\\\":\\\"https://github.com/MCG-NJU/p-MoD\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":43,\\\"forks_count\\\":2,\\\"open_issues_count\\\":1,\\\"updated_at\\\":\\\"2025-10-03T13:42:44Z\\\",\\\"created_at\\\":\\\"2024-12-02T12:31:35Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":110621731,\\\"name\\\":\\\"Symbolic-Regression\\\",\\\"full_name\\\":\\\"twhughes/Symbolic-Regression\\\",\\\"description\\\":\\\"predicting equations from raw data with deep learning\\\",\\\"html_url\\\":\\\"https://github.com/twhughes/Symbolic-Regression\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":57,\\\"forks_count\\\":10,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-08-28T09:18:47Z\\\",\\\"created_at\\\":\\\"2017-11-14T01:01:39Z\\\",\\\"topics\\\":[\\\"decoding\\\",\\\"equation\\\",\\\"neural-network\\\",\\\"symbolic-regression\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":43090439,\\\"name\\\":\\\"carcassonne-shading\\\",\\\"full_name\\\":\\\"tomkinsc/carcassonne-shading\\\",\\\"description\\\":\\\"Small JS app to manipulate SVG files for printing or laser-cutting tiles for the board game Carcassonne.\\\",\\\"html_url\\\":\\\"https://github.com/tomkinsc/carcassonne-shading\\\",\\\"language\\\":\\\"JavaScript\\\",\\\"stargazers_count\\\":24,\\\"forks_count\\\":6,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2024-12-07T12:18:28Z\\\",\\\"created_at\\\":\\\"2015-09-24T20:08:33Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"gh-pages\\\"},{\\\"id\\\":214648555,\\\"name\\\":\\\"HumanPoseMemo\\\",\\\"full_name\\\":\\\"chingswy/HumanPoseMemo\\\",\\\"description\\\":\\\"Memo about 3d human pose estimation, record of datasets, papers, codes.\\\",\\\"html_url\\\":\\\"https://github.com/chingswy/HumanPoseMemo\\\",\\\"language\\\":\\\"TeX\\\",\\\"stargazers_count\\\":175,\\\"forks_count\\\":20,\\\"open_issues_count\\\":1,\\\"updated_at\\\":\\\"2025-07-29T07:46:26Z\\\",\\\"created_at\\\":\\\"2019-10-12T13:04:52Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":460351353,\\\"name\\\":\\\"Research-Space\\\",\\\"full_name\\\":\\\"DicardoX/Research-Space\\\",\\\"description\\\":\\\"This repository is established to store personal notes and annotated papers during daily research.\\\",\\\"html_url\\\":\\\"https://github.com/DicardoX/Research-Space\\\",\\\"stargazers_count\\\":155,\\\"forks_count\\\":13,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-10-14T07:49:44Z\\\",\\\"created_at\\\":\\\"2022-02-17T08:45:55Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":138352180,\\\"name\\\":\\\"mimic_classify\\\",\\\"full_name\\\":\\\"rajatsen91/mimic_classify\\\",\\\"description\\\":\\\"Mimic \\\\u0026 Classify CI Test\\\",\\\"html_url\\\":\\\"https://github.com/rajatsen91/mimic_classify\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":6,\\\"forks_count\\\":5,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2022-03-14T18:44:33Z\\\",\\\"created_at\\\":\\\"2018-06-22T22:07:46Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":816686884,\\\"name\\\":\\\"pytorch-mixtures\\\",\\\"full_name\\\":\\\"jaisidhsingh/pytorch-mixtures\\\",\\\"description\\\":\\\"One-stop solutions for Mixture of Experts and Mixture of Depth modules in PyTorch.\\\",\\\"html_url\\\":\\\"https://github.com/jaisidhsingh/pytorch-mixtures\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":25,\\\"forks_count\\\":1,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-10-14T22:09:37Z\\\",\\\"created_at\\\":\\\"2024-06-18T08:05:26Z\\\",\\\"topics\\\":[\\\"language-model\\\",\\\"large-language-models\\\",\\\"mixture-of-depths\\\",\\\"mixture-of-experts\\\",\\\"nlp\\\",\\\"pip-package\\\",\\\"pytorch\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":255601979,\\\"name\\\":\\\"Awesome-Self-Supervised-Papers\\\",\\\"full_name\\\":\\\"dev-sungman/Awesome-Self-Supervised-Papers\\\",\\\"description\\\":\\\"Paper bank for Self-Supervised Learning\\\",\\\"html_url\\\":\\\"https://github.com/dev-sungman/Awesome-Self-Supervised-Papers\\\",\\\"stargazers_count\\\":580,\\\"forks_count\\\":58,\\\"open_issues_count\\\":1,\\\"updated_at\\\":\\\"2025-10-15T20:29:22Z\\\",\\\"created_at\\\":\\\"2020-04-14T12:23:38Z\\\",\\\"topics\\\":[\\\"awesome\\\",\\\"cvpr2020\\\",\\\"self-supervised\\\",\\\"self-supervised-learning\\\",\\\"sota\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":218470804,\\\"name\\\":\\\"awesome-image-registration\\\",\\\"full_name\\\":\\\"Awesome-Image-Registration-Organization/awesome-image-registration\\\",\\\"description\\\":\\\"image registration related books, papers, videos, and toolboxes \\\",\\\"html_url\\\":\\\"https://github.com/Awesome-Image-Registration-Organization/awesome-image-registration\\\",\\\"stargazers_count\\\":1451,\\\"forks_count\\\":174,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-10-15T07:15:15Z\\\",\\\"created_at\\\":\\\"2019-10-30T07:47:02Z\\\",\\\"topics\\\":[\\\"awesome\\\",\\\"awesome-list\\\",\\\"image\\\",\\\"image-processing\\\",\\\"image-registration\\\",\\\"medical-images\\\",\\\"natural-images\\\",\\\"point-cloud-registration\\\",\\\"remote-sensing-image\\\",\\\"slam\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":623618719,\\\"name\\\":\\\"Awesome-Foundation-Models\\\",\\\"full_name\\\":\\\"uncbiag/Awesome-Foundation-Models\\\",\\\"description\\\":\\\"A curated list of foundation models for vision and language tasks\\\",\\\"html_url\\\":\\\"https://github.com/uncbiag/Awesome-Foundation-Models\\\",\\\"stargazers_count\\\":1102,\\\"forks_count\\\":57,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-10-16T12:53:54Z\\\",\\\"created_at\\\":\\\"2023-04-04T18:27:06Z\\\",\\\"topics\\\":[\\\"foundation-models\\\",\\\"large-language-models\\\",\\\"multimodal-models\\\",\\\"transformer-models\\\",\\\"vision-transformer\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":785695282,\\\"name\\\":\\\"voltronformers\\\",\\\"full_name\\\":\\\"OpenAccess-AI-Collective/voltronformers\\\",\\\"description\\\":\\\"Assembling the best SotA AI techniques into a unified model\\\",\\\"html_url\\\":\\\"https://github.com/OpenAccess-AI-Collective/voltronformers\\\",\\\"stargazers_count\\\":12,\\\"forks_count\\\":1,\\\"open_issues_count\\\":1,\\\"updated_at\\\":\\\"2025-01-29T21:36:09Z\\\",\\\"created_at\\\":\\\"2024-04-12T12:30:21Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":165509352,\\\"name\\\":\\\"awesome-transformer-nlp\\\",\\\"full_name\\\":\\\"cedrickchee/awesome-transformer-nlp\\\",\\\"description\\\":\\\"A curated list of NLP resources focused on Transformer networks, attention mechanism, GPT, BERT, ChatGPT, LLMs, and transfer learning.\\\",\\\"html_url\\\":\\\"https://github.com/cedrickchee/awesome-transformer-nlp\\\",\\\"stargazers_count\\\":1117,\\\"forks_count\\\":132,\\\"open_issues_count\\\":1,\\\"updated_at\\\":\\\"2025-10-04T12:42:05Z\\\",\\\"created_at\\\":\\\"2019-01-13T13:37:08Z\\\",\\\"topics\\\":[\\\"attention-mechanism\\\",\\\"awesome\\\",\\\"awesome-list\\\",\\\"bert\\\",\\\"chatgpt\\\",\\\"gpt-2\\\",\\\"gpt-3\\\",\\\"gpt-4\\\",\\\"language-model\\\",\\\"llama\\\",\\\"natural-language-processing\\\",\\\"neural-networks\\\",\\\"nlp\\\",\\\"pre-trained-language-models\\\",\\\"transfer-learning\\\",\\\"transformer\\\",\\\"xlnet\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":18563333,\\\"name\\\":\\\"romstools\\\",\\\"full_name\\\":\\\"trondkr/romstools\\\",\\\"description\\\":\\\"A variety of tools for plotting and analysing output from the Regional Ocean Model (ROMS)\\\",\\\"html_url\\\":\\\"https://github.com/trondkr/romstools\\\",\\\"language\\\":\\\"Jupyter Notebook\\\",\\\"stargazers_count\\\":35,\\\"forks_count\\\":20,\\\"open_issues_count\\\":1,\\\"updated_at\\\":\\\"2025-02-03T23:15:46Z\\\",\\\"created_at\\\":\\\"2014-04-08T15:19:59Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":421092333,\\\"name\\\":\\\"tapas\\\",\\\"full_name\\\":\\\"wfinsinger/tapas\\\",\\\"description\\\":\\\"Trend And PeAkS (TAPAS) analysis of temporal series (e.g. paleoecological records) to assess long-term trend and detect events (and eventually estimate event-return intervals)\\\",\\\"html_url\\\":\\\"https://github.com/wfinsinger/tapas\\\",\\\"language\\\":\\\"R\\\",\\\"stargazers_count\\\":11,\\\"forks_count\\\":2,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2024-08-19T08:12:43Z\\\",\\\"created_at\\\":\\\"2021-10-25T16:02:09Z\\\",\\\"topics\\\":[\\\"paleoecology\\\",\\\"paleofire\\\",\\\"peak-detection\\\",\\\"timeseries-analysis\\\",\\\"trend\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":849748717,\\\"name\\\":\\\"praxis\\\",\\\"full_name\\\":\\\"0-5788719150923125/praxis\\\",\\\"description\\\":\\\"for all, none\\\",\\\"html_url\\\":\\\"https://github.com/0-5788719150923125/praxis\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":15,\\\"forks_count\\\":2,\\\"open_issues_count\\\":14,\\\"updated_at\\\":\\\"2025-10-15T04:48:03Z\\\",\\\"created_at\\\":\\\"2024-08-30T07:05:55Z\\\",\\\"topics\\\":[\\\"ai\\\",\\\"devops\\\",\\\"docker\\\",\\\"hivemind\\\",\\\"ml\\\",\\\"moe\\\",\\\"python\\\",\\\"ryan\\\",\\\"transformers\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":930300532,\\\"name\\\":\\\"UniMoD\\\",\\\"full_name\\\":\\\"showlab/UniMoD\\\",\\\"description\\\":\\\"The code repository of UniMoD\\\",\\\"html_url\\\":\\\"https://github.com/showlab/UniMoD\\\",\\\"stargazers_count\\\":10,\\\"forks_count\\\":0,\\\"open_issues_count\\\":1,\\\"updated_at\\\":\\\"2025-06-29T04:19:10Z\\\",\\\"created_at\\\":\\\"2025-02-10T12:14:15Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":90380488,\\\"name\\\":\\\"CNVcaller\\\",\\\"full_name\\\":\\\"JiangYuLab/CNVcaller\\\",\\\"html_url\\\":\\\"https://github.com/JiangYuLab/CNVcaller\\\",\\\"language\\\":\\\"Perl\\\",\\\"stargazers_count\\\":42,\\\"forks_count\\\":14,\\\"open_issues_count\\\":13,\\\"updated_at\\\":\\\"2025-07-19T15:44:21Z\\\",\\\"created_at\\\":\\\"2017-05-05T13:49:32Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":679673318,\\\"name\\\":\\\"awesome-adaptive-computation\\\",\\\"full_name\\\":\\\"koayon/awesome-adaptive-computation\\\",\\\"description\\\":\\\"A curated reading list of research in Adaptive Computation, Inference-Time Computation \\\\u0026 Mixture of Experts (MoE).\\\",\\\"html_url\\\":\\\"https://github.com/koayon/awesome-adaptive-computation\\\",\\\"stargazers_count\\\":154,\\\"forks_count\\\":10,\\\"open_issues_count\\\":1,\\\"updated_at\\\":\\\"2025-10-04T14:21:32Z\\\",\\\"created_at\\\":\\\"2023-08-17T11:11:32Z\\\",\\\"topics\\\":[\\\"adaptive-computation\\\",\\\"computer-vision\\\",\\\"machine-learning\\\",\\\"mixture-of-experts\\\",\\\"nlp\\\",\\\"pytorch\\\",\\\"tensorflow\\\",\\\"transformers\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":784596511,\\\"name\\\":\\\"Mixture-of-Depths-Jax\\\",\\\"full_name\\\":\\\"dtunai/Mixture-of-Depths-Jax\\\",\\\"description\\\":\\\"Jax module for the paper: \\\\\\\"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\\\\\\\"\\\",\\\"html_url\\\":\\\"https://github.com/dtunai/Mixture-of-Depths-Jax\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":3,\\\"forks_count\\\":0,\\\"open_issues_count\\\":13,\\\"updated_at\\\":\\\"2024-07-18T20:50:24Z\\\",\\\"created_at\\\":\\\"2024-04-10T06:56:02Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":126408408,\\\"name\\\":\\\"ScubaDiving_SE\\\",\\\"full_name\\\":\\\"garc0062/ScubaDiving_SE\\\",\\\"html_url\\\":\\\"https://github.com/garc0062/ScubaDiving_SE\\\",\\\"language\\\":\\\"Java\\\",\\\"stargazers_count\\\":0,\\\"forks_count\\\":3,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2018-05-29T12:00:09Z\\\",\\\"created_at\\\":\\\"2018-03-22T23:53:41Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":172594443,\\\"name\\\":\\\"BAMMSC\\\",\\\"full_name\\\":\\\"CHPGenetics/BAMMSC\\\",\\\"html_url\\\":\\\"https://github.com/CHPGenetics/BAMMSC\\\",\\\"language\\\":\\\"C++\\\",\\\"stargazers_count\\\":10,\\\"forks_count\\\":3,\\\"open_issues_count\\\":1,\\\"updated_at\\\":\\\"2024-01-06T13:08:49Z\\\",\\\"created_at\\\":\\\"2019-02-25T22:19:34Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":513566064,\\\"name\\\":\\\"wf-human-variation\\\",\\\"full_name\\\":\\\"epi2me-labs/wf-human-variation\\\",\\\"html_url\\\":\\\"https://github.com/epi2me-labs/wf-human-variation\\\",\\\"language\\\":\\\"Nextflow\\\",\\\"stargazers_count\\\":134,\\\"forks_count\\\":72,\\\"open_issues_count\\\":19,\\\"updated_at\\\":\\\"2025-09-12T06:00:35Z\\\",\\\"created_at\\\":\\\"2022-07-13T15:02:36Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":783865510,\\\"name\\\":\\\"Mixture-Of-Depths\\\",\\\"full_name\\\":\\\"paulilioaica/Mixture-Of-Depths\\\",\\\"html_url\\\":\\\"https://github.com/paulilioaica/Mixture-Of-Depths\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":2,\\\"forks_count\\\":0,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-06-29T09:06:21Z\\\",\\\"created_at\\\":\\\"2024-04-08T18:16:21Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":808493339,\\\"name\\\":\\\"Efficient-Diffusion-Model-Survey\\\",\\\"full_name\\\":\\\"AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey\\\",\\\"description\\\":\\\"[TMLR 2025] Efficient Diffusion Models: A Survey\\\",\\\"html_url\\\":\\\"https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey\\\",\\\"stargazers_count\\\":117,\\\"forks_count\\\":5,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-10-15T10:01:10Z\\\",\\\"created_at\\\":\\\"2024-05-31T07:26:39Z\\\",\\\"topics\\\":[\\\"diffusion-models\\\",\\\"efficient-deep-learning\\\",\\\"generative-ai\\\",\\\"machine-learning-systems\\\",\\\"survey\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":95475048,\\\"name\\\":\\\"remixt\\\",\\\"full_name\\\":\\\"amcpherson/remixt\\\",\\\"description\\\":\\\"Clone-specific genomic structure estimation in cancer\\\",\\\"html_url\\\":\\\"https://github.com/amcpherson/remixt\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":8,\\\"forks_count\\\":3,\\\"open_issues_count\\\":18,\\\"updated_at\\\":\\\"2025-07-03T17:39:56Z\\\",\\\"created_at\\\":\\\"2017-06-26T18:08:10Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":814422048,\\\"name\\\":\\\"Easy_Fintune_LLM_using_SageMaker_with_LLama_Factory\\\",\\\"full_name\\\":\\\"aws-samples/Easy_Fintune_LLM_using_SageMaker_with_LLama_Factory\\\",\\\"html_url\\\":\\\"https://github.com/aws-samples/Easy_Fintune_LLM_using_SageMaker_with_LLama_Factory\\\",\\\"language\\\":\\\"Jupyter Notebook\\\",\\\"stargazers_count\\\":14,\\\"forks_count\\\":2,\\\"open_issues_count\\\":1,\\\"updated_at\\\":\\\"2025-07-27T12:21:13Z\\\",\\\"created_at\\\":\\\"2024-06-13T01:44:44Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":223818793,\\\"name\\\":\\\"awesome-bioinformatics-benchmarks\\\",\\\"full_name\\\":\\\"j-andrews7/awesome-bioinformatics-benchmarks\\\",\\\"description\\\":\\\"A curated and summarized list of bioinformatics bench-marking papers and resources.\\\",\\\"html_url\\\":\\\"https://github.com/j-andrews7/awesome-bioinformatics-benchmarks\\\",\\\"stargazers_count\\\":344,\\\"forks_count\\\":62,\\\"open_issues_count\\\":23,\\\"updated_at\\\":\\\"2025-10-11T10:12:42Z\\\",\\\"created_at\\\":\\\"2019-11-24T22:20:17Z\\\",\\\"topics\\\":[\\\"benchmarking-study\\\",\\\"bioinformatics\\\",\\\"bioinformatics-benchmarks\\\",\\\"collection\\\",\\\"genomics\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":215084762,\\\"name\\\":\\\"heinsen_routing\\\",\\\"full_name\\\":\\\"glassroom/heinsen_routing\\\",\\\"description\\\":\\\"Reference implementation of \\\\\\\"An Algorithm for Routing Vectors in Sequences\\\\\\\" (Heinsen, 2022) and \\\\\\\"An Algorithm for Routing Capsules in All Domains\\\\\\\" (Heinsen, 2019), for composing deep neural networks.\\\",\\\"html_url\\\":\\\"https://github.com/glassroom/heinsen_routing\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":172,\\\"forks_count\\\":17,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-07-23T21:12:52Z\\\",\\\"created_at\\\":\\\"2019-10-14T15:52:45Z\\\",\\\"topics\\\":[\\\"associative-memory\\\",\\\"capsule-network\\\",\\\"capsules\\\",\\\"deep-learning\\\",\\\"dense-associative-memory\\\",\\\"em-routing\\\",\\\"paper\\\",\\\"pytorch\\\",\\\"routing-algorithm\\\",\\\"society-of-mind\\\",\\\"vector-routing\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"master\\\"},{\\\"id\\\":736966436,\\\"name\\\":\\\"cfDNA_deconv_benchmark\\\",\\\"full_name\\\":\\\"LiymLab/cfDNA_deconv_benchmark\\\",\\\"html_url\\\":\\\"https://github.com/LiymLab/cfDNA_deconv_benchmark\\\",\\\"language\\\":\\\"Jupyter Notebook\\\",\\\"stargazers_count\\\":3,\\\"forks_count\\\":2,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-09-20T12:55:47Z\\\",\\\"created_at\\\":\\\"2023-12-29T11:55:13Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":318541083,\\\"name\\\":\\\"so-fronts\\\",\\\"full_name\\\":\\\"so-wise/so-fronts\\\",\\\"description\\\":\\\"Full scripts to generate figures for \\\\\\\"Defining Southern Ocean fronts using unsupervised classification\\\\\\\" https://doi.org/10.5194/os-17-1545-2021\\\",\\\"html_url\\\":\\\"https://github.com/so-wise/so-fronts\\\",\\\"language\\\":\\\"Python\\\",\\\"stargazers_count\\\":10,\\\"forks_count\\\":2,\\\"open_issues_count\\\":2,\\\"updated_at\\\":\\\"2025-08-13T09:15:47Z\\\",\\\"created_at\\\":\\\"2020-12-04T14:29:10Z\\\",\\\"topics\\\":[\\\"fronts\\\",\\\"gaussian-mixture-models\\\",\\\"oceanography\\\",\\\"sobel-edge-detector\\\"],\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":922129389,\\\"name\\\":\\\"Notable-LLM-Research-Papers\\\",\\\"full_name\\\":\\\"itsual/Notable-LLM-Research-Papers\\\",\\\"description\\\":\\\"Curated list of research papers published in 2024 related to Large Language Models (LLM)\\\",\\\"html_url\\\":\\\"https://github.com/itsual/Notable-LLM-Research-Papers\\\",\\\"stargazers_count\\\":30,\\\"forks_count\\\":4,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-10-07T02:01:54Z\\\",\\\"created_at\\\":\\\"2025-01-25T11:53:42Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"},{\\\"id\\\":855021859,\\\"name\\\":\\\"mixplat\\\",\\\"full_name\\\":\\\"seasideGargantua/mixplat\\\",\\\"html_url\\\":\\\"https://github.com/seasideGargantua/mixplat\\\",\\\"language\\\":\\\"Cuda\\\",\\\"stargazers_count\\\":10,\\\"forks_count\\\":0,\\\"open_issues_count\\\":0,\\\"updated_at\\\":\\\"2025-09-30T06:23:28Z\\\",\\\"created_at\\\":\\\"2024-09-10T07:02:47Z\\\",\\\"private\\\":false,\\\"fork\\\":false,\\\"archived\\\":false,\\\"default_branch\\\":\\\"main\\\"}]}\",\"annotations\":null}",
            "tool_call_id": "call_2uCkZYV2xk1LvAao7VAvd4bT"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_KsEJrMDjKDU5WNF1NtBjZIjZ",
                    "type": "function",
                    "function": {
                        "name": "filesystem-write_file",
                        "arguments": "{\"path\":\"/workspace/dumps/workspace/result.json\",\"content\":\"{\\n    \\\"URL\\\": \\\"https://github.com/astramind-ai/Mixture-of-depths\\\"\\n}\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Successfully wrote to /workspace/dumps/workspace/result.json\",\"annotations\":null}",
            "tool_call_id": "call_KsEJrMDjKDU5WNF1NtBjZIjZ"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "call_iCFdLnWUUceSG7nDd2ySFZH3",
                    "type": "function",
                    "function": {
                        "name": "local-claim_done",
                        "arguments": "{}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "you have claimed the task is done!",
            "tool_call_id": "call_iCFdLnWUUceSG7nDd2ySFZH3"
        },
        {
            "role": "assistant",
            "content": "I found the most-starred unofficial implementation on GitHub for the paper \u201cMixture-of-Depths: Dynamically allocating compute in transformer-based language models\u201d and updated result.json accordingly.\n\nTop unofficial repo:\n- https://github.com/astramind-ai/Mixture-of-depths (most stars among Mixture-of-Depths unofficial implementations)\n\nresult.json has been updated to:\n{\n    \"URL\": \"https://github.com/astramind-ai/Mixture-of-depths\"\n}"
        }
    ]
}