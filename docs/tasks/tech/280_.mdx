---
title: "LLM Training Dataset"
description: "Organize pre-training datasets used by LLaMA and GPT-Neo into the ptdata sheet, sorted by size descending."
---

<Card>
<div className="tools-container">
<div className="mcp-servers-container">
<div className="mcp-servers-title">
MCP Servers
</div>
<div className="mcp-servers-grid">
<div className="mcp-server-item">
<img src="/icons/fetch.png" style={{height: "20px", width: "20px", margin: 0, padding: 0, display: 'inline-block'}} />
<span className="mcp-server-name">fetch</span>
</div>
<div className="mcp-server-item">
<img src="/icons/scholar.png" style={{height: "20px", width: "20px", margin: 0, padding: 0, display: 'inline-block'}} />
<span className="mcp-server-name">scholarly</span>
</div>
<div className="mcp-server-item">
<img src="/icons/google_sheet.png" style={{height: "20px", width: "20px", margin: 0, padding: 0, display: 'inline-block'}} />
<span className="mcp-server-name">google_sheet</span>
</div>
<div className="mcp-server-item">
<img src="/icons/playwright.png" style={{height: "20px", width: "20px", margin: 0, padding: 0, display: 'inline-block'}} />
<span className="mcp-server-name">playwright_with_chunk</span>
</div>
</div>
</div>
<div className="local-tools-container">
<div className="mcp-servers-title">
Local Tools
</div>
<div className="local-tools-grid">
<div className="local-tool-item">
<img src="/icons/history.png" style={{height: "20px", width: "20px", margin: 0, padding: 0, display: 'inline-block'}} />
<span className="local-tool-name">history</span>
</div>
<div className="local-tool-item">
<img src="/icons/google_search.png" style={{height: "20px", width: "20px", margin: 0, padding: 0, display: 'inline-block'}} />
<span className="local-tool-name">web_search</span>
</div>
<div className="local-tool-item">
<img src="/icons/claim_done.png" style={{height: "20px", width: "20px", margin: 0, padding: 0, display: 'inline-block'}} />
<span className="local-tool-name">claim_done</span>
</div>
<div className="local-tool-item">
<img src="/icons/python.png" style={{height: "20px", width: "20px", margin: 0, padding: 0, display: 'inline-block'}} />
<span className="local-tool-name">python_execute</span>
</div>
<div className="local-tool-item">
<Icon icon="bars-progress" size={20} color="#4286f6" />
<span className="local-tool-name">manage_context</span>
</div>
<div className="local-tool-item">
<Icon icon="filter-list" size={20} color="#4286f6" />
<span className="local-tool-name">handle_overlong_tool_outputs</span>
</div>
</div>
</div>
</div>
</Card>








## Instruction

I've been learning about large-scale language models recently, and I've decided to train a decoder-only language model on my own. The first step is to prepare pre-training data, so I need your help organizing the pre-training data for llama and gpt-neo into the `ptdata` sheet in `LLM Pre-training Data` spreadsheet. The pre-training data should be sorted in descending order by data size, with columns named `name`, `use in llm` (the value should only include llama and gpt-neo; if used by both models, it should be either gpt-neo or llama), `size` (the numeric value in GB, but do not include "GB" in cell), and `link` in order. The link to huggingface is preferred; if huggingface doesn't have the dataset, we'll try to provide other links.

## Model Trajectory
