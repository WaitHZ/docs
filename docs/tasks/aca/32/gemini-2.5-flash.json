{
    "pass": false,
    "messages": [
        {
            "role": "user",
            "content": "I am a graduate student in computer vision, and I have several papers on image generation in my workspace. I need your help to summarize the experimental results of the models in these papers on ImageNet 256 and store the results in a LaTeX table. The table should include: Model Name (When multiple models are included, report the one with the lowest FID result in the paper.), Method Category (determine if it belongs to VAE, GAN, Diffusion, Flow-based, or AR), Model Parameters, FID-50K, and Inception Score. Finally, please save the results in survey.tex in my workspace (only the LaTeX table source code is needed), and the order of the table should be in descending order of FID. The example structure of the table is format.tex."
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "function-call-3983557058090073373",
                    "type": "function",
                    "function": {
                        "name": "filesystem-list_directory",
                        "arguments": "{\"path\":\"/workspace/dumps/workspace\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"[DIR] .pdf_tools_tempfiles\\n[FILE] 2212.09748v2.pdf\\n[FILE] 2406.06525v1.pdf\\n[FILE] format.tex\",\"annotations\":null}",
            "tool_call_id": "function-call-3983557058090073373"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "function-call-7621436806546376493",
                    "type": "function",
                    "function": {
                        "name": "filesystem-read_file",
                        "arguments": "{\"path\":\"/workspace/dumps/workspace/format.tex\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"\\\\begin{table}[!ht]\\n    \\\\centering\\n    \\\\begin{tabular}{ccccc}\\n        \\\\toprule\\n        Model & Class & \\\\#Param & FID-50K & Inception Score \\\\\\\\\\n        \\\\midrule\\n        A & GAN & 300M & 40.20 & 150.00 \\\\\\\\\\n        B & Diffusion & 2B & 20.20 & 190.00 \\\\\\\\\\n        \\\\bottomrule\\n    \\\\end{tabular}\\n\\\\end{table}\",\"annotations\":null}",
            "tool_call_id": "function-call-7621436806546376493"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "function-call-15889490510775380703",
                    "type": "function",
                    "function": {
                        "name": "pdf-tools-read_pdf_pages",
                        "arguments": "{\"pdf_file_path\":\"/workspace/dumps/workspace/2212.09748v2.pdf\",\"start_page\":1,\"end_page\":5}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"PDF total pages: 25\\nExtracted page range: 1-5\\n\\n=== Page 1 ===\\nScalable Diffusion Models with Transformers\\nWilliam Peebles*\\nUC BerkeleySaining Xie\\nNew York University\\nFigure 1. Diffusion models with transformer backbones achieve state-of-the-art image quality. We show selected samples from two\\nof our class-conditional DiT-XL/2 models trained on ImageNet at 512 \\u0002512 and 256\\u0002256 resolution, respectively.\\nAbstract\\nWe explore a new class of diffusion models based on the\\ntransformer architecture. We train latent diffusion models\\nof images, replacing the commonly-used U-Net backbone\\nwith a transformer that operates on latent patches. We an-\\nalyze the scalability of our Diffusion Transformers (DiTs)\\nthrough the lens of forward pass complexity as measured by\\nG\ufb02ops. We \ufb01nd that DiTs with higher G\ufb02ops\u2014through in-\\ncreased transformer depth/width or increased number of in-\\nput tokens\u2014consistently have lower FID. In addition to pos-\\nsessing good scalability properties, our largest DiT-XL/2\\nmodels outperform all prior diffusion models on the class-\\nconditional ImageNet 512 \\u0002512 and 256\\u0002256 benchmarks,\\nachieving a state-of-the-art FID of 2.27 on the latter.1. Introduction\\nMachine learning is experiencing a renaissance powered\\nby transformers. Over the past \ufb01ve years, neural architec-\\ntures for natural language processing [8, 42], vision [10]\\nand several other domains have largely been subsumed by\\ntransformers [60]. Many classes of image-level genera-\\ntive models remain holdouts to the trend, though\u2014while\\ntransformers see widespread use in autoregressive mod-\\nels [3,6,43,47], they have seen less adoption in other gener-\\native modeling frameworks. For example, diffusion models\\nhave been at the forefront of recent advances in image-level\\ngenerative models [9,46]; yet, they all adopt a convolutional\\nU-Net architecture as the de-facto choice of backbone.\\n*Work done during an internship at Meta AI, FAIR Team.\\nCode and project page available here.\\n1arXiv:2212.09748v2  [cs.CV]  2 Mar 2023\\n\\n=== Page 2 ===\\n520 80 320GflopsDiameterFigure 2. ImageNet generation with Diffusion Transformers (DiTs). Bubble area indicates the \ufb02ops of the diffusion model. Left:\\nFID-50K (lower is better) of our DiT models at 400K training iterations. Performance steadily improves in FID as model \ufb02ops increase.\\nRight: Our best model, DiT-XL/2, is compute-ef\ufb01cient and outperforms all prior U-Net-based diffusion models, like ADM and LDM.\\nThe seminal work of Ho et al. [19] \ufb01rst introduced the\\nU-Net backbone for diffusion models. Having initially seen\\nsuccess within pixel-level autoregressive models and con-\\nditional GANs [23], the U-Net was inherited from Pixel-\\nCNN++ [52, 58] with a few changes. The model is con-\\nvolutional, comprised primarily of ResNet [15] blocks. In\\ncontrast to the standard U-Net [49], additional spatial self-\\nattention blocks, which are essential components in trans-\\nformers, are interspersed at lower resolutions. Dhariwal and\\nNichol [9] ablated several architecture choices for the U-\\nNet, such as the use of adaptive normalization layers [40] to\\ninject conditional information and channel counts for con-\\nvolutional layers. However, the high-level design of the U-\\nNet from Ho et al. has largely remained intact.\\nWith this work, we aim to demystify the signi\ufb01cance of\\narchitectural choices in diffusion models and offer empiri-\\ncal baselines for future generative modeling research. We\\nshow that the U-Net inductive bias is notcrucial to the per-\\nformance of diffusion models, and they can be readily re-\\nplaced with standard designs such as transformers. As a\\nresult, diffusion models are well-poised to bene\ufb01t from the\\nrecent trend of architecture uni\ufb01cation\u2014e.g., by inheriting\\nbest practices and training recipes from other domains, as\\nwell as retaining favorable properties like scalability, ro-\\nbustness and ef\ufb01ciency. A standardized architecture would\\nalso open up new possibilities for cross-domain research.\\nIn this paper, we focus on a new class of diffusion models\\nbased on transformers. We call them Diffusion Transform-\\ners, or DiTs for short. DiTs adhere to the best practices of\\nVision Transformers (ViTs) [10], which have been shown to\\nscale more effectively for visual recognition than traditional\\nconvolutional networks (e.g., ResNet [15]).More speci\ufb01cally, we study the scaling behavior of trans-\\nformers with respect to network complexity vs. sample\\nquality . We show that by constructing and benchmark-\\ning the DiT design space under the Latent Diffusion Mod-\\nels(LDMs) [48] framework, where diffusion models are\\ntrained within a V AE\u2019s latent space, we can successfully\\nreplace the U-Net backbone with a transformer. We further\\nshow that DiTs are scalable architectures for diffusion mod-\\nels: there is a strong correlation between the network com-\\nplexity (measured by G\ufb02ops) vs. sample quality (measured\\nby FID). By simply scaling-up DiT and training an LDM\\nwith a high-capacity backbone (118.6 G\ufb02ops), we are able\\nto achieve a state-of-the-art result of 2.27 FID on the class-\\nconditional 256\\u0002256ImageNet generation benchmark.\\n2. Related Work\\nTransformers. Transformers [60] have replaced domain-\\nspeci\ufb01c architectures across language, vision [10], rein-\\nforcement learning [5, 25] and meta-learning [39]. They\\nhave shown remarkable scaling properties under increas-\\ning model size, training compute and data in the language\\ndomain [26], as generic autoregressive models [17] and\\nas ViTs [63]. Beyond language, transformers have been\\ntrained to autoregressively predict pixels [6, 7, 38]. They\\nhave also been trained on discrete codebooks [59] as both\\nautoregressive models [11,47] and masked generative mod-\\nels [4, 14]; the former has shown excellent scaling behavior\\nup to 20B parameters [62]. Finally, transformers have been\\nexplored in DDPMs to synthesize non-spatial data; e.g., to\\ngenerate CLIP image embeddings in DALL \\u0001E 2 [41,46]. In\\nthis paper, we study the scaling properties of transformers\\nwhen used as the backbone of diffusion models of images.\\n2\\n\\n=== Page 3 ===\\nMulti-Head Self-AttentionLayer NormScale, ShiftMLPPointwise FeedforwardLayer NormScale, ShiftScale++\\nScale\ud835\udefe!,\ud835\udefd!\ud835\udefc!\ud835\udefe\\\",\ud835\udefd\\\"\ud835\udefc\\\"\\nInput TokensConditioningDiTBlock with adaLN-ZeroLatent Diffusion TransformerTimestep \ud835\udc61Label \ud835\udc66DiTBlockN xPatchifyLayer NormLinear and ReshapeEmbedNoise\u03a332 x 32 x 432 x 32 x 4\\nNoised Latent32 x 32 x 4Multi-Head Self-AttentionLayer NormPointwise Feedforward\\nLayer Norm++\\nInput TokensConditioningDiTBlock with Cross-AttentionMulti-Head Cross-AttentionLayer Norm+Multi-Head Self-AttentionLayer NormPointwise Feedforward++\\nInput TokensConditioningDiTBlock with In-Context ConditioningLayer Norm\\nConcatenate on Sequence DimensionFigure 3. The Diffusion Transformer (DiT) architecture. Left: We train conditional latent DiT models. The input latent is decomposed\\ninto patches and processed by several DiT blocks. Right: Details of our DiT blocks. We experiment with variants of standard transformer\\nblocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens. Adaptive layer norm works best.\\nDenoising diffusion probabilistic models (DDPMs).\\nDiffusion [19, 54] and score-based generative models [22,\\n56] have been particularly successful as generative models\\nof images [35,46,48,50], in many cases outperforming gen-\\nerative adversarial networks (GANs) [12] which had previ-\\nously been state-of-the-art. Improvements in DDPMs over\\nthe past two years have largely been driven by improved\\nsampling techniques [19, 27, 55], most notably classi\ufb01er-\\nfree guidance [21], reformulating diffusion models to pre-\\ndict noise instead of pixels [19] and using cascaded DDPM\\npipelines where low-resolution base diffusion models are\\ntrained in parallel with upsamplers [9, 20]. For all the dif-\\nfusion models listed above, convolutional U-Nets [49] are\\nthe de-facto choice of backbone architecture. Concurrent\\nwork [24] introduced a novel, ef\ufb01cient architecture based\\non attention for DDPMs; we explore pure transformers.\\nArchitecture complexity. When evaluating architecture\\ncomplexity in the image generation literature, it is fairly\\ncommon practice to use parameter counts. In general, pa-\\nrameter counts can be poor proxies for the complexity of\\nimage models since they do not account for, e.g., image res-\\nolution which signi\ufb01cantly impacts performance [44, 45].\\nInstead, much of the model complexity analysis in this pa-\\nper is through the lens of theoretical G\ufb02ops. This brings us\\nin-line with the architecture design literature where G\ufb02ops\\nare widely-used to gauge complexity. In practice, the\\ngolden complexity metric is still up for debate as it fre-\\nquently depends on particular application scenarios. Nichol\\nand Dhariwal\u2019s seminal work improving diffusion mod-\\nels [9, 36] is most related to us\u2014there, they analyzed the\\nscalability and G\ufb02op properties of the U-Net architecture\\nclass. In this paper, we focus on the transformer class.3. Diffusion Transformers\\n3.1. Preliminaries\\nDiffusion formulation. Before introducing our architec-\\nture, we brie\ufb02y review some basic concepts needed to\\nunderstand diffusion models (DDPMs) [19, 54]. Gaus-\\nsian diffusion models assume a forward noising process\\nwhich gradually applies noise to real data x0:q(xtjx0) =\\nN(xt;p\\u0016\\u000btx0;(1\\u0000\\u0016\\u000bt)I), where constants \\u0016\\u000btare hyperpa-\\nrameters. By applying the reparameterization trick, we can\\nsamplext=p\\u0016\\u000btx0+p1\\u0000\\u0016\\u000bt\\u000ft, where\\u000ft\\u0018N(0;I).\\nDiffusion models are trained to learn the reverse process\\nthat inverts forward process corruptions: p\\u0012(xt\\u00001jxt) =\\nN(\\u0016\\u0012(xt);\\u0006\\u0012(xt)), where neural networks are used to pre-\\ndict the statistics of p\\u0012. The reverse process model is\\ntrained with the variational lower bound [30] of the log-\\nlikelihood of x0, which reduces to L(\\u0012) =\\u0000p(x0jx1) +P\\ntDKL(q\\u0003(xt\\u00001jxt;x0)jjp\\u0012(xt\\u00001jxt)), excluding an ad-\\nditional term irrelevant for training. Since both q\\u0003andp\\u0012\\nare Gaussian,DKLcan be evaluated with the mean and co-\\nvariance of the two distributions. By reparameterizing \\u0016\\u0012as\\na noise prediction network \\u000f\\u0012, the model can be trained us-\\ning simple mean-squared error between the predicted noise\\n\\u000f\\u0012(xt)and the ground truth sampled Gaussian noise \\u000ft:\\nLsimple (\\u0012) =jj\\u000f\\u0012(xt)\\u0000\\u000ftjj2\\n2. But, in order to train diffu-\\nsion models with a learned reverse process covariance \\u0006\\u0012,\\nthe fullDKLterm needs to be optimized. We follow Nichol\\nand Dhariwal\u2019s approach [36]: train \\u000f\\u0012withLsimple , and\\ntrain\\u0006\\u0012with the fullL. Oncep\\u0012is trained, new images can\\nbe sampled by initializing xtmax\\u0018 N (0;I)and sampling\\nxt\\u00001\\u0018p\\u0012(xt\\u00001jxt)via the reparameterization trick.\\n3\\n\\n=== Page 4 ===\\nClassi\ufb01er-free guidance. Conditional diffusion models\\ntake extra information as input, such as a class label c.\\nIn this case, the reverse process becomes p\\u0012(xt\\u00001jxt;c),\\nwhere\\u000f\\u0012and\\u0006\\u0012are conditioned on c. In this setting,\\nclassi\ufb01er-free guidance can be used to encourage the sam-\\npling procedure to \ufb01nd xsuch that logp(cjx)is high [21].\\nBy Bayes Rule, logp(cjx)/logp(xjc)\\u0000logp(x), and\\nhencerxlogp(cjx)/rxlogp(xjc)\\u0000rxlogp(x). By in-\\nterpreting the output of diffusion models as the score func-\\ntion, the DDPM sampling procedure can be guided to sam-\\nplexwith highp(xjc)by:^\\u000f\\u0012(xt;c) =\\u000f\\u0012(xt;;) +s\\u0001\\nrxlogp(xjc)/\\u000f\\u0012(xt;;)+s\\u0001(\\u000f\\u0012(xt;c)\\u0000\\u000f\\u0012(xt;;)), where\\ns>1indicates the scale of the guidance (note that s= 1re-\\ncovers standard sampling). Evaluating the diffusion model\\nwithc=;is done by randomly dropping out cduring\\ntraining and replacing it with a learned \u201cnull\u201d embedding\\n;. Classi\ufb01er-free guidance is widely-known to yield sig-\\nni\ufb01cantly improved samples over generic sampling tech-\\nniques [21, 35, 46], and the trend holds for our DiT models.\\nLatent diffusion models. Training diffusion models di-\\nrectly in high-resolution pixel space can be computationally\\nprohibitive. Latent diffusion models (LDMs) [48] tackle this\\nissue with a two-stage approach: (1) learn an autoencoder\\nthat compresses images into smaller spatial representations\\nwith a learned encoder E; (2) train a diffusion model of\\nrepresentations z=E(x)instead of a diffusion model of\\nimagesx(Eis frozen). New images can then be generated\\nby sampling a representation zfrom the diffusion model\\nand subsequently decoding it to an image with the learned\\ndecoderx=D(z).\\nAs shown in Figure 2, LDMs achieve good performance\\nwhile using a fraction of the G\ufb02ops of pixel space diffusion\\nmodels like ADM. Since we are concerned with compute\\nef\ufb01ciency, this makes them an appealing starting point for\\narchitecture exploration. In this paper, we apply DiTs to\\nlatent space, although they could be applied to pixel space\\nwithout modi\ufb01cation as well. This makes our image genera-\\ntion pipeline a hybrid-based approach; we use off-the-shelf\\nconvolutional V AEs and transformer-based DDPMs.\\n3.2. Diffusion Transformer Design Space\\nWe introduce Diffusion Transformers (DiTs), a new ar-\\nchitecture for diffusion models. We aim to be as faithful to\\nthe standard transformer architecture as possible to retain\\nits scaling properties. Since our focus is training DDPMs of\\nimages (speci\ufb01cally, spatial representations of images), DiT\\nis based on the Vision Transformer (ViT) architecture which\\noperates on sequences of patches [10]. DiT retains many of\\nthe best practices of ViTs. Figure 3 shows an overview of\\nthe complete DiT architecture. In this section, we describe\\nthe forward pass of DiT, as well as the components of the\\ndesign space of the DiT class.\\n\ud835\udc5d\ud835\udc3c\ud835\udc5d\ud835\udc3c\ud835\udc47=\ud835\udc3c/\ud835\udc5d!Noised LatentI\u00d7I\u00d7CInput Tokens T\u00d7dDiTBlockFigure 4. Input speci\ufb01cations for DiT. Given patch size p\\u0002p,\\na spatial representation (the noised latent from the V AE) of shape\\nI\\u0002I\\u0002Cis \u201cpatchi\ufb01ed\u201d into a sequence of length T= (I=p)2\\nwith hidden dimension d. A smaller patch size presults in a longer\\nsequence length and thus more G\ufb02ops.\\nPatchify. The input to DiT is a spatial representation z\\n(for256\\u0002256\\u00023images,zhas shape 32\\u000232\\u00024). The\\n\ufb01rst layer of DiT is \u201cpatchify,\u201d which converts the spatial\\ninput into a sequence of Ttokens, each of dimension d,\\nby linearly embedding each patch in the input. Following\\npatchify, we apply standard ViT frequency-based positional\\nembeddings (the sine-cosine version) to all input tokens.\\nThe number of tokens Tcreated by patchify is determined\\nby the patch size hyperparameter p. As shown in Figure 4,\\nhalvingpwill quadruple T, and thus at least quadruple total\\ntransformer G\ufb02ops. Although it has a signi\ufb01cant impact on\\nG\ufb02ops, note that changing phas no meaningful impact on\\ndownstream parameter counts.\\nWe addp= 2;4;8to the DiT design space.\\nDiT block design. Following patchify, the input tokens\\nare processed by a sequence of transformer blocks. In ad-\\ndition to noised image inputs, diffusion models sometimes\\nprocess additional conditional information such as noise\\ntimestepst, class labels c, natural language, etc. We explore\\nfour variants of transformer blocks that process conditional\\ninputs differently. The designs introduce small, but impor-\\ntant, modi\ufb01cations to the standard ViT block design. The\\ndesigns of all blocks are shown in Figure 3.\\n\u2013In-context conditioning. We simply append the vec-\\ntor embeddings of tandcas two additional tokens in\\nthe input sequence, treating them no differently from\\nthe image tokens. This is similar to cls tokens in\\nViTs, and it allows us to use standard ViT blocks with-\\nout modi\ufb01cation. After the \ufb01nal block, we remove the\\nconditioning tokens from the sequence. This approach\\nintroduces negligible new G\ufb02ops to the model.\\n4\\n\\n=== Page 5 ===\\n100K 200K 300K 400K\\nTraining Steps20406080100FID-50K\\nXL/2 In-Context\\nXL/2 Cross-Attention\\nXL/2 adaLN\\nXL/2 adaLN-ZeroFigure 5. Comparing different conditioning strategies. adaLN-\\nZero outperforms cross-attention and in-context conditioning at all\\nstages of training.\\n\u2013Cross-attention block. We concatenate the embeddings\\noftandcinto a length-two sequence, separate from\\nthe image token sequence. The transformer block is\\nmodi\ufb01ed to include an additional multi-head cross-\\nattention layer following the multi-head self-attention\\nblock, similar to the original design from Vaswani et\\nal.[60], and also similar to the one used by LDM for\\nconditioning on class labels. Cross-attention adds the\\nmost G\ufb02ops to the model, roughly a 15% overhead.\\n\u2013Adaptive layer norm (adaLN) block. Following\\nthe widespread usage of adaptive normalization lay-\\ners [40] in GANs [2, 28] and diffusion models with U-\\nNet backbones [9], we explore replacing standard layer\\nnorm layers in transformer blocks with adaptive layer\\nnorm (adaLN). Rather than directly learn dimension-\\nwise scale and shift parameters \\rand\\f, we regress\\nthem from the sum of the embedding vectors of tand\\nc. Of the three block designs we explore, adaLN adds\\nthe least G\ufb02ops and is thus the most compute-ef\ufb01cient.\\nIt is also the only conditioning mechanism that is re-\\nstricted to apply the same function to all tokens.\\n\u2013adaLN-Zero block. Prior work on ResNets has found\\nthat initializing each residual block as the identity\\nfunction is bene\ufb01cial. For example, Goyal et al. found\\nthat zero-initializing the \ufb01nal batch norm scale factor \\r\\nin each block accelerates large-scale training in the su-\\npervised learning setting [13]. Diffusion U-Net mod-\\nels use a similar initialization strategy, zero-initializing\\nthe \ufb01nal convolutional layer in each block prior to any\\nresidual connections. We explore a modi\ufb01cation of\\nthe adaLN DiT block which does the same. In addi-\\ntion to regressing \\rand\\f, we also regress dimension-\\nwise scaling parameters \\u000bthat are applied immediately\\nprior to any residual connections within the DiT block.Model Layers N Hidden size dHeads G\ufb02ops (I=32,p=4)\\nDiT-S 12 384 6 1.4\\nDiT-B 12 768 12 5.6\\nDiT-L 24 1024 16 19.7\\nDiT-XL 28 1152 16 29.1\\nTable 1. Details of DiT models. We follow ViT [10] model con-\\n\ufb01gurations for the Small (S), Base (B) and Large (L) variants; we\\nalso introduce an XLarge (XL) con\ufb01g as our largest model.\\nWe initialize the MLP to output the zero-vector for all\\n\\u000b; this initializes the full DiT block as the identity\\nfunction. As with the vanilla adaLN block, adaLN-\\nZero adds negligible G\ufb02ops to the model.\\nWe include the in-context, cross-attention, adaptive layer\\nnorm and adaLN-Zero blocks in the DiT design space.\\nModel size. We apply a sequence of NDiT blocks, each\\noperating at the hidden dimension size d. Following ViT,\\nwe use standard transformer con\ufb01gs that jointly scale N,\\ndand attention heads [10, 63]. Speci\ufb01cally, we use four\\ncon\ufb01gs: DiT-S, DiT-B, DiT-L and DiT-XL. They cover a\\nwide range of model sizes and \ufb02op allocations, from 0.3\\nto 118.6 G\ufb02ops, allowing us to gauge scaling performance.\\nTable 1 gives details of the con\ufb01gs.\\nWe add B, S, L and XL con\ufb01gs to the DiT design space.\\nTransformer decoder. After the \ufb01nal DiT block, we need\\nto decode our sequence of image tokens into an output noise\\nprediction and an output diagonal covariance prediction.\\nBoth of these outputs have shape equal to the original spa-\\ntial input. We use a standard linear decoder to do this; we\\napply the \ufb01nal layer norm (adaptive if using adaLN) and lin-\\nearly decode each token into a p\\u0002p\\u00022Ctensor, where Cis\\nthe number of channels in the spatial input to DiT. Finally,\\nwe rearrange the decoded tokens into their original spatial\\nlayout to get the predicted noise and covariance.\\nThe complete DiT design space we explore is patch size,\\ntransformer block architecture and model size.\\n4. Experimental Setup\\nWe explore the DiT design space and study the scaling\\nproperties of our model class. Our models are named ac-\\ncording to their con\ufb01gs and latent patch sizes p; for exam-\\nple, DiT-XL/2 refers to the XLarge con\ufb01g and p= 2.\\nTraining. We train class-conditional latent DiT models at\\n256\\u0002256 and512\\u0002512 image resolution on the Ima-\\ngeNet dataset [31], a highly-competitive generative mod-\\neling benchmark. We initialize the \ufb01nal linear layer with\\nzeros and otherwise use standard weight initialization tech-\\nniques from ViT. We train all models with AdamW [29,33].\\n5\",\"annotations\":null}",
            "tool_call_id": "function-call-15889490510775380703"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "function-call-4285941714311021190",
                    "type": "function",
                    "function": {
                        "name": "pdf-tools-search_pdf_content",
                        "arguments": "{\"pattern\":\"parameters\",\"pdf_file_path\":\"/workspace/dumps/workspace/2212.09748v2.pdf\"}"
                    }
                },
                {
                    "id": "function-call-4285941714311021173",
                    "type": "function",
                    "function": {
                        "name": "pdf-tools-search_pdf_content",
                        "arguments": "{\"pattern\":\"Inception Score\",\"pdf_file_path\":\"/workspace/dumps/workspace/2212.09748v2.pdf\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Search ID: e408bc76\\nPattern: parameters\\nTotal matches: 11\\nPage: 1/2\\nResults per page: 10\\n\\nMatch 1:\\n  Page: 2\\n  Text: \\\"parameters\\\"\\n  Context: ...] and masked generative mod-\\nels [4, 14]; the former has shown excellent scaling behavior\\nup to 20B [parameters] [62]. Finally, transformers have been\\nexplored in DDPMs to synthesize non-spatial data; e.g., to\\nge...\\n\\nMatch 2:\\n  Page: 5\\n  Text: \\\"parameters\\\"\\n  Context: ...blocks with adaptive layer\\nnorm (adaLN). Rather than directly learn dimension-\\nwise scale and shift [parameters] \\rand\\f, we regress\\nthem from the sum of the embedding vectors of tand\\nc. Of the three block designs ...\\n\\nMatch 3:\\n  Page: 5\\n  Text: \\\"parameters\\\"\\n  Context: ...ock which does the same. In addi-\\ntion to regressing \\rand\\f, we also regress dimension-\\nwise scaling [parameters] \\u000bthat are applied immediately\\nprior to any residual connections within the DiT block.Model Layers N...\\n\\nMatch 4:\\n  Page: 6\\n  Text: \\\"parameters\\\"\\n  Context: ...ng with a decay of 0.9999. All results\\nreported use the EMA model. We use identical training hy-\\nper[parameters] across all DiT model sizes and patch sizes.\\nOur training hyperparameters are almost entirely retain...\\n\\nMatch 5:\\n  Page: 6\\n  Text: \\\"parameters\\\"\\n  Context: ... identical training hy-\\nperparameters across all DiT model sizes and patch sizes.\\nOur training hyper[parameters] are almost entirely retained\\nfrom ADM. We did not tune learning rates, decay/warm-up\\nschedules, Ada...\\n\\nMatch 6:\\n  Page: 6\\n  Text: \\\"parameters\\\"\\n  Context: ...r diffusion model, we decode it\\nto pixels using the V AE decoder x=D(z). We retain diffu-\\nsion hyper[parameters] from ADM [9]; speci\ufb01cally, we use a\\ntmax= 1000 linear variance schedule ranging from 1\\u000210\\u00004\\nto2\\u000210\\u0000...\\n\\nMatch 7:\\n  Page: 8\\n  Text: \\\"parameters\\\"\\n  Context: ...improvements throughout train-\\ning by simply scaling the number of tokens processed by\\nDiT, holding [parameters] approximately \ufb01xed.\\nDiT G\ufb02ops are critical to improving performance. The\\nresults of Figure 6 sugges...\\n\\nMatch 8:\\n  Page: 8\\n  Text: \\\"parameters\\\"\\n  Context: ... a DiT model. As model\\nsize is held constant and patch size is decreased, the trans-\\nformer\u2019s total [parameters] are effectively unchanged (actu-\\nally, total parameters slightly decrease ), and only G\ufb02ops are\\ninc...\\n\\nMatch 9:\\n  Page: 8\\n  Text: \\\"parameters\\\"\\n  Context: ...ize is decreased, the trans-\\nformer\u2019s total parameters are effectively unchanged (actu-\\nally, total [parameters] slightly decrease ), and only G\ufb02ops are\\nincreased. These results indicate that scaling model G\ufb02ops\\n...\\n\\nMatch 10:\\n  Page: 9\\n  Text: \\\"parameters\\\"\\n  Context: ...e train a new DiT-XL/2 model on\\nImageNet at 512\\u0002512resolution for 3M iterations with\\nidentical hyper[parameters] as the 256\\u0002256model. With a\\npatch size of 2, this XL/2 model processes a total of 1024\\ntokens after...\\n\\nUse search_pdf_next_page, search_pdf_prev_page, or search_pdf_go_page with search_id 'e408bc76' to navigate.\",\"annotations\":null}",
            "tool_call_id": "function-call-4285941714311021190"
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Search ID: d4f172a8\\nPattern: Inception Score\\nTotal matches: 4\\nPage: 1/1\\nResults per page: 10\\n\\nMatch 1:\\n  Page: 6\\n  Text: \\\"Inception Score\\\"\\n  Context: ...s section do notuse classi\ufb01er-free guid-\\nance except where otherwise stated. We additionally report\\n[Inception Score] [51], sFID [34] and Precision/Recall [32]\\nas secondary metrics.\\nCompute. We implement all models in...\\n\\nMatch 2:\\n  Page: 8\\n  Text: \\\"Inception Score\\\"\\n  Context: ...roved DiT models. In Fig-\\nure 12 (appendix), we \ufb01nd that this trend holds for other\\nmetrics such as [Inception Score].\\n107108109101010111012\\nTraining Compute (Gflops)0255075100125150175200FID-50K\\n1015202530\\nS/8\\nS/4\\nS/...\\n\\nMatch 3:\\n  Page: 13\\n  Text: \\\"Inception Score\\\"\\n  Context: ...ond FID. In Figure 12,\\nwe show the effects of DiT scale on a suite of evaluation\\nmetrics\u2014FID, sFID, [Inception Score], Precision and Recall.\\nWe \ufb01nd that our FID-driven analysis in the main paper gen-\\neralizes to the o...\\n\\nMatch 4:\\n  Page: 14\\n  Text: \\\"Inception Score\\\"\\n  Context: ...ing metrics. Left: We plot model performance as a function of total training\\ncompute for FID, sFID, [Inception Score], Precision and Recall. Right: We plot model performance at 400K training steps for all 12 DiT\\nvaria...\\n\\n\",\"annotations\":null}",
            "tool_call_id": "function-call-4285941714311021173"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "function-call-17936800516011562199",
                    "type": "function",
                    "function": {
                        "name": "pdf-tools-search_pdf_content",
                        "arguments": "{\"pdf_file_path\":\"/workspace/dumps/workspace/2212.09748v2.pdf\",\"pattern\":\"ImageNet 256\"}"
                    }
                },
                {
                    "id": "function-call-17936800516011564832",
                    "type": "function",
                    "function": {
                        "name": "pdf-tools-search_pdf_content",
                        "arguments": "{\"pdf_file_path\":\"/workspace/dumps/workspace/2212.09748v2.pdf\",\"pattern\":\"FID\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Search ID: 37677867\\nPattern: ImageNet 256\\nTotal matches: 4\\nPage: 1/1\\nResults per page: 10\\n\\nMatch 1:\\n  Page: 9\\n  Text: \\\"ImageNet 256\\\"\\n  Context: ...Class-Conditional [ImageNet 256] \\u0002256\\nModel FID # sFID# IS\\\" Precision\\\"Recall\\\"\\nBigGAN-deep [2] 6.95 7.36 171.4 0.87 0.28\\nStyleGAN-XL ...\\n\\nMatch 2:\\n  Page: 9\\n  Text: \\\"ImageNet 256\\\"\\n  Context: ...G (cfg=1.50) 2.27 4.60 278.24 0.83 0.57\\nTable 2. Benchmarking class-conditional image generation on\\n[ImageNet 256]\\u0002256. DiT-XL/2 achieves state-of-the-art FID.\\nClass-Conditional ImageNet 512 \\u0002512\\nModel FID # sFID# ...\\n\\nMatch 3:\\n  Page: 13\\n  Text: \\\"ImageNet 256\\\"\\n  Context: ...or our \ufb01nal metrics re-\\nported in Tables 2 and 3. In this section, we ablate threeClass-Conditional [ImageNet 256] \\u0002256, DiT-XL/2-G (cfg=1.5)\\nDecoder FID#sFID# IS\\\" Precision\\\" Recall\\\"\\noriginal 2.46 5.18 271.56 0.82 ...\\n\\nMatch 4:\\n  Page: 13\\n  Text: \\\"ImageNet 256\\\"\\n  Context: ...co/\\nstabilityai/sd-vae-ft-mse . Different pre-trained de-\\ncoder weights yield comparable results on [ImageNet 256]\\u0002256.\\nDiffusion U-Net Model Complexities\\nModel Image Resolution Base Flops (G) Upsampler Flops (G) T...\\n\\n\",\"annotations\":null}",
            "tool_call_id": "function-call-17936800516011562199"
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Search ID: 3d0da2df\\nPattern: FID\\nTotal matches: 61\\nPage: 1/7\\nResults per page: 10\\n\\nMatch 1:\\n  Page: 1\\n  Text: \\\"FID\\\"\\n  Context: ...h in-\\ncreased transformer depth/width or increased number of in-\\nput tokens\u2014consistently have lower [FID]. In addition to pos-\\nsessing good scalability properties, our largest DiT-XL/2\\nmodels outperform al...\\n\\nMatch 2:\\n  Page: 1\\n  Text: \\\"FID\\\"\\n  Context: ...ls on the class-\\nconditional ImageNet 512 \\u0002512 and 256\\u0002256 benchmarks,\\nachieving a state-of-the-art [FID] of 2.27 on the latter.1. Introduction\\nMachine learning is experiencing a renaissance powered\\nby tra...\\n\\nMatch 3:\\n  Page: 2\\n  Text: \\\"FID\\\"\\n  Context: ...on with Diffusion Transformers (DiTs). Bubble area indicates the \ufb02ops of the diffusion model. Left:\\n[FID]-50K (lower is better) of our DiT models at 400K training iterations. Performance steadily improves ...\\n\\nMatch 4:\\n  Page: 2\\n  Text: \\\"FID\\\"\\n  Context: ...K (lower is better) of our DiT models at 400K training iterations. Performance steadily improves in [FID] as model \ufb02ops increase.\\nRight: Our best model, DiT-XL/2, is compute-ef\ufb01cient and outperforms all pr...\\n\\nMatch 5:\\n  Page: 2\\n  Text: \\\"FID\\\"\\n  Context: ...ng correlation between the network com-\\nplexity (measured by G\ufb02ops) vs. sample quality (measured\\nby [FID]). By simply scaling-up DiT and training an LDM\\nwith a high-capacity backbone (118.6 G\ufb02ops), we are ...\\n\\nMatch 6:\\n  Page: 2\\n  Text: \\\"FID\\\"\\n  Context: ...th a high-capacity backbone (118.6 G\ufb02ops), we are able\\nto achieve a state-of-the-art result of 2.27 [FID] on the class-\\nconditional 256\\u0002256ImageNet generation benchmark.\\n2. Related Work\\nTransformers. Trans...\\n\\nMatch 7:\\n  Page: 5\\n  Text: \\\"FID\\\"\\n  Context: ...100K 200K 300K 400K\\nTraining Steps20406080100[FID]-50K\\nXL/2 In-Context\\nXL/2 Cross-Attention\\nXL/2 adaLN\\nXL/2 adaLN-ZeroFigure 5. Comparing different co...\\n\\nMatch 8:\\n  Page: 6\\n  Text: \\\"FID\\\"\\n  Context: ...Figure 6. Scaling the DiT model improves [FID] at all stages of training. We show FID-50K over training iterations for 12 of our DiT\\nmodels. Top r...\\n\\nMatch 9:\\n  Page: 6\\n  Text: \\\"FID\\\"\\n  Context: ...Figure 6. Scaling the DiT model improves FID at all stages of training. We show [FID]-50K over training iterations for 12 of our DiT\\nmodels. Top row: We compare FID holding patch size c...\\n\\nMatch 10:\\n  Page: 6\\n  Text: \\\"FID\\\"\\n  Context: ...of training. We show FID-50K over training iterations for 12 of our DiT\\nmodels. Top row: We compare [FID] holding patch size constant. Bottom row: We compare FID holding model size constant. Scaling the\\ntr...\\n\\nUse search_pdf_next_page, search_pdf_prev_page, or search_pdf_go_page with search_id '3d0da2df' to navigate.\",\"annotations\":null}",
            "tool_call_id": "function-call-17936800516011564832"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "function-call-12632574060029822720",
                    "type": "function",
                    "function": {
                        "name": "pdf-tools-read_pdf_pages",
                        "arguments": "{\"end_page\":9,\"start_page\":9,\"pdf_file_path\":\"/workspace/dumps/workspace/2212.09748v2.pdf\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"PDF total pages: 25\\nExtracted page range: 9-9\\n\\n=== Page 9 ===\\nClass-Conditional ImageNet 256 \\u0002256\\nModel FID # sFID# IS\\\" Precision\\\"Recall\\\"\\nBigGAN-deep [2] 6.95 7.36 171.4 0.87 0.28\\nStyleGAN-XL [53] 2.30 4.02 265.12 0.78 0.53\\nADM [9] 10.94 6.02 100.98 0.69 0.63\\nADM-U 7.49 5.13 127.49 0.72 0.63\\nADM-G 4.59 5.25 186.70 0.82 0.52\\nADM-G, ADM-U 3.94 6.14 215.84 0.83 0.53\\nCDM [20] 4.88 - 158.71 - -\\nLDM-8 [48] 15.51 - 79.03 0.65 0.63\\nLDM-8-G 7.76 - 209.52 0.84 0.35\\nLDM-4 10.56 - 103.49 0.71 0.62\\nLDM-4-G (cfg=1.25) 3.95 - 178.22 0.81 0.55\\nLDM-4-G (cfg=1.50) 3.60 - 247.67 0.87 0.48\\nDiT-XL/2 9.62 6.85 121.50 0.67 0.67\\nDiT-XL/2-G (cfg=1.25) 3.22 5.28 201.77 0.76 0.62\\nDiT-XL/2-G (cfg=1.50) 2.27 4.60 278.24 0.83 0.57\\nTable 2. Benchmarking class-conditional image generation on\\nImageNet 256\\u0002256. DiT-XL/2 achieves state-of-the-art FID.\\nClass-Conditional ImageNet 512 \\u0002512\\nModel FID # sFID# IS\\\" Precision\\\"Recall\\\"\\nBigGAN-deep [2] 8.43 8.13 177.90 0.88 0.29\\nStyleGAN-XL [53] 2.41 4.06 267.75 0.77 0.52\\nADM [9] 23.24 10.19 58.06 0.73 0.60\\nADM-U 9.96 5.62 121.78 0.75 0.64\\nADM-G 7.72 6.57 172.71 0.87 0.42\\nADM-G, ADM-U 3.85 5.86 221.72 0.84 0.53\\nDiT-XL/2 12.03 7.12 105.25 0.75 0.64\\nDiT-XL/2-G (cfg=1.25) 4.64 5.77 174.77 0.81 0.57\\nDiT-XL/2-G (cfg=1.50) 3.04 5.02 240.82 0.84 0.54\\nTable 3. Benchmarking class-conditional image generation on\\nImageNet 512\\u0002512. Note that prior work [9] measures Precision\\nand Recall using 1000 real samples for 512\\u0002512resolution; for\\nconsistency, we do the same.\\nOur method achieves the lowest FID of all prior generative\\nmodels, including the previous state-of-the-art StyleGAN-\\nXL [53]. Finally, we also observe that DiT-XL/2 achieves\\nhigher recall values at all tested classi\ufb01er-free guidance\\nscales compared to LDM-4 and LDM-8. When trained for\\nonly 2.35M steps (similar to ADM), XL/2 still outperforms\\nall prior diffusion models with an FID of 2.55.\\n512\\u0002512 ImageNet. We train a new DiT-XL/2 model on\\nImageNet at 512\\u0002512resolution for 3M iterations with\\nidentical hyperparameters as the 256\\u0002256model. With a\\npatch size of 2, this XL/2 model processes a total of 1024\\ntokens after patchifying the 64\\u000264\\u00024input latent (524.6\\nG\ufb02ops). Table 3 shows comparisons against state-of-the-art\\nmethods. XL/2 again outperforms all prior diffusion models\\nat this resolution, improving the previous best FID of 3.85\\nachieved by ADM to 3.04. Even with the increased num-\\nber of tokens, XL/2 remains compute-ef\ufb01cient. For exam-\\nple, ADM uses 1983 G\ufb02ops and ADM-U uses 2813 G\ufb02ops;\\nXL/2 uses 524.6 G\ufb02ops. We show samples from the high-\\nresolution XL/2 model in Figure 1 and the appendix.\\n101\\n102\\n103\\n104\\n105\\nSampling Compute (Gflops)20406080100120140160180FID-10K\\nS/8\\nS/4\\nS/2B/8\\nB/4\\nB/2L/8\\nL/4\\nL/2XL/8\\nXL/4\\nXL/2Figure 10. Scaling-up sampling compute does not compensate\\nfor a lack of model compute. For each of our DiT models trained\\nfor 400K iterations, we compute FID-10K using [16, 32, 64, 128,\\n256, 1000] sampling steps. For each number of steps, we plot the\\nFID as well as the G\ufb02ops used to sample each image. Small mod-\\nels cannot close the performance gap with our large models, even\\nif they sample with more test-time G\ufb02ops than the large models.\\n5.2. Scaling Model vs. Sampling Compute\\nDiffusion models are unique in that they can use addi-\\ntional compute after training by increasing the number of\\nsampling steps when generating an image. Given the im-\\npact of model G\ufb02ops on sample quality, in this section we\\nstudy if smaller- model compute DiTs can outperform larger\\nones by using more sampling compute . We compute FID\\nfor all 12 of our DiT models after 400K training steps, us-\\ning [16, 32, 64, 128, 256, 1000] sampling steps per-image.\\nThe main results are in Figure 10. Consider DiT-L/2 us-\\ning 1000 sampling steps versus DiT-XL/2 using 128 steps.\\nIn this case, L/2 uses 80:7T\ufb02ops to sample each image;\\nXL/2 uses 5\\u0002less compute\u2014 15:2T\ufb02ops\u2014to sample each\\nimage. Nonetheless, XL/2 has the better FID-10K (23.7\\nvs 25.9). In general, scaling-up sampling compute cannot\\ncompensate for a lack of model compute.\\n6. Conclusion\\nWe introduce Diffusion Transformers (DiTs), a simple\\ntransformer-based backbone for diffusion models that out-\\nperforms prior U-Net models and inherits the excellent scal-\\ning properties of the transformer model class. Given the\\npromising scaling results in this paper, future work should\\ncontinue to scale DiTs to larger models and token counts.\\nDiT could also be explored as a drop-in backbone for text-\\nto-image models like DALL \\u0001E 2 and Stable Diffusion.\\nAcknowledgements. We thank Kaiming He, Ronghang\\nHu, Alexander Berg, Shoubhik Debnath, Tim Brooks, Ilija\\nRadosavovic and Tete Xiao for helpful discussions. William\\nPeebles is supported by the NSF GRFP.\\n9\",\"annotations\":null}",
            "tool_call_id": "function-call-12632574060029822720"
        }
    ]
}