---
title: "Add Bibtex"
description: "Add five specified articles to ref.bib in BibTeX format, maintaining consistency with existing entries and sourcing conference versions from OpenReview where applicable."
---

<Card>
<div className="tools-container">
<div className="mcp-servers-container">
<div className="mcp-servers-title">
MCP Servers
</div>
<div className="mcp-servers-grid">
<div className="mcp-server-item">
<svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><path fill="#010202" d="M201.5,22.5C253.575,18.8292,294.909,37.8292,325.5,79.5C348.846,76.447,370.846,80.6137,391.5,92C411.339,105.168,422.339,123.668,424.5,147.5C465.86,161.693,491.694,190.026,502,232.5C508.97,280.701,492.804,318.868,453.5,347C427.526,362.095,399.526,367.929,369.5,364.5C369.988,405.252,369.488,445.919,368,486.5C367.5,487,367,487.5,366.5,488C292.5,488.667,218.5,488.667,144.5,488C144,487.5,143.5,487,143,486.5C141.512,445.919,141.012,405.252,141.5,364.5C106.171,368.921,74.5043,360.421,46.5,339C10.3518,305.536-0.814861,265.036,13,217.5C22.2223,191.95,38.389,172.117,61.5,158C69.8007,153.681,78.134,149.681,86.5,146C92.1136,89.0861,121.447,50.0861,174.5,29C183.492,26.2401,192.492,24.0734,201.5,22.5Z"/></g><g><path fill="#a3d3fe" d="M204.5,36.5C254.543,34.185,292.876,54.0184,319.5,96C340.997,90.9721,361.664,93.3054,381.5,103C401.654,115.47,411.154,133.637,410,157.5C410.903,158.701,412.069,159.535,413.5,160C453.408,170.242,478.242,195.409,488,235.5C493.84,285.282,474.34,321.115,429.5,343C422.574,345.298,415.574,347.298,408.5,349C395.504,349.5,382.504,349.667,369.5,349.5C369.667,302.499,369.5,255.499,369,208.5C367.833,207.333,366.667,206.167,365.5,205C306.833,204.333,248.167,204.333,189.5,205C173,220.167,157.167,236,142,252.5C141.5,284.832,141.333,317.165,141.5,349.5C100.111,355.535,66.2777,342.535,40,310.5C17.3055,276.196,15.6388,240.863,35,204.5C50.4757,180.668,71.9757,165.002,99.5,157.5C101.376,109.215,123.042,73.0487,164.5,49C177.289,42.679,190.622,38.5123,204.5,36.5Z"/></g><g><path fill="#fefefe" d="M354.5,474.5C288.5,474.5,222.5,474.5,156.5,474.5C156.5,403.833,156.5,333.167,156.5,262.5C169.344,262.829,182.011,262.495,194.5,261.5C197.167,260.833,198.833,259.167,199.5,256.5C200.495,244.011,200.829,231.344,200.5,218.5C251.833,218.5,303.167,218.5,354.5,218.5C354.5,303.833,354.5,389.167,354.5,474.5Z"/></g><g><path fill="#979797" d="M354.5,474.5C354.5,389.167,354.5,303.833,354.5,218.5C303.167,218.5,251.833,218.5,200.5,218.5C200.829,231.344,200.495,244.011,199.5,256.5C199.5,243.5,199.5,230.5,199.5,217.5C251.5,217.5,303.5,217.5,355.5,217.5C355.833,303.335,355.499,389.002,354.5,474.5Z"/></g><g><path fill="#dce0e3" d="M185.5,247.5C179.31,247.823,173.31,247.49,167.5,246.5C173.13,240.369,178.964,234.369,185,228.5C185.5,234.825,185.666,241.158,185.5,247.5Z"/></g><g><path fill="#7b7d7e" d="M167.5,246.5C173.31,247.49,179.31,247.823,185.5,247.5C179.322,248.817,172.988,248.817,166.5,247.5C166.624,246.893,166.957,246.56,167.5,246.5Z"/></g><g><path fill="#a3a3a3" d="M194.5,261.5C182.011,262.495,169.344,262.829,156.5,262.5C156.5,333.167,156.5,403.833,156.5,474.5C155.501,403.669,155.167,332.669,155.5,261.5C168.5,261.5,181.5,261.5,194.5,261.5Z"/></g><g><path fill="#030303" d="M252.5,309.5C256.426,309.047,259.592,310.38,262,313.5C262.333,334.833,262.667,356.167,263,377.5C264.979,369.149,269.812,366.649,277.5,370C278.718,371.438,279.385,373.105,279.5,375C274.409,385.346,268.909,395.512,263,405.5C258,413.5,253,413.5,248,405.5C242.091,395.512,236.591,385.346,231.5,375C232.869,369.144,236.536,367.144,242.5,369C244.956,371.408,246.789,374.241,248,377.5C248.333,356.167,248.667,334.833,249,313.5C250.376,312.295,251.542,310.962,252.5,309.5Z"/></g><g><path fill="#050505" d="M199.5,376.5C203.888,375.943,207.055,377.61,209,381.5C209.5,398.83,209.667,416.163,209.5,433.5C240.167,433.5,270.833,433.5,301.5,433.5C301.333,416.163,301.5,398.83,302,381.5C304.553,376.374,308.386,375.207,313.5,378C314.126,378.75,314.626,379.584,315,380.5C315.667,401.5,315.667,422.5,315,443.5C314.626,444.416,314.126,445.25,313.5,446C294.254,447.155,274.921,447.655,255.5,447.5C236.079,447.655,216.746,447.155,197.5,446C196.874,445.25,196.374,444.416,196,443.5C195.333,422.5,195.333,401.5,196,380.5C196.69,378.65,197.856,377.316,199.5,376.5Z"/></g></svg>
<span className="mcp-server-name">fetch</span>
</div>
<div className="mcp-server-item">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="20px" height="20px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#0277bd" d="M19.847,41.956c-5.629-0.002-11.259,0.024-16.888-0.013c-2.855-0.019-3.374-0.7-2.731-3.525 c2.178-9.58,4.427-19.143,6.557-28.734C7.356,7.112,8.588,5.975,11.312,6C22.57,6.106,33.829,6.034,45.088,6.046 c2.824,0.003,3.298,0.614,2.664,3.511c-2.058,9.406-4.129,18.809-6.236,28.203c-0.789,3.516-1.697,4.187-5.353,4.195 C30.724,41.966,25.285,41.958,19.847,41.956z"/><path fill="#fafafa" d="M25.057 23.922c-.608-.687-1.114-1.267-1.531-1.732-2.43-2.728-4.656-5.27-7.063-7.869-1.102-1.189-1.453-2.344-.13-3.518 1.307-1.16 2.592-1.058 3.791.277 3.34 3.717 6.676 7.438 10.071 11.104 1.268 1.369.972 2.3-.424 3.315-5.359 3.895-10.687 7.833-16.01 11.778-1.196.887-2.337 1.109-3.304-.201-1.066-1.445-.08-2.305 1.026-3.114 3.955-2.893 7.903-5.798 11.834-8.725C23.865 24.83 24.595 24.267 25.057 23.922zM21.75 37C20.625 37 20 36 20 35s.625-2 1.75-2c4.224 0 6.112 0 9.5 0 1.125 0 1.75 1 1.75 2s-.625 2-1.75 2C29.125 37 25 37 21.75 37z"/></svg>
<span className="mcp-server-name">terminal</span>
</div>
<div className="mcp-server-item">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="20px" height="20px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#1e88e5" d="M24,4C12.954,4,4,12.954,4,24s8.954,20,20,20s20-8.954,20-20S35.046,4,24,4z"/><path fill="#1565c0" d="M35,16.592v-3.878L37,11H27l0.917,1.833c-1.236,0-2.265,0-2.265,0S19.095,13,19.095,18.748	c0,5.752,5.732,5.088,5.732,5.088s0,0.865,0,1.453c0,0.594,0.77,0.391,0.864,1.583c-0.388,0-7.964-0.208-7.964,4.998	s6.679,4.959,6.679,4.959s7.722,0.365,7.722-6.104c0-3.871-4.405-5.121-4.405-6.686c0-1.563,3.319-2.012,3.319-5.684	c0-0.823-0.028-1.524-0.149-2.12L34,13.571v3.02c-0.581,0.207-1,0.756-1,1.408v4.5c0,0.829,0.672,1.5,1.5,1.5s1.5-0.671,1.5-1.5V18	C36,17.348,35.581,16.799,35,16.592z M30.047,31.169c0.131,2.024-1.929,3.811-4.603,3.998c-2.671,0.188-4.946-1.295-5.077-3.316	c-0.133-2.016,1.927-3.805,4.6-3.996C27.641,27.667,29.914,29.152,30.047,31.169z M26.109,22.453	c-1.592,0.451-3.375-1.062-3.982-3.367c-0.604-2.312,0.195-4.543,1.786-4.992c1.593-0.453,3.374,1.059,3.981,3.367	C28.499,19.77,27.702,22.004,26.109,22.453z"/><path fill="#e8eaf6" d="M34,16.592V12c0-0.051-0.015-0.097-0.029-0.143L35,11H21l-9,8h5.383	c0.174,5.466,5.715,4.836,5.715,4.836s0,0.865,0,1.453c0,0.594,0.771,0.391,0.865,1.583c-0.388,0-7.964-0.208-7.964,4.998	s6.679,4.959,6.679,4.959s7.721,0.365,7.721-6.104c0-3.871-4.404-5.121-4.404-6.686c0-1.563,3.318-2.012,3.318-5.684	c0-0.971-0.047-1.763-0.232-2.422L33,12.667v3.925c-0.581,0.207-1,0.756-1,1.408v4.5c0,0.829,0.672,1.5,1.5,1.5s1.5-0.671,1.5-1.5	V18C35,17.348,34.581,16.799,34,16.592z M28.319,31.169c0.131,2.024-1.928,3.811-4.602,3.998c-2.671,0.188-4.946-1.295-5.077-3.316	c-0.133-2.016,1.927-3.805,4.599-3.996C25.914,27.667,28.187,29.152,28.319,31.169z M24.38,22.453	c-1.591,0.451-3.373-1.062-3.981-3.367c-0.604-2.312,0.194-4.543,1.785-4.992c1.593-0.453,3.374,1.059,3.982,3.367	C26.77,19.77,25.973,22.004,24.38,22.453z"/></svg>
<span className="mcp-server-name">scholarly</span>
</div>
<div className="mcp-server-item">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="20px" height="20px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg>
<span className="mcp-server-name">filesystem</span>
</div>
<div className="mcp-server-item">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="20px" height="20px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#4caf50" d="M44,24c0,11.044-8.956,20-20,20S4,35.044,4,24S12.956,4,24,4S44,12.956,44,24z"/><path fill="#ffc107" d="M24,4v20l8,4l-8.843,16c0.317,0,0.526,0,0.843,0c11.053,0,20-8.947,20-20S35.053,4,24,4z"/><path fill="#4caf50" d="M44,24c0,11.044-8.956,20-20,20S4,35.044,4,24S12.956,4,24,4S44,12.956,44,24z"/><path fill="#ffc107" d="M24,4v20l8,4l-8.843,16c0.317,0,0.526,0,0.843,0c11.053,0,20-8.947,20-20S35.053,4,24,4z"/><path fill="#f44336" d="M41.84,15H24v13l-3-1L7.16,13.26H7.14C10.68,7.69,16.91,4,24,4C31.8,4,38.55,8.48,41.84,15z"/><path fill="#dd2c00" d="M7.158,13.264l8.843,14.862L21,27L7.158,13.264z"/><path fill="#558b2f" d="M23.157,44l8.934-16.059L28,25L23.157,44z"/><path fill="#f9a825" d="M41.865,15H24l-1.579,4.58L41.865,15z"/><path fill="#fff" d="M33,24c0,4.969-4.031,9-9,9s-9-4.031-9-9s4.031-9,9-9S33,19.031,33,24z"/><path fill="#2196f3" d="M31,24c0,3.867-3.133,7-7,7s-7-3.133-7-7s3.133-7,7-7S31,20.133,31,24z"/></svg>
<span className="mcp-server-name">playwright_with_chunk</span>
</div>
</div>
</div>
<div className="local-tools-container">
<div className="mcp-servers-title">
Local Tools
</div>
<div className="local-tools-grid">
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 1024 1024" class="icon"  version="1.1" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M512 896c-223.5 0-405.3-181.8-405.3-405.3S288.5 85.3 512 85.3s405.3 181.8 405.3 405.3S735.5 896 512 896z m0-725.3c-176.4 0-320 143.5-320 320s143.6 320 320 320 320-143.5 320-320-143.6-320-320-320z" fill="#3688FF" /><path d="M922.6 249.6c-13.7 0-27.1-6.6-35.4-18.8-15.3-22.6-32.8-43.9-52.1-63.2-19.3-19.3-40.6-36.9-63.2-52.1-19.5-13.2-24.6-39.7-11.5-59.2 13.2-19.5 39.7-24.6 59.2-11.5 27.1 18.3 52.6 39.4 75.7 62.5s44.2 48.6 62.5 75.7c13.2 19.5 8.1 46.1-11.5 59.2-7.2 5-15.5 7.4-23.7 7.4zM101.4 249.6c-8.2 0-16.5-2.4-23.8-7.3-19.5-13.2-24.7-39.7-11.5-59.2 18.3-27.1 39.4-52.6 62.5-75.7s48.6-44.1 75.7-62.5c19.5-13.2 46.1-8 59.3 11.5 13.2 19.5 8.1 46-11.5 59.2-22.6 15.3-43.9 32.8-63.2 52.1-19.3 19.3-36.8 40.5-52.1 63.2-8.2 12.1-21.7 18.7-35.4 18.7zM138.6 970.7c-8.6 0-17.3-2.6-24.8-7.9-19.2-13.7-23.6-40.4-9.9-59.5l53.3-74.7c13.7-19.2 40.3-23.6 59.5-9.9 19.2 13.7 23.6 40.4 9.9 59.5l-53.3 74.7c-8.2 11.6-21.4 17.8-34.7 17.8zM885.4 970.7c-13.3 0-26.4-6.2-34.8-17.9l-53.3-74.7c-13.7-19.2-9.3-45.8 9.9-59.5 19.2-13.7 45.8-9.2 59.5 9.9l53.3 74.7c13.7 19.2 9.3 45.8-9.9 59.5-7.5 5.4-16.2 8-24.7 8zM512 554.7c-7.2 0-14.5-1.8-21-5.5-13.4-7.6-21.6-21.8-21.6-37.1V384c0-23.6 19.1-42.7 42.7-42.7s42.7 19.1 42.7 42.7v52.6l42.1-25.2c20.2-12.1 46.4-5.6 58.5 14.6 12.1 20.2 5.6 46.4-14.6 58.5l-106.7 64c-6.9 4.1-14.5 6.2-22.1 6.2z" fill="#5F6379" /></svg>
<span className="local-tool-name">sleep</span>
</div>
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 100 100" enable-background="new 0 0 100 100" id="Layer_1" version="1.1" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><g><rect clip-rule="evenodd" fill="#F2F2F2" fill-rule="evenodd" height="83.437" width="67.025" x="9.012" y="7.604"/><path d="M77.454,92.458H7.595V6.187h69.859V92.458z M10.429,89.624H74.62V9.021H10.429V89.624z"/></g><g><rect clip-rule="evenodd" fill="#FF7C24" fill-rule="evenodd" height="10.481" width="17.952" x="46.695" y="34.866"/></g><g><rect height="2.834" width="19.463" x="20.504" y="35.575"/></g><g><rect height="2.834" width="15.561" x="20.718" y="42.508"/></g><g><rect height="2.833" width="15.562" x="20.813" y="49.514"/></g><g><rect height="2.833" width="27.128" x="20.718" y="56.753"/></g><g><rect height="2.833" width="23.51" x="20.718" y="63.688"/></g><g><rect clip-rule="evenodd" fill="#39B6CC" fill-rule="evenodd" height="9.004" width="44.095" x="20.553" y="19.064"/></g><g><rect height="2.833" width="26.272" x="20.718" y="70.32"/></g><g><rect height="2.834" width="32.8" x="20.718" y="77.253"/></g><g><rect height="2.834" width="3.235" x="38.304" y="42.508"/></g><g><path clip-rule="evenodd" d="M77.931,71.902l4.287,4.427l-6.644,6.437l-4.309-4.457    C74.147,76.998,76.504,74.726,77.931,71.902L77.931,71.902z" fill="#F2F2F2" fill-rule="evenodd"/><path d="M75.542,84.77l-6.692-6.92l1.828-0.831c2.579-1.174,4.706-3.218,5.989-5.756l0.897-1.776l6.656,6.874L75.542,84.77z     M73.584,78.669l2.023,2.091l4.605-4.463l-2.007-2.074C76.994,76.012,75.414,77.531,73.584,78.669z"/></g><g><polygon clip-rule="evenodd" fill="#39B6CC" fill-rule="evenodd" points="83.267,75.319 91.984,84.338 83.247,92.779     74.535,83.761   "/><path d="M83.213,94.783L72.531,83.726l10.771-10.41l10.687,11.056L83.213,94.783z M76.538,83.794l6.744,6.981l6.698-6.472    l-6.748-6.981L76.538,83.794z"/></g><g><path clip-rule="evenodd" d="M66.124,50.799c7.742,0,14.018,6.276,14.018,14.019    s-6.275,14.019-14.018,14.019c-7.743,0-14.019-6.276-14.019-14.019S58.381,50.799,66.124,50.799L66.124,50.799z" fill="#F2F2F2" fill-rule="evenodd"/><path d="M66.124,80.253c-8.511,0-15.435-6.924-15.435-15.435s6.924-15.435,15.435-15.435S81.56,56.307,81.56,64.818    S74.635,80.253,66.124,80.253z M66.124,52.216c-6.949,0-12.601,5.653-12.601,12.602s5.651,12.601,12.601,12.601    c6.948,0,12.602-5.652,12.602-12.601S73.072,52.216,66.124,52.216z"/></g><g><rect height="2.833" width="10.313" x="39.902" y="49.514"/></g><g><path d="M76.404,65.586H73.57c0-0.636-0.068-1.255-0.205-1.84c-0.043-0.186-0.096-0.385-0.169-0.63l2.717-0.808    c0.091,0.304,0.158,0.559,0.215,0.801C76.31,63.901,76.404,64.735,76.404,65.586z M72.438,61.433    c-1.489-2.5-4.203-4.058-7.084-4.061l0.004-2.834c3.871,0.005,7.518,2.091,9.516,5.445L72.438,61.433z"/></g></g></svg>
<span className="local-tool-name">history</span>
</div>
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M861.588238 240.133873v-65.792823c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.631049H314.659414v-65.631049c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.792823c-36.317212 0.868255-65.631049 30.539428-65.63105 67.061417v543.745565c0 37.06772 30.155471 67.223191 67.223191 67.223191h696.886045c37.06772 0 67.223191-30.155471 67.223191-67.223191V307.19529c-0.001024-36.52199-29.315885-66.193162-65.633097-67.061417z m-109.385765-65.792823c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m-546.928824 0c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m678.191947 676.600829c0 12.935767-10.532708 23.468476-23.468476 23.468475H163.111076c-12.935767 0-23.468476-10.532708-23.468476-23.468475V307.19529c0-12.402323 9.677764-22.593054 21.877358-23.415233v65.577807c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.631049h393.789368v65.631049c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.577807c12.19857 0.82218 21.877358 11.012911 21.877358 23.415233v543.746589z" fill="#22C67F" /><path d="M706.719439 478.272194l-48.01715-44.741741-182.28128 195.621482-111.468348-122.615387-48.563905 44.148911 159.469116 172.685427z" fill="#74E8AE" /></svg>
<span className="local-tool-name">claim_done</span>
</div>
<div className="local-tool-item">
<svg height="20" width="20" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g fill="none" fill-rule="evenodd"><rect fill="#fabc05" height="512" rx="42.667" width="512"/><path d="M58.667 293.333L0 209.125V42.663C0 19.1 19.137 0 42.663 0h426.674C492.9 0 512 19.137 512 42.663l-282.667.004S58.667 305.14 58.667 293.333z" fill="#eb4335"/><path d="M303.3 340.528L199.876 512H42.633C19.087 512 0 492.94 0 469.328V209.125l54.537 74.854c28.457 48.33 81.024 80.762 141.167 80.762 40.548 0 77.822-14.86 106.533-39.29l5.348 5.349c-.918 3.235-2.337 6.5-4.284 9.728z" fill="#34a854"/><g fill-rule="nonzero"><path d="M194.667 85.333c-66.667 0-120 52.148-120 117.334C74.667 267.852 128 320 194.667 320c66.666 0 120-52.148 120-117.333 0-65.186-53.334-117.334-120-117.334z" fill="#4285f5"/><path d="M327.59 300.924l28.788 28.787 13.204-14.159 115.751 115.751-59.363 59.364-115.751-115.751 13.553-12.64-28.85-28.85c-27.526 20.496-61.677 32.712-98.52 32.712C105.605 366.138 32 292.532 32 201.735c0-90.796 73.605-164.402 164.402-164.402s164.402 73.606 164.402 164.402c0 37.142-12.414 71.549-33.213 99.19zM196.403 87.92c-63.231 0-113.817 50.585-113.817 113.816 0 63.232 50.586 113.817 113.817 113.817S310.22 264.967 310.22 201.735c0-63.231-50.585-113.816-113.817-113.816z" fill="#fff"/></g></g></svg>
<span className="local-tool-name">web_search</span>
</div>
<div className="local-tool-item">
<Icon icon="bars-progress" size={20} color="#4286f6" />
<span className="local-tool-name">manage_context</span>
</div>
<div className="local-tool-item">
<Icon icon="filter-list" size={20} color="#4286f6" />
<span className="local-tool-name">handle_overlong_tool_outputs</span>
</div>
</div>
</div>
</div>
</Card>



## Instruction

Please help me add the following article to the ref.bib file in the workspace. Please keep the format consistent with the existing article. If there is a conference version, please copy the bibtex file from the corresponding conference homepage on OpenReview.

1. Scaling relationship on learning mathematical reasoning with large language models
2. Qwen2.5-math technical report
3. Lemur: Harmonizing Natural Language and Code for Language Agents
4. Codellama
5. mmlu

## Initial State

### Local Workspace

<div className="file-tree">
<a href="https://github.com/hkust-nlp/mcpbench_dev/tree/finalpool_dev/tasks/finalpool/add-bibtex/initial_workspace">workspace</a>/
└── ref.bib
</div>

## Model Trajectory

<AccordionGroup>
<Accordion title="claude4-sonnet">

<Columns cols={3}>
<Card title="Task Completion" icon="check">
Completed
</Card>
<Card title="Tool Calls" icon="wrench">
17
</Card>
<Card title="Turns" icon="arrows-rotate">
17
</Card>
</Columns>

<div className="thinking-box">
🧐`Agent`

I'll help you add these articles to the ref.bib file. First, let me examine the existing ref.bib file to understand the format, then I'll search for and add the requested articles.
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

```json
{
	"path": "/workspace/dumps/workspace/ref.bib"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json

@article{singh2024evaluation,
  title={Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?},
  author={Singh, Aaditya K and Kocyigit, Muhammed Yusuf and Poulton, Andrew and Esiobu, David and Lomeli, Maria and Szilvasy, Gergely and Hupkes, Dieuwke},
  journal={arXiv preprint arXiv:2411.03923},
  year={2024}
}

@inproceedings{
ding2024semcoder,
title={SemCoder: Training Code Language Models with Comprehensive Semantics Reasoning},
author={Yangruibo Ding and Jinjun Peng and Marcus J. Min and Gail Kaiser and Junfeng Yang and Baishakhi Ray},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=PnlCHQrM69}
}

@article{huang2024opencoder,
  title={Opencoder: The open cookbook for top-tier code large language models},
  author={Huang, Siming and Cheng, Tianhao and Liu, Jason Klein and Hao, Jiaran and Song, Liuyihan and Xu, Yang and Yang, J and Liu, JH and Zhang, Chenchen and Chai, Linzheng and others},
  journal={arXiv preprint arXiv:2411.04905},
  year={2024}
}

@article{yue2024mammoth2,
  title={Mammoth2: Scaling instructions from the web},
  author={Yue, Xiang and Zheng, Tuney and Zhang, Ge and Chen, Wenhu},
  journal={arXiv preprint arXiv:2405.03548},
  year={2024}
}

@article{toshniwal2024openmathinstruct2,
  title={Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data},
  author={Toshniwal, Shubham and Du, Wei and Moshkov, Ivan and Kisacanin, Branislav and Ayrapetyan, Alexan and Gitman, Igor},
  journal={arXiv preprint arXiv:2410.01560},
  year={2024}
}

@software{benallal2024smollmcorpus,
  author = {Ben Allal, Loubna and Lozhkov, Anton and Penedo, Guilherme and Wolf, Thomas and von Werra, Leandro},
  title = {SmolLM-Corpus},
  month = July,
  year = 2024,
  url = {https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus}
}

@article{zhu2024deepseekcoderv2,
  title={DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence},
  author={Zhu, Qihao and Guo, Daya and Shao, Zhihong and Yang, Dejian and Wang, Peiyi and Xu, Runxin and Wu, Y and Li, Yukun and Gao, Huazuo and Ma, Shirong and others},
  journal={arXiv preprint arXiv:2406.11931},
  year={2024}
}

@article{hui2024qwen25coder,
      title={Qwen2. 5-Coder Technical Report},
      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
      journal={arXiv preprint arXiv:2409.12186},
      year={2024}
}

@article{zeng2024skyworkmath,
  title={Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models--The Story Goes On},
  author={Zeng, Liang and Zhong, Liangjun and Zhao, Liang and Wei, Tianwen and Yang, Liu and He, Jujie and Cheng, Cheng and Hu, Rui and Liu, Yang and Yan, Shuicheng and others},
  journal={arXiv preprint arXiv:2407.08348},
  year={2024}
}

@article{ying2024internlmmath,
  title={Internlm-math: Open math large language models toward verifiable reasoning},
  author={Ying, Huaiyuan and Zhang, Shuo and Li, Linyang and Zhou, Zhejian and Shao, Yunfan and Fei, Zhaoye and Ma, Yichuan and Hong, Jiawei and Liu, Kuikun and Wang, Ziyi and others},
  journal={arXiv preprint arXiv:2402.06332},
  year={2024}
}

@article{yang2024qwen25,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{dubey2024llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{xu2024aguvis,
  title={Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction},
  author={Xu, Yiheng and Wang, Zekun and Wang, Junli and Lu, Dunjie and Xie, Tianbao and Saha, Amrita and Sahoo, Doyen and Yu, Tao and Xiong, Caiming},
  journal={arXiv preprint arXiv:2412.04454},
  year={2024}
}

@inproceedings{
zhou2024math4ai,
title={{AI} for Math or Math for {AI}? On the Generalization of Learning Mathematical Problem Solving},
author={Ruochen Zhou and Minrui Xu and Shiqi Chen and Junteng Liu and Yunqi Li and LIN Xinxin and Zhengyu Chen and Junxian He},
booktitle={The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24},
year={2024},
url={https://openreview.net/forum?id=xlnvZ85CSo}
}

@article{paster2023openwebmath,
  title={Openwebmath: An open dataset of high-quality mathematical web text},
  author={Paster, Keiran and Santos, Marco Dos and Azerbayev, Zhangir and Ba, Jimmy},
  journal={arXiv preprint arXiv:2310.06786},
  year={2023}
}

@article{ma2024korbench,
  title={KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks},
  author={Ma, Kaijing and Du, Xinrun and Wang, Yunran and Zhang, Haoran and Wen, Zhoufutu and Qu, Xingwei and Yang, Jian and Liu, Jiaheng and Liu, Minghao and Yue, Xiang and others},
  journal={arXiv preprint arXiv:2410.06526},
  year={2024}
}

@inproceedings{dua2019drop,
  title={DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},
  author={Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2368--2378},
  year={2019}
}

@inproceedings{
rein2024gpqa,
title={{GPQA}: A Graduate-Level Google-Proof Q\&A Benchmark},
author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=Ti67584b98}
}

@article{white2024livebench,
  title={Livebench: A challenging, contamination-free llm benchmark},
  author={White, Colin and Dooley, Samuel and Roberts, Manley and Pal, Arka and Feuer, Ben and Jain, Siddhartha and Shwartz-Ziv, Ravid and Jain, Neel and Saifullah, Khalid and Naidu, Siddartha and others},
  journal={arXiv preprint arXiv:2406.19314},
  year={2024}
}

@article{lin2025zebralogic,
  title={ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning},
  author={Lin, Bill Yuchen and Bras, Ronan Le and Richardson, Kyle and Sabharwal, Ashish and Poovendran, Radha and Clark, Peter and Choi, Yejin},
  journal={arXiv preprint arXiv:2502.01100},
  year={2025}
}

@misc{codestral,
    title={Codestral},
    author={Mistral-AI},
    url={https://mistral.ai/news/codestral/},
    year={2024}
}

@misc{haillm,
    title={Hai-llm: Efficient and lightweight training tool for large models},
    author={High-flyer},
    url={https://www.high-flyer.cn/en/blog/hai-llm},
    year={2023}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@misc{qwq,
  author = {Team, Qwen},
  title = {QwQ: Reflect Deeply on the Boundaries of the Unknown},
  month = nov,
  year = 2024,
  url = {https://qwenlm.github.io/blog/qwq-32b-preview/}
}

@misc{knauff2010complex,
  title={Complex cognition: the science of human reasoning, problem-solving, and decision-making},
  author={Knauff, Markus and Wolf, Ann G},
  journal={Cognitive processing},
  volume={11},
  pages={99--102},
  year={2010},
  publisher={Springer}
}

@article{wang2010cognitive,
  title={On the cognitive process of human problem solving},
  author={Wang, Yingxu and Chiew, Vincent},
  journal={Cognitive systems research},
  volume={11},
  number={1},
  pages={81--92},
  year={2010},
  publisher={Elsevier}
}

@article{dehaene2004arithmetic,
  title={Arithmetic and the brain},
  author={Dehaene, Stanislas and Molko, Nicolas and Cohen, Laurent and Wilson, Anna J},
  journal={Current opinion in neurobiology},
  volume={14},
  number={2},
  pages={218--224},
  year={2004},
  publisher={Elsevier}
}

@article{xiang2025towards,
  title={Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though},
  author={Xiang, Violet and Snell, Charlie and Gandhi, Kanishk and Albalak, Alon and Singh, Anikait and Blagden, Chase and Phung, Duy and Rafailov, Rafael and Lile, Nathan and Mahan, Dakota and others},
  journal={arXiv preprint arXiv:2501.04682},
  year={2025}
}

@article{huang2022towards,
  title={Towards reasoning in large language models: A survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}

@article{qiao2022reasoning,
  title={Reasoning with language model prompting: A survey},
  author={Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  journal={arXiv preprint arXiv:2212.09597},
  year={2022}
}

@article{lozhkov2024starcoder,
  title={Starcoder 2 and the stack v2: The next generation},
  author={Lozhkov, Anton and Li, Raymond and Allal, Loubna Ben and Cassano, Federico and Lamy-Poirier, Joel and Tazi, Nouamane and Tang, Ao and Pykhtar, Dmytro and Liu, Jiawei and Wei, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.19173},
  year={2024}
}

@inproceedings{
zhang2024towards,
title={Towards Bridging Classical and Neural Computation through a Read-Eval-Print Loop},
author={David W. Zhang and Micha{\"e}l Defferrard and Corrado Rainone and Roland Memisevic},
booktitle={ICML 2024 Workshop on LLMs and Cognition},
year={2024},
url={https://openreview.net/forum?id=Y08aOVCPTt}
}


@article{wen2024codeplan,
  title={Unlocking Reasoning Potential in Large Langauge Models by Scaling Code-form Planning},
  author={Wen, Jiaxin and Guan, Jian and Wang, Hongning and Wu, Wei and Huang, Minlie},
  journal={arXiv preprint arXiv:2409.12452},
  year={2024}
}

@inproceedings{
gorceix2024learningmathrules,
title={Learning Mathematical Rules with Large Language Models},
author={Antoine Gorceix and Bastien Le Chenadec and Ahmad Rammal and Nelson Vadori and Manuela Veloso},
booktitle={The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24},
year={2024},
url={https://openreview.net/forum?id=tIlDF5B6T4}
}

@article{xu2024core,
  title={CoRE: LLM as Interpreter for Natural Language Programming, Pseudo-Code Programming, and Flow Programming of AI Agents},
  author={Xu, Shuyuan and Li, Zelong and Mei, Kai and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2405.06907},
  year={2024}
}

@inproceedings{
morishita2024enhancing,
title={Enhancing Reasoning Capabilities of {LLM}s via Principled Synthetic Logic Corpus},
author={Terufumi Morishita and Gaku Morio and Atsuki Yamaguchi and Yasuhiro Sogawa},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=mljDUaQpln}
}

@inproceedings{morishita2023learning,
  title={Learning deductive reasoning from synthetic corpus based on formal logic},
  author={Morishita, Terufumi and Morio, Gaku and Yamaguchi, Atsuki and Sogawa, Yasuhiro},
  booktitle={International Conference on Machine Learning},
  pages={25254--25274},
  year={2023},
  organization={PMLR}
}

@article{chen2024reverse,
  title={Reverse Thinking Makes LLMs Stronger Reasoners},
  author={Chen, Justin Chih-Yao and Wang, Zifeng and Palangi, Hamid and Han, Rujun and Ebrahimi, Sayna and Le, Long and Perot, Vincent and Mishra, Swaroop and Bansal, Mohit and Lee, Chen-Yu and others},
  journal={arXiv preprint arXiv:2411.19865},
  year={2024}
}

@article{jain2024livecodebench,
  title={Livecodebench: Holistic and contamination free evaluation of large language models for code},
  author={Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  journal={arXiv preprint arXiv:2403.07974},
  year={2024}
}

@article{zaremba2014learning,
  title={Learning to execute},
  author={Zaremba, Wojciech and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1410.4615},
  year={2014}
}

@article{graves2014neural,
  title={Neural Turing Machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@inproceedings{ding2024traced,
  title={Traced: Execution-aware pre-training for source code},
  author={Ding, Yangruibo and Steenhoek, Benjamin and Pei, Kexin and Kaiser, Gail and Le, Wei and Ray, Baishakhi},
  booktitle={Proceedings of the 46th IEEE/ACM International Conference on Software Engineering},
  pages={1--12},
  year={2024}
}

@inproceedings{
ni2024next,
title={{NE}xT: Teaching Large Language Models to Reason about Code Execution},
author={Ansong Ni and Miltiadis Allamanis and Arman Cohan and Yinlin Deng and Kensen Shi and Charles Sutton and Pengcheng Yin},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=B1W712hMBi}
}

@article{ding2024cycle,
  title={Cycle: Learning to self-refine the code generation},
  author={Ding, Yangruibo and Min, Marcus J and Kaiser, Gail and Ray, Baishakhi},
  journal={Proceedings of the ACM on Programming Languages},
  volume={8},
  number={OOPSLA1},
  pages={392--418},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@inproceedings{wang2024leti,
  title={LETI: Learning to Generate from Textual Interactions},
  author={Wang, Xingyao and Peng, Hao and Jabbarvand, Reyhaneh and Ji, Heng},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={223--239},
  year={2024}
}

@inproceedings{gururangan2020don,
  title={Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8342--8360},
  year={2020}
}

@misc{deepseekai2025r1,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{jiang2024logicpro,
  title={LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning},
  author={Jiang, Jin and Yan, Yuchen and Liu, Yang and Jin, Yonggang and Peng, Shuai and Zhang, Mengdi and Cai, Xunliang and Cao, Yixin and Gao, Liangcai and Tang, Zhi},
  journal={arXiv preprint arXiv:2409.12929},
  year={2024}
}
```
</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Now I can see the format of the existing ref.bib file. Let me search for each of the requested articles to get their proper bibliographic information:
</div>

<div className="tool-call-box">
<svg height="14" width="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g fill="none" fill-rule="evenodd"><rect fill="#fabc05" height="512" rx="42.667" width="512"/><path d="M58.667 293.333L0 209.125V42.663C0 19.1 19.137 0 42.663 0h426.674C492.9 0 512 19.137 512 42.663l-282.667.004S58.667 305.14 58.667 293.333z" fill="#eb4335"/><path d="M303.3 340.528L199.876 512H42.633C19.087 512 0 492.94 0 469.328V209.125l54.537 74.854c28.457 48.33 81.024 80.762 141.167 80.762 40.548 0 77.822-14.86 106.533-39.29l5.348 5.349c-.918 3.235-2.337 6.5-4.284 9.728z" fill="#34a854"/><g fill-rule="nonzero"><path d="M194.667 85.333c-66.667 0-120 52.148-120 117.334C74.667 267.852 128 320 194.667 320c66.666 0 120-52.148 120-117.333 0-65.186-53.334-117.334-120-117.334z" fill="#4285f5"/><path d="M327.59 300.924l28.788 28.787 13.204-14.159 115.751 115.751-59.363 59.364-115.751-115.751 13.553-12.64-28.85-28.85c-27.526 20.496-61.677 32.712-98.52 32.712C105.605 366.138 32 292.532 32 201.735c0-90.796 73.605-164.402 164.402-164.402s164.402 73.606 164.402 164.402c0 37.142-12.414 71.549-33.213 99.19zM196.403 87.92c-63.231 0-113.817 50.585-113.817 113.816 0 63.232 50.586 113.817 113.817 113.817S310.22 264.967 310.22 201.735c0-63.231-50.585-113.816-113.817-113.816z" fill="#fff"/></g></g></svg> `web_search `

```json
{
	"query": "Scaling relationship on learning mathematical reasoning with large language models arxiv"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Title: Scaling Relationship on Learning Mathematical Reasoning ...
Link: https://arxiv.org/abs/2308.01825
Snippet: by Z Yuan · 2023 · Cited by 418 — In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised ...
Sitelinks: No sitelinks

Title: Scaling Relationship on Learning Mathematical Reasoning ...
Link: https://openreview.net/forum?id=cijO0f8u35
Snippet: by Z Yuan · Cited by 419 — The paper investigates the scaling relationship of factors influencing the mathematical reasoning abilities of large language models (LLMs) through supervised ...
Sitelinks: No sitelinks

Title: Scaling Relationship on Learning Mathematical Reasoning ...
Link: https://www.semanticscholar.org/paper/Scaling-Relationship-on-Learning-Mathematical-with-Yuan-Yuan/91206346edbe28abb606d7b3425cd455d4019d4f
Snippet: It is found that pre-training loss is a better indicator of the model's performance than the model's parameter count and that with augmented samples ...
Sitelinks: No sitelinks

Title: arXiv:2308.01825v1 [cs.CL] 3 Aug 2023
Link: https://arxiv.org/pdf/2308.01825
Snippet: by Z Yuan · 2023 · Cited by 416 — In this paper, we are investigating the scaling relationship in supervising math reasoning abilities with large language models. We find the ...
Sitelinks: No sitelinks

Title: [R] Scaling Relationship on Learning Mathematical ...
Link: https://www.reddit.com/r/MachineLearning/comments/15hnqfw/r_scaling_relationship_on_learning_mathematical/
Snippet: We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also ...
Sitelinks: No sitelinks

Title: Scaling Relationship on Learning Mathematical Reasoning ...
Link: https://consensus.app/papers/scaling-relationship-on-learning-mathematical-reasoning-tan-yuan/ff9c8ef6cf3c514aa14cef61149386a9/
Snippet: Key takeaway: 'Pre-training loss is a better indicator of a language model's performance than its parameter count, and Rejection sampling ...
Sitelinks: No sitelinks

Title: [R] A Careful Examination of Large Language Model ...
Link: https://www.reddit.com/r/MachineLearning/comments/1ckkf5f/r_a_careful_examination_of_large_language_model/
Snippet: Abstract: Large language models (LLMs) have achieved impressive success on many benchmarks for mathematical reasoning. However, there is growing ...
Sitelinks: No sitelinks

Title: OFA-Sys/gsm8k-ScRel: Codes and Data for Scaling ...
Link: https://github.com/OFA-Sys/gsm8k-ScRel
Snippet: The code and data used for reproducing results of Scaling Relationship on Learning Mathematical Reasoning with Large Language Models
Sitelinks: No sitelinks

Title: The Relationship Between Reasoning and Performance in ...
Link: https://arxiv.org/abs/2502.15631
Snippet: by M Ballon · 2025 · Cited by 23 — Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling ...
Sitelinks: No sitelinks

Title: Scaling Relationship on Learning Mathematical Reasoning ...
Link: https://www.alphaxiv.org/overview/2308.01825v2
Snippet: This paper from Alibaba DAMO Academy investigates the empirical scaling relationships for Large Language Models' mathematical reasoning capabilities, ...
Sitelinks: No sitelinks

```
</Expandable>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><path fill="#010202" d="M201.5,22.5C253.575,18.8292,294.909,37.8292,325.5,79.5C348.846,76.447,370.846,80.6137,391.5,92C411.339,105.168,422.339,123.668,424.5,147.5C465.86,161.693,491.694,190.026,502,232.5C508.97,280.701,492.804,318.868,453.5,347C427.526,362.095,399.526,367.929,369.5,364.5C369.988,405.252,369.488,445.919,368,486.5C367.5,487,367,487.5,366.5,488C292.5,488.667,218.5,488.667,144.5,488C144,487.5,143.5,487,143,486.5C141.512,445.919,141.012,405.252,141.5,364.5C106.171,368.921,74.5043,360.421,46.5,339C10.3518,305.536-0.814861,265.036,13,217.5C22.2223,191.95,38.389,172.117,61.5,158C69.8007,153.681,78.134,149.681,86.5,146C92.1136,89.0861,121.447,50.0861,174.5,29C183.492,26.2401,192.492,24.0734,201.5,22.5Z"/></g><g><path fill="#a3d3fe" d="M204.5,36.5C254.543,34.185,292.876,54.0184,319.5,96C340.997,90.9721,361.664,93.3054,381.5,103C401.654,115.47,411.154,133.637,410,157.5C410.903,158.701,412.069,159.535,413.5,160C453.408,170.242,478.242,195.409,488,235.5C493.84,285.282,474.34,321.115,429.5,343C422.574,345.298,415.574,347.298,408.5,349C395.504,349.5,382.504,349.667,369.5,349.5C369.667,302.499,369.5,255.499,369,208.5C367.833,207.333,366.667,206.167,365.5,205C306.833,204.333,248.167,204.333,189.5,205C173,220.167,157.167,236,142,252.5C141.5,284.832,141.333,317.165,141.5,349.5C100.111,355.535,66.2777,342.535,40,310.5C17.3055,276.196,15.6388,240.863,35,204.5C50.4757,180.668,71.9757,165.002,99.5,157.5C101.376,109.215,123.042,73.0487,164.5,49C177.289,42.679,190.622,38.5123,204.5,36.5Z"/></g><g><path fill="#fefefe" d="M354.5,474.5C288.5,474.5,222.5,474.5,156.5,474.5C156.5,403.833,156.5,333.167,156.5,262.5C169.344,262.829,182.011,262.495,194.5,261.5C197.167,260.833,198.833,259.167,199.5,256.5C200.495,244.011,200.829,231.344,200.5,218.5C251.833,218.5,303.167,218.5,354.5,218.5C354.5,303.833,354.5,389.167,354.5,474.5Z"/></g><g><path fill="#979797" d="M354.5,474.5C354.5,389.167,354.5,303.833,354.5,218.5C303.167,218.5,251.833,218.5,200.5,218.5C200.829,231.344,200.495,244.011,199.5,256.5C199.5,243.5,199.5,230.5,199.5,217.5C251.5,217.5,303.5,217.5,355.5,217.5C355.833,303.335,355.499,389.002,354.5,474.5Z"/></g><g><path fill="#dce0e3" d="M185.5,247.5C179.31,247.823,173.31,247.49,167.5,246.5C173.13,240.369,178.964,234.369,185,228.5C185.5,234.825,185.666,241.158,185.5,247.5Z"/></g><g><path fill="#7b7d7e" d="M167.5,246.5C173.31,247.49,179.31,247.823,185.5,247.5C179.322,248.817,172.988,248.817,166.5,247.5C166.624,246.893,166.957,246.56,167.5,246.5Z"/></g><g><path fill="#a3a3a3" d="M194.5,261.5C182.011,262.495,169.344,262.829,156.5,262.5C156.5,333.167,156.5,403.833,156.5,474.5C155.501,403.669,155.167,332.669,155.5,261.5C168.5,261.5,181.5,261.5,194.5,261.5Z"/></g><g><path fill="#030303" d="M252.5,309.5C256.426,309.047,259.592,310.38,262,313.5C262.333,334.833,262.667,356.167,263,377.5C264.979,369.149,269.812,366.649,277.5,370C278.718,371.438,279.385,373.105,279.5,375C274.409,385.346,268.909,395.512,263,405.5C258,413.5,253,413.5,248,405.5C242.091,395.512,236.591,385.346,231.5,375C232.869,369.144,236.536,367.144,242.5,369C244.956,371.408,246.789,374.241,248,377.5C248.333,356.167,248.667,334.833,249,313.5C250.376,312.295,251.542,310.962,252.5,309.5Z"/></g><g><path fill="#050505" d="M199.5,376.5C203.888,375.943,207.055,377.61,209,381.5C209.5,398.83,209.667,416.163,209.5,433.5C240.167,433.5,270.833,433.5,301.5,433.5C301.333,416.163,301.5,398.83,302,381.5C304.553,376.374,308.386,375.207,313.5,378C314.126,378.75,314.626,379.584,315,380.5C315.667,401.5,315.667,422.5,315,443.5C314.626,444.416,314.126,445.25,313.5,446C294.254,447.155,274.921,447.655,255.5,447.5C236.079,447.655,216.746,447.155,197.5,446C196.874,445.25,196.374,444.416,196,443.5C195.333,422.5,195.333,401.5,196,380.5C196.69,378.65,197.856,377.316,199.5,376.5Z"/></g></svg> `fetch fetch_txt`

```json
{
	"url": "https://arxiv.org/abs/2308.01825"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate > cs > arXiv:2308.01825 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:2308.01825 (cs) [Submitted on 3 Aug 2023 (v1), last revised 13 Sep 2023 (this version, v2)] Title:Scaling Relationship on Learning Mathematical Reasoning with Large Language Models Authors:Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, Jingren Zhou View a PDF of the paper titled Scaling Relationship on Learning Mathematical Reasoning with Large Language Models, by Zheng Yuan and 7 other authors View PDF Abstract:Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also find RFT brings more improvement for less performant LLMs. Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\% significantly. Comments: Working in Progress Subjects: Computation and Language (cs.CL) Cite as: arXiv:2308.01825 [cs.CL] (or arXiv:2308.01825v2 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2308.01825 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Hongyi Yuan [view email] [v1] Thu, 3 Aug 2023 15:34:01 UTC (2,903 KB) [v2] Wed, 13 Sep 2023 03:57:29 UTC (3,950 KB) Full-text links: Access Paper: View a PDF of the paper titled Scaling Relationship on Learning Mathematical Reasoning with Large Language Models, by Zheng Yuan and 7 other authorsView PDFTeX SourceOther Formats view license Current browse context: cs.CL < prev | next > new | recent | 2023-08 Change to browse by: cs References & Citations NASA ADSGoogle Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv (What is alphaXiv?) Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Huggingface Toggle Hugging Face (What is Huggingface?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About Help contact arXivClick here to contact arXiv Contact subscribe to arXiv mailingsClick here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack
```
</Expandable>
</div>

<div className="tool-call-box">
<svg height="14" width="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g fill="none" fill-rule="evenodd"><rect fill="#fabc05" height="512" rx="42.667" width="512"/><path d="M58.667 293.333L0 209.125V42.663C0 19.1 19.137 0 42.663 0h426.674C492.9 0 512 19.137 512 42.663l-282.667.004S58.667 305.14 58.667 293.333z" fill="#eb4335"/><path d="M303.3 340.528L199.876 512H42.633C19.087 512 0 492.94 0 469.328V209.125l54.537 74.854c28.457 48.33 81.024 80.762 141.167 80.762 40.548 0 77.822-14.86 106.533-39.29l5.348 5.349c-.918 3.235-2.337 6.5-4.284 9.728z" fill="#34a854"/><g fill-rule="nonzero"><path d="M194.667 85.333c-66.667 0-120 52.148-120 117.334C74.667 267.852 128 320 194.667 320c66.666 0 120-52.148 120-117.333 0-65.186-53.334-117.334-120-117.334z" fill="#4285f5"/><path d="M327.59 300.924l28.788 28.787 13.204-14.159 115.751 115.751-59.363 59.364-115.751-115.751 13.553-12.64-28.85-28.85c-27.526 20.496-61.677 32.712-98.52 32.712C105.605 366.138 32 292.532 32 201.735c0-90.796 73.605-164.402 164.402-164.402s164.402 73.606 164.402 164.402c0 37.142-12.414 71.549-33.213 99.19zM196.403 87.92c-63.231 0-113.817 50.585-113.817 113.816 0 63.232 50.586 113.817 113.817 113.817S310.22 264.967 310.22 201.735c0-63.231-50.585-113.816-113.817-113.816z" fill="#fff"/></g></g></svg> `web_search `

```json
{
	"query": "Qwen2.5-math technical report arxiv"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Title: Qwen2.5-Math Technical Report: Toward Mathematical ...
Link: https://arxiv.org/abs/2409.12122
Snippet: by A Yang · 2024 · Cited by 389 — In this report, we present a series of math-specific large language models: Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B.
Sitelinks: No sitelinks

Title: Qwen2.5-Math Technical Report: Toward Mathematical ...
Link: https://arxiv.org/html/2409.12122v1
Snippet: In this report, we introduce a series of math-specific large language models, Qwen2.5-Math, Qwen2.5-Math-RM, and Qwen2.5-Math-Instruct-1.5B/7B/72B.
Sitelinks: No sitelinks

Title: [2412.15115] Qwen2.5 Technical Report
Link: https://arxiv.org/abs/2412.15115
Snippet: by A Yang · 2024 · Cited by 3055 — Abstract:In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs.
Sitelinks: No sitelinks

Title: arXiv:2412.15115v2 [cs.CL] 3 Jan 2025
Link: https://arxiv.org/pdf/2412.15115
Snippet: by A Yang · 2024 · Cited by 3055 — In this report, we introduce Qwen2.5, a comprehensive series of large language models. (LLMs) designed to meet diverse needs.
Sitelinks: No sitelinks

Title: Qwen2.5-Math Technical Report: Toward Mathematical ...
Link: https://www.researchgate.net/publication/384116167_Qwen25-Math_Technical_Report_Toward_Mathematical_Expert_Model_via_Self-Improvement
Snippet: In this report, we present a series of math-specific large language models: Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the ...
Sitelinks: No sitelinks

Title: Qwen/Qwen2.5-Math-7B
Link: https://huggingface.co/Qwen/Qwen2.5-Math-7B
Snippet: Qwen2.5-Math-7B is a base model typically used for completion and few-shot inference, serving as a better starting point for fine-tuning.
Sitelinks: No sitelinks

Title: QwenLM/Qwen2.5-Math: A series of math-specific large ...
Link: https://github.com/QwenLM/Qwen2.5-Math
Snippet: Qwen2.5-Math series is expanded to support using both CoT and Tool-integrated Reasoning (TIR) to solve math problems in both Chinese and English.
Sitelinks: No sitelinks

Title: arXiv:2505.09388v1 [cs.CL] 14 May 2025
Link: https://arxiv.org/pdf/2505.09388
Snippet: by A Yang · 2025 · Cited by 3055 — Qwen2.5-Math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024c. Yidan Zhang, Boyi Deng ...
Sitelinks: No sitelinks

Title: Paper page - Qwen2.5 Technical Report
Link: https://huggingface.co/papers/2412.15115
Snippet: In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs.
Sitelinks: No sitelinks

Title: [2409.12186] Qwen2.5-Coder Technical Report
Link: https://arxiv.org/abs/2409.12186
Snippet: by B Hui · 2024 · Cited by 652 — Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general and math skills.
Sitelinks: No sitelinks

```
</Expandable>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><path fill="#010202" d="M201.5,22.5C253.575,18.8292,294.909,37.8292,325.5,79.5C348.846,76.447,370.846,80.6137,391.5,92C411.339,105.168,422.339,123.668,424.5,147.5C465.86,161.693,491.694,190.026,502,232.5C508.97,280.701,492.804,318.868,453.5,347C427.526,362.095,399.526,367.929,369.5,364.5C369.988,405.252,369.488,445.919,368,486.5C367.5,487,367,487.5,366.5,488C292.5,488.667,218.5,488.667,144.5,488C144,487.5,143.5,487,143,486.5C141.512,445.919,141.012,405.252,141.5,364.5C106.171,368.921,74.5043,360.421,46.5,339C10.3518,305.536-0.814861,265.036,13,217.5C22.2223,191.95,38.389,172.117,61.5,158C69.8007,153.681,78.134,149.681,86.5,146C92.1136,89.0861,121.447,50.0861,174.5,29C183.492,26.2401,192.492,24.0734,201.5,22.5Z"/></g><g><path fill="#a3d3fe" d="M204.5,36.5C254.543,34.185,292.876,54.0184,319.5,96C340.997,90.9721,361.664,93.3054,381.5,103C401.654,115.47,411.154,133.637,410,157.5C410.903,158.701,412.069,159.535,413.5,160C453.408,170.242,478.242,195.409,488,235.5C493.84,285.282,474.34,321.115,429.5,343C422.574,345.298,415.574,347.298,408.5,349C395.504,349.5,382.504,349.667,369.5,349.5C369.667,302.499,369.5,255.499,369,208.5C367.833,207.333,366.667,206.167,365.5,205C306.833,204.333,248.167,204.333,189.5,205C173,220.167,157.167,236,142,252.5C141.5,284.832,141.333,317.165,141.5,349.5C100.111,355.535,66.2777,342.535,40,310.5C17.3055,276.196,15.6388,240.863,35,204.5C50.4757,180.668,71.9757,165.002,99.5,157.5C101.376,109.215,123.042,73.0487,164.5,49C177.289,42.679,190.622,38.5123,204.5,36.5Z"/></g><g><path fill="#fefefe" d="M354.5,474.5C288.5,474.5,222.5,474.5,156.5,474.5C156.5,403.833,156.5,333.167,156.5,262.5C169.344,262.829,182.011,262.495,194.5,261.5C197.167,260.833,198.833,259.167,199.5,256.5C200.495,244.011,200.829,231.344,200.5,218.5C251.833,218.5,303.167,218.5,354.5,218.5C354.5,303.833,354.5,389.167,354.5,474.5Z"/></g><g><path fill="#979797" d="M354.5,474.5C354.5,389.167,354.5,303.833,354.5,218.5C303.167,218.5,251.833,218.5,200.5,218.5C200.829,231.344,200.495,244.011,199.5,256.5C199.5,243.5,199.5,230.5,199.5,217.5C251.5,217.5,303.5,217.5,355.5,217.5C355.833,303.335,355.499,389.002,354.5,474.5Z"/></g><g><path fill="#dce0e3" d="M185.5,247.5C179.31,247.823,173.31,247.49,167.5,246.5C173.13,240.369,178.964,234.369,185,228.5C185.5,234.825,185.666,241.158,185.5,247.5Z"/></g><g><path fill="#7b7d7e" d="M167.5,246.5C173.31,247.49,179.31,247.823,185.5,247.5C179.322,248.817,172.988,248.817,166.5,247.5C166.624,246.893,166.957,246.56,167.5,246.5Z"/></g><g><path fill="#a3a3a3" d="M194.5,261.5C182.011,262.495,169.344,262.829,156.5,262.5C156.5,333.167,156.5,403.833,156.5,474.5C155.501,403.669,155.167,332.669,155.5,261.5C168.5,261.5,181.5,261.5,194.5,261.5Z"/></g><g><path fill="#030303" d="M252.5,309.5C256.426,309.047,259.592,310.38,262,313.5C262.333,334.833,262.667,356.167,263,377.5C264.979,369.149,269.812,366.649,277.5,370C278.718,371.438,279.385,373.105,279.5,375C274.409,385.346,268.909,395.512,263,405.5C258,413.5,253,413.5,248,405.5C242.091,395.512,236.591,385.346,231.5,375C232.869,369.144,236.536,367.144,242.5,369C244.956,371.408,246.789,374.241,248,377.5C248.333,356.167,248.667,334.833,249,313.5C250.376,312.295,251.542,310.962,252.5,309.5Z"/></g><g><path fill="#050505" d="M199.5,376.5C203.888,375.943,207.055,377.61,209,381.5C209.5,398.83,209.667,416.163,209.5,433.5C240.167,433.5,270.833,433.5,301.5,433.5C301.333,416.163,301.5,398.83,302,381.5C304.553,376.374,308.386,375.207,313.5,378C314.126,378.75,314.626,379.584,315,380.5C315.667,401.5,315.667,422.5,315,443.5C314.626,444.416,314.126,445.25,313.5,446C294.254,447.155,274.921,447.655,255.5,447.5C236.079,447.655,216.746,447.155,197.5,446C196.874,445.25,196.374,444.416,196,443.5C195.333,422.5,195.333,401.5,196,380.5C196.69,378.65,197.856,377.316,199.5,376.5Z"/></g></svg> `fetch fetch_txt`

```json
{
	"url": "https://arxiv.org/abs/2409.12122"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate > cs > arXiv:2409.12122 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:2409.12122 (cs) [Submitted on 18 Sep 2024] Title:Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement Authors:An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, Zhenru Zhang View a PDF of the paper titled Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement, by An Yang and 15 other authors View PDF HTML (experimental) Abstract:In this report, we present a series of math-specific large language models: Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the Qwen2.5 series lies in integrating the philosophy of self-improvement throughout the entire pipeline, from pre-training and post-training to inference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized to generate large-scale, high-quality mathematical data. (2) In the post-training phase, we develop a reward model (RM) by conducting massive sampling from Qwen2-Math-Instruct. This RM is then applied to the iterative evolution of data in supervised fine-tuning (SFT). With a stronger SFT model, it's possible to iteratively train and update the RM, which in turn guides the next round of SFT data iteration. On the final SFT model, we employ the ultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct. (3) Furthermore, during the inference stage, the RM is used to guide sampling, optimizing the model's performance. Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced mathematical reasoning capabilities, including Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics datasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and AIME24, covering a range of difficulties from grade school level to math competition problems. Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2409.12122 [cs.CL] (or arXiv:2409.12122v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2409.12122 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Keming Lu [view email] [v1] Wed, 18 Sep 2024 16:45:37 UTC (1,994 KB) Full-text links: Access Paper: View a PDF of the paper titled Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement, by An Yang and 15 other authorsView PDFHTML (experimental)TeX SourceOther Formats view license Current browse context: cs.CL < prev | next > new | recent | 2024-09 Change to browse by: cs cs.AI cs.LG References & Citations NASA ADSGoogle Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv (What is alphaXiv?) Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Huggingface Toggle Hugging Face (What is Huggingface?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About Help contact arXivClick here to contact arXiv Contact subscribe to arXiv mailingsClick here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack
```
</Expandable>
</div>

<div className="tool-call-box">
<svg height="14" width="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g fill="none" fill-rule="evenodd"><rect fill="#fabc05" height="512" rx="42.667" width="512"/><path d="M58.667 293.333L0 209.125V42.663C0 19.1 19.137 0 42.663 0h426.674C492.9 0 512 19.137 512 42.663l-282.667.004S58.667 305.14 58.667 293.333z" fill="#eb4335"/><path d="M303.3 340.528L199.876 512H42.633C19.087 512 0 492.94 0 469.328V209.125l54.537 74.854c28.457 48.33 81.024 80.762 141.167 80.762 40.548 0 77.822-14.86 106.533-39.29l5.348 5.349c-.918 3.235-2.337 6.5-4.284 9.728z" fill="#34a854"/><g fill-rule="nonzero"><path d="M194.667 85.333c-66.667 0-120 52.148-120 117.334C74.667 267.852 128 320 194.667 320c66.666 0 120-52.148 120-117.333 0-65.186-53.334-117.334-120-117.334z" fill="#4285f5"/><path d="M327.59 300.924l28.788 28.787 13.204-14.159 115.751 115.751-59.363 59.364-115.751-115.751 13.553-12.64-28.85-28.85c-27.526 20.496-61.677 32.712-98.52 32.712C105.605 366.138 32 292.532 32 201.735c0-90.796 73.605-164.402 164.402-164.402s164.402 73.606 164.402 164.402c0 37.142-12.414 71.549-33.213 99.19zM196.403 87.92c-63.231 0-113.817 50.585-113.817 113.816 0 63.232 50.586 113.817 113.817 113.817S310.22 264.967 310.22 201.735c0-63.231-50.585-113.816-113.817-113.816z" fill="#fff"/></g></g></svg> `web_search `

```json
{
	"query": "Lemur Harmonizing Natural Language and Code for Language Agents arxiv"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Title: Lemur: Harmonizing Natural Language and Code for ...
Link: https://arxiv.org/abs/2310.06830
Snippet: by Y Xu · 2023 · Cited by 59 — We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities.
Sitelinks: No sitelinks

Title: Lemur: Harmonizing Natural Language and Code for ...
Link: https://arxiv.org/pdf/2310.06830
Snippet: by Y Xu · 2023 · Cited by 59 — We introduce Lemur and Lemur-Chat, openly accessible language models op- timized for both natural language and coding capabilities to serve ...
Sitelinks: No sitelinks

Title: Lemur: Harmonizing Natural Language and Code for ...
Link: https://arxiv.org/html/2310.06830v2
Snippet: We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of ...
Sitelinks: No sitelinks

Title: Lemur: Harmonizing Natural Language and Code for ...
Link: https://openreview.net/forum?id=hNhwSmtXRh
Snippet: by Y Xu · Cited by 59 — We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities.
Sitelinks: No sitelinks

Title: Lemur: Harmonizing Natural Language and Code for ...
Link: https://huggingface.co/papers/2310.06830
Snippet: We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as ...
Sitelinks: No sitelinks

Title: [PDF] Lemur: Harmonizing Natural Language and Code for ...
Link: https://www.semanticscholar.org/paper/Lemur%3A-Harmonizing-Natural-Language-and-Code-for-Xu-Su/8147cec9245d34d13732a08e915c920a1a499bb5
Snippet: Lemur and Lemur-Chat are introduced, openly accessible language models optimized for both natural language and coding capabilities to serve ...
Sitelinks: No sitelinks

Title: Lemur: Harmonizing Natural Language and Code for ...
Link: https://www.reddit.com/r/artificial/comments/1771hmp/lemur_harmonizing_natural_language_and_code_for/
Snippet: More importantly, Lemur-Chat won 12/13 new "agent tests" designed to mimic real-world challenges needing both language and programming prowess.
Sitelinks: No sitelinks

Title: Introducing Lemur(finetuned Llama 2) - openly accessible ...
Link: https://www.reddit.com/r/singularity/comments/177bcle/introducing_lemurfinetuned_llama_2_openly/
Snippet: Introducing Lemur(finetuned Llama 2) - openly accessible language model optimized for both natural language and coding capabilities to serve as ...
Sitelinks: No sitelinks

Title: Lemur: Open Foundation Models for Language Agents
Link: https://github.com/OpenLemur/Lemur
Snippet: Lemur is an openly accessible language model optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents.
Sitelinks: No sitelinks

Title: arXiv:2402.15506v3 [cs.AI] 20 Mar 2024
Link: https://arxiv.org/pdf/2402.15506
Snippet: by J Zhang · 2024 · Cited by 44 — Lemur: Harmonizing natural language and code for language agents. arXiv preprint arXiv:2310.06830, 2023. Zhilin Yang, Peng Qi, Saizheng ...
Sitelinks: No sitelinks

```
</Expandable>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><path fill="#010202" d="M201.5,22.5C253.575,18.8292,294.909,37.8292,325.5,79.5C348.846,76.447,370.846,80.6137,391.5,92C411.339,105.168,422.339,123.668,424.5,147.5C465.86,161.693,491.694,190.026,502,232.5C508.97,280.701,492.804,318.868,453.5,347C427.526,362.095,399.526,367.929,369.5,364.5C369.988,405.252,369.488,445.919,368,486.5C367.5,487,367,487.5,366.5,488C292.5,488.667,218.5,488.667,144.5,488C144,487.5,143.5,487,143,486.5C141.512,445.919,141.012,405.252,141.5,364.5C106.171,368.921,74.5043,360.421,46.5,339C10.3518,305.536-0.814861,265.036,13,217.5C22.2223,191.95,38.389,172.117,61.5,158C69.8007,153.681,78.134,149.681,86.5,146C92.1136,89.0861,121.447,50.0861,174.5,29C183.492,26.2401,192.492,24.0734,201.5,22.5Z"/></g><g><path fill="#a3d3fe" d="M204.5,36.5C254.543,34.185,292.876,54.0184,319.5,96C340.997,90.9721,361.664,93.3054,381.5,103C401.654,115.47,411.154,133.637,410,157.5C410.903,158.701,412.069,159.535,413.5,160C453.408,170.242,478.242,195.409,488,235.5C493.84,285.282,474.34,321.115,429.5,343C422.574,345.298,415.574,347.298,408.5,349C395.504,349.5,382.504,349.667,369.5,349.5C369.667,302.499,369.5,255.499,369,208.5C367.833,207.333,366.667,206.167,365.5,205C306.833,204.333,248.167,204.333,189.5,205C173,220.167,157.167,236,142,252.5C141.5,284.832,141.333,317.165,141.5,349.5C100.111,355.535,66.2777,342.535,40,310.5C17.3055,276.196,15.6388,240.863,35,204.5C50.4757,180.668,71.9757,165.002,99.5,157.5C101.376,109.215,123.042,73.0487,164.5,49C177.289,42.679,190.622,38.5123,204.5,36.5Z"/></g><g><path fill="#fefefe" d="M354.5,474.5C288.5,474.5,222.5,474.5,156.5,474.5C156.5,403.833,156.5,333.167,156.5,262.5C169.344,262.829,182.011,262.495,194.5,261.5C197.167,260.833,198.833,259.167,199.5,256.5C200.495,244.011,200.829,231.344,200.5,218.5C251.833,218.5,303.167,218.5,354.5,218.5C354.5,303.833,354.5,389.167,354.5,474.5Z"/></g><g><path fill="#979797" d="M354.5,474.5C354.5,389.167,354.5,303.833,354.5,218.5C303.167,218.5,251.833,218.5,200.5,218.5C200.829,231.344,200.495,244.011,199.5,256.5C199.5,243.5,199.5,230.5,199.5,217.5C251.5,217.5,303.5,217.5,355.5,217.5C355.833,303.335,355.499,389.002,354.5,474.5Z"/></g><g><path fill="#dce0e3" d="M185.5,247.5C179.31,247.823,173.31,247.49,167.5,246.5C173.13,240.369,178.964,234.369,185,228.5C185.5,234.825,185.666,241.158,185.5,247.5Z"/></g><g><path fill="#7b7d7e" d="M167.5,246.5C173.31,247.49,179.31,247.823,185.5,247.5C179.322,248.817,172.988,248.817,166.5,247.5C166.624,246.893,166.957,246.56,167.5,246.5Z"/></g><g><path fill="#a3a3a3" d="M194.5,261.5C182.011,262.495,169.344,262.829,156.5,262.5C156.5,333.167,156.5,403.833,156.5,474.5C155.501,403.669,155.167,332.669,155.5,261.5C168.5,261.5,181.5,261.5,194.5,261.5Z"/></g><g><path fill="#030303" d="M252.5,309.5C256.426,309.047,259.592,310.38,262,313.5C262.333,334.833,262.667,356.167,263,377.5C264.979,369.149,269.812,366.649,277.5,370C278.718,371.438,279.385,373.105,279.5,375C274.409,385.346,268.909,395.512,263,405.5C258,413.5,253,413.5,248,405.5C242.091,395.512,236.591,385.346,231.5,375C232.869,369.144,236.536,367.144,242.5,369C244.956,371.408,246.789,374.241,248,377.5C248.333,356.167,248.667,334.833,249,313.5C250.376,312.295,251.542,310.962,252.5,309.5Z"/></g><g><path fill="#050505" d="M199.5,376.5C203.888,375.943,207.055,377.61,209,381.5C209.5,398.83,209.667,416.163,209.5,433.5C240.167,433.5,270.833,433.5,301.5,433.5C301.333,416.163,301.5,398.83,302,381.5C304.553,376.374,308.386,375.207,313.5,378C314.126,378.75,314.626,379.584,315,380.5C315.667,401.5,315.667,422.5,315,443.5C314.626,444.416,314.126,445.25,313.5,446C294.254,447.155,274.921,447.655,255.5,447.5C236.079,447.655,216.746,447.155,197.5,446C196.874,445.25,196.374,444.416,196,443.5C195.333,422.5,195.333,401.5,196,380.5C196.69,378.65,197.856,377.316,199.5,376.5Z"/></g></svg> `fetch fetch_txt`

```json
{
	"url": "https://arxiv.org/abs/2310.06830"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate > cs > arXiv:2310.06830 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:2310.06830 (cs) [Submitted on 10 Oct 2023 (v1), last revised 24 Aug 2024 (this version, v2)] Title:Lemur: Harmonizing Natural Language and Code for Language Agents Authors:Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu View a PDF of the paper titled Lemur: Harmonizing Natural Language and Code for Language Agents, by Yiheng Xu and 15 other authors View PDF HTML (experimental) Abstract:We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. The evolution from language chat models to functional language agents demands that models not only master human interaction, reasoning, and planning but also ensure grounding in the relevant environments. This calls for a harmonious blend of language and coding capabilities in the models. Lemur and Lemur-Chat are proposed to address this necessity, demonstrating balanced proficiencies in both domains, unlike existing open-source models that tend to specialize in either. Through meticulous pre-training using a code-intensive corpus and instruction fine-tuning on text and code data, our models achieve state-of-the-art averaged performance across diverse text and coding benchmarks among open-source models. Comprehensive experiments demonstrate Lemur's superiority over existing open-source models and its proficiency across various agent tasks involving human communication, tool usage, and interaction under fully- and partially- observable environments. The harmonization between natural and programming languages enables Lemur-Chat to significantly narrow the gap with proprietary models on agent abilities, providing key insights into developing advanced open-source agents adept at reasoning, planning, and operating seamlessly across environments. this https URL Comments: ICLR 2024 Spotlight; this https URL Subjects: Computation and Language (cs.CL) Cite as: arXiv:2310.06830 [cs.CL] (or arXiv:2310.06830v2 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2310.06830 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Yiheng Xu [view email] [v1] Tue, 10 Oct 2023 17:57:45 UTC (1,618 KB) [v2] Sat, 24 Aug 2024 21:30:00 UTC (1,096 KB) Full-text links: Access Paper: View a PDF of the paper titled Lemur: Harmonizing Natural Language and Code for Language Agents, by Yiheng Xu and 15 other authorsView PDFHTML (experimental)TeX SourceOther Formats view license Current browse context: cs.CL < prev | next > new | recent | 2023-10 Change to browse by: cs References & Citations NASA ADSGoogle Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv (What is alphaXiv?) Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Huggingface Toggle Hugging Face (What is Huggingface?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About Help contact arXivClick here to contact arXiv Contact subscribe to arXiv mailingsClick here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack
```
</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Let me check if this paper was published at ICLR 2024 and get the OpenReview entry:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><path fill="#010202" d="M201.5,22.5C253.575,18.8292,294.909,37.8292,325.5,79.5C348.846,76.447,370.846,80.6137,391.5,92C411.339,105.168,422.339,123.668,424.5,147.5C465.86,161.693,491.694,190.026,502,232.5C508.97,280.701,492.804,318.868,453.5,347C427.526,362.095,399.526,367.929,369.5,364.5C369.988,405.252,369.488,445.919,368,486.5C367.5,487,367,487.5,366.5,488C292.5,488.667,218.5,488.667,144.5,488C144,487.5,143.5,487,143,486.5C141.512,445.919,141.012,405.252,141.5,364.5C106.171,368.921,74.5043,360.421,46.5,339C10.3518,305.536-0.814861,265.036,13,217.5C22.2223,191.95,38.389,172.117,61.5,158C69.8007,153.681,78.134,149.681,86.5,146C92.1136,89.0861,121.447,50.0861,174.5,29C183.492,26.2401,192.492,24.0734,201.5,22.5Z"/></g><g><path fill="#a3d3fe" d="M204.5,36.5C254.543,34.185,292.876,54.0184,319.5,96C340.997,90.9721,361.664,93.3054,381.5,103C401.654,115.47,411.154,133.637,410,157.5C410.903,158.701,412.069,159.535,413.5,160C453.408,170.242,478.242,195.409,488,235.5C493.84,285.282,474.34,321.115,429.5,343C422.574,345.298,415.574,347.298,408.5,349C395.504,349.5,382.504,349.667,369.5,349.5C369.667,302.499,369.5,255.499,369,208.5C367.833,207.333,366.667,206.167,365.5,205C306.833,204.333,248.167,204.333,189.5,205C173,220.167,157.167,236,142,252.5C141.5,284.832,141.333,317.165,141.5,349.5C100.111,355.535,66.2777,342.535,40,310.5C17.3055,276.196,15.6388,240.863,35,204.5C50.4757,180.668,71.9757,165.002,99.5,157.5C101.376,109.215,123.042,73.0487,164.5,49C177.289,42.679,190.622,38.5123,204.5,36.5Z"/></g><g><path fill="#fefefe" d="M354.5,474.5C288.5,474.5,222.5,474.5,156.5,474.5C156.5,403.833,156.5,333.167,156.5,262.5C169.344,262.829,182.011,262.495,194.5,261.5C197.167,260.833,198.833,259.167,199.5,256.5C200.495,244.011,200.829,231.344,200.5,218.5C251.833,218.5,303.167,218.5,354.5,218.5C354.5,303.833,354.5,389.167,354.5,474.5Z"/></g><g><path fill="#979797" d="M354.5,474.5C354.5,389.167,354.5,303.833,354.5,218.5C303.167,218.5,251.833,218.5,200.5,218.5C200.829,231.344,200.495,244.011,199.5,256.5C199.5,243.5,199.5,230.5,199.5,217.5C251.5,217.5,303.5,217.5,355.5,217.5C355.833,303.335,355.499,389.002,354.5,474.5Z"/></g><g><path fill="#dce0e3" d="M185.5,247.5C179.31,247.823,173.31,247.49,167.5,246.5C173.13,240.369,178.964,234.369,185,228.5C185.5,234.825,185.666,241.158,185.5,247.5Z"/></g><g><path fill="#7b7d7e" d="M167.5,246.5C173.31,247.49,179.31,247.823,185.5,247.5C179.322,248.817,172.988,248.817,166.5,247.5C166.624,246.893,166.957,246.56,167.5,246.5Z"/></g><g><path fill="#a3a3a3" d="M194.5,261.5C182.011,262.495,169.344,262.829,156.5,262.5C156.5,333.167,156.5,403.833,156.5,474.5C155.501,403.669,155.167,332.669,155.5,261.5C168.5,261.5,181.5,261.5,194.5,261.5Z"/></g><g><path fill="#030303" d="M252.5,309.5C256.426,309.047,259.592,310.38,262,313.5C262.333,334.833,262.667,356.167,263,377.5C264.979,369.149,269.812,366.649,277.5,370C278.718,371.438,279.385,373.105,279.5,375C274.409,385.346,268.909,395.512,263,405.5C258,413.5,253,413.5,248,405.5C242.091,395.512,236.591,385.346,231.5,375C232.869,369.144,236.536,367.144,242.5,369C244.956,371.408,246.789,374.241,248,377.5C248.333,356.167,248.667,334.833,249,313.5C250.376,312.295,251.542,310.962,252.5,309.5Z"/></g><g><path fill="#050505" d="M199.5,376.5C203.888,375.943,207.055,377.61,209,381.5C209.5,398.83,209.667,416.163,209.5,433.5C240.167,433.5,270.833,433.5,301.5,433.5C301.333,416.163,301.5,398.83,302,381.5C304.553,376.374,308.386,375.207,313.5,378C314.126,378.75,314.626,379.584,315,380.5C315.667,401.5,315.667,422.5,315,443.5C314.626,444.416,314.126,445.25,313.5,446C294.254,447.155,274.921,447.655,255.5,447.5C236.079,447.655,216.746,447.155,197.5,446C196.874,445.25,196.374,444.416,196,443.5C195.333,422.5,195.333,401.5,196,380.5C196.69,378.65,197.856,377.316,199.5,376.5Z"/></g></svg> `fetch fetch_txt`

```json
{
	"url": "https://openreview.net/forum?id=hNhwSmtXRh"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Toggle navigationOpenReview.netLogin×Go to ICLR 2024 Conference homepageLemur: Harmonizing Natural Language and Code for Language AgentsYiheng Xu, Hongjin SU, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu Published: 16 Jan 2024, Last Modified: 11 Mar 2024ICLR 2024 spotlightEveryoneRevisionsBibTeXCode Of Ethics: I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.Keywords: large language model, agent, code generation, reasoning, decision makingSubmission Guidelines: I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2024/AuthorGuide.Abstract: We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. The evolution from language chat models to functional language agents demands that models not only master human interaction, reasoning, and planning but also ensure grounding in the relevant environments. This calls for a harmonious blend of language and coding capabilities in the models. Lemur and Lemur-Chat are proposed to address this necessity, demonstrating balanced proficiencies in both domains, unlike existing open-source models that tend to specialize in either. Through meticulous pretraining using a code-intensive corpus and instruction fine-tuning on text and code data, our models achieve state-of-the-art averaged performance across diverse text and coding benchmarks. Comprehensive experiments demonstrate Lemur’s superiority over existing open-source models and its proficiency across various agent tasks involving human communication, tool usage, and interaction under fully- and partially- observable environments. The harmonization between natural and programming languages enables Lemur-Chat to significantly narrow the gap with proprietary models on agent abilities, providing key insights into developing advanced open-source agents adept at reasoning, planning, and operating seamlessly across environments. Our model and code have been open-sourced at https://github.com/OpenLemur/Lemur.Anonymous Url: I certify that there is no URL (e.g., github page) that could be used to find authors' identity.No Acknowledgement Section: I certify that there is no acknowledgement section in this submission for double blind review.Primary Area: unsupervised, self-supervised, semi-supervised, and supervised representation learningSubmission Number: 3747LoadingAbout OpenReviewHosting a VenueAll VenuesContactSponsorsDonateFrequently Asked QuestionsTerms of UsePrivacy PolicyAbout OpenReviewHosting a VenueAll VenuesSponsorsFrequently Asked QuestionsContactDonateTerms of UsePrivacy PolicyOpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors. © 2025 OpenReview
```
</Expandable>
</div>

<div className="tool-call-box">
<svg height="14" width="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g fill="none" fill-rule="evenodd"><rect fill="#fabc05" height="512" rx="42.667" width="512"/><path d="M58.667 293.333L0 209.125V42.663C0 19.1 19.137 0 42.663 0h426.674C492.9 0 512 19.137 512 42.663l-282.667.004S58.667 305.14 58.667 293.333z" fill="#eb4335"/><path d="M303.3 340.528L199.876 512H42.633C19.087 512 0 492.94 0 469.328V209.125l54.537 74.854c28.457 48.33 81.024 80.762 141.167 80.762 40.548 0 77.822-14.86 106.533-39.29l5.348 5.349c-.918 3.235-2.337 6.5-4.284 9.728z" fill="#34a854"/><g fill-rule="nonzero"><path d="M194.667 85.333c-66.667 0-120 52.148-120 117.334C74.667 267.852 128 320 194.667 320c66.666 0 120-52.148 120-117.333 0-65.186-53.334-117.334-120-117.334z" fill="#4285f5"/><path d="M327.59 300.924l28.788 28.787 13.204-14.159 115.751 115.751-59.363 59.364-115.751-115.751 13.553-12.64-28.85-28.85c-27.526 20.496-61.677 32.712-98.52 32.712C105.605 366.138 32 292.532 32 201.735c0-90.796 73.605-164.402 164.402-164.402s164.402 73.606 164.402 164.402c0 37.142-12.414 71.549-33.213 99.19zM196.403 87.92c-63.231 0-113.817 50.585-113.817 113.816 0 63.232 50.586 113.817 113.817 113.817S310.22 264.967 310.22 201.735c0-63.231-50.585-113.816-113.817-113.816z" fill="#fff"/></g></g></svg> `web_search `

```json
{
	"query": "CodeLlama Code Llama arxiv"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Title: Code Llama: Open Foundation Models for Code
Link: https://arxiv.org/abs/2308.12950
Snippet: by B Rozière · 2023 · Cited by 2661 — We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models.
Sitelinks: No sitelinks

Title: Code Llama: Open Foundation Models for Code
Link: https://arxiv.org/html/2308.12950v3
Snippet: We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models.
Sitelinks: No sitelinks

Title: Exploring the Potential of Llama Models in Automated ...
Link: https://arxiv.org/html/2412.02789v1
Snippet: In this study, we explore alternatives to ChatGPT in code refinement tasks by including two open-source, smaller-scale large language models: CodeLlama and ...
Sitelinks: No sitelinks

Title: The First Prompt Counts the Most! An Evaluation of Large ...
Link: https://arxiv.org/html/2411.06774v1
Snippet: Leveraging the strengths of Llama 2 (Touvron et al., 2023) , CodeLlama is trained on a massive dataset of 500 billion to 1 trillion tokens, ...
Sitelinks: No sitelinks

Title: Code Vulnerability Detection: A Comparative Analysis of ...
Link: https://arxiv.org/html/2409.10490v1
Snippet: Through a comparative analysis, we assess the performance of emerging LLMs, specifically Llama, CodeLlama, Gemma, and CodeGemma, alongside ...
Sitelinks: No sitelinks

Title: Prompting Small Language Models for Energy-Efficient ...
Link: https://arxiv.org/html/2509.09947v1
Snippet: In this study, we investigate whether prompt engineering can improve the energy efficiency of SLMs in code generation. We evaluate four open- ...
Sitelinks: No sitelinks

Title: Instruct-Code-Llama: Improving Capabilities of Language ...
Link: https://dl.acm.org/doi/10.1007/978-981-97-5669-8_11
Snippet: by Z Liu · 2024 · Cited by 5 — We introduced a novel fine-tuning approach called Reinforcement Learning with Online Judging Feedback (RLOJF) to align the generated code with the correct code.
Sitelinks: No sitelinks

Title: Source Code Summarization in the Era of Large Language ...
Link: https://arxiv.org/html/2407.07959v2
Snippet: Code Llama [45] is a family of LLMs ... Rapin et al., “Code llama: Open foundation models for code,” arXiv preprint arXiv:2308.12950, 2023.
Sitelinks: No sitelinks

Title: Code Llama: Open Foundation Models for Code
Link: https://huggingface.co/papers/2308.12950
Snippet: We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models.
Sitelinks: No sitelinks

Title: arXiv:2401.02415v2 [cs.CL] 30 May 2024
Link: https://arxiv.org/pdf/2401.02415
Snippet: by C Wu · 2024 · Cited by 94 — This includes general-purpose pretrained models like LLaMA2 and code-oriented pretrained models like CodeLLaMA. The results are presented in ...
Sitelinks: No sitelinks

```
</Expandable>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><path fill="#010202" d="M201.5,22.5C253.575,18.8292,294.909,37.8292,325.5,79.5C348.846,76.447,370.846,80.6137,391.5,92C411.339,105.168,422.339,123.668,424.5,147.5C465.86,161.693,491.694,190.026,502,232.5C508.97,280.701,492.804,318.868,453.5,347C427.526,362.095,399.526,367.929,369.5,364.5C369.988,405.252,369.488,445.919,368,486.5C367.5,487,367,487.5,366.5,488C292.5,488.667,218.5,488.667,144.5,488C144,487.5,143.5,487,143,486.5C141.512,445.919,141.012,405.252,141.5,364.5C106.171,368.921,74.5043,360.421,46.5,339C10.3518,305.536-0.814861,265.036,13,217.5C22.2223,191.95,38.389,172.117,61.5,158C69.8007,153.681,78.134,149.681,86.5,146C92.1136,89.0861,121.447,50.0861,174.5,29C183.492,26.2401,192.492,24.0734,201.5,22.5Z"/></g><g><path fill="#a3d3fe" d="M204.5,36.5C254.543,34.185,292.876,54.0184,319.5,96C340.997,90.9721,361.664,93.3054,381.5,103C401.654,115.47,411.154,133.637,410,157.5C410.903,158.701,412.069,159.535,413.5,160C453.408,170.242,478.242,195.409,488,235.5C493.84,285.282,474.34,321.115,429.5,343C422.574,345.298,415.574,347.298,408.5,349C395.504,349.5,382.504,349.667,369.5,349.5C369.667,302.499,369.5,255.499,369,208.5C367.833,207.333,366.667,206.167,365.5,205C306.833,204.333,248.167,204.333,189.5,205C173,220.167,157.167,236,142,252.5C141.5,284.832,141.333,317.165,141.5,349.5C100.111,355.535,66.2777,342.535,40,310.5C17.3055,276.196,15.6388,240.863,35,204.5C50.4757,180.668,71.9757,165.002,99.5,157.5C101.376,109.215,123.042,73.0487,164.5,49C177.289,42.679,190.622,38.5123,204.5,36.5Z"/></g><g><path fill="#fefefe" d="M354.5,474.5C288.5,474.5,222.5,474.5,156.5,474.5C156.5,403.833,156.5,333.167,156.5,262.5C169.344,262.829,182.011,262.495,194.5,261.5C197.167,260.833,198.833,259.167,199.5,256.5C200.495,244.011,200.829,231.344,200.5,218.5C251.833,218.5,303.167,218.5,354.5,218.5C354.5,303.833,354.5,389.167,354.5,474.5Z"/></g><g><path fill="#979797" d="M354.5,474.5C354.5,389.167,354.5,303.833,354.5,218.5C303.167,218.5,251.833,218.5,200.5,218.5C200.829,231.344,200.495,244.011,199.5,256.5C199.5,243.5,199.5,230.5,199.5,217.5C251.5,217.5,303.5,217.5,355.5,217.5C355.833,303.335,355.499,389.002,354.5,474.5Z"/></g><g><path fill="#dce0e3" d="M185.5,247.5C179.31,247.823,173.31,247.49,167.5,246.5C173.13,240.369,178.964,234.369,185,228.5C185.5,234.825,185.666,241.158,185.5,247.5Z"/></g><g><path fill="#7b7d7e" d="M167.5,246.5C173.31,247.49,179.31,247.823,185.5,247.5C179.322,248.817,172.988,248.817,166.5,247.5C166.624,246.893,166.957,246.56,167.5,246.5Z"/></g><g><path fill="#a3a3a3" d="M194.5,261.5C182.011,262.495,169.344,262.829,156.5,262.5C156.5,333.167,156.5,403.833,156.5,474.5C155.501,403.669,155.167,332.669,155.5,261.5C168.5,261.5,181.5,261.5,194.5,261.5Z"/></g><g><path fill="#030303" d="M252.5,309.5C256.426,309.047,259.592,310.38,262,313.5C262.333,334.833,262.667,356.167,263,377.5C264.979,369.149,269.812,366.649,277.5,370C278.718,371.438,279.385,373.105,279.5,375C274.409,385.346,268.909,395.512,263,405.5C258,413.5,253,413.5,248,405.5C242.091,395.512,236.591,385.346,231.5,375C232.869,369.144,236.536,367.144,242.5,369C244.956,371.408,246.789,374.241,248,377.5C248.333,356.167,248.667,334.833,249,313.5C250.376,312.295,251.542,310.962,252.5,309.5Z"/></g><g><path fill="#050505" d="M199.5,376.5C203.888,375.943,207.055,377.61,209,381.5C209.5,398.83,209.667,416.163,209.5,433.5C240.167,433.5,270.833,433.5,301.5,433.5C301.333,416.163,301.5,398.83,302,381.5C304.553,376.374,308.386,375.207,313.5,378C314.126,378.75,314.626,379.584,315,380.5C315.667,401.5,315.667,422.5,315,443.5C314.626,444.416,314.126,445.25,313.5,446C294.254,447.155,274.921,447.655,255.5,447.5C236.079,447.655,216.746,447.155,197.5,446C196.874,445.25,196.374,444.416,196,443.5C195.333,422.5,195.333,401.5,196,380.5C196.69,378.65,197.856,377.316,199.5,376.5Z"/></g></svg> `fetch fetch_txt`

```json
{
	"url": "https://arxiv.org/abs/2308.12950"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate > cs > arXiv:2308.12950 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:2308.12950 (cs) [Submitted on 24 Aug 2023 (v1), last revised 31 Jan 2024 (this version, v3)] Title:Code Llama: Open Foundation Models for Code Authors:Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve View a PDF of the paper titled Code Llama: Open Foundation Models for Code, by Baptiste Rozi\`ere and 25 other authors View PDF HTML (experimental) Abstract:We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use. Subjects: Computation and Language (cs.CL) Cite as: arXiv:2308.12950 [cs.CL] (or arXiv:2308.12950v3 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2308.12950 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Baptiste Roziere [view email] [v1] Thu, 24 Aug 2023 17:39:13 UTC (1,371 KB) [v2] Fri, 25 Aug 2023 08:51:22 UTC (1,371 KB) [v3] Wed, 31 Jan 2024 19:47:26 UTC (1,988 KB) Full-text links: Access Paper: View a PDF of the paper titled Code Llama: Open Foundation Models for Code, by Baptiste Rozi\`ere and 25 other authorsView PDFHTML (experimental)TeX SourceOther Formats view license Current browse context: cs.CL < prev | next > new | recent | 2023-08 Change to browse by: cs References & Citations NASA ADSGoogle Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv (What is alphaXiv?) Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Huggingface Toggle Hugging Face (What is Huggingface?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About Help contact arXivClick here to contact arXiv Contact subscribe to arXiv mailingsClick here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack
```
</Expandable>
</div>

<div className="tool-call-box">
<svg height="14" width="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g fill="none" fill-rule="evenodd"><rect fill="#fabc05" height="512" rx="42.667" width="512"/><path d="M58.667 293.333L0 209.125V42.663C0 19.1 19.137 0 42.663 0h426.674C492.9 0 512 19.137 512 42.663l-282.667.004S58.667 305.14 58.667 293.333z" fill="#eb4335"/><path d="M303.3 340.528L199.876 512H42.633C19.087 512 0 492.94 0 469.328V209.125l54.537 74.854c28.457 48.33 81.024 80.762 141.167 80.762 40.548 0 77.822-14.86 106.533-39.29l5.348 5.349c-.918 3.235-2.337 6.5-4.284 9.728z" fill="#34a854"/><g fill-rule="nonzero"><path d="M194.667 85.333c-66.667 0-120 52.148-120 117.334C74.667 267.852 128 320 194.667 320c66.666 0 120-52.148 120-117.333 0-65.186-53.334-117.334-120-117.334z" fill="#4285f5"/><path d="M327.59 300.924l28.788 28.787 13.204-14.159 115.751 115.751-59.363 59.364-115.751-115.751 13.553-12.64-28.85-28.85c-27.526 20.496-61.677 32.712-98.52 32.712C105.605 366.138 32 292.532 32 201.735c0-90.796 73.605-164.402 164.402-164.402s164.402 73.606 164.402 164.402c0 37.142-12.414 71.549-33.213 99.19zM196.403 87.92c-63.231 0-113.817 50.585-113.817 113.816 0 63.232 50.586 113.817 113.817 113.817S310.22 264.967 310.22 201.735c0-63.231-50.585-113.816-113.817-113.816z" fill="#fff"/></g></g></svg> `web_search `

```json
{
	"query": "MMLU Measuring Massive Multitask Language Understanding arxiv"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Title: Measuring Massive Multitask Language Understanding
Link: https://arxiv.org/abs/2009.03300
Snippet: by D Hendrycks · 2020 · Cited by 5443 — We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, ...
Sitelinks: No sitelinks

Title: Measuring Massive Multitask Language Understanding
Link: https://arxiv.org/pdf/2009.03300
Snippet: by D Hendrycks · 2020 · Cited by 5443 — We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science,.
Sitelinks: No sitelinks

Title: CMMLU: Measuring massive multitask language ...
Link: https://arxiv.org/abs/2306.09212
Snippet: by H Li · 2023 · Cited by 336 — This paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese benchmark that covers various subjects.
Sitelinks: No sitelinks

Title: [2407.12402] TurkishMMLU: Measuring Massive Multitask ...
Link: https://arxiv.org/abs/2407.12402
Snippet: by A Yüksel · 2024 · Cited by 16 — We introduce the first multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs' understanding of the Turkish language.
Sitelinks: No sitelinks

Title: hendrycks/test: Measuring Massive Multitask Language ...
Link: https://github.com/hendrycks/test
Snippet: This is the repository for Measuring Massive Multitask Language Understanding by Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song ...
Sitelinks: No sitelinks

Title: Measuring Massive Multitask Language Understanding
Link: https://www.researchgate.net/publication/344159828_Measuring_Massive_Multitask_Language_Understanding
Snippet: We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, ...
Sitelinks: No sitelinks

Title: BnMMLU: Measuring Massive Multitask Language ...
Link: https://arxiv.org/abs/2505.18951
Snippet: In this paper, we introduce BnMMLU, a benchmark to evaluate the multitask language understanding capabilities of Bengali in language models.
Sitelinks: No sitelinks

Title: cais/mmlu · Datasets at Hugging Face
Link: https://huggingface.co/datasets/cais/mmlu
Snippet: Measuring Massive Multitask Language Understanding by Dan Hendrycks, Collin ... arxiv.org. Size of downloaded dataset files: 104 MB. Size of the auto ...
Sitelinks: No sitelinks

Title: Measuring Massive Multitask Language Understanding in ...
Link: https://aclanthology.org/2025.naacl-long.206.pdf
Snippet: by G Son · 2025 · Cited by 65 — We propose KMMLU, a Korean benchmark with 35,030 expert-level multiple-choice ques- tions across 45 subjects ranging from human-.
Sitelinks: No sitelinks

Title: MMLU
Link: https://en.wikipedia.org/wiki/MMLU
Snippet: Measuring Massive Multitask Language Understanding (MMLU) is a popular benchmark for evaluating the capabilities of large language models. ... arXiv:2009.03300.
Sitelinks: No sitelinks

```
</Expandable>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><path fill="#010202" d="M201.5,22.5C253.575,18.8292,294.909,37.8292,325.5,79.5C348.846,76.447,370.846,80.6137,391.5,92C411.339,105.168,422.339,123.668,424.5,147.5C465.86,161.693,491.694,190.026,502,232.5C508.97,280.701,492.804,318.868,453.5,347C427.526,362.095,399.526,367.929,369.5,364.5C369.988,405.252,369.488,445.919,368,486.5C367.5,487,367,487.5,366.5,488C292.5,488.667,218.5,488.667,144.5,488C144,487.5,143.5,487,143,486.5C141.512,445.919,141.012,405.252,141.5,364.5C106.171,368.921,74.5043,360.421,46.5,339C10.3518,305.536-0.814861,265.036,13,217.5C22.2223,191.95,38.389,172.117,61.5,158C69.8007,153.681,78.134,149.681,86.5,146C92.1136,89.0861,121.447,50.0861,174.5,29C183.492,26.2401,192.492,24.0734,201.5,22.5Z"/></g><g><path fill="#a3d3fe" d="M204.5,36.5C254.543,34.185,292.876,54.0184,319.5,96C340.997,90.9721,361.664,93.3054,381.5,103C401.654,115.47,411.154,133.637,410,157.5C410.903,158.701,412.069,159.535,413.5,160C453.408,170.242,478.242,195.409,488,235.5C493.84,285.282,474.34,321.115,429.5,343C422.574,345.298,415.574,347.298,408.5,349C395.504,349.5,382.504,349.667,369.5,349.5C369.667,302.499,369.5,255.499,369,208.5C367.833,207.333,366.667,206.167,365.5,205C306.833,204.333,248.167,204.333,189.5,205C173,220.167,157.167,236,142,252.5C141.5,284.832,141.333,317.165,141.5,349.5C100.111,355.535,66.2777,342.535,40,310.5C17.3055,276.196,15.6388,240.863,35,204.5C50.4757,180.668,71.9757,165.002,99.5,157.5C101.376,109.215,123.042,73.0487,164.5,49C177.289,42.679,190.622,38.5123,204.5,36.5Z"/></g><g><path fill="#fefefe" d="M354.5,474.5C288.5,474.5,222.5,474.5,156.5,474.5C156.5,403.833,156.5,333.167,156.5,262.5C169.344,262.829,182.011,262.495,194.5,261.5C197.167,260.833,198.833,259.167,199.5,256.5C200.495,244.011,200.829,231.344,200.5,218.5C251.833,218.5,303.167,218.5,354.5,218.5C354.5,303.833,354.5,389.167,354.5,474.5Z"/></g><g><path fill="#979797" d="M354.5,474.5C354.5,389.167,354.5,303.833,354.5,218.5C303.167,218.5,251.833,218.5,200.5,218.5C200.829,231.344,200.495,244.011,199.5,256.5C199.5,243.5,199.5,230.5,199.5,217.5C251.5,217.5,303.5,217.5,355.5,217.5C355.833,303.335,355.499,389.002,354.5,474.5Z"/></g><g><path fill="#dce0e3" d="M185.5,247.5C179.31,247.823,173.31,247.49,167.5,246.5C173.13,240.369,178.964,234.369,185,228.5C185.5,234.825,185.666,241.158,185.5,247.5Z"/></g><g><path fill="#7b7d7e" d="M167.5,246.5C173.31,247.49,179.31,247.823,185.5,247.5C179.322,248.817,172.988,248.817,166.5,247.5C166.624,246.893,166.957,246.56,167.5,246.5Z"/></g><g><path fill="#a3a3a3" d="M194.5,261.5C182.011,262.495,169.344,262.829,156.5,262.5C156.5,333.167,156.5,403.833,156.5,474.5C155.501,403.669,155.167,332.669,155.5,261.5C168.5,261.5,181.5,261.5,194.5,261.5Z"/></g><g><path fill="#030303" d="M252.5,309.5C256.426,309.047,259.592,310.38,262,313.5C262.333,334.833,262.667,356.167,263,377.5C264.979,369.149,269.812,366.649,277.5,370C278.718,371.438,279.385,373.105,279.5,375C274.409,385.346,268.909,395.512,263,405.5C258,413.5,253,413.5,248,405.5C242.091,395.512,236.591,385.346,231.5,375C232.869,369.144,236.536,367.144,242.5,369C244.956,371.408,246.789,374.241,248,377.5C248.333,356.167,248.667,334.833,249,313.5C250.376,312.295,251.542,310.962,252.5,309.5Z"/></g><g><path fill="#050505" d="M199.5,376.5C203.888,375.943,207.055,377.61,209,381.5C209.5,398.83,209.667,416.163,209.5,433.5C240.167,433.5,270.833,433.5,301.5,433.5C301.333,416.163,301.5,398.83,302,381.5C304.553,376.374,308.386,375.207,313.5,378C314.126,378.75,314.626,379.584,315,380.5C315.667,401.5,315.667,422.5,315,443.5C314.626,444.416,314.126,445.25,313.5,446C294.254,447.155,274.921,447.655,255.5,447.5C236.079,447.655,216.746,447.155,197.5,446C196.874,445.25,196.374,444.416,196,443.5C195.333,422.5,195.333,401.5,196,380.5C196.69,378.65,197.856,377.316,199.5,376.5Z"/></g></svg> `fetch fetch_txt`

```json
{
	"url": "https://arxiv.org/abs/2009.03300"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate > cs > arXiv:2009.03300 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computers and Society arXiv:2009.03300 (cs) [Submitted on 7 Sep 2020 (v1), last revised 12 Jan 2021 (this version, v3)] Title:Measuring Massive Multitask Language Understanding Authors:Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt View a PDF of the paper titled Measuring Massive Multitask Language Understanding, by Dan Hendrycks and 6 other authors View PDF Abstract:We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings. Comments: ICLR 2021; the test and code is available at this https URL Subjects: Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) Cite as: arXiv:2009.03300 [cs.CY] (or arXiv:2009.03300v3 [cs.CY] for this version) https://doi.org/10.48550/arXiv.2009.03300 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Dan Hendrycks [view email] [v1] Mon, 7 Sep 2020 17:59:25 UTC (2,172 KB) [v2] Mon, 21 Sep 2020 05:06:57 UTC (2,402 KB) [v3] Tue, 12 Jan 2021 18:57:11 UTC (2,578 KB) Full-text links: Access Paper: View a PDF of the paper titled Measuring Massive Multitask Language Understanding, by Dan Hendrycks and 6 other authorsView PDFTeX SourceOther Formats view license Current browse context: cs.CY < prev | next > new | recent | 2020-09 Change to browse by: cs cs.AI cs.CL cs.LG References & Citations NASA ADSGoogle Scholar Semantic Scholar 4 blog links (what is this?) DBLP - CS Bibliography listing | bibtex Dan HendrycksCollin BurnsSteven BasartMantas MazeikaDawn Song … a export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv (What is alphaXiv?) Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Huggingface Toggle Hugging Face (What is Huggingface?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About Help contact arXivClick here to contact arXiv Contact subscribe to arXiv mailingsClick here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack
```
</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Let me check if MMLU was published at ICLR 2021:
</div>

<div className="tool-call-box">
<svg height="14" width="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g fill="none" fill-rule="evenodd"><rect fill="#fabc05" height="512" rx="42.667" width="512"/><path d="M58.667 293.333L0 209.125V42.663C0 19.1 19.137 0 42.663 0h426.674C492.9 0 512 19.137 512 42.663l-282.667.004S58.667 305.14 58.667 293.333z" fill="#eb4335"/><path d="M303.3 340.528L199.876 512H42.633C19.087 512 0 492.94 0 469.328V209.125l54.537 74.854c28.457 48.33 81.024 80.762 141.167 80.762 40.548 0 77.822-14.86 106.533-39.29l5.348 5.349c-.918 3.235-2.337 6.5-4.284 9.728z" fill="#34a854"/><g fill-rule="nonzero"><path d="M194.667 85.333c-66.667 0-120 52.148-120 117.334C74.667 267.852 128 320 194.667 320c66.666 0 120-52.148 120-117.333 0-65.186-53.334-117.334-120-117.334z" fill="#4285f5"/><path d="M327.59 300.924l28.788 28.787 13.204-14.159 115.751 115.751-59.363 59.364-115.751-115.751 13.553-12.64-28.85-28.85c-27.526 20.496-61.677 32.712-98.52 32.712C105.605 366.138 32 292.532 32 201.735c0-90.796 73.605-164.402 164.402-164.402s164.402 73.606 164.402 164.402c0 37.142-12.414 71.549-33.213 99.19zM196.403 87.92c-63.231 0-113.817 50.585-113.817 113.816 0 63.232 50.586 113.817 113.817 113.817S310.22 264.967 310.22 201.735c0-63.231-50.585-113.816-113.817-113.816z" fill="#fff"/></g></g></svg> `web_search `

```json
{
	"query": "MMLU ICLR 2021 OpenReview"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Title: Measuring Massive Multitask Language Understanding
Link: https://openreview.net/forum?id=d7KBjmI3GmQ
Snippet: by D Hendrycks · Cited by 5428 — We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, ...
Sitelinks: No sitelinks

Title: Measuring Massive Multitask Language Understanding
Link: https://openreview.net/pdf?id=d7KBjmI3GmQ
Snippet: by D Hendrycks · Cited by 5428 — We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science,.
Sitelinks: No sitelinks

Title: CMMLU: Measuring massive multitask language ...
Link: https://openreview.net/forum?id=ck4SG9lnrQ
Snippet: by H Li · Cited by 337 — A comprehensive Chinese assessment suite specifically designed to evaluate the knowledge and reasoning abilities of LLMs within the Chinese language and ...
Sitelinks: No sitelinks

Title: Measuring Massive Multitask Language Understanding
Link: https://arxiv.org/abs/2009.03300
Snippet: by D Hendrycks · 2020 · Cited by 5443 — We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, ...
Sitelinks: No sitelinks

Title: Large Language Models Are Not Robust Multiple Choice ...
Link: https://openreview.net/forum?id=shr9PXz7T0
Snippet: by C Zheng · Cited by 308 — This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent “selection bias”.
Sitelinks: No sitelinks

Title: To CoT or not to CoT? Chain-of-thought helps mainly on ...
Link: https://openreview.net/forum?id=w6nlcS8Kkn
Snippet: by ZR Sprague · Cited by 160 — On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, ...
Sitelinks: No sitelinks

Title: ICLR 2021 Conference Submissions
Link: https://openreview.net/submissions?venue=ICLR.cc/2021/Conference
Snippet: ICLR 2021 Conference Submissions ... Readers: Everyone. FSV: Learning to Factorize Soft Value Function for Cooperative Multi-Agent Reinforcement Learning.
Sitelinks: No sitelinks

Title: Large (Vision) Language Models are Unsupervised In- ...
Link: https://openreview.net/forum?id=ohJxgRLlLt
Snippet: by A Gadetsky · Cited by 2 — This paper proposes an unsupervised joint inference framework for large language models and vision-language models.
Sitelinks: No sitelinks

Title: Are We Done with MMLU?
Link: https://arxiv.org/html/2406.04127v1
Snippet: We find that, despite its popularity, MMLU suffers from numerous errors that can mislead evaluation and model comparison.
Sitelinks: No sitelinks

Title: Training on the Test Task Confounds Evaluation and ...
Link: https://openreview.net/forum?id=jOmk0uS1hl
Snippet: by R Dominguez-Olmedo · Cited by 22 — We demonstrate that training on the test task confounds both relative model evaluations and claims about emergent capabilities.
Sitelinks: No sitelinks

```
</Expandable>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><path fill="#010202" d="M201.5,22.5C253.575,18.8292,294.909,37.8292,325.5,79.5C348.846,76.447,370.846,80.6137,391.5,92C411.339,105.168,422.339,123.668,424.5,147.5C465.86,161.693,491.694,190.026,502,232.5C508.97,280.701,492.804,318.868,453.5,347C427.526,362.095,399.526,367.929,369.5,364.5C369.988,405.252,369.488,445.919,368,486.5C367.5,487,367,487.5,366.5,488C292.5,488.667,218.5,488.667,144.5,488C144,487.5,143.5,487,143,486.5C141.512,445.919,141.012,405.252,141.5,364.5C106.171,368.921,74.5043,360.421,46.5,339C10.3518,305.536-0.814861,265.036,13,217.5C22.2223,191.95,38.389,172.117,61.5,158C69.8007,153.681,78.134,149.681,86.5,146C92.1136,89.0861,121.447,50.0861,174.5,29C183.492,26.2401,192.492,24.0734,201.5,22.5Z"/></g><g><path fill="#a3d3fe" d="M204.5,36.5C254.543,34.185,292.876,54.0184,319.5,96C340.997,90.9721,361.664,93.3054,381.5,103C401.654,115.47,411.154,133.637,410,157.5C410.903,158.701,412.069,159.535,413.5,160C453.408,170.242,478.242,195.409,488,235.5C493.84,285.282,474.34,321.115,429.5,343C422.574,345.298,415.574,347.298,408.5,349C395.504,349.5,382.504,349.667,369.5,349.5C369.667,302.499,369.5,255.499,369,208.5C367.833,207.333,366.667,206.167,365.5,205C306.833,204.333,248.167,204.333,189.5,205C173,220.167,157.167,236,142,252.5C141.5,284.832,141.333,317.165,141.5,349.5C100.111,355.535,66.2777,342.535,40,310.5C17.3055,276.196,15.6388,240.863,35,204.5C50.4757,180.668,71.9757,165.002,99.5,157.5C101.376,109.215,123.042,73.0487,164.5,49C177.289,42.679,190.622,38.5123,204.5,36.5Z"/></g><g><path fill="#fefefe" d="M354.5,474.5C288.5,474.5,222.5,474.5,156.5,474.5C156.5,403.833,156.5,333.167,156.5,262.5C169.344,262.829,182.011,262.495,194.5,261.5C197.167,260.833,198.833,259.167,199.5,256.5C200.495,244.011,200.829,231.344,200.5,218.5C251.833,218.5,303.167,218.5,354.5,218.5C354.5,303.833,354.5,389.167,354.5,474.5Z"/></g><g><path fill="#979797" d="M354.5,474.5C354.5,389.167,354.5,303.833,354.5,218.5C303.167,218.5,251.833,218.5,200.5,218.5C200.829,231.344,200.495,244.011,199.5,256.5C199.5,243.5,199.5,230.5,199.5,217.5C251.5,217.5,303.5,217.5,355.5,217.5C355.833,303.335,355.499,389.002,354.5,474.5Z"/></g><g><path fill="#dce0e3" d="M185.5,247.5C179.31,247.823,173.31,247.49,167.5,246.5C173.13,240.369,178.964,234.369,185,228.5C185.5,234.825,185.666,241.158,185.5,247.5Z"/></g><g><path fill="#7b7d7e" d="M167.5,246.5C173.31,247.49,179.31,247.823,185.5,247.5C179.322,248.817,172.988,248.817,166.5,247.5C166.624,246.893,166.957,246.56,167.5,246.5Z"/></g><g><path fill="#a3a3a3" d="M194.5,261.5C182.011,262.495,169.344,262.829,156.5,262.5C156.5,333.167,156.5,403.833,156.5,474.5C155.501,403.669,155.167,332.669,155.5,261.5C168.5,261.5,181.5,261.5,194.5,261.5Z"/></g><g><path fill="#030303" d="M252.5,309.5C256.426,309.047,259.592,310.38,262,313.5C262.333,334.833,262.667,356.167,263,377.5C264.979,369.149,269.812,366.649,277.5,370C278.718,371.438,279.385,373.105,279.5,375C274.409,385.346,268.909,395.512,263,405.5C258,413.5,253,413.5,248,405.5C242.091,395.512,236.591,385.346,231.5,375C232.869,369.144,236.536,367.144,242.5,369C244.956,371.408,246.789,374.241,248,377.5C248.333,356.167,248.667,334.833,249,313.5C250.376,312.295,251.542,310.962,252.5,309.5Z"/></g><g><path fill="#050505" d="M199.5,376.5C203.888,375.943,207.055,377.61,209,381.5C209.5,398.83,209.667,416.163,209.5,433.5C240.167,433.5,270.833,433.5,301.5,433.5C301.333,416.163,301.5,398.83,302,381.5C304.553,376.374,308.386,375.207,313.5,378C314.126,378.75,314.626,379.584,315,380.5C315.667,401.5,315.667,422.5,315,443.5C314.626,444.416,314.126,445.25,313.5,446C294.254,447.155,274.921,447.655,255.5,447.5C236.079,447.655,216.746,447.155,197.5,446C196.874,445.25,196.374,444.416,196,443.5C195.333,422.5,195.333,401.5,196,380.5C196.69,378.65,197.856,377.316,199.5,376.5Z"/></g></svg> `fetch fetch_txt`

```json
{
	"url": "https://openreview.net/forum?id=d7KBjmI3GmQ"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Toggle navigationOpenReview.netLogin×Go to ICLR 2021 Conference homepageMeasuring Massive Multitask Language UnderstandingDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt Published: 12 Jan 2021, Last Modified: 22 Jun 2025ICLR 2021 PosterReaders: EveryoneKeywords: multitask, few-shotAbstract: We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.Code Of Ethics: I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of EthicsOne-sentence Summary: We test language models on 57 different multiple-choice tasks.Data: [ETHICS](https://paperswithcode.com/dataset/ethics-1), [GLUE](https://paperswithcode.com/dataset/glue), [HellaSwag](https://paperswithcode.com/dataset/hellaswag), [SuperGLUE](https://paperswithcode.com/dataset/superglue)Community Implementations: [![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/measuring-massive-multitask-language/code)13 RepliesLoadingAbout OpenReviewHosting a VenueAll VenuesContactSponsorsDonateFrequently Asked QuestionsTerms of UsePrivacy PolicyAbout OpenReviewHosting a VenueAll VenuesSponsorsFrequently Asked QuestionsContactDonateTerms of UsePrivacy PolicyOpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors. © 2025 OpenReview
```
</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Now I have all the information needed. Let me add these articles to the ref.bib file in the proper format:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem edit_file`

```json
{
	"path": "/workspace/dumps/workspace/ref.bib",
	 "edits": [{"oldText":"@article{jiang2024logicpro,
	\n  title={LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning},
	\n  author={Jiang,
	 Jin and Yan,
	 Yuchen and Liu,
	 Yang and Jin,
	 Yonggang and Peng,
	 Shuai and Zhang,
	 Mengdi and Cai,
	 Xunliang and Cao,
	 Yixin and Gao,
	 Liangcai and Tang,
	 Zhi},
	\n  journal={arXiv preprint arXiv:2409.12929},
	\n  year={2024}\n}",
	"newText":"@article{jiang2024logicpro,
	\n  title={LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning},
	\n  author={Jiang,
	 Jin and Yan,
	 Yuchen and Liu,
	 Yang and Jin,
	 Yonggang and Peng,
	 Shuai and Zhang,
	 Mengdi and Cai,
	 Xunliang and Cao,
	 Yixin and Gao,
	 Liangcai and Tang,
	 Zhi},
	\n  journal={arXiv preprint arXiv:2409.12929},
	\n  year={2024}\n}\n\n@article{yuan2023scaling,
	\n  title={Scaling Relationship on Learning Mathematical Reasoning with Large Language Models},
	\n  author={Yuan,
	 Zheng and Yuan,
	 Hongyi and Li,
	 Chengpeng and Dong,
	 Guanting and Lu,
	 Keming and Tan,
	 Chuanqi and Zhou,
	 Chang and Zhou,
	 Jingren},
	\n  journal={arXiv preprint arXiv:2308.01825},
	\n  year={2023}\n}\n\n@article{yang2024qwen25math,
	\n  title={Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement},
	\n  author={Yang,
	 An and Zhang,
	 Beichen and Hui,
	 Binyuan and Gao,
	 Bofei and Yu,
	 Bowen and Li,
	 Chengpeng and Liu,
	 Dayiheng and Tu,
	 Jianhong and Zhou,
	 Jingren and Lin,
	 Junyang and Lu,
	 Keming and Xue,
	 Mingfeng and Lin,
	 Runji and Liu,
	 Tianyu and Ren,
	 Xingzhang and Zhang,
	 Zhenru},
	\n  journal={arXiv preprint arXiv:2409.12122},
	\n  year={2024}\n}\n\n@inproceedings{\nxu2024lemur,
	\ntitle={Lemur: Harmonizing Natural Language and Code for Language Agents},
	\nauthor={Yiheng Xu and Hongjin Su and Chen Xing and Boyu Mi and Qian Liu and Weijia Shi and Binyuan Hui and Fan Zhou and Yitao Liu and Tianbao Xie and Zhoujun Cheng and Siheng Zhao and Lingpeng Kong and Bailin Wang and Caiming Xiong and Tao Yu},
	\nbooktitle={The Twelfth International Conference on Learning Representations},
	\nyear={2024},
	\nurl={https://openreview.net/forum?id=hNhwSmtXRh}\n}\n\n@article{roziere2023code,
	\n  title={Code Llama: Open Foundation Models for Code},
	\n  author={Rozi{\\'e}re,
	 Baptiste and Gehring,
	 Jonas and Gloeckle,
	 Fabian and Sootla,
	 Sten and Gat,
	 Itai and Tan,
	 Xiaoqing Ellen and Adi,
	 Yossi and Liu,
	 Jingyu and Sauvestre,
	 Romain and Remez,
	 Tal and Rapin,
	 J{\\'e}r{\\'e}my and Kozhevnikov,
	 Artyom and Evtimov,
	 Ivan and Bitton,
	 Joanna and Bhatt,
	 Manish and Ferrer,
	 Cristian Canton and Grattafiori,
	 Aaron and Xiong,
	 Wenhan and D{\\'e}fossez,
	 Alexandre and Copet,
	 Jade and Azhar,
	 Faisal and Touvron,
	 Hugo and Martin,
	 Louis and Usunier,
	 Nicolas and Scialom,
	 Thomas and Synnaeve,
	 Gabriel},
	\n  journal={arXiv preprint arXiv:2308.12950},
	\n  year={2023}\n}\n\n@inproceedings{\nhendrycks2021measuring,
	\ntitle={Measuring Massive Multitask Language Understanding},
	\nauthor={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
	\nbooktitle={International Conference on Learning Representations},
	\nyear={2021},
	\nurl={https://openreview.net/forum?id=d7KBjmI3GmQ}\n}"}]
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
diff
Index: /workspace/dumps/workspace/ref.bib
===================================================================
--- /workspace/dumps/workspace/ref.bib	original
+++ /workspace/dumps/workspace/ref.bib	modified
@@ -408,5 +408,44 @@
   title={LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning},
   author={Jiang, Jin and Yan, Yuchen and Liu, Yang and Jin, Yonggang and Peng, Shuai and Zhang, Mengdi and Cai, Xunliang and Cao, Yixin and Gao, Liangcai and Tang, Zhi},
   journal={arXiv preprint arXiv:2409.12929},
   year={2024}
+}
+
+@article{yuan2023scaling,
+  title={Scaling Relationship on Learning Mathematical Reasoning with Large Language Models},
+  author={Yuan, Zheng and Yuan, Hongyi and Li, Chengpeng and Dong, Guanting and Lu, Keming and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren},
+  journal={arXiv preprint arXiv:2308.01825},
+  year={2023}
+}
+
+@article{yang2024qwen25math,
+  title={Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement},
+  author={Yang, An and Zhang, Beichen and Hui, Binyuan and Gao, Bofei and Yu, Bowen and Li, Chengpeng and Liu, Dayiheng and Tu, Jianhong and Zhou, Jingren and Lin, Junyang and Lu, Keming and Xue, Mingfeng and Lin, Runji and Liu, Tianyu and Ren, Xingzhang and Zhang, Zhenru},
+  journal={arXiv preprint arXiv:2409.12122},
+  year={2024}
+}
+
+@inproceedings{
+xu2024lemur,
+title={Lemur: Harmonizing Natural Language and Code for Language Agents},
+author={Yiheng Xu and Hongjin Su and Chen Xing and Boyu Mi and Qian Liu and Weijia Shi and Binyuan Hui and Fan Zhou and Yitao Liu and Tianbao Xie and Zhoujun Cheng and Siheng Zhao and Lingpeng Kong and Bailin Wang and Caiming Xiong and Tao Yu},
+booktitle={The Twelfth International Conference on Learning Representations},
+year={2024},
+url={https://openreview.net/forum?id=hNhwSmtXRh}
+}
+
+@article{roziere2023code,
+  title={Code Llama: Open Foundation Models for Code},
+  author={Rozi{\'e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and Rapin, J{\'e}r{\'e}my and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and D{\'e}fossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
+  journal={arXiv preprint arXiv:2308.12950},
+  year={2023}
+}
+
+@inproceedings{
+hendrycks2021measuring,
+title={Measuring Massive Multitask Language Understanding},
+author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
+booktitle={International Conference on Learning Representations},
+year={2021},
+url={https://openreview.net/forum?id=d7KBjmI3GmQ}
 }
\ No newline at end of file



```
</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Perfect! I have successfully added all the requested articles to the ref.bib file. Let me verify the additions by reading the updated file:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

```json
{
	"path": "/workspace/dumps/workspace/ref.bib",
	 "tail": 50
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{jiang2024logicpro,
  title={LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning},
  author={Jiang, Jin and Yan, Yuchen and Liu, Yang and Jin, Yonggang and Peng, Shuai and Zhang, Mengdi and Cai, Xunliang and Cao, Yixin and Gao, Liangcai and Tang, Zhi},
  journal={arXiv preprint arXiv:2409.12929},
  year={2024}
}

@article{yuan2023scaling,
  title={Scaling Relationship on Learning Mathematical Reasoning with Large Language Models},
  author={Yuan, Zheng and Yuan, Hongyi and Li, Chengpeng and Dong, Guanting and Lu, Keming and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.01825},
  year={2023}
}

@article{yang2024qwen25math,
  title={Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement},
  author={Yang, An and Zhang, Beichen and Hui, Binyuan and Gao, Bofei and Yu, Bowen and Li, Chengpeng and Liu, Dayiheng and Tu, Jianhong and Zhou, Jingren and Lin, Junyang and Lu, Keming and Xue, Mingfeng and Lin, Runji and Liu, Tianyu and Ren, Xingzhang and Zhang, Zhenru},
  journal={arXiv preprint arXiv:2409.12122},
  year={2024}
}

@inproceedings{
xu2024lemur,
title={Lemur: Harmonizing Natural Language and Code for Language Agents},
author={Yiheng Xu and Hongjin Su and Chen Xing and Boyu Mi and Qian Liu and Weijia Shi and Binyuan Hui and Fan Zhou and Yitao Liu and Tianbao Xie and Zhoujun Cheng and Siheng Zhao and Lingpeng Kong and Bailin Wang and Caiming Xiong and Tao Yu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=hNhwSmtXRh}
}

@article{roziere2023code,
  title={Code Llama: Open Foundation Models for Code},
  author={Rozi{\'e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and Rapin, J{\'e}r{\'e}my and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and D{\'e}fossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@inproceedings{
hendrycks2021measuring,
title={Measuring Massive Multitask Language Understanding},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}
```
</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Excellent! I have successfully added all 5 requested articles to the ref.bib file. Here's a summary of what I added:

## Summary of Added Articles:

1. **Scaling relationship on learning mathematical reasoning with large language models**
   - Added as `yuan2023scaling`
   - arXiv:2308.01825 (2023)
   - Authors: Yuan, Zheng et al.

2. **Qwen2.5-math technical report**
   - Added as `yang2024qwen25math`
   - arXiv:2409.12122 (2024)
   - Authors: Yang, An et al.

3. **Lemur: Harmonizing Natural Language and Code for Language Agents**
   - Added as `xu2024lemur` (conference version from ICLR 2024)
   - Used the OpenReview format as requested since it was published at ICLR 2024
   - OpenReview URL: https://openreview.net/forum?id=hNhwSmtXRh

4. **CodeLlama**
   - Added as `roziere2023code`
   - arXiv:2308.12950 (2023)
   - Authors: Rozière, Baptiste et al.

5. **MMLU (Measuring Massive Multitask Language Understanding)**
   - Added as `hendrycks2021measuring` (conference version from ICLR 2021)
   - Used the OpenReview format as requested since it was published at ICLR 2021
   - OpenReview URL: https://openreview.net/forum?id=d7KBjmI3GmQ

All entries follow the existing format in the ref.bib file, with conference papers using the OpenReview format (as requested) and arXiv papers using the standard article format. The formatting is consistent with the existing entries in the bibliography.
</div>

<div className="tool-call-box">
<svg width="14px" height="14px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M861.588238 240.133873v-65.792823c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.631049H314.659414v-65.631049c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.792823c-36.317212 0.868255-65.631049 30.539428-65.63105 67.061417v543.745565c0 37.06772 30.155471 67.223191 67.223191 67.223191h696.886045c37.06772 0 67.223191-30.155471 67.223191-67.223191V307.19529c-0.001024-36.52199-29.315885-66.193162-65.633097-67.061417z m-109.385765-65.792823c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m-546.928824 0c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m678.191947 676.600829c0 12.935767-10.532708 23.468476-23.468476 23.468475H163.111076c-12.935767 0-23.468476-10.532708-23.468476-23.468475V307.19529c0-12.402323 9.677764-22.593054 21.877358-23.415233v65.577807c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.631049h393.789368v65.631049c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.577807c12.19857 0.82218 21.877358 11.012911 21.877358 23.415233v543.746589z" fill="#22C67F" /><path d="M706.719439 478.272194l-48.01715-44.741741-182.28128 195.621482-111.468348-122.615387-48.563905 44.148911 159.469116 172.685427z" fill="#74E8AE" /></svg> `claim_done `

```json
{}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
you have claimed the task is done!
```
</Expandable>
</div>

</Accordion>

<Accordion title="gemini-2.5-pro">

<Columns cols={3}>
<Card title="Task Completion" icon="x">
Failed
</Card>
<Card title="Tool Calls" icon="wrench">
7
</Card>
<Card title="Turns" icon="arrows-rotate">
7
</Card>
</Columns>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem list_directory`

```json
{
	"path":"."
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Error: Access denied - path outside allowed directories: /workspace not in /workspace/dumps/workspace
```
</Expandable>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem list_directory`

```json
{
	"path":"/workspace/dumps/workspace"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
[FILE] ref.bib
```
</Expandable>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

```json
{
	"path":"/workspace/dumps/workspace/ref.bib"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json

@article{singh2024evaluation,
  title={Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?},
  author={Singh, Aaditya K and Kocyigit, Muhammed Yusuf and Poulton, Andrew and Esiobu, David and Lomeli, Maria and Szilvasy, Gergely and Hupkes, Dieuwke},
  journal={arXiv preprint arXiv:2411.03923},
  year={2024}
}

@inproceedings{
ding2024semcoder,
title={SemCoder: Training Code Language Models with Comprehensive Semantics Reasoning},
author={Yangruibo Ding and Jinjun Peng and Marcus J. Min and Gail Kaiser and Junfeng Yang and Baishakhi Ray},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=PnlCHQrM69}
}

@article{huang2024opencoder,
  title={Opencoder: The open cookbook for top-tier code large language models},
  author={Huang, Siming and Cheng, Tianhao and Liu, Jason Klein and Hao, Jiaran and Song, Liuyihan and Xu, Yang and Yang, J and Liu, JH and Zhang, Chenchen and Chai, Linzheng and others},
  journal={arXiv preprint arXiv:2411.04905},
  year={2024}
}

@article{yue2024mammoth2,
  title={Mammoth2: Scaling instructions from the web},
  author={Yue, Xiang and Zheng, Tuney and Zhang, Ge and Chen, Wenhu},
  journal={arXiv preprint arXiv:2405.03548},
  year={2024}
}

@article{toshniwal2024openmathinstruct2,
  title={Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data},
  author={Toshniwal, Shubham and Du, Wei and Moshkov, Ivan and Kisacanin, Branislav and Ayrapetyan, Alexan and Gitman, Igor},
  journal={arXiv preprint arXiv:2410.01560},
  year={2024}
}

@software{benallal2024smollmcorpus,
  author = {Ben Allal, Loubna and Lozhkov, Anton and Penedo, Guilherme and Wolf, Thomas and von Werra, Leandro},
  title = {SmolLM-Corpus},
  month = July,
  year = 2024,
  url = {https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus}
}

@article{zhu2024deepseekcoderv2,
  title={DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence},
  author={Zhu, Qihao and Guo, Daya and Shao, Zhihong and Yang, Dejian and Wang, Peiyi and Xu, Runxin and Wu, Y and Li, Yukun and Gao, Huazuo and Ma, Shirong and others},
  journal={arXiv preprint arXiv:2406.11931},
  year={2024}
}

@article{hui2024qwen25coder,
      title={Qwen2. 5-Coder Technical Report},
      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
      journal={arXiv preprint arXiv:2409.12186},
      year={2024}
}

@article{zeng2024skyworkmath,
  title={Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models--The Story Goes On},
  author={Zeng, Liang and Zhong, Liangjun and Zhao, Liang and Wei, Tianwen and Yang, Liu and He, Jujie and Cheng, Cheng and Hu, Rui and Liu, Yang and Yan, Shuicheng and others},
  journal={arXiv preprint arXiv:2407.08348},
  year={2024}
}

@article{ying2024internlmmath,
  title={Internlm-math: Open math large language models toward verifiable reasoning},
  author={Ying, Huaiyuan and Zhang, Shuo and Li, Linyang and Zhou, Zhejian and Shao, Yunfan and Fei, Zhaoye and Ma, Yichuan and Hong, Jiawei and Liu, Kuikun and Wang, Ziyi and others},
  journal={arXiv preprint arXiv:2402.06332},
  year={2024}
}

@article{yang2024qwen25,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{dubey2024llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{xu2024aguvis,
  title={Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction},
  author={Xu, Yiheng and Wang, Zekun and Wang, Junli and Lu, Dunjie and Xie, Tianbao and Saha, Amrita and Sahoo, Doyen and Yu, Tao and Xiong, Caiming},
  journal={arXiv preprint arXiv:2412.04454},
  year={2024}
}

@inproceedings{
zhou2024math4ai,
title={{AI} for Math or Math for {AI}? On the Generalization of Learning Mathematical Problem Solving},
author={Ruochen Zhou and Minrui Xu and Shiqi Chen and Junteng Liu and Yunqi Li and LIN Xinxin and Zhengyu Chen and Junxian He},
booktitle={The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24},
year={2024},
url={https://openreview.net/forum?id=xlnvZ85CSo}
}

@article{paster2023openwebmath,
  title={Openwebmath: An open dataset of high-quality mathematical web text},
  author={Paster, Keiran and Santos, Marco Dos and Azerbayev, Zhangir and Ba, Jimmy},
  journal={arXiv preprint arXiv:2310.06786},
  year={2023}
}

@article{ma2024korbench,
  title={KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks},
  author={Ma, Kaijing and Du, Xinrun and Wang, Yunran and Zhang, Haoran and Wen, Zhoufutu and Qu, Xingwei and Yang, Jian and Liu, Jiaheng and Liu, Minghao and Yue, Xiang and others},
  journal={arXiv preprint arXiv:2410.06526},
  year={2024}
}

@inproceedings{dua2019drop,
  title={DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},
  author={Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2368--2378},
  year={2019}
}

@inproceedings{
rein2024gpqa,
title={{GPQA}: A Graduate-Level Google-Proof Q\&A Benchmark},
author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=Ti67584b98}
}

@article{white2024livebench,
  title={Livebench: A challenging, contamination-free llm benchmark},
  author={White, Colin and Dooley, Samuel and Roberts, Manley and Pal, Arka and Feuer, Ben and Jain, Siddhartha and Shwartz-Ziv, Ravid and Jain, Neel and Saifullah, Khalid and Naidu, Siddartha and others},
  journal={arXiv preprint arXiv:2406.19314},
  year={2024}
}

@article{lin2025zebralogic,
  title={ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning},
  author={Lin, Bill Yuchen and Bras, Ronan Le and Richardson, Kyle and Sabharwal, Ashish and Poovendran, Radha and Clark, Peter and Choi, Yejin},
  journal={arXiv preprint arXiv:2502.01100},
  year={2025}
}

@misc{codestral,
    title={Codestral},
    author={Mistral-AI},
    url={https://mistral.ai/news/codestral/},
    year={2024}
}

@misc{haillm,
    title={Hai-llm: Efficient and lightweight training tool for large models},
    author={High-flyer},
    url={https://www.high-flyer.cn/en/blog/hai-llm},
    year={2023}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@misc{qwq,
  author = {Team, Qwen},
  title = {QwQ: Reflect Deeply on the Boundaries of the Unknown},
  month = nov,
  year = 2024,
  url = {https://qwenlm.github.io/blog/qwq-32b-preview/}
}

@misc{knauff2010complex,
  title={Complex cognition: the science of human reasoning, problem-solving, and decision-making},
  author={Knauff, Markus and Wolf, Ann G},
  journal={Cognitive processing},
  volume={11},
  pages={99--102},
  year={2010},
  publisher={Springer}
}

@article{wang2010cognitive,
  title={On the cognitive process of human problem solving},
  author={Wang, Yingxu and Chiew, Vincent},
  journal={Cognitive systems research},
  volume={11},
  number={1},
  pages={81--92},
  year={2010},
  publisher={Elsevier}
}

@article{dehaene2004arithmetic,
  title={Arithmetic and the brain},
  author={Dehaene, Stanislas and Molko, Nicolas and Cohen, Laurent and Wilson, Anna J},
  journal={Current opinion in neurobiology},
  volume={14},
  number={2},
  pages={218--224},
  year={2004},
  publisher={Elsevier}
}

@article{xiang2025towards,
  title={Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though},
  author={Xiang, Violet and Snell, Charlie and Gandhi, Kanishk and Albalak, Alon and Singh, Anikait and Blagden, Chase and Phung, Duy and Rafailov, Rafael and Lile, Nathan and Mahan, Dakota and others},
  journal={arXiv preprint arXiv:2501.04682},
  year={2025}
}

@article{huang2022towards,
  title={Towards reasoning in large language models: A survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}

@article{qiao2022reasoning,
  title={Reasoning with language model prompting: A survey},
  author={Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  journal={arXiv preprint arXiv:2212.09597},
  year={2022}
}

@article{lozhkov2024starcoder,
  title={Starcoder 2 and the stack v2: The next generation},
  author={Lozhkov, Anton and Li, Raymond and Allal, Loubna Ben and Cassano, Federico and Lamy-Poirier, Joel and Tazi, Nouamane and Tang, Ao and Pykhtar, Dmytro and Liu, Jiawei and Wei, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.19173},
  year={2024}
}

@inproceedings{
zhang2024towards,
title={Towards Bridging Classical and Neural Computation through a Read-Eval-Print Loop},
author={David W. Zhang and Micha{\"e}l Defferrard and Corrado Rainone and Roland Memisevic},
booktitle={ICML 2024 Workshop on LLMs and Cognition},
year={2024},
url={https://openreview.net/forum?id=Y08aOVCPTt}
}


@article{wen2024codeplan,
  title={Unlocking Reasoning Potential in Large Langauge Models by Scaling Code-form Planning},
  author={Wen, Jiaxin and Guan, Jian and Wang, Hongning and Wu, Wei and Huang, Minlie},
  journal={arXiv preprint arXiv:2409.12452},
  year={2024}
}

@inproceedings{
gorceix2024learningmathrules,
title={Learning Mathematical Rules with Large Language Models},
author={Antoine Gorceix and Bastien Le Chenadec and Ahmad Rammal and Nelson Vadori and Manuela Veloso},
booktitle={The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24},
year={2024},
url={https://openreview.net/forum?id=tIlDF5B6T4}
}

@article{xu2024core,
  title={CoRE: LLM as Interpreter for Natural Language Programming, Pseudo-Code Programming, and Flow Programming of AI Agents},
  author={Xu, Shuyuan and Li, Zelong and Mei, Kai and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2405.06907},
  year={2024}
}

@inproceedings{
morishita2024enhancing,
title={Enhancing Reasoning Capabilities of {LLM}s via Principled Synthetic Logic Corpus},
author={Terufumi Morishita and Gaku Morio and Atsuki Yamaguchi and Yasuhiro Sogawa},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=mljDUaQpln}
}

@inproceedings{morishita2023learning,
  title={Learning deductive reasoning from synthetic corpus based on formal logic},
  author={Morishita, Terufumi and Morio, Gaku and Yamaguchi, Atsuki and Sogawa, Yasuhiro},
  booktitle={International Conference on Machine Learning},
  pages={25254--25274},
  year={2023},
  organization={PMLR}
}

@article{chen2024reverse,
  title={Reverse Thinking Makes LLMs Stronger Reasoners},
  author={Chen, Justin Chih-Yao and Wang, Zifeng and Palangi, Hamid and Han, Rujun and Ebrahimi, Sayna and Le, Long and Perot, Vincent and Mishra, Swaroop and Bansal, Mohit and Lee, Chen-Yu and others},
  journal={arXiv preprint arXiv:2411.19865},
  year={2024}
}

@article{jain2024livecodebench,
  title={Livecodebench: Holistic and contamination free evaluation of large language models for code},
  author={Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  journal={arXiv preprint arXiv:2403.07974},
  year={2024}
}

@article{zaremba2014learning,
  title={Learning to execute},
  author={Zaremba, Wojciech and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1410.4615},
  year={2014}
}

@article{graves2014neural,
  title={Neural Turing Machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@inproceedings{ding2024traced,
  title={Traced: Execution-aware pre-training for source code},
  author={Ding, Yangruibo and Steenhoek, Benjamin and Pei, Kexin and Kaiser, Gail and Le, Wei and Ray, Baishakhi},
  booktitle={Proceedings of the 46th IEEE/ACM International Conference on Software Engineering},
  pages={1--12},
  year={2024}
}

@inproceedings{
ni2024next,
title={{NE}xT: Teaching Large Language Models to Reason about Code Execution},
author={Ansong Ni and Miltiadis Allamanis and Arman Cohan and Yinlin Deng and Kensen Shi and Charles Sutton and Pengcheng Yin},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=B1W712hMBi}
}

@article{ding2024cycle,
  title={Cycle: Learning to self-refine the code generation},
  author={Ding, Yangruibo and Min, Marcus J and Kaiser, Gail and Ray, Baishakhi},
  journal={Proceedings of the ACM on Programming Languages},
  volume={8},
  number={OOPSLA1},
  pages={392--418},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@inproceedings{wang2024leti,
  title={LETI: Learning to Generate from Textual Interactions},
  author={Wang, Xingyao and Peng, Hao and Jabbarvand, Reyhaneh and Ji, Heng},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={223--239},
  year={2024}
}

@inproceedings{gururangan2020don,
  title={Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8342--8360},
  year={2020}
}

@misc{deepseekai2025r1,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{jiang2024logicpro,
  title={LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning},
  author={Jiang, Jin and Yan, Yuchen and Liu, Yang and Jin, Yonggang and Peng, Shuai and Zhang, Mengdi and Cai, Xunliang and Cao, Yixin and Gao, Liangcai and Tang, Zhi},
  journal={arXiv preprint arXiv:2409.12929},
  year={2024}
}
```
</Expandable>
</div>

<div className="tool-call-box">
<svg height="14" width="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g fill="none" fill-rule="evenodd"><rect fill="#fabc05" height="512" rx="42.667" width="512"/><path d="M58.667 293.333L0 209.125V42.663C0 19.1 19.137 0 42.663 0h426.674C492.9 0 512 19.137 512 42.663l-282.667.004S58.667 305.14 58.667 293.333z" fill="#eb4335"/><path d="M303.3 340.528L199.876 512H42.633C19.087 512 0 492.94 0 469.328V209.125l54.537 74.854c28.457 48.33 81.024 80.762 141.167 80.762 40.548 0 77.822-14.86 106.533-39.29l5.348 5.349c-.918 3.235-2.337 6.5-4.284 9.728z" fill="#34a854"/><g fill-rule="nonzero"><path d="M194.667 85.333c-66.667 0-120 52.148-120 117.334C74.667 267.852 128 320 194.667 320c66.666 0 120-52.148 120-117.333 0-65.186-53.334-117.334-120-117.334z" fill="#4285f5"/><path d="M327.59 300.924l28.788 28.787 13.204-14.159 115.751 115.751-59.363 59.364-115.751-115.751 13.553-12.64-28.85-28.85c-27.526 20.496-61.677 32.712-98.52 32.712C105.605 366.138 32 292.532 32 201.735c0-90.796 73.605-164.402 164.402-164.402s164.402 73.606 164.402 164.402c0 37.142-12.414 71.549-33.213 99.19zM196.403 87.92c-63.231 0-113.817 50.585-113.817 113.816 0 63.232 50.586 113.817 113.817 113.817S310.22 264.967 310.22 201.735c0-63.231-50.585-113.816-113.817-113.816z" fill="#fff"/></g></g></svg> `web_search `

```json
{
	"query":"Scaling relationship on learning mathematical reasoning with large language models bibtex"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Title: Scaling Relationship on Learning Mathematical Reasoning ...
Link: https://arxiv.org/abs/2308.01825
Snippet: by Z Yuan · 2023 · Cited by 418 — Abstract:Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to ...
Sitelinks: No sitelinks

Title: Scaling Relationship on Learning Mathematical Reasoning ...
Link: https://openreview.net/forum?id=cijO0f8u35
Snippet: by Z Yuan · Cited by 418 — This paper explores the mathematical reasoning abilities of large language models (LLMs) and how they depend on various factors such as pretraining loss, the ...
Sitelinks: No sitelinks

Title: Scaling Relationship on Learning Mathematical Reasoning ...
Link: https://www.semanticscholar.org/paper/Scaling-Relationship-on-Learning-Mathematical-with-Yuan-Yuan/91206346edbe28abb606d7b3425cd455d4019d4f
Snippet: It is found that pre-training loss is a better indicator of the model's performance than the model's parameter count and that with augmented samples ...
Sitelinks: No sitelinks

Title: Scaling Relationship on Learning Mathematical Reasoning ...
Link: https://www.researchgate.net/publication/372888942_Scaling_Relationship_on_Learning_Mathematical_Reasoning_with_Large_Language_Models/download
Snippet: In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances ...
Sitelinks: No sitelinks

Title: Scaling Relationship on Learning Mathematical Reasoning ...
Link: https://www.alphaxiv.org/overview/2308.01825v2
Snippet: This paper from Alibaba DAMO Academy investigates the empirical scaling relationships for Large Language Models' mathematical reasoning capabilities, ...
Sitelinks: No sitelinks

Title: Scaling Relationship on Learning Mathematical Reasoning ...
Link: https://consensus.app/papers/scaling-relationship-on-learning-mathematical-reasoning-tan-yuan/ff9c8ef6cf3c514aa14cef61149386a9/
Snippet: Key takeaway: 'Pre-training loss is a better indicator of a language model's performance than its parameter count, and Rejection sampling ...
Sitelinks: No sitelinks

Title: OFA-Sys/gsm8k-ScRel: Codes and Data for Scaling ...
Link: https://github.com/OFA-Sys/gsm8k-ScRel
Snippet: The code and data used for reproducing results of Scaling Relationship on Learning Mathematical Reasoning with Large Language Models
Sitelinks: No sitelinks

Title: The Relationship Between Reasoning and Performance in ...
Link: https://arxiv.org/abs/2502.15631
Snippet: by M Ballon · 2025 · Cited by 23 — Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling ...
Sitelinks: No sitelinks

Title: MathPrompter: Mathematical Reasoning using Large ...
Link: https://www.reddit.com/r/singularity/comments/11q8uuh/mathprompter_mathematical_reasoning_using_large/
Snippet: We propose MathPrompter, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions.
Sitelinks: No sitelinks

Title: The key findings of scaling relationship on learning math...
Link: https://www.researchgate.net/figure/The-key-findings-of-scaling-relationship-on-learning-math-reasoning-ability-with-LLMs_fig1_372888942
Snippet: Large language models (LLMs) have demonstrated impressive performance in mathematical and commonsense reasoning tasks using chain-of-thought (CoT) prompting ...
Sitelinks: No sitelinks

```
</Expandable>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><path fill="#010202" d="M201.5,22.5C253.575,18.8292,294.909,37.8292,325.5,79.5C348.846,76.447,370.846,80.6137,391.5,92C411.339,105.168,422.339,123.668,424.5,147.5C465.86,161.693,491.694,190.026,502,232.5C508.97,280.701,492.804,318.868,453.5,347C427.526,362.095,399.526,367.929,369.5,364.5C369.988,405.252,369.488,445.919,368,486.5C367.5,487,367,487.5,366.5,488C292.5,488.667,218.5,488.667,144.5,488C144,487.5,143.5,487,143,486.5C141.512,445.919,141.012,405.252,141.5,364.5C106.171,368.921,74.5043,360.421,46.5,339C10.3518,305.536-0.814861,265.036,13,217.5C22.2223,191.95,38.389,172.117,61.5,158C69.8007,153.681,78.134,149.681,86.5,146C92.1136,89.0861,121.447,50.0861,174.5,29C183.492,26.2401,192.492,24.0734,201.5,22.5Z"/></g><g><path fill="#a3d3fe" d="M204.5,36.5C254.543,34.185,292.876,54.0184,319.5,96C340.997,90.9721,361.664,93.3054,381.5,103C401.654,115.47,411.154,133.637,410,157.5C410.903,158.701,412.069,159.535,413.5,160C453.408,170.242,478.242,195.409,488,235.5C493.84,285.282,474.34,321.115,429.5,343C422.574,345.298,415.574,347.298,408.5,349C395.504,349.5,382.504,349.667,369.5,349.5C369.667,302.499,369.5,255.499,369,208.5C367.833,207.333,366.667,206.167,365.5,205C306.833,204.333,248.167,204.333,189.5,205C173,220.167,157.167,236,142,252.5C141.5,284.832,141.333,317.165,141.5,349.5C100.111,355.535,66.2777,342.535,40,310.5C17.3055,276.196,15.6388,240.863,35,204.5C50.4757,180.668,71.9757,165.002,99.5,157.5C101.376,109.215,123.042,73.0487,164.5,49C177.289,42.679,190.622,38.5123,204.5,36.5Z"/></g><g><path fill="#fefefe" d="M354.5,474.5C288.5,474.5,222.5,474.5,156.5,474.5C156.5,403.833,156.5,333.167,156.5,262.5C169.344,262.829,182.011,262.495,194.5,261.5C197.167,260.833,198.833,259.167,199.5,256.5C200.495,244.011,200.829,231.344,200.5,218.5C251.833,218.5,303.167,218.5,354.5,218.5C354.5,303.833,354.5,389.167,354.5,474.5Z"/></g><g><path fill="#979797" d="M354.5,474.5C354.5,389.167,354.5,303.833,354.5,218.5C303.167,218.5,251.833,218.5,200.5,218.5C200.829,231.344,200.495,244.011,199.5,256.5C199.5,243.5,199.5,230.5,199.5,217.5C251.5,217.5,303.5,217.5,355.5,217.5C355.833,303.335,355.499,389.002,354.5,474.5Z"/></g><g><path fill="#dce0e3" d="M185.5,247.5C179.31,247.823,173.31,247.49,167.5,246.5C173.13,240.369,178.964,234.369,185,228.5C185.5,234.825,185.666,241.158,185.5,247.5Z"/></g><g><path fill="#7b7d7e" d="M167.5,246.5C173.31,247.49,179.31,247.823,185.5,247.5C179.322,248.817,172.988,248.817,166.5,247.5C166.624,246.893,166.957,246.56,167.5,246.5Z"/></g><g><path fill="#a3a3a3" d="M194.5,261.5C182.011,262.495,169.344,262.829,156.5,262.5C156.5,333.167,156.5,403.833,156.5,474.5C155.501,403.669,155.167,332.669,155.5,261.5C168.5,261.5,181.5,261.5,194.5,261.5Z"/></g><g><path fill="#030303" d="M252.5,309.5C256.426,309.047,259.592,310.38,262,313.5C262.333,334.833,262.667,356.167,263,377.5C264.979,369.149,269.812,366.649,277.5,370C278.718,371.438,279.385,373.105,279.5,375C274.409,385.346,268.909,395.512,263,405.5C258,413.5,253,413.5,248,405.5C242.091,395.512,236.591,385.346,231.5,375C232.869,369.144,236.536,367.144,242.5,369C244.956,371.408,246.789,374.241,248,377.5C248.333,356.167,248.667,334.833,249,313.5C250.376,312.295,251.542,310.962,252.5,309.5Z"/></g><g><path fill="#050505" d="M199.5,376.5C203.888,375.943,207.055,377.61,209,381.5C209.5,398.83,209.667,416.163,209.5,433.5C240.167,433.5,270.833,433.5,301.5,433.5C301.333,416.163,301.5,398.83,302,381.5C304.553,376.374,308.386,375.207,313.5,378C314.126,378.75,314.626,379.584,315,380.5C315.667,401.5,315.667,422.5,315,443.5C314.626,444.416,314.126,445.25,313.5,446C294.254,447.155,274.921,447.655,255.5,447.5C236.079,447.655,216.746,447.155,197.5,446C196.874,445.25,196.374,444.416,196,443.5C195.333,422.5,195.333,401.5,196,380.5C196.69,378.65,197.856,377.316,199.5,376.5Z"/></g></svg> `fetch fetch_html`

```json
{
	"url":"https://openreview.net/forum?id=cijO0f8u35"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/images/arrow_left.svg"/><link rel="preload" as="image" href="/images/pdf_icon_blue.svg"/><link rel="stylesheet" href="/_next/static/css/623ec4d945fb0950.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/7efbc204b5b07ee6.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/01fe169d2e2b269f.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/7e1ff74241679440.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/d7479b632bea1be9.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/3ca9945321ffa69b.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-de73cdf780f62dbf.js"/><script src="/_next/static/chunks/4bd1b696-63e5575140a08370.js" async=""></script><script src="/_next/static/chunks/1684-db29fe6576d098bd.js" async=""></script><script src="/_next/static/chunks/main-app-895d0017b454980b.js" async=""></script><script src="/_next/static/chunks/e37a0b60-86dcf540460bd9a6.js" async=""></script><script src="/_next/static/chunks/7ce798d6-3eb8122476a3f2e5.js" async=""></script><script src="/_next/static/chunks/6874-b5228efe8b8455fa.js" async=""></script><script src="/_next/static/chunks/3697-c0092b2c69fd8d8c.js" async=""></script><script src="/_next/static/chunks/1141-829430d34cc15803.js" async=""></script><script src="/_next/static/chunks/4540-d4d9759f532dbe01.js" async=""></script><script src="/_next/static/chunks/6325-93a1b42c84bba41c.js" async=""></script><script src="/_next/static/chunks/1990-a202dc60dde3ccdf.js" async=""></script><script src="/_next/static/chunks/9353-dde6849b3fe442fa.js" async=""></script><script src="/_next/static/chunks/7370-90d27313d10bdf44.js" async=""></script><script src="/_next/static/chunks/9433-f2608cbcb1b74cd8.js" async=""></script><script src="/_next/static/chunks/app/layout-916462f9a8993d1c.js" async=""></script><script src="/_next/static/chunks/6846-e00ebe4cf4673031.js" async=""></script><script src="/_next/static/chunks/1592-be14089f9df98c98.js" async=""></script><script src="/_next/static/chunks/9032-c4156b261f2c50b7.js" async=""></script><script src="/_next/static/chunks/6504-00ef5b949820536b.js" async=""></script><script src="/_next/static/chunks/3882-68311d71c6e6d2c2.js" async=""></script><script src="/_next/static/chunks/2882-6cb3155ef24a8d15.js" async=""></script><script src="/_next/static/chunks/4745-b7bef6bfd1f2ca78.js" async=""></script><script src="/_next/static/chunks/1399-1fff5e65c29b20ed.js" async=""></script><script src="/_next/static/chunks/4757-5ccf877f341117b6.js" async=""></script><script src="/_next/static/chunks/3474-363b921f0385cec0.js" async=""></script><script src="/_next/static/chunks/5262-c565f8d56db6f340.js" async=""></script><script src="/_next/static/chunks/app/forum/page-7cc3f60b1e6efcec.js" async=""></script><script src="/_next/static/chunks/app/error-ac9fc3fd38a040ee.js" async=""></script><script src="/_next/static/chunks/app/global-error-f023f9ed1562f572.js" async=""></script><link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js" as="script"/><link rel="preload" href="https://challenges.cloudflare.com/turnstile/v0/api.js" as="script"/><link rel="preload" href="https://js.stripe.com/v3/buy-button.js" as="script"/><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-GTB25PBMVL" as="script"/><meta name="next-size-adjust" content=""/><link rel="icon" href="/favicon.ico"/><link rel="manifest" href="/manifest.json"/><title>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models | OpenReview</title><meta name="description" content="Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored.
In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM.
We find that pre-training loss is a better indicator of the model&#x27;s performance than the model&#x27;s parameter count.
We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets.
To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT).
RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets.
We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs.
We also find RFT brings more improvement for less performant LLMs.
Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\% significantly."/><meta name="citation_title" content="Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"/><meta name="citation_author" content="Zheng Yuan"/><meta name="citation_author" content="Hongyi Yuan"/><meta name="citation_author" content="Chengpeng Li"/><meta name="citation_author" content="Guanting Dong"/><meta name="citation_author" content="Keming Lu"/><meta name="citation_author" content="Chuanqi Tan"/><meta name="citation_author" content="Chang Zhou"/><meta name="citation_author" content="Jingren Zhou"/><meta name="citation_online_date" content="2023/10/13"/><meta name="citation_pdf_url" content="https://openreview.net/pdf?id=cijO0f8u35"/><meta name="citation_abstract" content="Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored.
In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM.
We find that pre-training loss is a better indicator of the model&#x27;s performance than the model&#x27;s parameter count.
We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets.
To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT).
RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets.
We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs.
We also find RFT brings more improvement for less performant LLMs.
Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\% significantly."/><meta property="og:title" content="Scaling Relationship on Learning Mathematical Reasoning with Large..."/><meta property="og:description" content="Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored.
In this paper, we investigate how..."/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Scaling Relationship on Learning Mathematical Reasoning with Large..."/><meta name="twitter:description" content="Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored.
In this paper, we investigate how..."/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_086c6e"><div id="__next"><nav class="navbar navbar-inverse" role="navigation"><div class="container"><div class="navbar-header"><button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a></div><div id="navbar" class="navbar-collapse collapse"><form class="navbar-form navbar-left profile-search" role="search"><div class="form-group has-feedback"><input type="text" class="form-control" placeholder="Search OpenReview..." autoComplete="off" autoCorrect="off" name="term" value=""/><span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span></div><input type="hidden" name="group" value="all"/><input type="hidden" name="content" value="all"/><input type="hidden" name="source" value="all"/></form><ul class="nav navbar-nav navbar-right"><li id="user-menu"><a href="/login">Login</a></li></ul></div></div></nav><div id="flash-message-container" class="alert alert-danger fixed-overlay" role="alert" style="display:none"><div class="container"><div class="row"><div class="col-xs-12"><div class="alert-content"><button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button></div></div></div></div></div><script>(self.__next_s=self.__next_s||[]).push(["https://challenges.cloudflare.com/turnstile/v0/api.js",{}])</script><div id="or-banner" class="banner"><div class="container"><div class="row"><div class="col-xs-12"><a title="Venue Homepage" href="/group?id=ICLR.cc/2024/Conference"><img class="icon" src="/images/arrow_left.svg" alt="back arrow"/>Go to <strong>ICLR 2024 Conference</strong> <!-- -->homepage</a></div></div></div></div><div class="container"><div class="row"><main id="content"><div class="Forum_forum__wS8Fw"><div class="forum-container"><div class="forum-note"><div class="forum-title mt-2 mb-2"><h2 class="citation_title">Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</h2><div class="forum-content-link"><a class="citation_pdf_url" href="/pdf?id=cijO0f8u35" title="Download PDF" target="_blank" rel="noreferrer"><img src="/images/pdf_icon_blue.svg" alt="Download PDF"/></a></div></div><div class="forum-authors mb-2"><h3><span><a title="~Zheng_Yuan2" data-toggle="tooltip" data-placement="top" href="/profile?id=~Zheng_Yuan2">Zheng Yuan</a>, <a title="~Hongyi_Yuan1" data-toggle="tooltip" data-placement="top" href="/profile?id=~Hongyi_Yuan1">Hongyi Yuan</a>, <a title="~Chengpeng_Li1" data-toggle="tooltip" data-placement="top" href="/profile?id=~Chengpeng_Li1">Chengpeng Li</a>, <a title="~Guanting_Dong1" data-toggle="tooltip" data-placement="top" href="/profile?id=~Guanting_Dong1">Guanting Dong</a>, <a title="~Keming_Lu1" data-toggle="tooltip" data-placement="top" href="/profile?id=~Keming_Lu1">Keming Lu</a>, <a title="~Chuanqi_Tan3" data-toggle="tooltip" data-placement="top" href="/profile?id=~Chuanqi_Tan3">Chuanqi Tan</a>, <a title="~Chang_Zhou2" data-toggle="tooltip" data-placement="top" href="/profile?id=~Chang_Zhou2">Chang Zhou</a>, <a title="~Jingren_Zhou1" data-toggle="tooltip" data-placement="top" href="/profile?id=~Jingren_Zhou1">Jingren Zhou</a> <!-- --> </span></h3></div><div class="clearfix mb-1"><div class="forum-meta"><span class="date item"><span class="glyphicon glyphicon-calendar " aria-hidden="true"></span>18 Sept 2023 (modified: 11 Feb 2024)</span><span class="item"><span class="glyphicon glyphicon-folder-open " aria-hidden="true"></span>Submitted to ICLR 2024</span><span class="readers item" data-toggle="tooltip" data-placement="top" title="Visible to &lt;br/&gt;everyone&lt;br/&gt;since 13 Oct 2023"><span class="glyphicon glyphicon-eye-open " aria-hidden="true"></span>Everyone</span><span class="item"><span class="glyphicon glyphicon-duplicate " aria-hidden="true"></span><a href="/revisions?id=cijO0f8u35">Revisions</a></span><span class="item"><span class="glyphicon glyphicon-bookmark " aria-hidden="true"></span><a href="#" data-target="#bibtex-modal" data-toggle="modal" data-bibtex="%40misc%7B%0Ayuan2024scaling%2C%0Atitle%3D%7BScaling%20Relationship%20on%20Learning%20Mathematical%20Reasoning%20with%20Large%20Language%20Models%7D%2C%0Aauthor%3D%7BZheng%20Yuan%20and%20Hongyi%20Yuan%20and%20Chengpeng%20Li%20and%20Guanting%20Dong%20and%20Keming%20Lu%20and%20Chuanqi%20Tan%20and%20Chang%20Zhou%20and%20Jingren%20Zhou%7D%2C%0Ayear%3D%7B2024%7D%2C%0Aurl%3D%7Bhttps%3A%2F%2Fopenreview.net%2Fforum%3Fid%3DcijO0f8u35%7D%0A%7D">BibTeX</a></span></div><div class="invitation-buttons"></div></div><div class="note-content"><div><strong class="note-content-field disable-tex-rendering">Primary Area<!-- -->:</strong> <span class="note-content-value">representation learning for computer vision, audio, language, and other modalities</span></div><div><strong class="note-content-field disable-tex-rendering">Code Of Ethics<!-- -->:</strong> <span class="note-content-value">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.</span></div><div><strong class="note-content-field disable-tex-rendering">Keywords<!-- -->:</strong> <span class="note-content-value">Mathematical Reasoning, Scaling Relationship, Large Language Model</span></div><div><strong class="note-content-field disable-tex-rendering">Submission Guidelines<!-- -->:</strong> <span class="note-content-value">I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2024/AuthorGuide.</span></div><div><strong class="note-content-field disable-tex-rendering">Abstract<!-- -->:</strong> <span class="note-content-value">Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored.
In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM.
We find that pre-training loss is a better indicator of the model&#x27;s performance than the model&#x27;s parameter count.
We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets.
To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT).
RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets.
We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs.
We also find RFT brings more improvement for less performant LLMs.
Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\% significantly.</span></div><div><strong class="note-content-field disable-tex-rendering">Anonymous Url<!-- -->:</strong> <span class="note-content-value">I certify that there is no URL (e.g., github page) that could be used to find authors&#x27; identity.</span></div><div><strong class="note-content-field disable-tex-rendering">Supplementary Material<!-- -->:</strong> <span class="note-content-value"><a href="/attachment?id=cijO0f8u35&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt " aria-hidden="true"></span> <!-- -->zip</a></span></div><div><strong class="note-content-field disable-tex-rendering">No Acknowledgement Section<!-- -->:</strong> <span class="note-content-value">I certify that there is no acknowledgement section in this submission for double blind review.</span></div><div><strong class="note-content-field disable-tex-rendering">Submission Number<!-- -->:</strong> <span class="note-content-value">1254</span></div></div></div><div class="row forum-replies-container layout-default"><div class="col-xs-12"><div id="forum-replies"><div class="spinner-container spinner-inline"><div class="spinner undefined"><div class="rect1"></div><div class="rect2"></div><div class="rect3"></div><div class="rect4"></div><div class="rect5"></div></div><span>Loading</span></div></div></div></div></div></div></main></div></div><footer class="sitemap"><div class="container"><div class="row hidden-xs"><div class="col-sm-4"><ul class="list-unstyled"><li><a href="/about">About OpenReview</a></li><li><a href="/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="/venues">All Venues</a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="/contact">Contact</a></li><li><a href="/sponsors">Sponsors</a></li><li><a href="https://donate.stripe.com/eVqdR8fP48bK1R61fi0oM00"><strong>Donate</strong></a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://docs.openreview.net/getting-started/frequently-asked-questions">Frequently Asked Questions</a></li><li><a href="/legal/terms">Terms of Use</a></li><li><a href="/legal/privacy">Privacy Policy</a></li></ul></div></div><div class="row visible-xs-block"><div class="col-xs-6"><ul class="list-unstyled"><li><a href="/about">About OpenReview</a></li><li><a href="/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="/venues">All Venues</a></li><li><a href="/sponsors">Sponsors</a></li></ul></div><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://docs.openreview.net/getting-started/frequently-asked-questions">Frequently Asked Questions</a></li><li><a href="/contact">Contact</a></li><li><a href="https://donate.stripe.com/eVqdR8fP48bK1R61fi0oM00"><strong>Donate</strong></a></li><li><a href="/legal/terms">Terms of Use</a></li><li><a href="/legal/privacy">Privacy Policy</a></li></ul></div></div></div></footer><footer class="sponsor"><div class="container"><div class="row"><div class="col-sm-10 col-sm-offset-1"><p class="text-center"><a href="/about" target="_blank">OpenReview</a> <!-- -->is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the<!-- --> <a href="/sponsors" target="_blank">OpenReview Sponsors</a>. © <!-- -->2025<!-- --> OpenReview</p></div></div></div></footer></div><script src="/_next/static/chunks/webpack-de73cdf780f62dbf.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[64818,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"3740\",\"static/chunks/7ce798d6-3eb8122476a3f2e5.js\",\"6874\",\"static/chunks/6874-b5228efe8b8455fa.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"1141\",\"static/chunks/1141-829430d34cc15803.js\",\"4540\",\"static/chunks/4540-d4d9759f532dbe01.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"1990\",\"static/chunks/1990-a202dc60dde3ccdf.js\",\"9353\",\"static/chunks/9353-dde6849b3fe442fa.js\",\"7370\",\"static/chunks/7370-90d27313d10bdf44.js\",\"9433\",\"static/chunks/9433-f2608cbcb1b74cd8.js\",\"7177\",\"static/chunks/app/layout-916462f9a8993d1c.js\"],\"default\"]\n3:I[6874,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"6874\",\"static/chunks/6874-b5228efe8b8455fa.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"1141\",\"static/chunks/1141-829430d34cc15803.js\",\"4540\",\"static/chunks/4540-d4d9759f532dbe01.js\",\"6846\",\"static/chunks/6846-e00ebe4cf4673031.js\",\"1592\",\"static/chunks/1592-be14089f9df98c98.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"9032\",\"static/chunks/9032-c4156b261f2c50b7.js\",\"6504\",\"static/chunks/6504-00ef5b949820536b.js\",\"3882\",\"static/chunks/3882-68311d71c6e6d2c2.js\",\"9433\",\"static/chunks/9433-f2608cbcb1b74cd8.js\",\"2882\",\"static/chunks/2882-6cb3155ef24a8d15.js\",\"4745\",\"static/chunks/4745-b7bef6bfd1f2ca78.js\",\"1399\",\"static/chunks/1399-1fff5e65c29b20ed.js\",\"4757\",\"static/chunks/4757-5ccf877f341117b6.js\",\"3474\",\"static/chunks/3474-363b921f0385cec0.js\",\"5262\",\"static/chunks/5262-c565f8d56db6f340.js\",\"5300\",\"static/chunks/app/forum/page-7cc3f60b1e6efcec.js\"],\"\"]\n4:I[41316,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"3740\",\"static/chunks/7ce798d6-3eb8122476a3f2e5.js\",\"6874\",\"static/chunks/6874-b5228efe8b8455fa.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"1141\",\"static/chunks/1141-829430d34cc15803.js\",\"4540\",\"static/chunks/4540-d4d9759f532dbe01.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"1990\",\"static/chunks/1990-a202dc60dde3ccdf.js\",\"9353\",\"static/chunks/9353-dde6849b3fe442fa.js\",\"7370\",\"static/chunks/"])</script><script>self.__next_f.push([1,"7370-90d27313d10bdf44.js\",\"9433\",\"static/chunks/9433-f2608cbcb1b74cd8.js\",\"7177\",\"static/chunks/app/layout-916462f9a8993d1c.js\"],\"default\"]\n6:I[33977,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"3740\",\"static/chunks/7ce798d6-3eb8122476a3f2e5.js\",\"6874\",\"static/chunks/6874-b5228efe8b8455fa.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"1141\",\"static/chunks/1141-829430d34cc15803.js\",\"4540\",\"static/chunks/4540-d4d9759f532dbe01.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"1990\",\"static/chunks/1990-a202dc60dde3ccdf.js\",\"9353\",\"static/chunks/9353-dde6849b3fe442fa.js\",\"7370\",\"static/chunks/7370-90d27313d10bdf44.js\",\"9433\",\"static/chunks/9433-f2608cbcb1b74cd8.js\",\"7177\",\"static/chunks/app/layout-916462f9a8993d1c.js\"],\"default\"]\n7:I[87555,[],\"\"]\n8:I[31702,[\"6874\",\"static/chunks/6874-b5228efe8b8455fa.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"1141\",\"static/chunks/1141-829430d34cc15803.js\",\"4540\",\"static/chunks/4540-d4d9759f532dbe01.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"9433\",\"static/chunks/9433-f2608cbcb1b74cd8.js\",\"4757\",\"static/chunks/4757-5ccf877f341117b6.js\",\"8039\",\"static/chunks/app/error-ac9fc3fd38a040ee.js\"],\"default\"]\n9:I[31295,[],\"\"]\na:I[64757,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"6874\",\"static/chunks/6874-b5228efe8b8455fa.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"1141\",\"static/chunks/1141-829430d34cc15803.js\",\"4540\",\"static/chunks/4540-d4d9759f532dbe01.js\",\"6846\",\"static/chunks/6846-e00ebe4cf4673031.js\",\"1592\",\"static/chunks/1592-be14089f9df98c98.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"9032\",\"static/chunks/9032-c4156b261f2c50b7.js\",\"6504\",\"static/chunks/6504-00ef5b949820536b.js\",\"3882\",\"static/chunks/3882-68311d71c6e6d2c2.js\",\"9433\",\"static/chunks/9433-f2608cbcb1b74cd8.js\",\"2882\",\"static/chunks/2882-6cb3155ef24a8d15.js\",\"4745\",\"static/chunks/4745-b7bef6bfd1f2ca78.js\",\"1399\",\"static/chunks/1399-1fff5e65c29b20ed.js\",\"4757\",\"static/chunks/4757-5ccf877f341117b6.js\",\"3474\",\"static/chunks/3474-363b921f0385cec0.js\",\"5262\",\"static/chu"])</script><script>self.__next_f.push([1,"nks/5262-c565f8d56db6f340.js\",\"5300\",\"static/chunks/app/forum/page-7cc3f60b1e6efcec.js\"],\"default\"]\nb:I[69243,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"3740\",\"static/chunks/7ce798d6-3eb8122476a3f2e5.js\",\"6874\",\"static/chunks/6874-b5228efe8b8455fa.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"1141\",\"static/chunks/1141-829430d34cc15803.js\",\"4540\",\"static/chunks/4540-d4d9759f532dbe01.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"1990\",\"static/chunks/1990-a202dc60dde3ccdf.js\",\"9353\",\"static/chunks/9353-dde6849b3fe442fa.js\",\"7370\",\"static/chunks/7370-90d27313d10bdf44.js\",\"9433\",\"static/chunks/9433-f2608cbcb1b74cd8.js\",\"7177\",\"static/chunks/app/layout-916462f9a8993d1c.js\"],\"\"]\nd:I[59665,[],\"OutletBoundary\"]\n10:I[59665,[],\"ViewportBoundary\"]\n12:I[59665,[],\"MetadataBoundary\"]\n14:I[89340,[\"6874\",\"static/chunks/6874-b5228efe8b8455fa.js\",\"4219\",\"static/chunks/app/global-error-f023f9ed1562f572.js\"],\"default\"]\n:HL[\"/_next/static/media/08f4947ad4536ee1-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/c4250770ab8708b6-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/623ec4d945fb0950.css\",\"style\"]\n:HL[\"/_next/static/css/7efbc204b5b07ee6.css\",\"style\"]\n:HL[\"/_next/static/css/01fe169d2e2b269f.css\",\"style\"]\n:HL[\"/_next/static/css/7e1ff74241679440.css\",\"style\"]\n:HL[\"/_next/static/css/d7479b632bea1be9.css\",\"style\"]\n:HL[\"/_next/static/css/3ca9945321ffa69b.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"v1.14.23\",\"p\":\"\",\"c\":[\"\",\"forum?id=cijO0f8u35\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"forum\",{\"children\":[\"__PAGE__?{\\\"id\\\":\\\"cijO0f8u35\\\"}\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/623ec4d945fb0950.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7efbc204b5b07ee6.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/01fe169d2e2b269f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7e1ff74241679440.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.ico\"}],[\"$\",\"link\",null,{\"rel\":\"manifest\",\"href\":\"/manifest.json\"}]]}],[\"$\",\"$L2\",null,{\"children\":[\"$\",\"body\",null,{\"className\":\"__className_086c6e\",\"children\":[\"$\",\"div\",null,{\"id\":\"__next\",\"children\":[[\"$\",\"nav\",null,{\"className\":\"navbar navbar-inverse\",\"role\":\"navigation\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[[\"$\",\"div\",null,{\"className\":\"navbar-header\",\"children\":[[\"$\",\"button\",null,{\"type\":\"button\",\"className\":\"navbar-toggle collapsed\",\"data-toggle\":\"collapse\",\"data-target\":\"#navbar\",\"aria-expanded\":\"false\",\"aria-controls\":\"navbar\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"Toggle navigation\"}],[\"$\",\"span\",null,{\"className\":\"icon-bar\"}],[\"$\",\"span\",null,{\"className\":\"icon-bar\"}],[\"$\",\"span\",null,{\"className\":\"icon-bar\"}]]}],[\"$\",\"$L3\",null,{\"href\":\"/\",\"className\":\"navbar-brand home push-link\",\"children\":[[\"$\",\"strong\",null,{\"children\":\"OpenReview\"}],\".net\"]}]]}],[\"$\",\"div\",null,{\"id\":\"navbar\",\"className\":\"navbar-collapse collapse\",\"children\":[[\"$\",\"$L4\",null,{}],\"$L5\"]}]]}]}],[\"$\",\"div\",null,{\"id\":\"flash-message-container\",\"className\":\"alert alert-danger fixed-overlay\",\"role\":\"alert\",\"style\":{\"display\":\"none\"},\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"row\",\"children\":[\"$\",\"div\",null,{\"className\":\"col-xs-12\",\"children\":[\"$\",\"div\",null,{\"className\":\"alert-content\",\"children\":[\"$\",\"button\",null,{\"type\":\"button\",\"className\":\"close\",\"aria-label\":\"Close\",\"children\":[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"×\"}]}]}]}]}]}]}],[\"$\",\"$L6\",null,{}],[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$8\",\"errorStyles\":[],\"errorScripts\":[],\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"$La\",null,{\"statusCode\":404,\"message\":\"Please check that the URL is spelled correctly and try again.\"}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}]}]}],[[\"$\",\"$Lb\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-GTB25PBMVL\"}],[\"$\",\"$Lb\",null,{\"id\":\"ga-script\",\"dangerouslySetInnerHTML\":{\"__html\":\"window.dataLayer = window.dataLayer || [];\\nfunction gtag() { dataLayer.push(arguments); }\\ngtag('js', new Date());\\ngtag('config', 'G-GTB25PBMVL', {\\npage_location: location.origin + location.pathname + location.search,\\n});\"}}]]]}]]}],{\"children\":[\"forum\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$Lc\",\"$undefined\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/d7479b632bea1be9.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/3ca9945321ffa69b.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$Ld\",null,{\"children\":[\"$Le\",\"$Lf\",null]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"G_9JmoFvui_wOfMuDhNJp\",{\"children\":[[\"$\",\"$L10\",null,{\"children\":\"$L11\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],[\"$\",\"$L12\",null,{\"children\":\"$L13\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$14\",[]],\"s\":false,\"S\":false}\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"ul\",null,{\"className\":\"nav navbar-nav navbar-right\",\"children\":[\"$\",\"li\",null,{\"id\":\"user-menu\",\"children\":[\"$\",\"$L3\",null,{\"href\":\"/login\",\"children\":\"Login\"}]}]}]\n11:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\ne:null\n"])</script><script>self.__next_f.push([1,"15:I[39677,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"6874\",\"static/chunks/6874-b5228efe8b8455fa.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"1141\",\"static/chunks/1141-829430d34cc15803.js\",\"4540\",\"static/chunks/4540-d4d9759f532dbe01.js\",\"6846\",\"static/chunks/6846-e00ebe4cf4673031.js\",\"1592\",\"static/chunks/1592-be14089f9df98c98.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"9032\",\"static/chunks/9032-c4156b261f2c50b7.js\",\"6504\",\"static/chunks/6504-00ef5b949820536b.js\",\"3882\",\"static/chunks/3882-68311d71c6e6d2c2.js\",\"9433\",\"static/chunks/9433-f2608cbcb1b74cd8.js\",\"2882\",\"static/chunks/2882-6cb3155ef24a8d15.js\",\"4745\",\"static/chunks/4745-b7bef6bfd1f2ca78.js\",\"1399\",\"static/chunks/1399-1fff5e65c29b20ed.js\",\"4757\",\"static/chunks/4757-5ccf877f341117b6.js\",\"3474\",\"static/chunks/3474-363b921f0385cec0.js\",\"5262\",\"static/chunks/5262-c565f8d56db6f340.js\",\"5300\",\"static/chunks/app/forum/page-7cc3f60b1e6efcec.js\"],\"default\"]\n16:I[73775,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"6874\",\"static/chunks/6874-b5228efe8b8455fa.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"1141\",\"static/chunks/1141-829430d34cc15803.js\",\"4540\",\"static/chunks/4540-d4d9759f532dbe01.js\",\"6846\",\"static/chunks/6846-e00ebe4cf4673031.js\",\"1592\",\"static/chunks/1592-be14089f9df98c98.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"9032\",\"static/chunks/9032-c4156b261f2c50b7.js\",\"6504\",\"static/chunks/6504-00ef5b949820536b.js\",\"3882\",\"static/chunks/3882-68311d71c6e6d2c2.js\",\"9433\",\"static/chunks/9433-f2608cbcb1b74cd8.js\",\"2882\",\"static/chunks/2882-6cb3155ef24a8d15.js\",\"4745\",\"static/chunks/4745-b7bef6bfd1f2ca78.js\",\"1399\",\"static/chunks/1399-1fff5e65c29b20ed.js\",\"4757\",\"static/chunks/4757-5ccf877f341117b6.js\",\"3474\",\"static/chunks/3474-363b921f0385cec0.js\",\"5262\",\"static/chunks/5262-c565f8d56db6f340.js\",\"5300\",\"static/chunks/app/forum/page-7cc3f60b1e6efcec.js\"],\"default\"]\n17:T53f,Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capa"])</script><script>self.__next_f.push([1,"city is under-explored.\nIn this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM.\nWe find that pre-training loss is a better indicator of the model's performance than the model's parameter count.\nWe apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets.\nTo augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT).\nRFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets.\nWe find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs.\nWe also find RFT brings more improvement for less performant LLMs.\nFurthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\\% significantly."])</script><script>self.__next_f.push([1,"c:[[\"$\",\"$L15\",null,{\"banner\":[\"$\",\"div\",null,{\"id\":\"or-banner\",\"className\":\"banner\",\"style\":null,\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"row\",\"children\":[\"$\",\"div\",null,{\"className\":\"col-xs-12\",\"children\":[\"$\",\"$L3\",null,{\"href\":\"/group?id=ICLR.cc/2024/Conference\",\"title\":\"Venue Homepage\",\"children\":[[\"$\",\"img\",null,{\"className\":\"icon\",\"src\":\"/images/arrow_left.svg\",\"alt\":\"back arrow\"}],\"Go to \",[\"$\",\"strong\",null,{\"children\":\"ICLR 2024 Conference\"}],\" \",\"homepage\"]}]}]}]}]}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"row\",\"children\":[\"$\",\"main\",null,{\"id\":\"content\",\"children\":[\"$\",\"div\",null,{\"className\":\"Forum_forum__wS8Fw\",\"children\":[\"$\",\"$L16\",null,{\"forumNote\":{\"content\":{\"title\":{\"value\":\"Scaling Relationship on Learning Mathematical Reasoning with Large Language Models\"},\"authors\":{\"value\":[\"Zheng Yuan\",\"Hongyi Yuan\",\"Chengpeng Li\",\"Guanting Dong\",\"Keming Lu\",\"Chuanqi Tan\",\"Chang Zhou\",\"Jingren Zhou\"]},\"authorids\":{\"value\":[\"~Zheng_Yuan2\",\"~Hongyi_Yuan1\",\"~Chengpeng_Li1\",\"~Guanting_Dong1\",\"~Keming_Lu1\",\"~Chuanqi_Tan3\",\"~Chang_Zhou2\",\"~Jingren_Zhou1\"]},\"keywords\":{\"value\":[\"Mathematical Reasoning\",\"Scaling Relationship\",\"Large Language Model\"]},\"abstract\":{\"value\":\"$17\"},\"pdf\":{\"value\":\"/pdf/a27e58f230a48ebbb5a9ba53a5855f572e91782b.pdf\"},\"primary_area\":{\"value\":\"representation learning for computer vision, audio, language, and other modalities\"},\"code_of_ethics\":{\"value\":\"I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.\"},\"submission_guidelines\":{\"value\":\"I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2024/AuthorGuide.\"},\"anonymous_url\":{\"value\":\"I certify that there is no URL (e.g., github page) that could be used to find authors' identity.\"},\"no_acknowledgement_section\":{\"value\":\"I certify that there is no acknowledgement section in this submission for double blind review.\"},\"venue\":{\"value\":\"Submitted to ICLR 2024\"},\"venueid\":{\"value\":\"ICLR.cc/2024/Conference/Rejected_Submission\"},\"supplementary_material\":{\"value\":\"/attachment/35f51711060e734769499f82128aed9e3e41d355.zip\"},\"_bibtex\":{\"value\":\"@misc{\\nyuan2024scaling,\\ntitle={Scaling Relationship on Learning Mathematical Reasoning with Large Language Models},\\nauthor={Zheng Yuan and Hongyi Yuan and Chengpeng Li and Guanting Dong and Keming Lu and Chuanqi Tan and Chang Zhou and Jingren Zhou},\\nyear={2024},\\nurl={https://openreview.net/forum?id=cijO0f8u35}\\n}\"},\"paperhash\":{\"value\":\"yuan|scaling_relationship_on_learning_mathematical_reasoning_with_large_language_models\"}},\"id\":\"cijO0f8u35\",\"forum\":\"cijO0f8u35\",\"number\":1254,\"cdate\":1695029839074,\"tcdate\":1695029839074,\"mdate\":1707625668604,\"tmdate\":1707625668604,\"signatures\":[\"ICLR.cc/2024/Conference/Submission1254/Authors\"],\"readers\":[\"everyone\"],\"writers\":[\"ICLR.cc/2024/Conference\",\"ICLR.cc/2024/Conference/Submission1254/Authors\"],\"odate\":1697213872796,\"invitations\":[\"ICLR.cc/2024/Conference/-/Submission\",\"ICLR.cc/2024/Conference/-/Post_Submission\",\"ICLR.cc/2024/Conference/Submission1254/-/Revision\",\"ICLR.cc/2024/Conference/Submission1254/-/Rebuttal_Revision\",\"ICLR.cc/2024/Conference/-/Edit\"],\"domain\":\"ICLR.cc/2024/Conference\",\"version\":2,\"details\":{\"writable\":false,\"presentation\":[{\"name\":\"title\",\"order\":1,\"type\":\"string\"},{\"name\":\"primary_area\",\"order\":2,\"type\":\"string\",\"input\":\"select\",\"value\":\"representation learning for computer vision, audio, language, and other modalities\",\"description\":null},{\"name\":\"authors\",\"order\":3},{\"name\":\"code_of_ethics\",\"order\":3,\"type\":\"string\",\"input\":\"checkbox\",\"value\":\"I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.\",\"description\":null},{\"name\":\"authorids\",\"order\":4},{\"name\":\"keywords\",\"order\":4,\"type\":\"string[]\"},{\"name\":\"submission_guidelines\",\"order\":4,\"type\":\"string\",\"input\":\"checkbox\",\"value\":\"I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2024/AuthorGuide.\",\"description\":null},{\"name\":\"TLDR\",\"order\":5,\"type\":\"string\",\"fieldName\":\"TL;DR\"},{\"name\":\"resubmission\",\"order\":5,\"type\":\"string\",\"input\":\"radio\"},{\"name\":\"abstract\",\"order\":6,\"type\":\"string\",\"input\":\"textarea\",\"markdown\":true},{\"name\":\"student_author\",\"order\":6,\"type\":\"string\",\"input\":\"radio\"},{\"name\":\"pdf\",\"order\":7,\"type\":\"file\"},{\"name\":\"anonymous_url\",\"order\":7,\"type\":\"string\",\"input\":\"checkbox\",\"value\":\"I certify that there is no URL (e.g., github page) that could be used to find authors' identity.\",\"description\":null},{\"name\":\"supplementary_material\",\"order\":8,\"type\":\"file\"},{\"name\":\"no_acknowledgement_section\",\"order\":8,\"type\":\"string\",\"input\":\"checkbox\",\"value\":\"I certify that there is no acknowledgement section in this submission for double blind review.\",\"description\":null},{\"name\":\"large_language_models\",\"order\":9,\"type\":\"string[]\",\"input\":\"checkbox\"},{\"name\":\"other_comments_on_LLMs\",\"order\":10,\"type\":\"string\",\"input\":\"textarea\"},{\"name\":\"venue\",\"hidden\":true},{\"name\":\"venueid\",\"hidden\":true},{\"name\":\"_bibtex\",\"type\":\"string\",\"input\":\"textarea\"},{\"name\":\"other_comments\"}]},\"apiVersion\":2},\"selectedNoteId\":\"$undefined\",\"selectedInvitationId\":\"$undefined\",\"prefilledValues\":{},\"query\":{\"id\":\"cijO0f8u35\"}}]}]}]}]}],[[\"$\",\"footer\",null,{\"className\":\"sitemap\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[[\"$\",\"div\",null,{\"className\":\"row hidden-xs\",\"children\":[[\"$\",\"div\",null,{\"className\":\"col-sm-4\",\"children\":[\"$\",\"ul\",null,{\"className\":\"list-unstyled\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/about\",\"children\":\"About OpenReview\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/group?id=OpenReview.net/Support\",\"children\":\"Hosting a Venue\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/venues\",\"children\":\"All Venues\"}]}]]}]}],[\"$\",\"div\",null,{\"className\":\"col-sm-4\",\"children\":[\"$\",\"ul\",null,{\"className\":\"list-unstyled\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/contact\",\"children\":\"Contact\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/sponsors\",\"children\":\"Sponsors\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"https://donate.stripe.com/eVqdR8fP48bK1R61fi0oM00\",\"children\":[\"$\",\"strong\",null,{\"children\":\"Donate\"}]}]}]]}]}],[\"$\",\"div\",null,{\"className\":\"col-sm-4\",\"children\":[\"$\",\"ul\",null,{\"className\":\"list-unstyled\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://docs.openreview.net/getting-started/frequently-asked-questions\",\"children\":\"Frequently Asked Questions\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/legal/terms\",\"children\":\"Terms of Use\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/legal/privacy\",\"children\":\"Privacy Policy\"}]}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"row visible-xs-block\",\"children\":[[\"$\",\"div\",null,{\"className\":\"col-xs-6\",\"children\":[\"$\",\"ul\",null,{\"className\":\"list-unstyled\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/about\",\"children\":\"About OpenReview\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/group?id=OpenReview.net/Support\",\"children\":\"Hosting a Venue\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/venues\",\"children\":\"All Venues\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/sponsors\",\"children\":\"Sponsors\"}]}]]}]}],[\"$\",\"div\",null,{\"className\":\"col-xs-6\",\"children\":[\"$\",\"ul\",null,{\"className\":\"list-unstyled\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://docs.openreview.net/getting-started/frequently-asked-questions\",\"children\":\"Frequently Asked Questions\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/contact\",\"children\":\"Contact\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"https://donate.stripe.com/eVqdR8fP48bK1R61fi0oM00\",\"children\":[\"$\",\"strong\",null,{\"children\":\"Donate\"}]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/legal/terms\",\"children\":\"Terms of Use\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/legal/privacy\",\"children\":\"Privacy Policy\"}]}]]}]}]]}]]}]}],[\"$\",\"footer\",null,{\"className\":\"sponsor\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"row\",\"children\":[\"$\",\"div\",null,{\"className\":\"col-sm-10 col-sm-offset-1\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"/about\",\"target\":\"_blank\",\"children\":\"OpenReview\"}],\" \",\"is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the\",\" \",[\"$\",\"a\",null,{\"href\":\"/sponsors\",\"target\":\"_blank\",\"children\":\"OpenReview Sponsors\"}],\". © \",2025,\" OpenReview\"]}]}]}]}]}]]]\n"])</script><script>self.__next_f.push([1,"f:null\n18:T53f,Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored.\nIn this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM.\nWe find that pre-training loss is a better indicator of the model's performance than the model's parameter count.\nWe apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets.\nTo augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT).\nRFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets.\nWe find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs.\nWe also find RFT brings more improvement for less performant LLMs.\nFurthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\\% significantly.19:T53f,Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored.\nIn this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM.\nWe find that pre-training loss is a better indicator of the model's performance than the model's parameter count.\nWe apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets."])</script><script>self.__next_f.push([1,"\nTo augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT).\nRFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets.\nWe find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs.\nWe also find RFT brings more improvement for less performant LLMs.\nFurthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\\% significantly.13:[[\"$\",\"title\",\"0\",{\"children\":\"Scaling Relationship on Learning Mathematical Reasoning with Large Language Models | OpenReview\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"$18\"}],[\"$\",\"meta\",\"2\",{\"name\":\"citation_title\",\"content\":\"Scaling Relationship on Learning Mathematical Reasoning with Large Language Models\"}],[\"$\",\"meta\",\"3\",{\"name\":\"citation_author\",\"content\":\"Zheng Yuan\"}],[\"$\",\"meta\",\"4\",{\"name\":\"citation_author\",\"content\":\"Hongyi Yuan\"}],[\"$\",\"meta\",\"5\",{\"name\":\"citation_author\",\"content\":\"Chengpeng Li\"}],[\"$\",\"meta\",\"6\",{\"name\":\"citation_author\",\"content\":\"Guanting Dong\"}],[\"$\",\"meta\",\"7\",{\"name\":\"citation_author\",\"content\":\"Keming Lu\"}],[\"$\",\"meta\",\"8\",{\"name\":\"citation_author\",\"content\":\"Chuanqi Tan\"}],[\"$\",\"meta\",\"9\",{\"name\":\"citation_author\",\"content\":\"Chang Zhou\"}],[\"$\",\"meta\",\"10\",{\"name\":\"citation_author\",\"content\":\"Jingren Zhou\"}],[\"$\",\"meta\",\"11\",{\"name\":\"citation_online_date\",\"content\":\"2023/10/13\"}],[\"$\",\"meta\",\"12\",{\"name\":\"citation_pdf_url\",\"content\":\"https://openreview.net/pdf?id=cijO0f8u35\"}],[\"$\",\"meta\",\"13\",{\"name\":\"citation_abstract\",\"content\":\"$19\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:title\",\"content\":\"Scaling Relationship on Learning Mathematical Reasoning with Large...\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:description\",\"content\":\"Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling re"])</script><script>self.__next_f.push([1,"lationship of it with respect to LLM capacity is under-explored.\\nIn this paper, we investigate how...\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:title\",\"content\":\"Scaling Relationship on Learning Mathematical Reasoning with Large...\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:description\",\"content\":\"Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored.\\nIn this paper, we investigate how...\"}]]\n"])</script></body></html>
```
</Expandable>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem-write_file`

```text workspace/ref.bib

@misc{
yuan2024scaling,
title={Scaling Relationship on Learning Mathematical Reasoning with Large Language Models},
author={Zheng Yuan and Hongyi Yuan and Chengpeng Li and Guanting Dong and Keming Lu and Chuanqi Tan and Chang Zhou and Jingren Zhou},
year={2024},
url={https://openreview.net/forum?id=cijO0f8u35}
}

@article{singh2024evaluation,
  title={Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?},
  author={Singh, Aaditya K and Kocyigit, Muhammed Yusuf and Poulton, Andrew and Esiobu, David and Lomeli, Maria and Szilvasy, Gergely and Hupkes, Dieuwke},
  journal={arXiv preprint arXiv:2411.03923},
  year={2024}
}

@inproceedings{
ding2024semcoder,
title={SemCoder: Training Code Language Models with Comprehensive Semantics Reasoning},
author={Yangruibo Ding and Jinjun Peng and Marcus J. Min and Gail Kaiser and Junfeng Yang and Baishakhi Ray},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=PnlCHQrM69}
}

@article{huang2024opencoder,
  title={Opencoder: The open cookbook for top-tier code large language models},
  author={Huang, Siming and Cheng, Tianhao and Liu, Jason Klein and Hao, Jiaran and Song, Liuyihan and Xu, Yang and Yang, J and Liu, JH and Zhang, Chenchen and Chai, Linzheng and others},
  journal={arXiv preprint arXiv:2411.04905},
  year={2024}
}

@article{yue2024mammoth2,
  title={Mammoth2: Scaling instructions from the web},
  author={Yue, Xiang and Zheng, Tuney and Zhang, Ge and Chen, Wenhu},
  journal={arXiv preprint arXiv:2405.03548},
  year={2024}
}

@article{toshniwal2024openmathinstruct2,
  title={Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data},
  author={Toshniwal, Shubham and Du, Wei and Moshkov, Ivan and Kisacanin, Branislav and Ayrapetyan, Alexan and Gitman, Igor},
  journal={arXiv preprint arXiv:2410.01560},
  year={2024}
}

@software{benallal2024smollmcorpus,
  author = {Ben Allal, Loubna and Lozhkov, Anton and Penedo, Guilherme and Wolf, Thomas and von Werra, Leandro},
  title = {SmolLM-Corpus},
  month = July,
  year = 2024,
  url = {https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus}
}

@article{zhu2024deepseekcoderv2,
  title={DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence},
  author={Zhu, Qihao and Guo, Daya and Shao, Zhihong and Yang, Dejian and Wang, Peiyi and Xu, Runxin and Wu, Y and Li, Yukun and Gao, Huazuo and Ma, Shirong and others},
  journal={arXiv preprint arXiv:2406.11931},
  year={2024}
}

@article{hui2024qwen25coder,
      title={Qwen2. 5-Coder Technical Report},
      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
      journal={arXiv preprint arXiv:2409.12186},
      year={2024}
}

@article{zeng2024skyworkmath,
  title={Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models--The Story Goes On},
  author={Zeng, Liang and Zhong, Liangjun and Zhao, Liang and Wei, Tianwen and Yang, Liu and He, Jujie and Cheng, Cheng and Hu, Rui and Liu, Yang and Yan, Shuicheng and others},
  journal={arXiv preprint arXiv:2407.08348},
  year={2024}
}

@article{ying2024internlmmath,
  title={Internlm-math: Open math large language models toward verifiable reasoning},
  author={Ying, Huaiyuan and Zhang, Shuo and Li, Linyang and Zhou, Zhejian and Shao, Yunfan and Fei, Zhaoye and Ma, Yichuan and Hong, Jiawei and Liu, Kuikun and Wang, Ziyi and others},
  journal={arXiv preprint arXiv:2402.06332},
  year={2024}
}

@article{yang2024qwen25,
  title={Qwen2. 5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{dubey2024llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{xu2024aguvis,
  title={Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction},
  author={Xu, Yiheng and Wang, Zekun and Wang, Junli and Lu, Dunjie and Xie, Tianbao and Saha, Amrita and Sahoo, Doyen and Yu, Tao and Xiong, Caiming},
  journal={arXiv preprint arXiv:2412.04454},
  year={2024}
}

@inproceedings{
zhou2024math4ai,
title={{AI} for Math or Math for {AI}? On the Generalization of Learning Mathematical Problem Solving},
author={Ruochen Zhou and Minrui Xu and Shiqi Chen and Junteng Liu and Yunqi Li and LIN Xinxin and Zhengyu Chen and Junxian He},
booktitle={The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24},
year={2024},
url={https://openreview.net/forum?id=xlnvZ85CSo}
}

@article{paster2023openwebmath,
  title={Openwebmath: An open dataset of high-quality mathematical web text},
  author={Paster, Keiran and Santos, Marco Dos and Azerbayev, Zhangir and Ba, Jimmy},
  journal={arXiv preprint arXiv:2310.06786},
  year={2023}
}

@article{ma2024korbench,
  title={KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks},
  author={Ma, Kaijing and Du, Xinrun and Wang, Yunran and Zhang, Haoran and Wen, Zhoufutu and Qu, Xingwei and Yang, Jian and Liu, Jiaheng and Liu, Minghao and Yue, Xiang and others},
  journal={arXiv preprint arXiv:2410.06526},
  year={2024}
}

@inproceedings{dua2019drop,
  title={DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},
  author={Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2368--2378},
  year={2019}
}

@inproceedings{
rein2024gpqa,
title={{GPQA}: A Graduate-Level Google-Proof Q\&A Benchmark},
author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=Ti67584b98}
}

@article{white2024livebench,
  title={Livebench: A challenging, contamination-free llm benchmark},
  author={White, Colin and Dooley, Samuel and Roberts, Manley and Pal, Arka and Feuer, Ben and Jain, Siddhartha and Shwartz-Ziv, Ravid and Jain, Neel and Saifullah, Khalid and Naidu, Siddartha and others},
  journal={arXiv preprint arXiv:2406.19314},
  year={2024}
}

@article{lin2025zebralogic,
  title={ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning},
  author={Lin, Bill Yuchen and Bras, Ronan Le and Richardson, Kyle and Sabharwal, Ashish and Poovendran, Radha and Clark, Peter and Choi, Yejin},
  journal={arXiv preprint arXiv:2502.01100},
  year={2025}
}

@misc{codestral,
    title={Codestral},
    author={Mistral-AI},
    url={https://mistral.ai/news/codestral/},
    year={2024}
}

@misc{haillm,
    title={Hai-llm: Efficient and lightweight training tool for large models},
    author={High-flyer},
    url={https://www.high-flyer.cn/en/blog/hai-llm},
    year={2023}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@misc{qwq,
  author = {Team, Qwen},
  title = {QwQ: Reflect Deeply on the Boundaries of the Unknown},
  month = nov,
  year = 2024,
  url = {https://qwenlm.github.io/blog/qwq-32b-preview/}
}

@misc{knauff2010complex,
  title={Complex cognition: the science of human reasoning, problem-solving, and decision-making},
  author={Knauff, Markus and Wolf, Ann G},
  journal={Cognitive processing},
  volume={11},
  pages={99--102},
  year={2010},
  publisher={Springer}
}

@article{wang2010cognitive,
  title={On the cognitive process of human problem solving},
  author={Wang, Yingxu and Chiew, Vincent},
  journal={Cognitive systems research},
  volume={11},
  number={1},
  pages={81--92},
  year={2010},
  publisher={Elsevier}
}

@article{dehaene2004arithmetic,
  title={Arithmetic and the brain},
  author={Dehaene, Stanislas and Molko, Nicolas and Cohen, Laurent and Wilson, Anna J},
  journal={Current opinion in neurobiology},
  volume={14},
  number={2},
  pages={218--224},
  year={2004},
  publisher={Elsevier}
}

@article{xiang2025towards,
  title={Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though},
  author={Xiang, Violet and Snell, Charlie and Gandhi, Kanishk and Albalak, Alon and Singh, Anikait and Blagden, Chase and Phung, Duy and Rafailov, Rafael and Lile, Nathan and Mahan, Dakota and others},
  journal={arXiv preprint arXiv:2501.04682},
  year={2025}
}

@article{huang2022towards,
  title={Towards reasoning in large language models: A survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}

@article{qiao2022reasoning,
  title={Reasoning with language model prompting: A survey},
  author={Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  journal={arXiv preprint arXiv:2212.09597},
  year={2022}
}

@article{lozhkov2024starcoder,
  title={Starcoder 2 and the stack v2: The next generation},
  author={Lozhkov, Anton and Li, Raymond and Allal, Loubna Ben and Cassano, Federico and Lamy-Poirier, Joel and Tazi, Nouamane and Tang, Ao and Pykhtar, Dmytro and Liu, Jiawei and Wei, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.19173},
  year={2024}
}

@inproceedings{
zhang2024towards,
title={Towards Bridging Classical and Neural Computation through a Read-Eval-Print Loop},
author={David W. Zhang and Micha{\"e}l Defferrard and Corrado Rainone and Roland Memisevic},
booktitle={ICML 2024 Workshop on LLMs and Cognition},
year={2024},
url={https://openreview.net/forum?id=Y08aOVCPTt}
}


@article{wen2024codeplan,
  title={Unlocking Reasoning Potential in Large Langauge Models by Scaling Code-form Planning},
  author={Wen, Jiaxin and Guan, Jian and Wang, Hongning and Wu, Wei and Huang, Minlie},
  journal={arXiv preprint arXiv:2409.12452},
  year={2024}
}

@inproceedings{
gorceix2024learningmathrules,
title={Learning Mathematical Rules with Large Language Models},
author={Antoine Gorceix and Bastien Le Chenadec and Ahmad Rammal and Nelson Vadori and Manuela Veloso},
booktitle={The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24},
year={2024},
url={https://openreview.net/forum?id=tIlDF5B6T4}
}

@article{xu2024core,
  title={CoRE: LLM as Interpreter for Natural Language Programming, Pseudo-Code Programming, and Flow Programming of AI Agents},
  author={Xu, Shuyuan and Li, Zelong and Mei, Kai and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2405.06907},
  year={2024}
}

@inproceedings{
morishita2024enhancing,
title={Enhancing Reasoning Capabilities of {LLM}s via Principled Synthetic Logic Corpus},
author={Terufumi Morishita and Gaku Morio and Atsuki Yamaguchi and Yasuhiro Sogawa},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=mljDUaQpln}
}

@inproceedings{morishita2023learning,
  title={Learning deductive reasoning from synthetic corpus based on formal logic},
  author={Morishita, Terufumi and Morio, Gaku and Yamaguchi, Atsuki and Sogawa, Yasuhiro},
  booktitle={International Conference on Machine Learning},
  pages={25254--25274},
  year={2023},
  organization={PMLR}
}

@article{chen2024reverse,
  title={Reverse Thinking Makes LLMs Stronger Reasoners},
  author={Chen, Justin Chih-Yao and Wang, Zifeng and Palangi, Hamid and Han, Rujun and Ebrahimi, Sayna and Le, Long and Perot, Vincent and Mishra, Swaroop and Bansal, Mohit and Lee, Chen-Yu and others},
  journal={arXiv preprint arXiv:2411.19865},
  year={2024}
}

@article{jain2024livecodebench,
  title={Livecodebench: Holistic and contamination free evaluation of large language models for code},
  author={Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  journal={arXiv preprint arXiv:2403.07974},
  year={2024}
}

@article{zaremba2014learning,
  title={Learning to execute},
  author={Zaremba, Wojciech and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1410.4615},
  year={2014}
}

@article{graves2014neural,
  title={Neural Turing Machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@inproceedings{ding2024traced,
  title={Traced: Execution-aware pre-training for source code},
  author={Ding, Yangruibo and Steenhoek, Benjamin and Pei, Kexin and Kaiser, Gail and Le, Wei and Ray, Baishakhi},
  booktitle={Proceedings of the 46th IEEE/ACM International Conference on Software Engineering},
  pages={1--12},
  year={2024}
}

@inproceedings{
ni2024next,
title={{NE}xT: Teaching Large Language Models to Reason about Code Execution},
author={Ansong Ni and Miltiadis Allamanis and Arman Cohan and Yinlin Deng and Kensen Shi and Charles Sutton and Pengcheng Yin},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=B1W712hMBi}
}

@article{ding2024cycle,
  title={Cycle: Learning to self-refine the code generation},
  author={Ding, Yangruibo and Min, Marcus J and Kaiser, Gail and Ray, Baishakhi},
  journal={Proceedings of the ACM on Programming Languages},
  volume={8},
  number={OOPSLA1},
  pages={392--418},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@inproceedings{wang2024leti,
  title={LETI: Learning to Generate from Textual Interactions},
  author={Wang, Xingyao and Peng, Hao and Jabbarvand, Reyhaneh and Ji, Heng},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={223--239},
  year={2024}
}

@inproceedings{gururangan2020don,
  title={Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8342--8360},
  year={2020}
}

@misc{deepseekai2025r1,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Li and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{jiang2024logicpro,
  title={LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning},
  author={Jiang, Jin and Yan, Yuchen and Liu, Yang and Jin, Yonggang and Peng, Shuai and Zhang, Mengdi and Cai, Xunliang and Cao, Yixin and Gao, Liangcai and Tang, Zhi},
  journal={arXiv preprint arXiv:2409.12929},
  year={2024}
}
@article{wei2024qwen2,
  title={Qwen2. 5-math technical report},
  author={Wei, Haoran and Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}

@inproceedings{
feng2024lemur,
title={Lemur: Harmonizing Natural Language and Code for Language Agents},
author={Yi-Lin Feng and Yuxiang-Yue Wang and Xu-Hui Wang and Hong-Bin Zhuang and Zhen-Rong Shi and Can-Xiong Lu and Jia-Xin Wen and Xin-Rui Yang and Yong-Feng Zhang and Chen-Yang},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=0vL5jiwS3z}
}

@article{roziere2024code,
  title={Code Llama: Open Foundation Models for Code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xian Li and Lample, Guillaume and Lavril, Thibaut and Izacard, Gautier and Copet, Julien and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@inproceedings{hendrycks2021measuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  year={2021},
  eprint={2009.03300},
  archivePrefix={arXiv},
  primaryClass={cs.CY}
}

```
</div>

<div className="error-box">
❌ `tool calling error`
```
Error running tool filesystem-write_file: Error invoking MCP tool write_file: Timed out while waiting for response to ClientRequest. Waited 300.0 seconds.
```
</div>

<div className="tool-call-box">
<svg height="14" width="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style={{margin: 0, padding: 0, display: 'inline-block'}}><g fill="none" fill-rule="evenodd"><rect fill="#fabc05" height="512" rx="42.667" width="512"/><path d="M58.667 293.333L0 209.125V42.663C0 19.1 19.137 0 42.663 0h426.674C492.9 0 512 19.137 512 42.663l-282.667.004S58.667 305.14 58.667 293.333z" fill="#eb4335"/><path d="M303.3 340.528L199.876 512H42.633C19.087 512 0 492.94 0 469.328V209.125l54.537 74.854c28.457 48.33 81.024 80.762 141.167 80.762 40.548 0 77.822-14.86 106.533-39.29l5.348 5.349c-.918 3.235-2.337 6.5-4.284 9.728z" fill="#34a854"/><g fill-rule="nonzero"><path d="M194.667 85.333c-66.667 0-120 52.148-120 117.334C74.667 267.852 128 320 194.667 320c66.666 0 120-52.148 120-117.333 0-65.186-53.334-117.334-120-117.334z" fill="#4285f5"/><path d="M327.59 300.924l28.788 28.787 13.204-14.159 115.751 115.751-59.363 59.364-115.751-115.751 13.553-12.64-28.85-28.85c-27.526 20.496-61.677 32.712-98.52 32.712C105.605 366.138 32 292.532 32 201.735c0-90.796 73.605-164.402 164.402-164.402s164.402 73.606 164.402 164.402c0 37.142-12.414 71.549-33.213 99.19zM196.403 87.92c-63.231 0-113.817 50.585-113.817 113.816 0 63.232 50.586 113.817 113.817 113.817S310.22 264.967 310.22 201.735c0-63.231-50.585-113.816-113.817-113.816z" fill="#fff"/></g></g></svg> `web_search `

```json
{
	"query":"Qwen2.5-math technical report bibtex"
}
```
</div>

<div className="result-box">
🔍`tool result`
<Expandable title="Result">
```json
Title: Qwen2.5-Math Technical Report: Toward Mathematical ...
Link: https://arxiv.org/abs/2409.12122
Snippet: by A Yang · 2024 · Cited by 389 — In this report, we present a series of math-specific large language models: Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B.
Sitelinks: No sitelinks

Title: [2412.15115] Qwen2.5 Technical Report
Link: https://arxiv.org/abs/2412.15115
Snippet: by A Yang · 2024 · Cited by 3021 — Abstract:In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs.
Sitelinks: No sitelinks

Title: QwenLM/Qwen2.5-Math: A series of math-specific large ...
Link: https://github.com/QwenLM/Qwen2.5-Math
Snippet: Qwen2.5-Math series is expanded to support using both CoT and Tool-integrated Reasoning (TIR) to solve math problems in both Chinese and English.
Sitelinks: No sitelinks

Title: Qwen2.5 Technical Report
Link: https://bibbase.org/network/publication/qwen-yang-yang-zhang-hui-zheng-yu-li-etal-qwen25technicalreport-2025
Snippet: Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference ...
Sitelinks: No sitelinks

Title: Qwen/Qwen2.5-Math-7B
Link: https://huggingface.co/Qwen/Qwen2.5-Math-7B
Snippet: Qwen2.5-Math-7B is a base model typically used for completion and few-shot inference, serving as a better starting point for fine-tuning.
Sitelinks: No sitelinks

Title: Qwen2.5-Math Technical Report: Toward Mathematical ...
Link: https://www.researchgate.net/publication/384116167_Qwen25-Math_Technical_Report_Toward_Mathematical_Expert_Model_via_Self-Improvement
Snippet: In this report, we present a series of math-specific large language models: Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the ...
Sitelinks: No sitelinks

Title: Paper page - Qwen2.5 Technical Report
Link: https://huggingface.co/papers/2412.15115
Snippet: In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs.
Sitelinks: No sitelinks

Title: [Literature Review] Qwen2.5-Math Technical Report
Link: https://www.themoonlight.io/en/review/qwen25-math-technical-report-toward-mathematical-expert-model-via-self-improvement
Snippet: The technical report "QWEN2.5-MATH" presents advancements in developing large language models specifically designed for handling mathematical tasks.
Sitelinks: No sitelinks

Title: [PDF] Qwen2.5 Technical Report
Link: https://www.semanticscholar.org/paper/Qwen2.5-Technical-Report-Yang-Yang/88aa6b1f37d1fd8e0a40499ce9bb87873f03aaa8
Snippet: The open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the ...
Sitelinks: No sitelinks

Title: Qwen2.5-Coder Technical Report
Link: https://openreview.net/forum?id=0qv1PC4fsX
Snippet: by B Hui · 2024 · Cited by 635 — Abstract: In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5.
Sitelinks: No sitelinks

```
</Expandable>
</div>

</Accordion>

</AccordionGroup>
