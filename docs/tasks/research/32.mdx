---
title: "ImageNet"
description: "Summarize the ImageNet 256 experimental results from my image generation papers into a LaTeX table."
mode: wide
---

<Card>
<div className="tools-container">
<div className="mcp-servers-container">
<div className="mcp-servers-title">
MCP Servers
</div>
<div className="mcp-servers-grid">
<div className="mcp-server-item">
<svg width="20px" height="20px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg>
<span className="mcp-server-name">pdf-tools</span>
</div>
<div className="mcp-server-item">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="20px" height="20px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg>
<span className="mcp-server-name">filesystem</span>
</div>
</div>
</div>
<div className="local-tools-container">
<div className="mcp-servers-title">
Local Tools
</div>
<div className="local-tools-grid">
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 100 100" enable-background="new 0 0 100 100" id="Layer_1" version="1.1" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><g><rect clip-rule="evenodd" fill="#F2F2F2" fill-rule="evenodd" height="83.437" width="67.025" x="9.012" y="7.604"/><path d="M77.454,92.458H7.595V6.187h69.859V92.458z M10.429,89.624H74.62V9.021H10.429V89.624z"/></g><g><rect clip-rule="evenodd" fill="#FF7C24" fill-rule="evenodd" height="10.481" width="17.952" x="46.695" y="34.866"/></g><g><rect height="2.834" width="19.463" x="20.504" y="35.575"/></g><g><rect height="2.834" width="15.561" x="20.718" y="42.508"/></g><g><rect height="2.833" width="15.562" x="20.813" y="49.514"/></g><g><rect height="2.833" width="27.128" x="20.718" y="56.753"/></g><g><rect height="2.833" width="23.51" x="20.718" y="63.688"/></g><g><rect clip-rule="evenodd" fill="#39B6CC" fill-rule="evenodd" height="9.004" width="44.095" x="20.553" y="19.064"/></g><g><rect height="2.833" width="26.272" x="20.718" y="70.32"/></g><g><rect height="2.834" width="32.8" x="20.718" y="77.253"/></g><g><rect height="2.834" width="3.235" x="38.304" y="42.508"/></g><g><path clip-rule="evenodd" d="M77.931,71.902l4.287,4.427l-6.644,6.437l-4.309-4.457    C74.147,76.998,76.504,74.726,77.931,71.902L77.931,71.902z" fill="#F2F2F2" fill-rule="evenodd"/><path d="M75.542,84.77l-6.692-6.92l1.828-0.831c2.579-1.174,4.706-3.218,5.989-5.756l0.897-1.776l6.656,6.874L75.542,84.77z     M73.584,78.669l2.023,2.091l4.605-4.463l-2.007-2.074C76.994,76.012,75.414,77.531,73.584,78.669z"/></g><g><polygon clip-rule="evenodd" fill="#39B6CC" fill-rule="evenodd" points="83.267,75.319 91.984,84.338 83.247,92.779     74.535,83.761   "/><path d="M83.213,94.783L72.531,83.726l10.771-10.41l10.687,11.056L83.213,94.783z M76.538,83.794l6.744,6.981l6.698-6.472    l-6.748-6.981L76.538,83.794z"/></g><g><path clip-rule="evenodd" d="M66.124,50.799c7.742,0,14.018,6.276,14.018,14.019    s-6.275,14.019-14.018,14.019c-7.743,0-14.019-6.276-14.019-14.019S58.381,50.799,66.124,50.799L66.124,50.799z" fill="#F2F2F2" fill-rule="evenodd"/><path d="M66.124,80.253c-8.511,0-15.435-6.924-15.435-15.435s6.924-15.435,15.435-15.435S81.56,56.307,81.56,64.818    S74.635,80.253,66.124,80.253z M66.124,52.216c-6.949,0-12.601,5.653-12.601,12.602s5.651,12.601,12.601,12.601    c6.948,0,12.602-5.652,12.602-12.601S73.072,52.216,66.124,52.216z"/></g><g><rect height="2.833" width="10.313" x="39.902" y="49.514"/></g><g><path d="M76.404,65.586H73.57c0-0.636-0.068-1.255-0.205-1.84c-0.043-0.186-0.096-0.385-0.169-0.63l2.717-0.808    c0.091,0.304,0.158,0.559,0.215,0.801C76.31,63.901,76.404,64.735,76.404,65.586z M72.438,61.433    c-1.489-2.5-4.203-4.058-7.084-4.061l0.004-2.834c3.871,0.005,7.518,2.091,9.516,5.445L72.438,61.433z"/></g></g></svg>
<span className="local-tool-name">history</span>
</div>
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M861.588238 240.133873v-65.792823c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.631049H314.659414v-65.631049c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.792823c-36.317212 0.868255-65.631049 30.539428-65.63105 67.061417v543.745565c0 37.06772 30.155471 67.223191 67.223191 67.223191h696.886045c37.06772 0 67.223191-30.155471 67.223191-67.223191V307.19529c-0.001024-36.52199-29.315885-66.193162-65.633097-67.061417z m-109.385765-65.792823c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m-546.928824 0c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m678.191947 676.600829c0 12.935767-10.532708 23.468476-23.468476 23.468475H163.111076c-12.935767 0-23.468476-10.532708-23.468476-23.468475V307.19529c0-12.402323 9.677764-22.593054 21.877358-23.415233v65.577807c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.631049h393.789368v65.631049c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.577807c12.19857 0.82218 21.877358 11.012911 21.877358 23.415233v543.746589z" fill="#22C67F" /><path d="M706.719439 478.272194l-48.01715-44.741741-182.28128 195.621482-111.468348-122.615387-48.563905 44.148911 159.469116 172.685427z" fill="#74E8AE" /></svg>
<span className="local-tool-name">claim_done</span>
</div>
<div className="local-tool-item">
<Icon icon="bars-progress" size={20} color="#4286f6" />
<span className="local-tool-name">manage_context</span>
</div>
<div className="local-tool-item">
<Icon icon="filter-list" size={20} color="#4286f6" />
<span className="local-tool-name">handle_overlong_tool_outputs</span>
</div>
</div>
</div>
</div>
</Card>









## Instruction

I am a graduate student in computer vision, and I have several papers on image generation in my workspace. I need your help to summarize the experimental results of the models in these papers on ImageNet 256 and store the results in a LaTeX table. The table should include: Model Name (When multiple models are included, report the one with the lowest FID result in the paper.), Method Category (determine if it belongs to VAE, GAN, Diffusion, Flow-based, or AR), Model Parameters, FID-50K, and Inception Score. Finally, please save the results in survey.tex in my workspace (only the LaTeX table source code is needed), and the order of the table should be in descending order of FID. The example structure of the table is format.tex.

## Initial State

### Local Workspace

<div className="file-tree">
<a href="https://github.com/hkust-nlp/mcpbench_dev/tree/finalpool_dev/tasks/finalpool/imagenet">workspace</a>/
├── 2212.09748v2.pdf
├── 2406.06525v1.pdf
└── format.tex
</div>

## Model Trajectory

<AccordionGroup>
<Accordion title="claude-4.5-sonnet">

<Columns cols={3}>
<Card title="Task Completion" icon="check">
Completed
</Card>
<Card title="Tool Calls" icon="wrench">
14
</Card>
<Card title="Turns" icon="arrows-rotate">
11
</Card>
</Columns>

<div className="thinking-box">
🧐`Agent`

I'll help you summarize the experimental results from the papers in your workspace. Let me start by exploring the workspace to find the papers and the format file.
</div>

<div className="result-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem list_directory`

<Expandable title="Details">
```json arguments
{
	"path": "/workspace/dumps/workspace"
}

```

```json output_result
[DIR] .pdf_tools_tempfiles
[FILE] 2212.09748v2.pdf
[FILE] 2406.06525v1.pdf
[FILE] format.tex
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Good! I can see there are 2 PDF papers and a format.tex file. Let me first check the format file to understand the table structure you want.
</div>

<div className="result-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

<Expandable title="Details">
```json arguments
{
	"path": "/workspace/dumps/workspace/format.tex"
}

```

```json output_result
\begin{table}[!ht]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        Model & Class & \#Param & FID-50K & Inception Score \\
        \midrule
        A & GAN & 300M & 40.20 & 150.00 \\
        B & Diffusion & 2B & 20.20 & 190.00 \\
        \bottomrule
    \end{tabular}
\end{table}
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Perfect! Now let me examine the PDF papers to extract the experimental results. Let me start by getting basic information about each PDF.
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools get_pdf_info`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf"
}

```

```json output_result
PDF file information:
Total pages: 25
Title: 
Author: 
Creator: LaTeX with hyperref
Creation date: D:20230303012649Z

```

</Expandable>
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools get_pdf_info`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2406.06525v1.pdf"
}

```

```json output_result
PDF file information:
Total pages: 26
Title: 
Author: 
Creator: LaTeX with hyperref
Creation date: D:20240611014653Z

```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me read the first few pages of each paper to understand what they're about and find the experimental results on ImageNet 256.
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	"start_page": 1,
	"end_page": 5
}

```

```json output_result
PDF total pages: 25
Extracted page range: 1-5

=== Page 1 ===
Scalable Diffusion Models with Transformers
William Peebles*
UC BerkeleySaining Xie
New York University
Figure 1. Diffusion models with transformer backbones achieve state-of-the-art image quality. We show selected samples from two
of our class-conditional DiT-XL/2 models trained on ImageNet at 512 512 and 256256 resolution, respectively.
Abstract
We explore a new class of diffusion models based on the
transformer architecture. We train latent diffusion models
of images, replacing the commonly-used U-Net backbone
with a transformer that operates on latent patches. We an-
alyze the scalability of our Diffusion Transformers (DiTs)
through the lens of forward pass complexity as measured by
Gﬂops. We ﬁnd that DiTs with higher Gﬂops—through in-
creased transformer depth/width or increased number of in-
put tokens—consistently have lower FID. In addition to pos-
sessing good scalability properties, our largest DiT-XL/2
models outperform all prior diffusion models on the class-
conditional ImageNet 512 512 and 256256 benchmarks,
achieving a state-of-the-art FID of 2.27 on the latter.1. Introduction
Machine learning is experiencing a renaissance powered
by transformers. Over the past ﬁve years, neural architec-
tures for natural language processing [8, 42], vision [10]
and several other domains have largely been subsumed by
transformers [60]. Many classes of image-level genera-
tive models remain holdouts to the trend, though—while
transformers see widespread use in autoregressive mod-
els [3,6,43,47], they have seen less adoption in other gener-
ative modeling frameworks. For example, diffusion models
have been at the forefront of recent advances in image-level
generative models [9,46]; yet, they all adopt a convolutional
U-Net architecture as the de-facto choice of backbone.
*Work done during an internship at Meta AI, FAIR Team.
Code and project page available here.
1arXiv:2212.09748v2  [cs.CV]  2 Mar 2023

=== Page 2 ===
520 80 320GflopsDiameterFigure 2. ImageNet generation with Diffusion Transformers (DiTs). Bubble area indicates the ﬂops of the diffusion model. Left:
FID-50K (lower is better) of our DiT models at 400K training iterations. Performance steadily improves in FID as model ﬂops increase.
Right: Our best model, DiT-XL/2, is compute-efﬁcient and outperforms all prior U-Net-based diffusion models, like ADM and LDM.
The seminal work of Ho et al. [19] ﬁrst introduced the
U-Net backbone for diffusion models. Having initially seen
success within pixel-level autoregressive models and con-
ditional GANs [23], the U-Net was inherited from Pixel-
CNN++ [52, 58] with a few changes. The model is con-
volutional, comprised primarily of ResNet [15] blocks. In
contrast to the standard U-Net [49], additional spatial self-
attention blocks, which are essential components in trans-
formers, are interspersed at lower resolutions. Dhariwal and
Nichol [9] ablated several architecture choices for the U-
Net, such as the use of adaptive normalization layers [40] to
inject conditional information and channel counts for con-
volutional layers. However, the high-level design of the U-
Net from Ho et al. has largely remained intact.
With this work, we aim to demystify the signiﬁcance of
architectural choices in diffusion models and offer empiri-
cal baselines for future generative modeling research. We
show that the U-Net inductive bias is notcrucial to the per-
formance of diffusion models, and they can be readily re-
placed with standard designs such as transformers. As a
result, diffusion models are well-poised to beneﬁt from the
recent trend of architecture uniﬁcation—e.g., by inheriting
best practices and training recipes from other domains, as
well as retaining favorable properties like scalability, ro-
bustness and efﬁciency. A standardized architecture would
also open up new possibilities for cross-domain research.
In this paper, we focus on a new class of diffusion models
based on transformers. We call them Diffusion Transform-
ers, or DiTs for short. DiTs adhere to the best practices of
Vision Transformers (ViTs) [10], which have been shown to
scale more effectively for visual recognition than traditional
convolutional networks (e.g., ResNet [15]).More speciﬁcally, we study the scaling behavior of trans-
formers with respect to network complexity vs. sample
quality . We show that by constructing and benchmark-
ing the DiT design space under the Latent Diffusion Mod-
els(LDMs) [48] framework, where diffusion models are
trained within a V AE’s latent space, we can successfully
replace the U-Net backbone with a transformer. We further
show that DiTs are scalable architectures for diffusion mod-
els: there is a strong correlation between the network com-
plexity (measured by Gﬂops) vs. sample quality (measured
by FID). By simply scaling-up DiT and training an LDM
with a high-capacity backbone (118.6 Gﬂops), we are able
to achieve a state-of-the-art result of 2.27 FID on the class-
conditional 256256ImageNet generation benchmark.
2. Related Work
Transformers. Transformers [60] have replaced domain-
speciﬁc architectures across language, vision [10], rein-
forcement learning [5, 25] and meta-learning [39]. They
have shown remarkable scaling properties under increas-
ing model size, training compute and data in the language
domain [26], as generic autoregressive models [17] and
as ViTs [63]. Beyond language, transformers have been
trained to autoregressively predict pixels [6, 7, 38]. They
have also been trained on discrete codebooks [59] as both
autoregressive models [11,47] and masked generative mod-
els [4, 14]; the former has shown excellent scaling behavior
up to 20B parameters [62]. Finally, transformers have been
explored in DDPMs to synthesize non-spatial data; e.g., to
generate CLIP image embeddings in DALL E 2 [41,46]. In
this paper, we study the scaling properties of transformers
when used as the backbone of diffusion models of images.
2

=== Page 3 ===
Multi-Head Self-AttentionLayer NormScale, ShiftMLPPointwise FeedforwardLayer NormScale, ShiftScale++
Scale𝛾!,𝛽!𝛼!𝛾",𝛽"𝛼"
Input TokensConditioningDiTBlock with adaLN-ZeroLatent Diffusion TransformerTimestep 𝑡Label 𝑦DiTBlockN xPatchifyLayer NormLinear and ReshapeEmbedNoiseΣ32 x 32 x 432 x 32 x 4
Noised Latent32 x 32 x 4Multi-Head Self-AttentionLayer NormPointwise Feedforward
Layer Norm++
Input TokensConditioningDiTBlock with Cross-AttentionMulti-Head Cross-AttentionLayer Norm+Multi-Head Self-AttentionLayer NormPointwise Feedforward++
Input TokensConditioningDiTBlock with In-Context ConditioningLayer Norm
Concatenate on Sequence DimensionFigure 3. The Diffusion Transformer (DiT) architecture. Left: We train conditional latent DiT models. The input latent is decomposed
into patches and processed by several DiT blocks. Right: Details of our DiT blocks. We experiment with variants of standard transformer
blocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens. Adaptive layer norm works best.
Denoising diffusion probabilistic models (DDPMs).
Diffusion [19, 54] and score-based generative models [22,
56] have been particularly successful as generative models
of images [35,46,48,50], in many cases outperforming gen-
erative adversarial networks (GANs) [12] which had previ-
ously been state-of-the-art. Improvements in DDPMs over
the past two years have largely been driven by improved
sampling techniques [19, 27, 55], most notably classiﬁer-
free guidance [21], reformulating diffusion models to pre-
dict noise instead of pixels [19] and using cascaded DDPM
pipelines where low-resolution base diffusion models are
trained in parallel with upsamplers [9, 20]. For all the dif-
fusion models listed above, convolutional U-Nets [49] are
the de-facto choice of backbone architecture. Concurrent
work [24] introduced a novel, efﬁcient architecture based
on attention for DDPMs; we explore pure transformers.
Architecture complexity. When evaluating architecture
complexity in the image generation literature, it is fairly
common practice to use parameter counts. In general, pa-
rameter counts can be poor proxies for the complexity of
image models since they do not account for, e.g., image res-
olution which signiﬁcantly impacts performance [44, 45].
Instead, much of the model complexity analysis in this pa-
per is through the lens of theoretical Gﬂops. This brings us
in-line with the architecture design literature where Gﬂops
are widely-used to gauge complexity. In practice, the
golden complexity metric is still up for debate as it fre-
quently depends on particular application scenarios. Nichol
and Dhariwal’s seminal work improving diffusion mod-
els [9, 36] is most related to us—there, they analyzed the
scalability and Gﬂop properties of the U-Net architecture
class. In this paper, we focus on the transformer class.3. Diffusion Transformers
3.1. Preliminaries
Diffusion formulation. Before introducing our architec-
ture, we brieﬂy review some basic concepts needed to
understand diffusion models (DDPMs) [19, 54]. Gaus-
sian diffusion models assume a forward noising process
which gradually applies noise to real data x0:q(xtjx0) =
N(xt;ptx0;(1 t)I), where constants tare hyperpa-
rameters. By applying the reparameterization trick, we can
samplext=ptx0+p1 tt, wheretN(0;I).
Diffusion models are trained to learn the reverse process
that inverts forward process corruptions: p(xt 1jxt) =
N((xt);(xt)), where neural networks are used to pre-
dict the statistics of p. The reverse process model is
trained with the variational lower bound [30] of the log-
likelihood of x0, which reduces to L() = p(x0jx1) +P
tDKL(q(xt 1jxt;x0)jjp(xt 1jxt)), excluding an ad-
ditional term irrelevant for training. Since both qandp
are Gaussian,DKLcan be evaluated with the mean and co-
variance of the two distributions. By reparameterizing as
a noise prediction network , the model can be trained us-
ing simple mean-squared error between the predicted noise
(xt)and the ground truth sampled Gaussian noise t:
Lsimple () =jj(xt) tjj2
2. But, in order to train diffu-
sion models with a learned reverse process covariance ,
the fullDKLterm needs to be optimized. We follow Nichol
and Dhariwal’s approach [36]: train withLsimple , and
trainwith the fullL. Oncepis trained, new images can
be sampled by initializing xtmax N (0;I)and sampling
xt 1p(xt 1jxt)via the reparameterization trick.
3

=== Page 4 ===
Classiﬁer-free guidance. Conditional diffusion models
take extra information as input, such as a class label c.
In this case, the reverse process becomes p(xt 1jxt;c),
whereandare conditioned on c. In this setting,
classiﬁer-free guidance can be used to encourage the sam-
pling procedure to ﬁnd xsuch that logp(cjx)is high [21].
By Bayes Rule, logp(cjx)/logp(xjc) logp(x), and
hencerxlogp(cjx)/rxlogp(xjc) rxlogp(x). By in-
terpreting the output of diffusion models as the score func-
tion, the DDPM sampling procedure can be guided to sam-
plexwith highp(xjc)by:^(xt;c) =(xt;;) +s
rxlogp(xjc)/(xt;;)+s((xt;c) (xt;;)), where
s>1indicates the scale of the guidance (note that s= 1re-
covers standard sampling). Evaluating the diffusion model
withc=;is done by randomly dropping out cduring
training and replacing it with a learned “null” embedding
;. Classiﬁer-free guidance is widely-known to yield sig-
niﬁcantly improved samples over generic sampling tech-
niques [21, 35, 46], and the trend holds for our DiT models.
Latent diffusion models. Training diffusion models di-
rectly in high-resolution pixel space can be computationally
prohibitive. Latent diffusion models (LDMs) [48] tackle this
issue with a two-stage approach: (1) learn an autoencoder
that compresses images into smaller spatial representations
with a learned encoder E; (2) train a diffusion model of
representations z=E(x)instead of a diffusion model of
imagesx(Eis frozen). New images can then be generated
by sampling a representation zfrom the diffusion model
and subsequently decoding it to an image with the learned
decoderx=D(z).
As shown in Figure 2, LDMs achieve good performance
while using a fraction of the Gﬂops of pixel space diffusion
models like ADM. Since we are concerned with compute
efﬁciency, this makes them an appealing starting point for
architecture exploration. In this paper, we apply DiTs to
latent space, although they could be applied to pixel space
without modiﬁcation as well. This makes our image genera-
tion pipeline a hybrid-based approach; we use off-the-shelf
convolutional V AEs and transformer-based DDPMs.
3.2. Diffusion Transformer Design Space
We introduce Diffusion Transformers (DiTs), a new ar-
chitecture for diffusion models. We aim to be as faithful to
the standard transformer architecture as possible to retain
its scaling properties. Since our focus is training DDPMs of
images (speciﬁcally, spatial representations of images), DiT
is based on the Vision Transformer (ViT) architecture which
operates on sequences of patches [10]. DiT retains many of
the best practices of ViTs. Figure 3 shows an overview of
the complete DiT architecture. In this section, we describe
the forward pass of DiT, as well as the components of the
design space of the DiT class.
𝑝𝐼𝑝𝐼𝑇=𝐼/𝑝!Noised LatentI×I×CInput Tokens T×dDiTBlockFigure 4. Input speciﬁcations for DiT. Given patch size pp,
a spatial representation (the noised latent from the V AE) of shape
IICis “patchiﬁed” into a sequence of length T= (I=p)2
with hidden dimension d. A smaller patch size presults in a longer
sequence length and thus more Gﬂops.
Patchify. The input to DiT is a spatial representation z
(for2562563images,zhas shape 32324). The
ﬁrst layer of DiT is “patchify,” which converts the spatial
input into a sequence of Ttokens, each of dimension d,
by linearly embedding each patch in the input. Following
patchify, we apply standard ViT frequency-based positional
embeddings (the sine-cosine version) to all input tokens.
The number of tokens Tcreated by patchify is determined
by the patch size hyperparameter p. As shown in Figure 4,
halvingpwill quadruple T, and thus at least quadruple total
transformer Gﬂops. Although it has a signiﬁcant impact on
Gﬂops, note that changing phas no meaningful impact on
downstream parameter counts.
We addp= 2;4;8to the DiT design space.
DiT block design. Following patchify, the input tokens
are processed by a sequence of transformer blocks. In ad-
dition to noised image inputs, diffusion models sometimes
process additional conditional information such as noise
timestepst, class labels c, natural language, etc. We explore
four variants of transformer blocks that process conditional
inputs differently. The designs introduce small, but impor-
tant, modiﬁcations to the standard ViT block design. The
designs of all blocks are shown in Figure 3.
–In-context conditioning. We simply append the vec-
tor embeddings of tandcas two additional tokens in
the input sequence, treating them no differently from
the image tokens. This is similar to cls tokens in
ViTs, and it allows us to use standard ViT blocks with-
out modiﬁcation. After the ﬁnal block, we remove the
conditioning tokens from the sequence. This approach
introduces negligible new Gﬂops to the model.
4

=== Page 5 ===
100K 200K 300K 400K
Training Steps20406080100FID-50K
XL/2 In-Context
XL/2 Cross-Attention
XL/2 adaLN
XL/2 adaLN-ZeroFigure 5. Comparing different conditioning strategies. adaLN-
Zero outperforms cross-attention and in-context conditioning at all
stages of training.
–Cross-attention block. We concatenate the embeddings
oftandcinto a length-two sequence, separate from
the image token sequence. The transformer block is
modiﬁed to include an additional multi-head cross-
attention layer following the multi-head self-attention
block, similar to the original design from Vaswani et
al.[60], and also similar to the one used by LDM for
conditioning on class labels. Cross-attention adds the
most Gﬂops to the model, roughly a 15% overhead.
–Adaptive layer norm (adaLN) block. Following
the widespread usage of adaptive normalization lay-
ers [40] in GANs [2, 28] and diffusion models with U-
Net backbones [9], we explore replacing standard layer
norm layers in transformer blocks with adaptive layer
norm (adaLN). Rather than directly learn dimension-
wise scale and shift parameters and, we regress
them from the sum of the embedding vectors of tand
c. Of the three block designs we explore, adaLN adds
the least Gﬂops and is thus the most compute-efﬁcient.
It is also the only conditioning mechanism that is re-
stricted to apply the same function to all tokens.
–adaLN-Zero block. Prior work on ResNets has found
that initializing each residual block as the identity
function is beneﬁcial. For example, Goyal et al. found
that zero-initializing the ﬁnal batch norm scale factor 
in each block accelerates large-scale training in the su-
pervised learning setting [13]. Diffusion U-Net mod-
els use a similar initialization strategy, zero-initializing
the ﬁnal convolutional layer in each block prior to any
residual connections. We explore a modiﬁcation of
the adaLN DiT block which does the same. In addi-
tion to regressing and, we also regress dimension-
wise scaling parameters that are applied immediately
prior to any residual connections within the DiT block.Model Layers N Hidden size dHeads Gﬂops (I=32,p=4)
DiT-S 12 384 6 1.4
DiT-B 12 768 12 5.6
DiT-L 24 1024 16 19.7
DiT-XL 28 1152 16 29.1
Table 1. Details of DiT models. We follow ViT [10] model con-
ﬁgurations for the Small (S), Base (B) and Large (L) variants; we
also introduce an XLarge (XL) conﬁg as our largest model.
We initialize the MLP to output the zero-vector for all
; this initializes the full DiT block as the identity
function. As with the vanilla adaLN block, adaLN-
Zero adds negligible Gﬂops to the model.
We include the in-context, cross-attention, adaptive layer
norm and adaLN-Zero blocks in the DiT design space.
Model size. We apply a sequence of NDiT blocks, each
operating at the hidden dimension size d. Following ViT,
we use standard transformer conﬁgs that jointly scale N,
dand attention heads [10, 63]. Speciﬁcally, we use four
conﬁgs: DiT-S, DiT-B, DiT-L and DiT-XL. They cover a
wide range of model sizes and ﬂop allocations, from 0.3
to 118.6 Gﬂops, allowing us to gauge scaling performance.
Table 1 gives details of the conﬁgs.
We add B, S, L and XL conﬁgs to the DiT design space.
Transformer decoder. After the ﬁnal DiT block, we need
to decode our sequence of image tokens into an output noise
prediction and an output diagonal covariance prediction.
Both of these outputs have shape equal to the original spa-
tial input. We use a standard linear decoder to do this; we
apply the ﬁnal layer norm (adaptive if using adaLN) and lin-
early decode each token into a pp2Ctensor, where Cis
the number of channels in the spatial input to DiT. Finally,
we rearrange the decoded tokens into their original spatial
layout to get the predicted noise and covariance.
The complete DiT design space we explore is patch size,
transformer block architecture and model size.
4. Experimental Setup
We explore the DiT design space and study the scaling
properties of our model class. Our models are named ac-
cording to their conﬁgs and latent patch sizes p; for exam-
ple, DiT-XL/2 refers to the XLarge conﬁg and p= 2.
Training. We train class-conditional latent DiT models at
256256 and512512 image resolution on the Ima-
geNet dataset [31], a highly-competitive generative mod-
eling benchmark. We initialize the ﬁnal linear layer with
zeros and otherwise use standard weight initialization tech-
niques from ViT. We train all models with AdamW [29,33].
5
```

</Expandable>
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2406.06525v1.pdf",
	"start_page": 1,
	"end_page": 5
}

```

```json output_result
PDF total pages: 26
Extracted page range: 1-5

=== Page 1 ===
Autoregressive Model Beats Diffusion: Llama for
Scalable Image Generation
Peize Sun1Yi Jiang2†Shoufa Chen1Shilong Zhang1Bingyue Peng2
Ping Luo1∗Zehuan Yuan2∗
1The University of Hong Kong2ByteDance
Codes and models: https://github.com/FoundationVision/LlamaGen
Figure 1: Image generation with vanilla autoregressive models . We show samples from our
class-conditional image (top row) and text-conditional image (bottom row) generation models.
Abstract
We introduce LlamaGen, a new family of image generation models that apply origi-
nal “next-token prediction” paradigm of large language models to visual generation
domain. It is an affirmative answer to whether vanilla autoregressive models, e.g.,
Llama, without inductive biases on visual signals can achieve state-of-the-art image
generation performance if scaling properly. We reexamine design spaces of image
tokenizers, scalability properties of image generation models, and their training
data quality. The outcome of this exploration consists of: (1) An image tokenizer
with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook
usage of 97% on ImageNet benchmark. (2) A series of class-conditional image
generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on
ImageNet 256 ×256 benchmarks, outperforming the popular diffusion models such
as LDM, DiT. (3) A text-conditional image generation model with 775M parame-
ters, from two-stage training on LAION-COCO and high aesthetics quality images,
demonstrating competitive performance of visual quality and text alignment. (4)
We verify the effectiveness of LLM serving frameworks in optimizing the inference
speed of image generation models and achieve 326% - 414% speedup. We release
all models and codes to facilitate open-source community of visual generation and
multimodal foundation models.
∗: Corresponding authors, †: project leadarXiv:2406.06525v1  [cs.CV]  10 Jun 2024

=== Page 2 ===
1 Introduction
Built upon autoregressive models, large language models (LLMs) [Vaswani et al. 2017; Devlin et al.
2018; Radford et al. 2018; Raffel et al. 2020; Radford et al. 2019; Brown et al. 2020; Zhang et al.
2022] generate the text by predicting the next token in a sequence. This “next-token prediction”
paradigm presents unprecedented capabilities in solving language tasks in a human-like conversational
manner [Ouyang et al. 2022; OpenAI 2022, 2023b; Google 2023; Anthropic 2023; Workshop et al.
2022; Touvron et al. 2023a,b; Bai et al. 2023a; Yang et al. 2023; Team 2023; Bi et al. 2024] and
incredible scalability [Kaplan et al. 2020; Henighan et al. 2020; Hoffmann et al. 2022; Wei et al. 2022;
Alabdulmohsin et al. 2022; Chowdhery et al. 2023; Anil et al. 2023], demonstrating a promising path
toward general-purpose artificial intelligence models.
Witnessed the scalability of autoregressive models on large language models, pioneering works
attempt to explore autoregressive models in image generation, for example, VQV AE [Van Den Oord
et al. 2017; Razavi et al. 2019], VQGAN [Esser et al. 2021; Lee et al. 2022], DALL-E [Ramesh et al.
2021], Parti [Yu et al. 2021, 2022]. They introduce image tokenizers to convert continuous images to
discrete tokens, and apply autoregressive models to generate image tokens in the way of next-token
prediction. They demonstrate strong performance among their contemporaries [Brock et al. 2018;
Ho et al. 2020; Dhariwal & Nichol 2021] in the year before 2022. However, their open-source
communities are not well developed, which largely limits their further improvements.
At the same period, another image generation method, diffusion models [Song & Ermon 2019; Ho
et al. 2020; Song et al. 2020; Dhariwal & Nichol 2021; Nichol et al. 2021; Lu et al. 2022a; Ho
et al. 2022a; Ho & Salimans 2022; Rombach et al. 2022; Ramesh et al. 2022; Saharia et al. 2022;
Rombach et al. 2022] develop rapidly. Along with their open-source communities, they dominate
the field of visual generation up to today. However, diffusion models share distinct paradigms with
autoregressive language models, which poses a huge challenge to building a unified model between
language and vision.
In this work, we are committed to pushing the envelope of autoregressive models on image generation
further: continuing its research methodology and contributing to open-source community. Reviewing
the literature on image generation in the year before 2024, we identify three keys to existing advanced
models [Peebles & Xie 2023; Podell et al. 2023; Xue et al. 2023; Chen et al. 2023b,c; Betker et al.
2023; Li et al. 2024; Esser et al. 2024]: 1) well-designed image compressors, 2) scalable image
generation models and 3) high-quality training data. Motivated by this, we reexamine the designs of
image tokenizers (image compressors for autoregressive models), the scalability properties of image
generation models, and the effects of training data.
Towards a potential unified model between language and vision, our design is reducing the inductive
biases on visual signals and adopting the same architecture as LLM. This belongs to a different re-
search philosophy with recent works [Chang et al. 2022; Yu et al. 2023b; Tian et al. 2024] that modify
the architectures under the guidance of vision-oriented designs. For example, MaskGIT [Chang et al.
2022], MAGVIT [Yu et al. 2023a,b] adopt the masked image modeling strategy, V AR [Tian et al.
2024] uses hierarchical multi-scale property. Although they have succeeded in achieving leading
image generation performance, and even better than diffusion models, it is still not clear whether
the original language model architectures are capable of this. Instead, our work reveals that vanilla
autoregressive models that apply the exactly same “next-token prediction” as language models are
also able to achieve state-of-the-art image generation performance. As a bonus, we can leverage the
techniques [Dao et al. 2022; Rasley et al. 2020; Shoeybi et al. 2019; Zhao et al. 2023; Kwon et al.
2023; Chen et al. 2023a; Dettmers 2022] developed in LLM community to optimize the training
recipes and inference speeds of our models.
In summary, our contributions to the community include:
1.Image tokenizer: An image tokenizer with downsample ratio of 16, achieves reconstruction
quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark. With the downsample
ratio of 8, our tokenizer is competitive or even better than continuous V AE [Rombach et al.
2022; Podell et al. 2023; OpenAI 2023a] used in diffusion models. This shows that discrete
representation in image tokenizers is no longer the bottleneck of the image reconstruction.
2.Scalable image generation model: A series of class-conditional image generation models,
ranging from 111M to 3.1B parameters, are developed based on Llama architecture [Touvron
2

=== Page 3 ===
et al. 2023a,b]. The largest model realizes 2.18 FID on ImageNet 256 ×256 benchmarks,
outperforming the popular diffusion models such as LDM [Rombach et al. 2022], DiT [Peebles
& Xie 2023]. This shows that vanilla autoregressive models without inductive biases on visual
signals can serve as the basis of image generation systems.
3.Hiqh-quality training data: A text-conditional image generation model with 775M parameters,
is firstly trained on a 50M subset of LAION-COCO [LAION 2022] and then fine-tuned on
10M internal high aesthetics quality images. It demonstrates competitive performance of visual
quality and text alignment.
4.Optimized inference speed: We adopt vLLM [Kwon et al. 2023], one of the most popular
LLM serving frameworks, to optimize the inference speed of our image generation models, and
remarkable 326% - 414% speedup is achieved.
We release all models and codes to facilitate the open-source community of visual generation and
multimodal foundation models. It is worth noticing that our released models are still behind state-of-
the-art visual generation models based on diffusion models [Alpha-VLLM 2024; Esser et al. 2024;
Brooks et al. 2024]. When more training data and computation resources are available in the future,
large-scale AR-based visual generation models, e.g., above 7B parameters, will be explored.
2 Autoregressive Models for Image Generation
2.1 Overview
Firstly, image pixels x∈RH×W×3are quantized into q∈Qh×wdiscrete tokens by the image
tokenizer [Van Den Oord et al. 2017; Esser et al. 2021; Yu et al. 2021], where h=H/p,w=W/p ,
pis downsample ratio of the image tokenizer, q(i,j)is indices of the image codebook. Then, these
image tokens are reshaped to a sequence of h·wtokens in raster scan ordering and used to train
Transformer [Vaswani et al. 2017]-based autoregressive models.
During image generation, image tokens (q1, q2, . . . , q h·w)are generated by autoregressive mod-
els [Radford et al. 2018, 2019; Brown et al. 2020; Touvron et al. 2023a] in the way of next-token
predictionQh·w
t=1p(qt|q<t, c), where cis class label embedding or text embedding. Finally, these
image tokens are converted to image pixels by the image tokenizer decoder.
2.2 Image Tokenizer
Quantized-Autoencoder architecture. We use the same architecture as VQGAN [Esser et al.
2021], encoder-quantizer-decoder. The encoder and the decoder are ConvNet with downsample ratio
p. The quantizer contains a codebook Z∈RK×CwithKlearnable vectors. The encoder projects
image pixels xto the feature map f. The quantization process maps each vector f(i,j)in the feature
map to the code index q(i,j)of its nearest vector z(i,j)in the codebook. During decoding, the code
index q(i,j)is remapped to the feature vector z(i,j)and the decoder converts these feature vectors
back to the image pixels ˆx.
The codebook has critical effects on image tokenization performance. Following [Yu et al. 2021], we
useℓ2-normalization to codebook vectors, low codebook vector dimension C, and large codebook
sizeK. These designs significantly improve reconstruction quality and codebook usage. More details
will be discussed in experiments.
Training losses. Since quantization is a non-differentiable operation, a straight-through gradient
estimator [Bengio et al. 2013] is used to preserve the gradient from the decoder to the encoder
z=sg[z−f] +f,sg[·]is stop-gradient operation. For codebook learning, LVQ=∥sg[f]−z∥2
2+
β∥f−sg[z]∥2
2, where the second term is commitment loss [Van Den Oord et al. 2017] to force feature
vectors extracted from the encoder to be close to codebook vectors, βis commitment loss weight.
For simplicity, we don’t add entropy loss [Yu et al. 2023a; Chang et al. 2022] in codebook learning.
For image reconstruction training, LAE=ℓ2(x,ˆx)+LP(x,ˆx)+λGLG(ˆx), where ℓ2is a reconstruction
loss on image pixels, LP(·)is a perceptual loss from LPIPS [Zhang et al. 2018], LG(·)is an adversarial
loss from a PatchGAN [Isola et al. 2017] discriminator trained at the same time with the image
tokenizer, and λGis adversarial loss weight.
3

=== Page 4 ===
Model Parameters Layers Hidden Size Heads
LlamaGen-B 111M 12 768 12
LlamaGen-L 343M 24 1024 16
LlamaGen-XL 775M 36 1280 20
LlamaGen-XXL 1.4B 48 1536 24
LlamaGen-3B 3.1B 24 3200 32
Table 1: Model sizes and architecture configurations of LlamaGen. The configurations are
following previous works [Radford et al. 2019; Touvron et al. 2023a; OpenLM-Research 2023].
2.3 Image Generation by Autoregressive Models
Llama architecture. Our model architecture is largely based on Llama [Touvron et al. 2023a,b],
applying pre-normalization using RMSNorm [Zhang & Sennrich 2019], SwiGLU activation func-
tion [Shazeer 2020], and rotary positional embeddings [Su et al. 2024]. Specifically, we use 2D RoPE
in at each layer of our model, following the implementation of [Lu et al. 2023; Fang et al. 2023]. We
do not use the technique of AdaLN [Peebles & Xie 2023] to keep our structure the same as LLM.
Class-conditional image generation. The class embedding is indexed from a set of learnable
embeddings [Peebles & Xie 2023; Esser et al. 2021] and is used as the prefilling token embedding.
Starting from this token embedding, the model generates the sequence of image tokens by next-token
prediction way, and stops at the location of the pre-defined maximum length.
Text-conditional image generation. To integrate the text condition into autoregressive models, we
use FLAN-T5 XL [Chung et al. 2024] as the text encoder, the encoded text feature is projected by an
additional MLP [Chen et al. 2023b,c] and is used as prefilling token embedding in autoregressive
models. We note that this design is not an ultimate design for multimodal foundation models, where
a unified vocabulary is established between language and vision [Lu et al. 2023; Team et al. 2023].
We leave it for future research.
Classifier-free guidance. Developed in the diffusion model community, classifier-free guidance [Ho
& Salimans 2022] is well-known for its improving visual quality and text-image alignment. We adopt
it in our models. During training, the conditional is randomly dropped and is replaced by a null
unconditional embedding [Peebles & Xie 2023; Chen et al. 2023b]. In inference, for each token, its
logitℓgis formed by ℓg=ℓu+s(ℓc−ℓu), where ℓcis conditional logit, ℓuis unconditional logit,
andsis scale of the classifier-free guidance.
It is worth noting that all design choices discussed so far are largely inspired by previous works, for
example, image tokenizer is borrowed from [Rombach et al. 2022; Yu et al. 2021], image generation
is from [Peebles & Xie 2023; Chen et al. 2023b; Esser et al. 2021]. A large portion of these techniques
are well studied in diffusion models but little in AR models. Our work adapts these advanced designs
collectively to AR-based visual generation models.
2.4 Scale Up
Our model architecture is almost the same as Llama, which allows us to seamlessly adopt optimization
techniques [Zhang & Sennrich 2019; Shazeer 2020; Su et al. 2024] and training recipes [Dao et al.
2022; Rasley et al. 2020; Shoeybi et al. 2019] in LLM community. As shown in Table 1, we scale the
model size up to 3.1B parameters in this work. All models are implemented with PyTorch 2 [Ansel
et al. 2024] and trained on 80GB A100 GPUs. For training the models with parameters below 1.4B,
we directly use DDP, otherwise, we adopt PyTorch FSDP [Zhao et al. 2023] to optimize GPU memory
usage.
2.5 Serving
Autoregressive models have always suffered from its low inference speed. With the rapid development
of large language models, advanced inference techniques [Kwon et al. 2023; Chen et al. 2023a;
Dettmers 2022] are proposed in the LLM community to optimize the inference speed.
4

=== Page 5 ===
Similar to training, inference techniques developed in the LLM community can also be adopted to
optimize our models. We verify the effectiveness of vLLM [Kwon et al. 2023], one of the most
popular LLM serving frameworks, on our image generation methods. As shown in Table 7, 326% -
414% speedup is achieved compared to the baseline setting.
3 Experiments
3.1 Image Tokenizer
Training setup. The training is on ImageNet [Deng et al. 2009] train set, using the resolution of
256×256 and random crop data augmentation. The image tokenizer model size is 72M and 70M
when the downsample ratio is 16 and 8, respectively. All models are trained with the same settings:
constant learning rate of 10−4, AdamW optimizer with β1= 0.9,β2= 0.95, weight decay = 0.05,
batch size of 128 and training epochs of 40. For the training losses, commitment loss weight is 0.25
and adversarial loss weight is 0.5. The adversarial loss is enabled after 20k training iterations.
Evaluation metrics. We use the popular ImageNet benchmark under the image resolution of 256
×256. The image reconstruction quality is measured by r-FID, reconstruction-FID on 256 ×256
ImageNet 50k validation set. The codebook usage is calculated as the percentage of used codes in the
queue of size 65536 over the whole codebook size. We also report PSNR and SSIM as the metrics of
reconstruction quality, following SDXL [Podell et al. 2023].
dim rFID↓PSNR↑SSIM↑usage↑
256 9.21 18.32 0.575 0.29%
32 3.22 19.98 0.646 20.9%
8 2.19 20.79 0.675 97.0%
4 9.88 19.39 0.593 82.0%
(a)Codebook vector dimension. Lower vector di-
mension (from 256 to 8) improves both reconstruction
quality and codebook usage significantly.size rFID↓PSNR↑SSIM↑usage↑
4096 3.02 19.99 0.643 100.0%
8192 2.91 20.41 0.654 75.0%
16384 2.19 20.79 0.675 97.0%
32768 2.26 20.59 0.663 85.0%
(b)Codebook size. Larger codebook size (from 4096
to 16384) benefits to the overall performance of image
tokenizers.
Table 2: Ablation studies on codebook designs in image tokenizers. . The evaluations are on
256×256 ImageNet 50k validation set. The default setting is codebook vector dimension is 8,
codebook size is 16384, downsample ratio is 16.
ratio img size tokens size rFID↓PSNR↑SSIM↑usage↑
256 256 (16 ×16) 2.19 20.79 0.675 97.0%
16 384 576 (24 ×24) 0.94 21.94 0.726 97.0%
512 1024 (32 ×32) 0.70 23.03 0.772 97.0%
256 1024 (32 ×32) 0.59 24.45 0.813 97.6%
8 384 2304 (48 ×48) 0.37 25.63 0.852 97.6%
512 4096 (64 ×64) 0.39 26.98 0.888 97.6%
Table 3: Number of tokens to represent the image. The number of tokens depends on downsample
ratio and input image size. The reconstructed image is always resized to 256 ×256 when evaluating
on ImageNet 50k validation set. The default setting is codebook vector dimension is 8, codebook
size is 16384.
Effect of image codebook designs. As shown in Table 2, when the codebook vector dimension is
reduced from 256 to 32 to 8, much better reconstruction quality and codebook usage are consistently
achieved. For codebook size, a larger size from 4096 to 16384 benefits the overall performance.
These observations are consistent with previous works [Yu et al. 2021, 2023b].
Effect of number of tokens to represent the image. Table 3 studies the effect of image token
number on image reconstruction quality. Using the same image tokenizer, for example, downsample
ratio as 16, representing an image with only 256 tokens (16 ×16) is not sufficient for good reconstruc-
tion quality, and increasing the number of tokens to 576 (24 ×24) could largely improve the image
quality from 2.43 to 0.99 rFID.
5
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Good! I can see the first paper is about DiT (Diffusion Transformers) and the second is about LlamaGen (Autoregressive models). Let me search for the experimental results on ImageNet 256×256 in both papers.
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools search_pdf_content`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	"pattern": "ImageNet 256.*FID|FID.*ImageNet 256|256.*256.*FID",
	"page_size": 20
}

```

```json output_result
Search ID: 14508e95
Pattern: ImageNet 256.*FID|FID.*ImageNet 256|256.*256.*FID
Total matches: 2
Page: 1/1
Results per page: 20

Match 1:
  Page: 9
  Text: "ImageNet 256256. DiT-XL/2 achieves state-of-the-art FID"
  Context: ...G (cfg=1.50) 2.27 4.60 278.24 0.83 0.57
Table 2. Benchmarking class-conditional image generation on
[ImageNet 256256. DiT-XL/2 achieves state-of-the-art FID].
Class-Conditional ImageNet 512 512
Model FID # sFID# IS" Precision"Recall"
BigGAN-deep [2] 8.43 8...

Match 2:
  Page: 13
  Text: "256256and512512DiT-XL/2 models, we never observed FID"
  Context: ...s exclude the V AE model which contains 84M parameters across the
encoder and decoder. For both the [256256and512512DiT-XL/2 models, we never observed FID] saturate and continued training
them as long as possible. Numbers reported in this table use the ft...


```

</Expandable>
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools search_pdf_content`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2406.06525v1.pdf",
	"pattern": "ImageNet 256.*FID|FID.*ImageNet 256|256.*256.*FID",
	"page_size": 20
}

```

```json output_result
Search ID: 61cb7014
Pattern: ImageNet 256.*FID|FID.*ImageNet 256|256.*256.*FID
Total matches: 1
Page: 1/1
Results per page: 20

Match 1:
  Page: 3
  Text: "FID on ImageNet 256"
  Context: ...et al. 2023a,b]. The largest model realizes 2.18 [FID on ImageNet 256] ×256 benchmarks,
outperforming the popular diffusion models such as LDM [Rombach et al. 2022], DiT ...


```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Great! Now let me read the pages with the detailed experimental results tables. Let me check page 9 of the DiT paper and look for more details in both papers.
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	"start_page": 8,
	"end_page": 10
}

```

```json output_result
PDF total pages: 25
Extracted page range: 8-10

=== Page 8 ===
100101102
Transformer Gflops20406080100120140160FID-50K
Correlation: -0.93S/8
S/4
S/2B/8
B/4
B/2L/8
L/4
L/2XL/8
XL/4
XL/2Figure 8. Transformer Gﬂops are strongly correlated with FID.
We plot the Gﬂops of each of our DiT models and each model’s
FID-50K after 400K training steps.
Scaling model size and patch size. We train 12 DiT mod-
els, sweeping over model conﬁgs (S, B, L, XL) and patch
sizes (8, 4, 2). Note that DiT-L and DiT-XL are signiﬁcantly
closer to each other in terms of relative Gﬂops than other
conﬁgs. Figure 2 (left) gives an overview of the Gﬂops of
each model and their FID at 400K training iterations. In
all cases, we ﬁnd that increasing model size and decreasing
patch size yields considerably improved diffusion models.
Figure 6 (top) demonstrates how FID changes as model
size is increased and patch size is held constant. Across all
four conﬁgs, signiﬁcant improvements in FID are obtained
over all stages of training by making the transformer deeper
and wider. Similarly, Figure 6 (bottom) shows FID as patch
size is decreased and model size is held constant. We again
observe considerable FID improvements throughout train-
ing by simply scaling the number of tokens processed by
DiT, holding parameters approximately ﬁxed.
DiT Gﬂops are critical to improving performance. The
results of Figure 6 suggest that parameter counts do not
uniquely determine the quality of a DiT model. As model
size is held constant and patch size is decreased, the trans-
former’s total parameters are effectively unchanged (actu-
ally, total parameters slightly decrease ), and only Gﬂops are
increased. These results indicate that scaling model Gﬂops
is actually the key to improved performance. To investi-
gate this further, we plot the FID-50K at 400K training steps
against model Gﬂops in Figure 8. The results demonstrate
that different DiT conﬁgs obtain similar FID values when
their total Gﬂops are similar (e.g., DiT-S/2 and DiT-B/4).
We ﬁnd a strong negative correlation between model Gﬂops
and FID-50K, suggesting that additional model compute is
the critical ingredient for improved DiT models. In Fig-
ure 12 (appendix), we ﬁnd that this trend holds for other
metrics such as Inception Score.
107108109101010111012
Training Compute (Gflops)0255075100125150175200FID-50K
1015202530
S/8
S/4
S/2B/8
B/4
B/2L/8
L/4
L/2XL/8
XL/4
XL/2Figure 9. Larger DiT models use large compute more efﬁ-
ciently. We plot FID as a function of total training compute.
Larger DiT models are more compute-efﬁcient. In
Figure 9, we plot FID as a function of total training compute
for all DiT models. We estimate training compute as model
Gﬂopsbatch sizetraining steps3, where the factor of
3 roughly approximates the backwards pass as being twice
as compute-heavy as the forward pass. We ﬁnd that small
DiT models, even when trained longer, eventually become
compute-inefﬁcient relative to larger DiT models trained for
fewer steps. Similarly, we ﬁnd that models that are identi-
cal except for patch size have different performance proﬁles
even when controlling for training Gﬂops. For example,
XL/4 is outperformed by XL/2 after roughly 1010Gﬂops.
Visualizing scaling. We visualize the effect of scaling on
sample quality in Figure 7. At 400K training steps, we sam-
ple an image from each of our 12 DiT models using iden-
tical starting noise xtmax, sampling noise and class labels.
This lets us visually interpret how scaling affects DiT sam-
ple quality. Indeed, scaling both model size and the number
of tokens yields notable improvements in visual quality.
5.1. State-of-the-Art Diffusion Models
256256 ImageNet. Following our scaling analysis, we
continue training our highest Gﬂop model, DiT-XL/2, for
7M steps. We show samples from the model in Figures 1,
and we compare against state-of-the-art class-conditional
generative models. We report results in Table 2. When us-
ing classiﬁer-free guidance, DiT-XL/2 outperforms all prior
diffusion models, decreasing the previous best FID-50K of
3.60 achieved by LDM to 2.27. Figure 2 (right) shows that
DiT-XL/2 (118.6 Gﬂops) is compute-efﬁcient relative to la-
tent space U-Net models like LDM-4 (103.6 Gﬂops) and
substantially more efﬁcient than pixel space U-Net mod-
els such as ADM (1120 Gﬂops) or ADM-U (742 Gﬂops).
8

=== Page 9 ===
Class-Conditional ImageNet 256 256
Model FID # sFID# IS" Precision"Recall"
BigGAN-deep [2] 6.95 7.36 171.4 0.87 0.28
StyleGAN-XL [53] 2.30 4.02 265.12 0.78 0.53
ADM [9] 10.94 6.02 100.98 0.69 0.63
ADM-U 7.49 5.13 127.49 0.72 0.63
ADM-G 4.59 5.25 186.70 0.82 0.52
ADM-G, ADM-U 3.94 6.14 215.84 0.83 0.53
CDM [20] 4.88 - 158.71 - -
LDM-8 [48] 15.51 - 79.03 0.65 0.63
LDM-8-G 7.76 - 209.52 0.84 0.35
LDM-4 10.56 - 103.49 0.71 0.62
LDM-4-G (cfg=1.25) 3.95 - 178.22 0.81 0.55
LDM-4-G (cfg=1.50) 3.60 - 247.67 0.87 0.48
DiT-XL/2 9.62 6.85 121.50 0.67 0.67
DiT-XL/2-G (cfg=1.25) 3.22 5.28 201.77 0.76 0.62
DiT-XL/2-G (cfg=1.50) 2.27 4.60 278.24 0.83 0.57
Table 2. Benchmarking class-conditional image generation on
ImageNet 256256. DiT-XL/2 achieves state-of-the-art FID.
Class-Conditional ImageNet 512 512
Model FID # sFID# IS" Precision"Recall"
BigGAN-deep [2] 8.43 8.13 177.90 0.88 0.29
StyleGAN-XL [53] 2.41 4.06 267.75 0.77 0.52
ADM [9] 23.24 10.19 58.06 0.73 0.60
ADM-U 9.96 5.62 121.78 0.75 0.64
ADM-G 7.72 6.57 172.71 0.87 0.42
ADM-G, ADM-U 3.85 5.86 221.72 0.84 0.53
DiT-XL/2 12.03 7.12 105.25 0.75 0.64
DiT-XL/2-G (cfg=1.25) 4.64 5.77 174.77 0.81 0.57
DiT-XL/2-G (cfg=1.50) 3.04 5.02 240.82 0.84 0.54
Table 3. Benchmarking class-conditional image generation on
ImageNet 512512. Note that prior work [9] measures Precision
and Recall using 1000 real samples for 512512resolution; for
consistency, we do the same.
Our method achieves the lowest FID of all prior generative
models, including the previous state-of-the-art StyleGAN-
XL [53]. Finally, we also observe that DiT-XL/2 achieves
higher recall values at all tested classiﬁer-free guidance
scales compared to LDM-4 and LDM-8. When trained for
only 2.35M steps (similar to ADM), XL/2 still outperforms
all prior diffusion models with an FID of 2.55.
512512 ImageNet. We train a new DiT-XL/2 model on
ImageNet at 512512resolution for 3M iterations with
identical hyperparameters as the 256256model. With a
patch size of 2, this XL/2 model processes a total of 1024
tokens after patchifying the 64644input latent (524.6
Gﬂops). Table 3 shows comparisons against state-of-the-art
methods. XL/2 again outperforms all prior diffusion models
at this resolution, improving the previous best FID of 3.85
achieved by ADM to 3.04. Even with the increased num-
ber of tokens, XL/2 remains compute-efﬁcient. For exam-
ple, ADM uses 1983 Gﬂops and ADM-U uses 2813 Gﬂops;
XL/2 uses 524.6 Gﬂops. We show samples from the high-
resolution XL/2 model in Figure 1 and the appendix.
101
102
103
104
105
Sampling Compute (Gflops)20406080100120140160180FID-10K
S/8
S/4
S/2B/8
B/4
B/2L/8
L/4
L/2XL/8
XL/4
XL/2Figure 10. Scaling-up sampling compute does not compensate
for a lack of model compute. For each of our DiT models trained
for 400K iterations, we compute FID-10K using [16, 32, 64, 128,
256, 1000] sampling steps. For each number of steps, we plot the
FID as well as the Gﬂops used to sample each image. Small mod-
els cannot close the performance gap with our large models, even
if they sample with more test-time Gﬂops than the large models.
5.2. Scaling Model vs. Sampling Compute
Diffusion models are unique in that they can use addi-
tional compute after training by increasing the number of
sampling steps when generating an image. Given the im-
pact of model Gﬂops on sample quality, in this section we
study if smaller- model compute DiTs can outperform larger
ones by using more sampling compute . We compute FID
for all 12 of our DiT models after 400K training steps, us-
ing [16, 32, 64, 128, 256, 1000] sampling steps per-image.
The main results are in Figure 10. Consider DiT-L/2 us-
ing 1000 sampling steps versus DiT-XL/2 using 128 steps.
In this case, L/2 uses 80:7Tﬂops to sample each image;
XL/2 uses 5less compute— 15:2Tﬂops—to sample each
image. Nonetheless, XL/2 has the better FID-10K (23.7
vs 25.9). In general, scaling-up sampling compute cannot
compensate for a lack of model compute.
6. Conclusion
We introduce Diffusion Transformers (DiTs), a simple
transformer-based backbone for diffusion models that out-
performs prior U-Net models and inherits the excellent scal-
ing properties of the transformer model class. Given the
promising scaling results in this paper, future work should
continue to scale DiTs to larger models and token counts.
DiT could also be explored as a drop-in backbone for text-
to-image models like DALL E 2 and Stable Diffusion.
Acknowledgements. We thank Kaiming He, Ronghang
Hu, Alexander Berg, Shoubhik Debnath, Tim Brooks, Ilija
Radosavovic and Tete Xiao for helpful discussions. William
Peebles is supported by the NSF GRFP.
9

=== Page 10 ===
References
[1] James Bradbury, Roy Frostig, Peter Hawkins,
Matthew James Johnson, Chris Leary, Dougal Maclau-
rin, George Necula, Adam Paszke, Jake VanderPlas, Skye
Wanderman-Milne, and Qiao Zhang. JAX: composable
transformations of Python+NumPy programs, 2018. 6
[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high ﬁdelity natural image synthesis.
InICLR , 2019. 5, 9
[3] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. In NeurIPS , 2020. 1
[4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T
Freeman. Maskgit: Masked generative image transformer. In
CVPR , pages 11315–11325, 2022. 2
[5] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,
Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srini-
vas, and Igor Mordatch. Decision transformer: Reinforce-
ment learning via sequence modeling. In NeurIPS , 2021. 2
[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever. Generative pre-
training from pixels. In ICML , 2020. 1, 2
[7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
Generating long sequences with sparse transformers. arXiv
preprint arXiv:1904.10509 , 2019. 2
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional trans-
formers for language understanding. In NAACL-HCT , 2019.
1
[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. In NeurIPS , 2021. 1, 2, 3, 5,
6, 9, 12
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In ICLR , 2020. 1, 2,
4, 5
[11] Patrick Esser, Robin Rombach, and Bj ¨orn Ommer. Taming
transformers for high-resolution image synthesis, 2020. 2
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS , 2014.
3
[13] Priya Goyal, Piotr Doll ´ar, Ross Girshick, Pieter Noord-
huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,
Yangqing Jia, and Kaiming He. Accurate, large minibatch
sgd: Training imagenet in 1 hour. arXiv:1706.02677 , 2017.
5
[14] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo
Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-
tor quantized diffusion model for text-to-image synthesis. In
CVPR , pages 10696–10706, 2022. 2
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 2[16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear
units (gelus). arXiv preprint arXiv:1606.08415 , 2016. 12
[17] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen,
Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B
Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws
for autoregressive generative modeling. arXiv preprint
arXiv:2010.14701 , 2020. 2
[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. 2017. 6
[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 2, 3
[20] Jonathan Ho, Chitwan Saharia, William Chan, David J
Fleet, Mohammad Norouzi, and Tim Salimans. Cas-
caded diffusion models for high ﬁdelity image generation.
arXiv:2106.15282 , 2021. 3, 9
[21] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications , 2021. 3, 4
[22] Aapo Hyv ¨arinen and Peter Dayan. Estimation of non-
normalized statistical models by score matching. Journal
of Machine Learning Research , 6(4), 2005. 3
[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1125–1134,
2017. 2
[24] Allan Jabri, David Fleet, and Ting Chen. Scalable adap-
tive computation for iterative generation. arXiv preprint
arXiv:2212.11972 , 2022. 3
[25] Michael Janner, Qiyang Li, and Sergey Levine. Ofﬂine rein-
forcement learning as one big sequence modeling problem.
InNeurIPS , 2021. 2
[26] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for
neural language models. arXiv:2001.08361 , 2020. 2, 13
[27] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models. In Proc. NeurIPS , 2022. 3
[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
CVPR , 2019. 5
[29] Diederik Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR , 2015. 5
[30] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 3, 6
[31] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NeurIPS , 2012. 5
[32] Tuomas Kynk ¨a¨anniemi, Tero Karras, Samuli Laine, Jaakko
Lehtinen, and Timo Aila. Improved precision and recall met-
ric for assessing generative models. In NeurIPS , 2019. 6
[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv:1711.05101 , 2017. 5
10
```

</Expandable>
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2406.06525v1.pdf",
	"start_page": 6,
	"end_page": 8
}

```

```json output_result
PDF total pages: 26
Extracted page range: 6-8

=== Page 6 ===
ratio method dim sizeImageNet COCO
rFID↓PSNR↑SSIM↑rFID↓PSNR↑SSIM↑
16VQGAN 256 1024 8.30 19.51 0.614 16.95 19.08 0.613
VQGAN 256 16384 4.99 20.00 0.629 12.29 19.57 0.630
MaskGIT 256 1024 2.28 - - - - -
Ours 8 16384 2.19 20.79 0.675 8.11 20.42 0.678
8VQGANoim.4 256 1.44 22.63 0.737 6.58 22.289 0.744
VQGANoim.4 16384 1.19 23.38 0.762 5.89 23.08 0.771
ViT-VQGAN 32 8192 1.28 - - - - -
Ours 8 16384 0.59 24.45 0.813 4.19 24.20 0.822
8SD-V AEukn.4 - 0.74 25.68 0.820 4.45 25.41 0.831
SDXL-V AEukn.4 - 0.68 26.04 0.834 4.07 25.76 0.845
OAI-Decoderukn.4 - 0.81 24.43 0.786 4.59 24.19 0.800
Table 4: Comparisons with other image tokenizers. The evaluations are on 256 ×256 ImageNet
50k validation set and COCO 5k val2017 set. All models are trained on ImageNet except “oim.” is
on OpenImage, “ukn.” is unknown training data.
Comparisons with other image tokenizers. We compare with other image tokenizers, including
VQGAN [Esser et al. 2021], MaskGIT [Chang et al. 2022], ViT-VQGAN [Yu et al. 2021]. As shown
in Table 4, our tokenizer outperforms previous image tokenizers. We also evaluate our tokenizer on
COCO val2017 [Lin et al. 2014] of 256 ×256 image resolution to verify the image reconstruction
quality, since COCO images contain more complex scenes. The comparison results are consistent
with those in ImageNet validation set. This shows our tokenizer is a generalizable image tokenizer
for both object-centric and scene-centric images.
Importantly, our tokenizer is competitive to continuous latent space representation, such as SD
V AE [Rombach et al. 2022], SDXL V AE [Podell et al. 2023], and Consistency Decoder from
OpenAI [OpenAI 2023a], which are widely used in diffusion models. This shows that discrete
representation in the image tokenizer is no longer the bottleneck of the image reconstruction.
3.2 Class-conditional Image Generation
Training setup. Our benchmark is the popular 256 ×256 ImageNet. All models are trained with
the similar settings: base learning rate of 10−4per 256 batch size, AdamW optimizer with β1= 0.9,
β2= 0.95,weight decay = 0.05, gradient clipping of 1.0. The dropout is always 0.1 for input
token embedding, attention module and FFN module. The class condition embedding dropout for
classifier-free guidance is 0.1.
Precomputing image codes. To accelerate the model training, we use the image tokenizer to precom-
pute image codes before training. To achieve the similar effect of random crop data augmentation,
we extract image codes of ten crops of the original image. During training, we randomly select one
copy code from the ten augmentations.
Evaluation metrics. We use Fréchet inception distance (FID) [Heusel et al. 2017] as the main
metric. We also report Inception Score (IS) [Salimans et al. 2016], sFID [Nash et al. 2021] and
Precision/Recall [Kynkäänniemi et al. 2019] as secondary metrics. All evaluations are implemented
using ADM’s TensorFlow scripts [Dhariwal & Nichol 2021] for fair comparisons.
Effect of image tokens. Although increasing the image tokens brings better image reconstruction
quality, it is not strongly correlated to image generation quality. As shown in Table 5, when the model
parameter is smaller than 1B, 256 (16 ×16) tokens bring better image generation performance than
576 (24 ×24). This shows the synergistic effect of scaling up model parameters and token numbers.
Nevertheless, fewer image tokens would limit the image generation performance, for example, 256
(16×16) tokens limit the FID at 3.06 FID, while 576 (24 ×24) could further improve the FID to a
lower value.
6

=== Page 7 ===
image token model FID↓ IS↑ Precision ↑Recall↑
image size: 256 ×256
tokens: 256 (16 ×16)
rFID: 2.19B 8.69 124.43 0.78 0.46
L 4.21 200.00 0.82 0.50
XL 3.39 227.08 0.81 0.54
XXL 3.09 253.60 0.82 0.52
3B 3.06 279.71 0.84 0.53
image size: 384 ×384
tokens: 576 (24 ×24)
rFID: 0.94B 12.89 92.44 0.73 0.48
L 5.01 167.31 0.78 0.52
XL 3.42 202.93 0.79 0.56
XXL 2.89 236.21 0.80 0.56
3B 2.61 251.90 0.80 0.56
Table 5: The effect of image tokens on image generation. The generated image is always resized to
256×256 when evaluating on ImageNet benchmark. We compare all models after training 50 epochs.
The inference setting is cfg = 1.75, top-k = 0 (all), top-p = 1.0, temperature = 1.0 for all experiments.
(a) without classifier-free guidance
 (b) with classifier-free guidance
Figure 2: Scaling model size. We show FID of 256 ×256 ImageNet benchmark over training epochs.
Scaling model size brings consistent improvement on FID during the whole training process. More
detailed evaluation metrics are in Appendix.
(a) classifier-free guidance
 (b) top-k sampling
Figure 3: The effect of sampling configuration. We show FID and Inception Score of 256 ×256
ImageNet benchmark over different sampling configurations. The model is LlamaGen-L, and the
default setting is cfg = 2.0, top-k = 0 (all), top-p = 1.0, temperature = 1.0.
Effect of model size. We train our models across five model sizes (B, L, XL, XXL, 3B) and evaluate
their performance with and without classifier-free guidance. Figure 2 illustrates how FID changes as
both the model sizes and the training epochs increase. Notable improvements in FID are observed
when scaling the model from LlamaGen-B to LlamaGen-XXL. Further scaling to 3B yields only
marginal improvements. A plausible explanation for this phenomenon could be the limitation in
dataset size: ImageNet [Deng et al. 2009] comprises approximately only 1 million images, expanding
the dataset or using stronger data augmentation could potentially lead to further improvements.
7

=== Page 8 ===
Type Model #Para. FID↓ IS↑ Precision ↑Recall↑
GANBigGAN [Brock et al. 2018] 112M 6.95 224.5 0.89 0.38
GigaGAN [Kang et al. 2023] 569M 3.45 225.5 0.84 0.61
StyleGan-XL [Sauer et al. 2022] 166M 2.30 265.1 0.78 0.53
DiffusionADM [Dhariwal & Nichol 2021] 554M 10.94 101.0 0.69 0.63
CDM [Ho et al. 2022b] − 4.88 158.7 − −
LDM-4 [Rombach et al. 2022] 400M 3.60 247.7 − −
DiT-XL/2 [Peebles & Xie 2023] 675M 2.27 278.2 0.83 0.57
Mask.MaskGIT [Chang et al. 2022] 227M 6.18 182.1 0.80 0.51
MaskGIT-re [Chang et al. 2022] 227M 4.02 355.6 − −
ARVQGAN [Esser et al. 2021] 227M 18.65 80.4 0.78 0.26
VQGAN [Esser et al. 2021] 1.4B 15.78 74.3 − −
VQGAN-re [Esser et al. 2021] 1.4B 5.20 280.3 − −
ViT-VQGAN [Yu et al. 2021] 1.7B 4.17 175.1 − −
ViT-VQGAN-re [Yu et al. 2021] 1.7B 3.04 227.4 − −
RQTran. [Lee et al. 2022] 3.8B 7.55 134.0 − −
RQTran.-re [Lee et al. 2022] 3.8B 3.80 323.7 − −
ARLlamaGen-B (cfg=2.00) 111M 5.46 193.61 0.83 0.45
LlamaGen-L (cfg=2.00) 343M 3.07 256.06 0.83 0.52
LlamaGen-XL (cfg=1.75) 775M 2.62 244.08 0.80 0.57
LlamaGen-XXL (cfg=1.75) 1.4B 2.34 253.90 0.80 0.59
LlamaGen-3B (cfg=1.65) 3.1B 2.18 263.33 0.81 0.58
LlamaGen-3B (cfg=1.75) 3.1B 2.32 280.10 0.82 0.56
LlamaGen-3B (cfg=2.00) 3.1B 2.81 311.59 0.84 0.54
Table 6: Model comparisons on class-conditional ImageNet 256 ×256 benchmark . Metrics include
Fréchet inception distance (FID), inception score (IS), precision and recall. “ ↓” or “↑” indicate lower
or higher values are better. “-re” means using rejection sampling. “cfg” means using classifier-free
guidance. More detailed results are in Appendix.
Effect of classifier-free guidance (CFG). First, as shown in Figure 2, using classifier-free guidance
can significantly enhance the visual quality across all model sizes. Moreover, Figure 3a illustrates
that the model achieves optimal FID at CFG = 2.0 and further increasing CFG would deteriorate FID,
which is consistent with previous findings [Dhariwal & Nichol 2021]. Additionally, the increment in
CFG results in a trade-off between diversity and fidelity, as evidenced by increased precision and
decreased recall, demonstrated in Table 10.
Effect of top-k sampling. As shown in Figure 3b, a small top-k value is not beneficial for FID
and IS. Increasing top-k continuously improves FID but decreases IS, which trades off fidelity for
diversity. We observe a similar trend when changing the parameter of top-p and temperature in
sampling. Since FID is our main metric, we use maximum value as the default top-k value, which is
the whole codebook size.
Comparisons with other image generation methods. In Table 6, we compare with popular image
generation models, including GAN [Brock et al. 2018; Kang et al. 2023; Sauer et al. 2022], Diffusion
models [Dhariwal & Nichol 2021; Ho et al. 2022b; Rombach et al. 2022; Peebles & Xie 2023],
and masked-prediction models [Chang et al. 2022]. Our models exhibit competitive performance
in all metrics of FID, IS, Precision and Recall. Notably, our 3B model outperforms the popular
diffusion models LDM [Rombach et al. 2022], DiT [Peebles & Xie 2023]. This shows that vanilla
autoregressive models can serve as the basis of advanced image generation systems.
When comparing with autoregressive models [Esser et al. 2021; Yu et al. 2021; Lee et al. 2022], our
model outperforms all previous models at different levels of model parameters. This benefits from
better designs of image tokenizers and better scalability of image generation models. We hope our
simple and effective implementation will serve as a solid baseline and help facilitate future research
in autoregressive models for image generations.
8
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Perfect! Now I have the detailed information from both papers. Let me search for more information about model parameters and check if there are any other relevant details I might have missed.
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	"start_page": 4,
	"end_page": 6
}

```

```json output_result
PDF total pages: 25
Extracted page range: 4-6

=== Page 4 ===
Classiﬁer-free guidance. Conditional diffusion models
take extra information as input, such as a class label c.
In this case, the reverse process becomes p(xt 1jxt;c),
whereandare conditioned on c. In this setting,
classiﬁer-free guidance can be used to encourage the sam-
pling procedure to ﬁnd xsuch that logp(cjx)is high [21].
By Bayes Rule, logp(cjx)/logp(xjc) logp(x), and
hencerxlogp(cjx)/rxlogp(xjc) rxlogp(x). By in-
terpreting the output of diffusion models as the score func-
tion, the DDPM sampling procedure can be guided to sam-
plexwith highp(xjc)by:^(xt;c) =(xt;;) +s
rxlogp(xjc)/(xt;;)+s((xt;c) (xt;;)), where
s>1indicates the scale of the guidance (note that s= 1re-
covers standard sampling). Evaluating the diffusion model
withc=;is done by randomly dropping out cduring
training and replacing it with a learned “null” embedding
;. Classiﬁer-free guidance is widely-known to yield sig-
niﬁcantly improved samples over generic sampling tech-
niques [21, 35, 46], and the trend holds for our DiT models.
Latent diffusion models. Training diffusion models di-
rectly in high-resolution pixel space can be computationally
prohibitive. Latent diffusion models (LDMs) [48] tackle this
issue with a two-stage approach: (1) learn an autoencoder
that compresses images into smaller spatial representations
with a learned encoder E; (2) train a diffusion model of
representations z=E(x)instead of a diffusion model of
imagesx(Eis frozen). New images can then be generated
by sampling a representation zfrom the diffusion model
and subsequently decoding it to an image with the learned
decoderx=D(z).
As shown in Figure 2, LDMs achieve good performance
while using a fraction of the Gﬂops of pixel space diffusion
models like ADM. Since we are concerned with compute
efﬁciency, this makes them an appealing starting point for
architecture exploration. In this paper, we apply DiTs to
latent space, although they could be applied to pixel space
without modiﬁcation as well. This makes our image genera-
tion pipeline a hybrid-based approach; we use off-the-shelf
convolutional V AEs and transformer-based DDPMs.
3.2. Diffusion Transformer Design Space
We introduce Diffusion Transformers (DiTs), a new ar-
chitecture for diffusion models. We aim to be as faithful to
the standard transformer architecture as possible to retain
its scaling properties. Since our focus is training DDPMs of
images (speciﬁcally, spatial representations of images), DiT
is based on the Vision Transformer (ViT) architecture which
operates on sequences of patches [10]. DiT retains many of
the best practices of ViTs. Figure 3 shows an overview of
the complete DiT architecture. In this section, we describe
the forward pass of DiT, as well as the components of the
design space of the DiT class.
𝑝𝐼𝑝𝐼𝑇=𝐼/𝑝!Noised LatentI×I×CInput Tokens T×dDiTBlockFigure 4. Input speciﬁcations for DiT. Given patch size pp,
a spatial representation (the noised latent from the V AE) of shape
IICis “patchiﬁed” into a sequence of length T= (I=p)2
with hidden dimension d. A smaller patch size presults in a longer
sequence length and thus more Gﬂops.
Patchify. The input to DiT is a spatial representation z
(for2562563images,zhas shape 32324). The
ﬁrst layer of DiT is “patchify,” which converts the spatial
input into a sequence of Ttokens, each of dimension d,
by linearly embedding each patch in the input. Following
patchify, we apply standard ViT frequency-based positional
embeddings (the sine-cosine version) to all input tokens.
The number of tokens Tcreated by patchify is determined
by the patch size hyperparameter p. As shown in Figure 4,
halvingpwill quadruple T, and thus at least quadruple total
transformer Gﬂops. Although it has a signiﬁcant impact on
Gﬂops, note that changing phas no meaningful impact on
downstream parameter counts.
We addp= 2;4;8to the DiT design space.
DiT block design. Following patchify, the input tokens
are processed by a sequence of transformer blocks. In ad-
dition to noised image inputs, diffusion models sometimes
process additional conditional information such as noise
timestepst, class labels c, natural language, etc. We explore
four variants of transformer blocks that process conditional
inputs differently. The designs introduce small, but impor-
tant, modiﬁcations to the standard ViT block design. The
designs of all blocks are shown in Figure 3.
–In-context conditioning. We simply append the vec-
tor embeddings of tandcas two additional tokens in
the input sequence, treating them no differently from
the image tokens. This is similar to cls tokens in
ViTs, and it allows us to use standard ViT blocks with-
out modiﬁcation. After the ﬁnal block, we remove the
conditioning tokens from the sequence. This approach
introduces negligible new Gﬂops to the model.
4

=== Page 5 ===
100K 200K 300K 400K
Training Steps20406080100FID-50K
XL/2 In-Context
XL/2 Cross-Attention
XL/2 adaLN
XL/2 adaLN-ZeroFigure 5. Comparing different conditioning strategies. adaLN-
Zero outperforms cross-attention and in-context conditioning at all
stages of training.
–Cross-attention block. We concatenate the embeddings
oftandcinto a length-two sequence, separate from
the image token sequence. The transformer block is
modiﬁed to include an additional multi-head cross-
attention layer following the multi-head self-attention
block, similar to the original design from Vaswani et
al.[60], and also similar to the one used by LDM for
conditioning on class labels. Cross-attention adds the
most Gﬂops to the model, roughly a 15% overhead.
–Adaptive layer norm (adaLN) block. Following
the widespread usage of adaptive normalization lay-
ers [40] in GANs [2, 28] and diffusion models with U-
Net backbones [9], we explore replacing standard layer
norm layers in transformer blocks with adaptive layer
norm (adaLN). Rather than directly learn dimension-
wise scale and shift parameters and, we regress
them from the sum of the embedding vectors of tand
c. Of the three block designs we explore, adaLN adds
the least Gﬂops and is thus the most compute-efﬁcient.
It is also the only conditioning mechanism that is re-
stricted to apply the same function to all tokens.
–adaLN-Zero block. Prior work on ResNets has found
that initializing each residual block as the identity
function is beneﬁcial. For example, Goyal et al. found
that zero-initializing the ﬁnal batch norm scale factor 
in each block accelerates large-scale training in the su-
pervised learning setting [13]. Diffusion U-Net mod-
els use a similar initialization strategy, zero-initializing
the ﬁnal convolutional layer in each block prior to any
residual connections. We explore a modiﬁcation of
the adaLN DiT block which does the same. In addi-
tion to regressing and, we also regress dimension-
wise scaling parameters that are applied immediately
prior to any residual connections within the DiT block.Model Layers N Hidden size dHeads Gﬂops (I=32,p=4)
DiT-S 12 384 6 1.4
DiT-B 12 768 12 5.6
DiT-L 24 1024 16 19.7
DiT-XL 28 1152 16 29.1
Table 1. Details of DiT models. We follow ViT [10] model con-
ﬁgurations for the Small (S), Base (B) and Large (L) variants; we
also introduce an XLarge (XL) conﬁg as our largest model.
We initialize the MLP to output the zero-vector for all
; this initializes the full DiT block as the identity
function. As with the vanilla adaLN block, adaLN-
Zero adds negligible Gﬂops to the model.
We include the in-context, cross-attention, adaptive layer
norm and adaLN-Zero blocks in the DiT design space.
Model size. We apply a sequence of NDiT blocks, each
operating at the hidden dimension size d. Following ViT,
we use standard transformer conﬁgs that jointly scale N,
dand attention heads [10, 63]. Speciﬁcally, we use four
conﬁgs: DiT-S, DiT-B, DiT-L and DiT-XL. They cover a
wide range of model sizes and ﬂop allocations, from 0.3
to 118.6 Gﬂops, allowing us to gauge scaling performance.
Table 1 gives details of the conﬁgs.
We add B, S, L and XL conﬁgs to the DiT design space.
Transformer decoder. After the ﬁnal DiT block, we need
to decode our sequence of image tokens into an output noise
prediction and an output diagonal covariance prediction.
Both of these outputs have shape equal to the original spa-
tial input. We use a standard linear decoder to do this; we
apply the ﬁnal layer norm (adaptive if using adaLN) and lin-
early decode each token into a pp2Ctensor, where Cis
the number of channels in the spatial input to DiT. Finally,
we rearrange the decoded tokens into their original spatial
layout to get the predicted noise and covariance.
The complete DiT design space we explore is patch size,
transformer block architecture and model size.
4. Experimental Setup
We explore the DiT design space and study the scaling
properties of our model class. Our models are named ac-
cording to their conﬁgs and latent patch sizes p; for exam-
ple, DiT-XL/2 refers to the XLarge conﬁg and p= 2.
Training. We train class-conditional latent DiT models at
256256 and512512 image resolution on the Ima-
geNet dataset [31], a highly-competitive generative mod-
eling benchmark. We initialize the ﬁnal linear layer with
zeros and otherwise use standard weight initialization tech-
niques from ViT. We train all models with AdamW [29,33].
5

=== Page 6 ===
Figure 6. Scaling the DiT model improves FID at all stages of training. We show FID-50K over training iterations for 12 of our DiT
models. Top row: We compare FID holding patch size constant. Bottom row: We compare FID holding model size constant. Scaling the
transformer backbone yields better generative models across all model sizes and patch sizes.
We use a constant learning rate of 110 4, no weight de-
cay and a batch size of 256. The only data augmentation
we use is horizontal ﬂips. Unlike much prior work with
ViTs [57, 61], we did not ﬁnd learning rate warmup nor
regularization necessary to train DiTs to high performance.
Even without these techniques, training was highly stable
across all model conﬁgs and we did not observe any loss
spikes commonly seen when training transformers. Follow-
ing common practice in the generative modeling literature,
we maintain an exponential moving average (EMA) of DiT
weights over training with a decay of 0.9999. All results
reported use the EMA model. We use identical training hy-
perparameters across all DiT model sizes and patch sizes.
Our training hyperparameters are almost entirely retained
from ADM. We did not tune learning rates, decay/warm-up
schedules, Adam 1/2or weight decays.
Diffusion. We use an off-the-shelf pre-trained variational
autoencoder (V AE) model [30] from Stable Diffusion [48].
The V AE encoder has a downsample factor of 8—given an
RGB image xwith shape 2562563,z=E(x)has
shape 32324. Across all experiments in this section,
our diffusion models operate in this Z-space. After sam-
pling a new latent from our diffusion model, we decode it
to pixels using the V AE decoder x=D(z). We retain diffu-
sion hyperparameters from ADM [9]; speciﬁcally, we use a
tmax= 1000 linear variance schedule ranging from 110 4
to210 2, ADM’s parameterization of the covariance 
and their method for embedding input timesteps and labels.
Evaluation metrics. We measure scaling performance
with Fr ´echet Inception Distance (FID) [18], the standard
metric for evaluating generative models of images.We follow convention when comparing against prior works
and report FID-50K using 250 DDPM sampling steps.
FID is known to be sensitive to small implementation de-
tails [37]; to ensure accurate comparisons, all values re-
ported in this paper are obtained by exporting samples and
using ADM’s TensorFlow evaluation suite [9]. FID num-
bers reported in this section do notuse classiﬁer-free guid-
ance except where otherwise stated. We additionally report
Inception Score [51], sFID [34] and Precision/Recall [32]
as secondary metrics.
Compute. We implement all models in JAX [1] and train
them using TPU-v3 pods. DiT-XL/2, our most compute-
intensive model, trains at roughly 5.7 iterations/second on a
TPU v3-256 pod with a global batch size of 256.
5. Experiments
DiT block design. We train four of our highest Gﬂop
DiT-XL/2 models, each using a different block design—
in-context (119.4 Gﬂops), cross-attention (137.6 Gﬂops),
adaptive layer norm (adaLN, 118.6 Gﬂops) or adaLN-zero
(118.6 Gﬂops). We measure FID over the course of training.
Figure 5 shows the results. The adaLN-Zero block yields
lower FID than both cross-attention and in-context condi-
tioning while being the most compute-efﬁcient. At 400K
training iterations, the FID achieved with the adaLN-Zero
model is nearly half that of the in-context model, demon-
strating that the conditioning mechanism critically affects
model quality. Initialization is also important—adaLN-
Zero, which initializes each DiT block as the identity func-
tion, signiﬁcantly outperforms vanilla adaLN. For the rest
of the paper, all models will use adaLN-Zero DiT blocks.
6
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me check the appendix or other sections to find the exact parameter count for DiT-XL/2:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools search_pdf_content`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	"pattern": "XL/2.*parameter|parameter.*XL/2|675M|558M",
	"page_size": 20
}

```

```json output_result
No matches found for pattern: XL/2.*parameter|parameter.*XL/2|675M|558M
```

</Expandable>
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	"start_page": 12,
	"end_page": 14
}

```

```json output_result
PDF total pages: 25
Extracted page range: 12-14

=== Page 12 ===
Figure 11. Additional selected samples from our 512 512 and 256256 resolution DiT-XL/2 models. We use a classiﬁer-free guidance
scale of 6.0 for the 512512model and 4.0 for the 256256model. Both models use the ft-EMA V AE decoder.
A. Additional Implementation Details
We include detailed information about all of our DiT
models in Table 4, including both 256256and512512
models. In Figure 13, we report DiT training loss curves.
Finally, we also include Gﬂop counts for DDPM U-Net
models from ADM and LDM in Table 6.
DiT model details. To embed input timesteps, we use
a 256-dimensional frequency embedding [9] followed by
a two-layer MLP with dimensionality equal to the trans-
former’s hidden size and SiLU activations. Each adaLN
layer feeds the sum of the timestep and class embeddings
into a SiLU nonlinearity and a linear layer with output neu-
rons equal to either 4(adaLN) or 6(adaLN-Zero) the
transformer’s hidden size. We use GELU nonlinearities (ap-
proximated with tanh) in the core transformer [16].
Classiﬁer-free guidance on a subset of channels. In our
experiments using classiﬁer-free guidance, we applied guid-
ance only to the ﬁrst three channels of the latents instead of
all four channels. Upon investigating, we found that three-
channel guidance and four-channel guidance give similarresults (in terms of FID) when simply adjusting the scale
factor. Speciﬁcally, three-channel guidance with a scale
of(1 +x)appears reasonably well-approximated by four-
channel guidance with a scale of (1 +3
4x)(e.g., three-
channel guidance with a scale of 1:5gives an FID-50K of
2.27, and four-channel guidance with a scale of 1:375gives
an FID-50K of 2.20). It is somewhat interesting that ap-
plying guidance to a subset of elements can still yield good
performance, and we leave it to future work to explore this
phenomenon further.
B. Model Samples
We show samples from our two DiT-XL/2 models at
512512and256256resolution trained for 3M and 7M
steps, respectively. Figures 1 and 11 show selected samples
from both models. Figures 14 through 33 show uncurated
samples from the two models across a range of classiﬁer-
free guidance scales and input class labels (generated with
250 DDPM sampling steps and the ft-EMA V AE decoder).
As with prior work using guidance, we observe that larger
scales increase visual ﬁdelity and decrease sample diversity.
12

=== Page 13 ===
Model Image Resolution Flops (G) Params (M) Training Steps (K) Batch Size Learning Rate DiT Block FID-50K (no guidance)
DiT-S/8 256256 0.36 33 400 256 110 4adaLN-Zero 153.60
DiT-S/4 256256 1.41 33 400 256 110 4adaLN-Zero 100.41
DiT-S/2 256256 6.06 33 400 256 110 4adaLN-Zero 68.40
DiT-B/8 256256 1.42 131 400 256 110 4adaLN-Zero 122.74
DiT-B/4 256256 5.56 130 400 256 110 4adaLN-Zero 68.38
DiT-B/2 256256 23.01 130 400 256 110 4adaLN-Zero 43.47
DiT-L/8 256256 5.01 459 400 256 110 4adaLN-Zero 118.87
DiT-L/4 256256 19.70 458 400 256 110 4adaLN-Zero 45.64
DiT-L/2 256256 80.71 458 400 256 110 4adaLN-Zero 23.33
DiT-XL/8 256256 7.39 676 400 256 110 4adaLN-Zero 106.41
DiT-XL/4 256256 29.05 675 400 256 110 4adaLN-Zero 43.01
DiT-XL/2 256256 118.64 675 400 256 110 4adaLN-Zero 19.47
DiT-XL/2 256256 119.37 449 400 256 110 4in-context 35.24
DiT-XL/2 256256 137.62 598 400 256 110 4cross-attention 26.14
DiT-XL/2 256256 118.56 600 400 256 110 4adaLN 25.21
DiT-XL/2 256256 118.64 675 2352 256 110 4adaLN-Zero 10.67
DiT-XL/2 256256 118.64 675 7000 256 110 4adaLN-Zero 9.62
DiT-XL/2 512512 524.60 675 1301 256 110 4adaLN-Zero 13.78
DiT-XL/2 512512 524.60 675 3000 256 110 4adaLN-Zero 11.93
Table 4. Details of all DiT models. We report detailed information about every DiT model in our paper. Note that FID-50K here is
computed without classiﬁer-free guidance. Parameter and ﬂop counts exclude the V AE model which contains 84M parameters across the
encoder and decoder. For both the 256256and512512DiT-XL/2 models, we never observed FID saturate and continued training
them as long as possible. Numbers reported in this table use the ft-MSE V AE decoder.
C. Additional Scaling Results
Impact of scaling on metrics beyond FID. In Figure 12,
we show the effects of DiT scale on a suite of evaluation
metrics—FID, sFID, Inception Score, Precision and Recall.
We ﬁnd that our FID-driven analysis in the main paper gen-
eralizes to the other metrics—across every metric, scaled-up
DiT models are more compute-efﬁcient and model Gﬂops
are highly-correlated with performance. In particular, In-
ception Score and Precision beneﬁt heavily from increased
model scale.
Impact of scaling on training loss. We also examine the
impact of scale on training loss in Figure 13. Increasing
DiT model Gﬂops (via transformer size or number of input
tokens) causes the training loss to decrease more rapidly and
saturate at a lower value. This phenomenon is consistent
with trends observed with language models, where scaled-
up transformers demonstrate both improved loss curves as
well as improved performance on downstream evaluation
suites [26].
D. V AE Decoder Ablations
We used off-the-shelf, pre-trained V AEs across our ex-
periments. The V AE models (ft-MSE and ft-EMA) are ﬁne-
tuned versions of the original LDM “f8” model (only the
decoder weights are ﬁne-tuned). We monitored metrics for
our scaling analysis in Section 5 using the ft-MSE decoder,
and we used the ft-EMA decoder for our ﬁnal metrics re-
ported in Tables 2 and 3. In this section, we ablate threeClass-Conditional ImageNet 256 256, DiT-XL/2-G (cfg=1.5)
Decoder FID#sFID# IS" Precision" Recall"
original 2.46 5.18 271.56 0.82 0.57
ft-MSE 2.30 4.73 276.09 0.83 0.57
ft-EMA 2.27 4.60 278.24 0.83 0.57
Table 5. Decoder ablation. We tested different pre-trained V AE
decoder weights available at https://huggingface.co/
stabilityai/sd-vae-ft-mse . Different pre-trained de-
coder weights yield comparable results on ImageNet 256256.
Diffusion U-Net Model Complexities
Model Image Resolution Base Flops (G) Upsampler Flops (G) Total Flops (G)
ADM 128128 307 - 307
ADM 256256 1120 - 1120
ADM 512512 1983 - 1983
ADM-U 256256 110 632 742
ADM-U 512512 307 2506 2813
LDM-4 256256 104 - 104
LDM-8 256256 57 - 57
Table 6. Gﬂop counts for baseline diffusion models that use U-
Net backbones. Note that we only count Flops for DDPM com-
ponents.
different choices of the V AE decoder; the original one used
by LDM and the two ﬁne-tuned decoders used by Stable
Diffusion. Because the encoders are identical across mod-
els, the decoders can be swapped-in without retraining the
diffusion model. Table 5 shows results; XL/2 continues to
outperform all prior diffusion models when using the LDM
decoder.
13

=== Page 14 ===
Figure 12. DiT scaling behavior on several generative modeling metrics. Left: We plot model performance as a function of total training
compute for FID, sFID, Inception Score, Precision and Recall. Right: We plot model performance at 400K training steps for all 12 DiT
variants against transformer Gﬂops, ﬁnding strong correlations across metrics. All values were computed using the ft-MSE V AE decoder.
14
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Perfect! Now I have all the information I need. Let me compile the data:

From the papers:
1. **DiT-XL/2** (2212.09748v2.pdf):
   - Model: DiT-XL/2
   - Category: Diffusion
   - Parameters: 675M
   - FID-50K: 2.27 (with cfg=1.50)
   - Inception Score: 278.24

2. **LlamaGen-3B** (2406.06525v1.pdf):
   - Model: LlamaGen-3B
   - Category: AR (Autoregressive)
   - Parameters: 3.1B
   - FID: 2.18 (with cfg=1.65)
   - Inception Score: 263.33

Now let me create the LaTeX table in the format specified, ordered by FID in descending order (worst FID first):
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
workspace/survey.tex
\begin{table}[!ht]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        Model & Class & \#Param & FID-50K & Inception Score \\
        \midrule
        DiT-XL/2 & Diffusion & 675M & 2.27 & 278.24 \\
        LlamaGen-3B & AR & 3.1B & 2.18 & 263.33 \\
        \bottomrule
    \end{tabular}
\end{table}

```

```json output_result
Successfully wrote to /workspace/dumps/workspace/survey.tex
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Perfect! I've successfully created the LaTeX table with the experimental results from both papers on ImageNet 256×256. Here's a summary of what I found:

## Summary

I analyzed two papers in your workspace:

1. **DiT (Diffusion Transformers)** - Paper ID: 2212.09748v2
   - Best model: DiT-XL/2
   - Method: Diffusion model with transformer backbone
   - Parameters: 675M
   - FID-50K: 2.27 (with classifier-free guidance scale 1.50)
   - Inception Score: 278.24

2. **LlamaGen** - Paper ID: 2406.06525v1
   - Best model: LlamaGen-3B
   - Method: Autoregressive (AR) model based on Llama architecture
   - Parameters: 3.1B
   - FID: 2.18 (with classifier-free guidance scale 1.65)
   - Inception Score: 263.33

The table has been saved to `survey.tex` in your workspace, ordered by FID in descending order (DiT-XL/2 with 2.27 FID first, then LlamaGen-3B with 2.18 FID). The table follows the format structure you provided in `format.tex`.
</div>

</Accordion>

</AccordionGroup>
