---
title: "ImageNet"
description: "Summarize the ImageNet 256 experimental results from my image generation papers into a LaTeX table."
mode: wide
---

<Card>
<div className="tools-container">
<div className="mcp-servers-container">
<div className="mcp-servers-title">
MCP Servers
</div>
<div className="mcp-servers-grid">
<div className="mcp-server-item">
<svg width="20px" height="20px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg>
<span className="mcp-server-name">pdf-tools</span>
</div>
<div className="mcp-server-item">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="20px" height="20px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg>
<span className="mcp-server-name">filesystem</span>
</div>
</div>
</div>
<div className="local-tools-container">
<div className="mcp-servers-title">
Local Tools
</div>
<div className="local-tools-grid">
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 100 100" enable-background="new 0 0 100 100" id="Layer_1" version="1.1" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><g><rect clip-rule="evenodd" fill="#F2F2F2" fill-rule="evenodd" height="83.437" width="67.025" x="9.012" y="7.604"/><path d="M77.454,92.458H7.595V6.187h69.859V92.458z M10.429,89.624H74.62V9.021H10.429V89.624z"/></g><g><rect clip-rule="evenodd" fill="#FF7C24" fill-rule="evenodd" height="10.481" width="17.952" x="46.695" y="34.866"/></g><g><rect height="2.834" width="19.463" x="20.504" y="35.575"/></g><g><rect height="2.834" width="15.561" x="20.718" y="42.508"/></g><g><rect height="2.833" width="15.562" x="20.813" y="49.514"/></g><g><rect height="2.833" width="27.128" x="20.718" y="56.753"/></g><g><rect height="2.833" width="23.51" x="20.718" y="63.688"/></g><g><rect clip-rule="evenodd" fill="#39B6CC" fill-rule="evenodd" height="9.004" width="44.095" x="20.553" y="19.064"/></g><g><rect height="2.833" width="26.272" x="20.718" y="70.32"/></g><g><rect height="2.834" width="32.8" x="20.718" y="77.253"/></g><g><rect height="2.834" width="3.235" x="38.304" y="42.508"/></g><g><path clip-rule="evenodd" d="M77.931,71.902l4.287,4.427l-6.644,6.437l-4.309-4.457    C74.147,76.998,76.504,74.726,77.931,71.902L77.931,71.902z" fill="#F2F2F2" fill-rule="evenodd"/><path d="M75.542,84.77l-6.692-6.92l1.828-0.831c2.579-1.174,4.706-3.218,5.989-5.756l0.897-1.776l6.656,6.874L75.542,84.77z     M73.584,78.669l2.023,2.091l4.605-4.463l-2.007-2.074C76.994,76.012,75.414,77.531,73.584,78.669z"/></g><g><polygon clip-rule="evenodd" fill="#39B6CC" fill-rule="evenodd" points="83.267,75.319 91.984,84.338 83.247,92.779     74.535,83.761   "/><path d="M83.213,94.783L72.531,83.726l10.771-10.41l10.687,11.056L83.213,94.783z M76.538,83.794l6.744,6.981l6.698-6.472    l-6.748-6.981L76.538,83.794z"/></g><g><path clip-rule="evenodd" d="M66.124,50.799c7.742,0,14.018,6.276,14.018,14.019    s-6.275,14.019-14.018,14.019c-7.743,0-14.019-6.276-14.019-14.019S58.381,50.799,66.124,50.799L66.124,50.799z" fill="#F2F2F2" fill-rule="evenodd"/><path d="M66.124,80.253c-8.511,0-15.435-6.924-15.435-15.435s6.924-15.435,15.435-15.435S81.56,56.307,81.56,64.818    S74.635,80.253,66.124,80.253z M66.124,52.216c-6.949,0-12.601,5.653-12.601,12.602s5.651,12.601,12.601,12.601    c6.948,0,12.602-5.652,12.602-12.601S73.072,52.216,66.124,52.216z"/></g><g><rect height="2.833" width="10.313" x="39.902" y="49.514"/></g><g><path d="M76.404,65.586H73.57c0-0.636-0.068-1.255-0.205-1.84c-0.043-0.186-0.096-0.385-0.169-0.63l2.717-0.808    c0.091,0.304,0.158,0.559,0.215,0.801C76.31,63.901,76.404,64.735,76.404,65.586z M72.438,61.433    c-1.489-2.5-4.203-4.058-7.084-4.061l0.004-2.834c3.871,0.005,7.518,2.091,9.516,5.445L72.438,61.433z"/></g></g></svg>
<span className="local-tool-name">history</span>
</div>
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M861.588238 240.133873v-65.792823c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.631049H314.659414v-65.631049c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.792823c-36.317212 0.868255-65.631049 30.539428-65.63105 67.061417v543.745565c0 37.06772 30.155471 67.223191 67.223191 67.223191h696.886045c37.06772 0 67.223191-30.155471 67.223191-67.223191V307.19529c-0.001024-36.52199-29.315885-66.193162-65.633097-67.061417z m-109.385765-65.792823c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m-546.928824 0c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m678.191947 676.600829c0 12.935767-10.532708 23.468476-23.468476 23.468475H163.111076c-12.935767 0-23.468476-10.532708-23.468476-23.468475V307.19529c0-12.402323 9.677764-22.593054 21.877358-23.415233v65.577807c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.631049h393.789368v65.631049c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.577807c12.19857 0.82218 21.877358 11.012911 21.877358 23.415233v543.746589z" fill="#22C67F" /><path d="M706.719439 478.272194l-48.01715-44.741741-182.28128 195.621482-111.468348-122.615387-48.563905 44.148911 159.469116 172.685427z" fill="#74E8AE" /></svg>
<span className="local-tool-name">claim_done</span>
</div>
<div className="local-tool-item">
<Icon icon="bars-progress" size={20} color="#4286f6" />
<span className="local-tool-name">manage_context</span>
</div>
<div className="local-tool-item">
<Icon icon="filter-list" size={20} color="#4286f6" />
<span className="local-tool-name">handle_overlong_tool_outputs</span>
</div>
</div>
</div>
</div>
</Card>









## Instruction

I am a graduate student in computer vision, and I have several papers on image generation in my workspace. I need your help to summarize the experimental results of the models in these papers on ImageNet 256 and store the results in a LaTeX table. The table should include: Model Name (When multiple models are included, report the one with the lowest FID result in the paper.), Method Category (determine if it belongs to VAE, GAN, Diffusion, Flow-based, or AR), Model Parameters, FID-50K, and Inception Score. Finally, please save the results in survey.tex in my workspace (only the LaTeX table source code is needed), and the order of the table should be in descending order of FID. The example structure of the table is format.tex.

## Initial State

### Local Workspace

<div className="file-tree">
<a href="https://github.com/hkust-nlp/mcpbench_dev/tree/finalpool_dev/tasks/finalpool/imagenet">workspace</a>/
├── 2212.09748v2.pdf
├── 2406.06525v1.pdf
└── format.tex
</div>

## Model Trajectory

<AccordionGroup>
<Accordion title="claude-4-sonnet">

<Columns cols={3}>
<Card title="Task Completion" icon="check">
Completed
</Card>
<Card title="Tool Calls" icon="wrench">
21
</Card>
<Card title="Turns" icon="arrows-rotate">
21
</Card>
</Columns>

<div className="thinking-box">
🧐`Agent`

I'll help you analyze the papers in your workspace and create a LaTeX table with the experimental results. Let me start by exploring your workspace to find the papers and the format file.
</div>

<div className="result-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem list_directory`

<Expandable title="Details">
```json arguments
{
	"path": "/workspace/dumps/workspace"
}

```

```json output_result
[DIR] .pdf_tools_tempfiles
[FILE] 2212.09748v2.pdf
[FILE] 2406.06525v1.pdf
[FILE] format.tex
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Let me first check the format.tex file to understand the table structure you want:
</div>

<div className="result-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

<Expandable title="Details">
```json arguments
{
	"path": "/workspace/dumps/workspace/format.tex"
}

```

```json output_result
\begin{table}[!ht]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        Model & Class & \#Param & FID-50K & Inception Score \\
        \midrule
        A & GAN & 300M & 40.20 & 150.00 \\
        B & Diffusion & 2B & 20.20 & 190.00 \\
        \bottomrule
    \end{tabular}
\end{table}
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me examine the PDF papers to extract the experimental results. Let me start with the first paper:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools get_pdf_info`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf"
}

```

```json output_result
PDF file information:
Total pages: 25
Title: 
Author: 
Creator: LaTeX with hyperref
Creation date: D:20230303012649Z

```

</Expandable>
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	 "start_page": 1,
	 "end_page": 3
}

```

```json output_result
PDF total pages: 25
Extracted page range: 1-3

=== Page 1 ===
Scalable Diffusion Models with Transformers
William Peebles*
UC BerkeleySaining Xie
New York University
Figure 1. Diffusion models with transformer backbones achieve state-of-the-art image quality. We show selected samples from two
of our class-conditional DiT-XL/2 models trained on ImageNet at 512 512 and 256256 resolution, respectively.
Abstract
We explore a new class of diffusion models based on the
transformer architecture. We train latent diffusion models
of images, replacing the commonly-used U-Net backbone
with a transformer that operates on latent patches. We an-
alyze the scalability of our Diffusion Transformers (DiTs)
through the lens of forward pass complexity as measured by
Gﬂops. We ﬁnd that DiTs with higher Gﬂops—through in-
creased transformer depth/width or increased number of in-
put tokens—consistently have lower FID. In addition to pos-
sessing good scalability properties, our largest DiT-XL/2
models outperform all prior diffusion models on the class-
conditional ImageNet 512 512 and 256256 benchmarks,
achieving a state-of-the-art FID of 2.27 on the latter.1. Introduction
Machine learning is experiencing a renaissance powered
by transformers. Over the past ﬁve years, neural architec-
tures for natural language processing [8, 42], vision [10]
and several other domains have largely been subsumed by
transformers [60]. Many classes of image-level genera-
tive models remain holdouts to the trend, though—while
transformers see widespread use in autoregressive mod-
els [3,6,43,47], they have seen less adoption in other gener-
ative modeling frameworks. For example, diffusion models
have been at the forefront of recent advances in image-level
generative models [9,46]; yet, they all adopt a convolutional
U-Net architecture as the de-facto choice of backbone.
*Work done during an internship at Meta AI, FAIR Team.
Code and project page available here.
1arXiv:2212.09748v2  [cs.CV]  2 Mar 2023

=== Page 2 ===
520 80 320GflopsDiameterFigure 2. ImageNet generation with Diffusion Transformers (DiTs). Bubble area indicates the ﬂops of the diffusion model. Left:
FID-50K (lower is better) of our DiT models at 400K training iterations. Performance steadily improves in FID as model ﬂops increase.
Right: Our best model, DiT-XL/2, is compute-efﬁcient and outperforms all prior U-Net-based diffusion models, like ADM and LDM.
The seminal work of Ho et al. [19] ﬁrst introduced the
U-Net backbone for diffusion models. Having initially seen
success within pixel-level autoregressive models and con-
ditional GANs [23], the U-Net was inherited from Pixel-
CNN++ [52, 58] with a few changes. The model is con-
volutional, comprised primarily of ResNet [15] blocks. In
contrast to the standard U-Net [49], additional spatial self-
attention blocks, which are essential components in trans-
formers, are interspersed at lower resolutions. Dhariwal and
Nichol [9] ablated several architecture choices for the U-
Net, such as the use of adaptive normalization layers [40] to
inject conditional information and channel counts for con-
volutional layers. However, the high-level design of the U-
Net from Ho et al. has largely remained intact.
With this work, we aim to demystify the signiﬁcance of
architectural choices in diffusion models and offer empiri-
cal baselines for future generative modeling research. We
show that the U-Net inductive bias is notcrucial to the per-
formance of diffusion models, and they can be readily re-
placed with standard designs such as transformers. As a
result, diffusion models are well-poised to beneﬁt from the
recent trend of architecture uniﬁcation—e.g., by inheriting
best practices and training recipes from other domains, as
well as retaining favorable properties like scalability, ro-
bustness and efﬁciency. A standardized architecture would
also open up new possibilities for cross-domain research.
In this paper, we focus on a new class of diffusion models
based on transformers. We call them Diffusion Transform-
ers, or DiTs for short. DiTs adhere to the best practices of
Vision Transformers (ViTs) [10], which have been shown to
scale more effectively for visual recognition than traditional
convolutional networks (e.g., ResNet [15]).More speciﬁcally, we study the scaling behavior of trans-
formers with respect to network complexity vs. sample
quality . We show that by constructing and benchmark-
ing the DiT design space under the Latent Diffusion Mod-
els(LDMs) [48] framework, where diffusion models are
trained within a V AE’s latent space, we can successfully
replace the U-Net backbone with a transformer. We further
show that DiTs are scalable architectures for diffusion mod-
els: there is a strong correlation between the network com-
plexity (measured by Gﬂops) vs. sample quality (measured
by FID). By simply scaling-up DiT and training an LDM
with a high-capacity backbone (118.6 Gﬂops), we are able
to achieve a state-of-the-art result of 2.27 FID on the class-
conditional 256256ImageNet generation benchmark.
2. Related Work
Transformers. Transformers [60] have replaced domain-
speciﬁc architectures across language, vision [10], rein-
forcement learning [5, 25] and meta-learning [39]. They
have shown remarkable scaling properties under increas-
ing model size, training compute and data in the language
domain [26], as generic autoregressive models [17] and
as ViTs [63]. Beyond language, transformers have been
trained to autoregressively predict pixels [6, 7, 38]. They
have also been trained on discrete codebooks [59] as both
autoregressive models [11,47] and masked generative mod-
els [4, 14]; the former has shown excellent scaling behavior
up to 20B parameters [62]. Finally, transformers have been
explored in DDPMs to synthesize non-spatial data; e.g., to
generate CLIP image embeddings in DALL E 2 [41,46]. In
this paper, we study the scaling properties of transformers
when used as the backbone of diffusion models of images.
2

=== Page 3 ===
Multi-Head Self-AttentionLayer NormScale, ShiftMLPPointwise FeedforwardLayer NormScale, ShiftScale++
Scale𝛾!,𝛽!𝛼!𝛾",𝛽"𝛼"
Input TokensConditioningDiTBlock with adaLN-ZeroLatent Diffusion TransformerTimestep 𝑡Label 𝑦DiTBlockN xPatchifyLayer NormLinear and ReshapeEmbedNoiseΣ32 x 32 x 432 x 32 x 4
Noised Latent32 x 32 x 4Multi-Head Self-AttentionLayer NormPointwise Feedforward
Layer Norm++
Input TokensConditioningDiTBlock with Cross-AttentionMulti-Head Cross-AttentionLayer Norm+Multi-Head Self-AttentionLayer NormPointwise Feedforward++
Input TokensConditioningDiTBlock with In-Context ConditioningLayer Norm
Concatenate on Sequence DimensionFigure 3. The Diffusion Transformer (DiT) architecture. Left: We train conditional latent DiT models. The input latent is decomposed
into patches and processed by several DiT blocks. Right: Details of our DiT blocks. We experiment with variants of standard transformer
blocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens. Adaptive layer norm works best.
Denoising diffusion probabilistic models (DDPMs).
Diffusion [19, 54] and score-based generative models [22,
56] have been particularly successful as generative models
of images [35,46,48,50], in many cases outperforming gen-
erative adversarial networks (GANs) [12] which had previ-
ously been state-of-the-art. Improvements in DDPMs over
the past two years have largely been driven by improved
sampling techniques [19, 27, 55], most notably classiﬁer-
free guidance [21], reformulating diffusion models to pre-
dict noise instead of pixels [19] and using cascaded DDPM
pipelines where low-resolution base diffusion models are
trained in parallel with upsamplers [9, 20]. For all the dif-
fusion models listed above, convolutional U-Nets [49] are
the de-facto choice of backbone architecture. Concurrent
work [24] introduced a novel, efﬁcient architecture based
on attention for DDPMs; we explore pure transformers.
Architecture complexity. When evaluating architecture
complexity in the image generation literature, it is fairly
common practice to use parameter counts. In general, pa-
rameter counts can be poor proxies for the complexity of
image models since they do not account for, e.g., image res-
olution which signiﬁcantly impacts performance [44, 45].
Instead, much of the model complexity analysis in this pa-
per is through the lens of theoretical Gﬂops. This brings us
in-line with the architecture design literature where Gﬂops
are widely-used to gauge complexity. In practice, the
golden complexity metric is still up for debate as it fre-
quently depends on particular application scenarios. Nichol
and Dhariwal’s seminal work improving diffusion mod-
els [9, 36] is most related to us—there, they analyzed the
scalability and Gﬂop properties of the U-Net architecture
class. In this paper, we focus on the transformer class.3. Diffusion Transformers
3.1. Preliminaries
Diffusion formulation. Before introducing our architec-
ture, we brieﬂy review some basic concepts needed to
understand diffusion models (DDPMs) [19, 54]. Gaus-
sian diffusion models assume a forward noising process
which gradually applies noise to real data x0:q(xtjx0) =
N(xt;ptx0;(1 t)I), where constants tare hyperpa-
rameters. By applying the reparameterization trick, we can
samplext=ptx0+p1 tt, wheretN(0;I).
Diffusion models are trained to learn the reverse process
that inverts forward process corruptions: p(xt 1jxt) =
N((xt);(xt)), where neural networks are used to pre-
dict the statistics of p. The reverse process model is
trained with the variational lower bound [30] of the log-
likelihood of x0, which reduces to L() = p(x0jx1) +P
tDKL(q(xt 1jxt;x0)jjp(xt 1jxt)), excluding an ad-
ditional term irrelevant for training. Since both qandp
are Gaussian,DKLcan be evaluated with the mean and co-
variance of the two distributions. By reparameterizing as
a noise prediction network , the model can be trained us-
ing simple mean-squared error between the predicted noise
(xt)and the ground truth sampled Gaussian noise t:
Lsimple () =jj(xt) tjj2
2. But, in order to train diffu-
sion models with a learned reverse process covariance ,
the fullDKLterm needs to be optimized. We follow Nichol
and Dhariwal’s approach [36]: train withLsimple , and
trainwith the fullL. Oncepis trained, new images can
be sampled by initializing xtmax N (0;I)and sampling
xt 1p(xt 1jxt)via the reparameterization trick.
3
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Let me continue reading this paper to find the experimental results:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools search_pdf_content`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	 "pattern": "ImageNet.*256.*FID|FID.*ImageNet.*256|Inception Score"
}

```

```json output_result
Search ID: cefec954
Pattern: ImageNet.*256.*FID|FID.*ImageNet.*256|Inception Score
Total matches: 5
Page: 1/1
Results per page: 10

Match 1:
  Page: 6
  Text: "Inception Score"
  Context: ...s section do notuse classiﬁer-free guid-
ance except where otherwise stated. We additionally report
[Inception Score] [51], sFID [34] and Precision/Recall [32]
as secondary metrics.
Compute. We implement all models in...

Match 2:
  Page: 8
  Text: "Inception Score"
  Context: ...roved DiT models. In Fig-
ure 12 (appendix), we ﬁnd that this trend holds for other
metrics such as [Inception Score].
107108109101010111012
Training Compute (Gflops)0255075100125150175200FID-50K
1015202530
S/8
S/4
S/...

Match 3:
  Page: 9
  Text: "ImageNet 256256. DiT-XL/2 achieves state-of-the-art FID"
  Context: ...G (cfg=1.50) 2.27 4.60 278.24 0.83 0.57
Table 2. Benchmarking class-conditional image generation on
[ImageNet 256256. DiT-XL/2 achieves state-of-the-art FID].
Class-Conditional ImageNet 512 512
Model FID # sFID# IS" Precision"Recall"
BigGAN-deep [2] 8.43 8...

Match 4:
  Page: 13
  Text: "Inception Score"
  Context: ...ond FID. In Figure 12,
we show the effects of DiT scale on a suite of evaluation
metrics—FID, sFID, [Inception Score], Precision and Recall.
We ﬁnd that our FID-driven analysis in the main paper gen-
eralizes to the o...

Match 5:
  Page: 14
  Text: "Inception Score"
  Context: ...ing metrics. Left: We plot model performance as a function of total training
compute for FID, sFID, [Inception Score], Precision and Recall. Right: We plot model performance at 400K training steps for all 12 DiT
varia...


```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Let me look at page 9 which seems to contain the benchmark results:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	 "start_page": 9,
	 "end_page": 9
}

```

```json output_result
PDF total pages: 25
Extracted page range: 9-9

=== Page 9 ===
Class-Conditional ImageNet 256 256
Model FID # sFID# IS" Precision"Recall"
BigGAN-deep [2] 6.95 7.36 171.4 0.87 0.28
StyleGAN-XL [53] 2.30 4.02 265.12 0.78 0.53
ADM [9] 10.94 6.02 100.98 0.69 0.63
ADM-U 7.49 5.13 127.49 0.72 0.63
ADM-G 4.59 5.25 186.70 0.82 0.52
ADM-G, ADM-U 3.94 6.14 215.84 0.83 0.53
CDM [20] 4.88 - 158.71 - -
LDM-8 [48] 15.51 - 79.03 0.65 0.63
LDM-8-G 7.76 - 209.52 0.84 0.35
LDM-4 10.56 - 103.49 0.71 0.62
LDM-4-G (cfg=1.25) 3.95 - 178.22 0.81 0.55
LDM-4-G (cfg=1.50) 3.60 - 247.67 0.87 0.48
DiT-XL/2 9.62 6.85 121.50 0.67 0.67
DiT-XL/2-G (cfg=1.25) 3.22 5.28 201.77 0.76 0.62
DiT-XL/2-G (cfg=1.50) 2.27 4.60 278.24 0.83 0.57
Table 2. Benchmarking class-conditional image generation on
ImageNet 256256. DiT-XL/2 achieves state-of-the-art FID.
Class-Conditional ImageNet 512 512
Model FID # sFID# IS" Precision"Recall"
BigGAN-deep [2] 8.43 8.13 177.90 0.88 0.29
StyleGAN-XL [53] 2.41 4.06 267.75 0.77 0.52
ADM [9] 23.24 10.19 58.06 0.73 0.60
ADM-U 9.96 5.62 121.78 0.75 0.64
ADM-G 7.72 6.57 172.71 0.87 0.42
ADM-G, ADM-U 3.85 5.86 221.72 0.84 0.53
DiT-XL/2 12.03 7.12 105.25 0.75 0.64
DiT-XL/2-G (cfg=1.25) 4.64 5.77 174.77 0.81 0.57
DiT-XL/2-G (cfg=1.50) 3.04 5.02 240.82 0.84 0.54
Table 3. Benchmarking class-conditional image generation on
ImageNet 512512. Note that prior work [9] measures Precision
and Recall using 1000 real samples for 512512resolution; for
consistency, we do the same.
Our method achieves the lowest FID of all prior generative
models, including the previous state-of-the-art StyleGAN-
XL [53]. Finally, we also observe that DiT-XL/2 achieves
higher recall values at all tested classiﬁer-free guidance
scales compared to LDM-4 and LDM-8. When trained for
only 2.35M steps (similar to ADM), XL/2 still outperforms
all prior diffusion models with an FID of 2.55.
512512 ImageNet. We train a new DiT-XL/2 model on
ImageNet at 512512resolution for 3M iterations with
identical hyperparameters as the 256256model. With a
patch size of 2, this XL/2 model processes a total of 1024
tokens after patchifying the 64644input latent (524.6
Gﬂops). Table 3 shows comparisons against state-of-the-art
methods. XL/2 again outperforms all prior diffusion models
at this resolution, improving the previous best FID of 3.85
achieved by ADM to 3.04. Even with the increased num-
ber of tokens, XL/2 remains compute-efﬁcient. For exam-
ple, ADM uses 1983 Gﬂops and ADM-U uses 2813 Gﬂops;
XL/2 uses 524.6 Gﬂops. We show samples from the high-
resolution XL/2 model in Figure 1 and the appendix.
101
102
103
104
105
Sampling Compute (Gflops)20406080100120140160180FID-10K
S/8
S/4
S/2B/8
B/4
B/2L/8
L/4
L/2XL/8
XL/4
XL/2Figure 10. Scaling-up sampling compute does not compensate
for a lack of model compute. For each of our DiT models trained
for 400K iterations, we compute FID-10K using [16, 32, 64, 128,
256, 1000] sampling steps. For each number of steps, we plot the
FID as well as the Gﬂops used to sample each image. Small mod-
els cannot close the performance gap with our large models, even
if they sample with more test-time Gﬂops than the large models.
5.2. Scaling Model vs. Sampling Compute
Diffusion models are unique in that they can use addi-
tional compute after training by increasing the number of
sampling steps when generating an image. Given the im-
pact of model Gﬂops on sample quality, in this section we
study if smaller- model compute DiTs can outperform larger
ones by using more sampling compute . We compute FID
for all 12 of our DiT models after 400K training steps, us-
ing [16, 32, 64, 128, 256, 1000] sampling steps per-image.
The main results are in Figure 10. Consider DiT-L/2 us-
ing 1000 sampling steps versus DiT-XL/2 using 128 steps.
In this case, L/2 uses 80:7Tﬂops to sample each image;
XL/2 uses 5less compute— 15:2Tﬂops—to sample each
image. Nonetheless, XL/2 has the better FID-10K (23.7
vs 25.9). In general, scaling-up sampling compute cannot
compensate for a lack of model compute.
6. Conclusion
We introduce Diffusion Transformers (DiTs), a simple
transformer-based backbone for diffusion models that out-
performs prior U-Net models and inherits the excellent scal-
ing properties of the transformer model class. Given the
promising scaling results in this paper, future work should
continue to scale DiTs to larger models and token counts.
DiT could also be explored as a drop-in backbone for text-
to-image models like DALL E 2 and Stable Diffusion.
Acknowledgements. We thank Kaiming He, Ronghang
Hu, Alexander Berg, Shoubhik Debnath, Tim Brooks, Ilija
Radosavovic and Tete Xiao for helpful discussions. William
Peebles is supported by the NSF GRFP.
9
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Great! Now let me check if there's information about the model parameters. Let me search for parameter information:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools search_pdf_content`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	 "pattern": "parameters|param|XL/2.*M|million|billion"
}

```

```json output_result
Search ID: 6fe46ca7
Pattern: parameters|param|XL/2.*M|million|billion
Total matches: 73
Page: 1/8
Results per page: 10

Match 1:
  Page: 1
  Text: "XL/2 models trained on Im"
  Context: ...ieve state-of-the-art image quality. We show selected samples from two
of our class-conditional DiT-[XL/2 models trained on Im]ageNet at 512 512 and 256256 resolution, respectively.
Abstract
We explore a new class of diffusio...

Match 2:
  Page: 2
  Text: "XL/2, is compute-efﬁcient and outperforms all prior U-Net-based diffusion models, like ADM and LDM"
  Context: ...iterations. Performance steadily improves in FID as model ﬂops increase.
Right: Our best model, DiT-[XL/2, is compute-efﬁcient and outperforms all prior U-Net-based diffusion models, like ADM and LDM].
The seminal work of Ho et al. [19] ﬁrst introduced the
U-Net backbone for diffusion models. Having...

Match 3:
  Page: 2
  Text: "parameters"
  Context: ...] and masked generative mod-
els [4, 14]; the former has shown excellent scaling behavior
up to 20B [parameters] [62]. Finally, transformers have been
explored in DDPMs to synthesize non-spatial data; e.g., to
ge...

Match 4:
  Page: 3
  Text: "param"
  Context: ...ing architecture
complexity in the image generation literature, it is fairly
common practice to use [param]eter counts. In general, pa-
rameter counts can be poor proxies for the complexity of
image models s...

Match 5:
  Page: 3
  Text: "param"
  Context: ...ta x0:q(xtjx0) =
N(xt;ptx0;(1 t)I), where constants tare hyperpa-
rameters. By applying the re[param]eterization trick, we can
samplext=ptx0+p1 tt, wheretN(0;I).
Diffusion models are trained to ...

Match 6:
  Page: 3
  Text: "param"
  Context: ...ndp
are Gaussian,DKLcan be evaluated with the mean and co-
variance of the two distributions. By re[param]eterizing as
a noise prediction network , the model can be trained us-
ing simple mean-squared e...

Match 7:
  Page: 3
  Text: "param"
  Context: ...ned, new images can
be sampled by initializing xtmax N (0;I)and sampling
xt 1p(xt 1jxt)via the re[param]eterization trick.
3...

Match 8:
  Page: 4
  Text: "param"
  Context: ...to all input tokens.
The number of tokens Tcreated by patchify is determined
by the patch size hyper[param]eter p. As shown in Figure 4,
halvingpwill quadruple T, and thus at least quadruple total
transforme...

Match 9:
  Page: 4
  Text: "param"
  Context: ...ugh it has a signiﬁcant impact on
Gﬂops, note that changing phas no meaningful impact on
downstream [param]eter counts.
We addp= 2;4;8to the DiT design space.
DiT block design. Following patchify, the input ...

Match 10:
  Page: 5
  Text: "XL/2 adaLN-ZeroFigure 5. Com"
  Context: ...00K 200K 300K 400K
Training Steps20406080100FID-50K
XL/2 In-Context
XL/2 Cross-Attention
XL/2 adaLN
[XL/2 adaLN-ZeroFigure 5. Com]paring different conditioning strategies. adaLN-
Zero outperforms cross-attention and in-context con...

Use search_pdf_next_page, search_pdf_prev_page, or search_pdf_go_page with search_id '6fe46ca7' to navigate.
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Let me search for more specific parameter information:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools search_pdf_content`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	 "pattern": "DiT-XL.*M|XL.*parameters|118.*M|675.*M"
}

```

```json output_result
Search ID: 70df9c6d
Pattern: DiT-XL.*M|XL.*parameters|118.*M|675.*M
Total matches: 42
Page: 1/5
Results per page: 10

Match 1:
  Page: 1
  Text: "DiT-XL/2 models trained on Im"
  Context: ... achieve state-of-the-art image quality. We show selected samples from two
of our class-conditional [DiT-XL/2 models trained on Im]ageNet at 512 512 and 256256 resolution, respectively.
Abstract
We explore a new class of diffusio...

Match 2:
  Page: 2
  Text: "DiT-XL/2, is compute-efﬁcient and outperforms all prior U-Net-based diffusion models, like ADM and LDM"
  Context: ...ing iterations. Performance steadily improves in FID as model ﬂops increase.
Right: Our best model, [DiT-XL/2, is compute-efﬁcient and outperforms all prior U-Net-based diffusion models, like ADM and LDM].
The seminal work of Ho et al. [19] ﬁrst introduced the
U-Net backbone for diffusion models. Having...

Match 3:
  Page: 5
  Text: "118.6 Gﬂops, allowing us to gauge scaling perform"
  Context: ...S, DiT-B, DiT-L and DiT-XL. They cover a
wide range of model sizes and ﬂop allocations, from 0.3
to [118.6 Gﬂops, allowing us to gauge scaling perform]ance.
Table 1 gives details of the conﬁgs.
We add B, S, L and XL conﬁgs to the DiT design space.
Tra...

Match 4:
  Page: 6
  Text: "DiT-XL/2, our most com"
  Context: ...as secondary metrics.
Compute. We implement all models in JAX [1] and train
them using TPU-v3 pods. [DiT-XL/2, our most com]pute-
intensive model, trains at roughly 5.7 iterations/second on a
TPU v3-256 pod with a global bat...

Match 5:
  Page: 6
  Text: "DiT-XL/2 m"
  Context: ...with a global batch size of 256.
5. Experiments
DiT block design. We train four of our highest Gﬂop
[DiT-XL/2 m]odels, each using a different block design—
in-context (119.4 Gﬂops), cross-attention (137.6 Gﬂops),...

Match 6:
  Page: 6
  Text: "118.6 Gﬂops). We m"
  Context: ...19.4 Gﬂops), cross-attention (137.6 Gﬂops),
adaptive layer norm (adaLN, 118.6 Gﬂops) or adaLN-zero
([118.6 Gﬂops). We m]easure FID over the course of training.
Figure 5 shows the results. The adaLN-Zero block yields
lowe...

Match 7:
  Page: 8
  Text: "DiT-XL/2 outperform"
  Context: ...-conditional
generative models. We report results in Table 2. When us-
ing classiﬁer-free guidance, [DiT-XL/2 outperform]s all prior
diffusion models, decreasing the previous best FID-50K of
3.60 achieved by LDM to 2.27. ...

Match 8:
  Page: 8
  Text: "DiT-XL/2 (118.6 Gﬂops) is com"
  Context: ..., decreasing the previous best FID-50K of
3.60 achieved by LDM to 2.27. Figure 2 (right) shows that
[DiT-XL/2 (118.6 Gﬂops) is com]pute-efﬁcient relative to la-
tent space U-Net models like LDM-4 (103.6 Gﬂops) and
substantially mor...

Match 9:
  Page: 9
  Text: "DiT-XL/2 m"
  Context: ... still outperforms
all prior diffusion models with an FID of 2.55.
512512 ImageNet. We train a new [DiT-XL/2 m]odel on
ImageNet at 512512resolution for 3M iterations with
identical hyperparameters as the 25625...

Match 10:
  Page: 12
  Text: "DiT-XL/2 m"
  Context: ...Figure 11. Additional selected samples from our 512 512 and 256256 resolution [DiT-XL/2 m]odels. We use a classiﬁer-free guidance
scale of 6.0 for the 512512model and 4.0 for the 256256mod...

Use search_pdf_next_page, search_pdf_prev_page, or search_pdf_go_page with search_id '70df9c6d' to navigate.
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Let me look for the table with model configurations to get parameter counts:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	 "start_page": 5,
	 "end_page": 6
}

```

```json output_result
PDF total pages: 25
Extracted page range: 5-6

=== Page 5 ===
100K 200K 300K 400K
Training Steps20406080100FID-50K
XL/2 In-Context
XL/2 Cross-Attention
XL/2 adaLN
XL/2 adaLN-ZeroFigure 5. Comparing different conditioning strategies. adaLN-
Zero outperforms cross-attention and in-context conditioning at all
stages of training.
–Cross-attention block. We concatenate the embeddings
oftandcinto a length-two sequence, separate from
the image token sequence. The transformer block is
modiﬁed to include an additional multi-head cross-
attention layer following the multi-head self-attention
block, similar to the original design from Vaswani et
al.[60], and also similar to the one used by LDM for
conditioning on class labels. Cross-attention adds the
most Gﬂops to the model, roughly a 15% overhead.
–Adaptive layer norm (adaLN) block. Following
the widespread usage of adaptive normalization lay-
ers [40] in GANs [2, 28] and diffusion models with U-
Net backbones [9], we explore replacing standard layer
norm layers in transformer blocks with adaptive layer
norm (adaLN). Rather than directly learn dimension-
wise scale and shift parameters and, we regress
them from the sum of the embedding vectors of tand
c. Of the three block designs we explore, adaLN adds
the least Gﬂops and is thus the most compute-efﬁcient.
It is also the only conditioning mechanism that is re-
stricted to apply the same function to all tokens.
–adaLN-Zero block. Prior work on ResNets has found
that initializing each residual block as the identity
function is beneﬁcial. For example, Goyal et al. found
that zero-initializing the ﬁnal batch norm scale factor 
in each block accelerates large-scale training in the su-
pervised learning setting [13]. Diffusion U-Net mod-
els use a similar initialization strategy, zero-initializing
the ﬁnal convolutional layer in each block prior to any
residual connections. We explore a modiﬁcation of
the adaLN DiT block which does the same. In addi-
tion to regressing and, we also regress dimension-
wise scaling parameters that are applied immediately
prior to any residual connections within the DiT block.Model Layers N Hidden size dHeads Gﬂops (I=32,p=4)
DiT-S 12 384 6 1.4
DiT-B 12 768 12 5.6
DiT-L 24 1024 16 19.7
DiT-XL 28 1152 16 29.1
Table 1. Details of DiT models. We follow ViT [10] model con-
ﬁgurations for the Small (S), Base (B) and Large (L) variants; we
also introduce an XLarge (XL) conﬁg as our largest model.
We initialize the MLP to output the zero-vector for all
; this initializes the full DiT block as the identity
function. As with the vanilla adaLN block, adaLN-
Zero adds negligible Gﬂops to the model.
We include the in-context, cross-attention, adaptive layer
norm and adaLN-Zero blocks in the DiT design space.
Model size. We apply a sequence of NDiT blocks, each
operating at the hidden dimension size d. Following ViT,
we use standard transformer conﬁgs that jointly scale N,
dand attention heads [10, 63]. Speciﬁcally, we use four
conﬁgs: DiT-S, DiT-B, DiT-L and DiT-XL. They cover a
wide range of model sizes and ﬂop allocations, from 0.3
to 118.6 Gﬂops, allowing us to gauge scaling performance.
Table 1 gives details of the conﬁgs.
We add B, S, L and XL conﬁgs to the DiT design space.
Transformer decoder. After the ﬁnal DiT block, we need
to decode our sequence of image tokens into an output noise
prediction and an output diagonal covariance prediction.
Both of these outputs have shape equal to the original spa-
tial input. We use a standard linear decoder to do this; we
apply the ﬁnal layer norm (adaptive if using adaLN) and lin-
early decode each token into a pp2Ctensor, where Cis
the number of channels in the spatial input to DiT. Finally,
we rearrange the decoded tokens into their original spatial
layout to get the predicted noise and covariance.
The complete DiT design space we explore is patch size,
transformer block architecture and model size.
4. Experimental Setup
We explore the DiT design space and study the scaling
properties of our model class. Our models are named ac-
cording to their conﬁgs and latent patch sizes p; for exam-
ple, DiT-XL/2 refers to the XLarge conﬁg and p= 2.
Training. We train class-conditional latent DiT models at
256256 and512512 image resolution on the Ima-
geNet dataset [31], a highly-competitive generative mod-
eling benchmark. We initialize the ﬁnal linear layer with
zeros and otherwise use standard weight initialization tech-
niques from ViT. We train all models with AdamW [29,33].
5

=== Page 6 ===
Figure 6. Scaling the DiT model improves FID at all stages of training. We show FID-50K over training iterations for 12 of our DiT
models. Top row: We compare FID holding patch size constant. Bottom row: We compare FID holding model size constant. Scaling the
transformer backbone yields better generative models across all model sizes and patch sizes.
We use a constant learning rate of 110 4, no weight de-
cay and a batch size of 256. The only data augmentation
we use is horizontal ﬂips. Unlike much prior work with
ViTs [57, 61], we did not ﬁnd learning rate warmup nor
regularization necessary to train DiTs to high performance.
Even without these techniques, training was highly stable
across all model conﬁgs and we did not observe any loss
spikes commonly seen when training transformers. Follow-
ing common practice in the generative modeling literature,
we maintain an exponential moving average (EMA) of DiT
weights over training with a decay of 0.9999. All results
reported use the EMA model. We use identical training hy-
perparameters across all DiT model sizes and patch sizes.
Our training hyperparameters are almost entirely retained
from ADM. We did not tune learning rates, decay/warm-up
schedules, Adam 1/2or weight decays.
Diffusion. We use an off-the-shelf pre-trained variational
autoencoder (V AE) model [30] from Stable Diffusion [48].
The V AE encoder has a downsample factor of 8—given an
RGB image xwith shape 2562563,z=E(x)has
shape 32324. Across all experiments in this section,
our diffusion models operate in this Z-space. After sam-
pling a new latent from our diffusion model, we decode it
to pixels using the V AE decoder x=D(z). We retain diffu-
sion hyperparameters from ADM [9]; speciﬁcally, we use a
tmax= 1000 linear variance schedule ranging from 110 4
to210 2, ADM’s parameterization of the covariance 
and their method for embedding input timesteps and labels.
Evaluation metrics. We measure scaling performance
with Fr ´echet Inception Distance (FID) [18], the standard
metric for evaluating generative models of images.We follow convention when comparing against prior works
and report FID-50K using 250 DDPM sampling steps.
FID is known to be sensitive to small implementation de-
tails [37]; to ensure accurate comparisons, all values re-
ported in this paper are obtained by exporting samples and
using ADM’s TensorFlow evaluation suite [9]. FID num-
bers reported in this section do notuse classiﬁer-free guid-
ance except where otherwise stated. We additionally report
Inception Score [51], sFID [34] and Precision/Recall [32]
as secondary metrics.
Compute. We implement all models in JAX [1] and train
them using TPU-v3 pods. DiT-XL/2, our most compute-
intensive model, trains at roughly 5.7 iterations/second on a
TPU v3-256 pod with a global batch size of 256.
5. Experiments
DiT block design. We train four of our highest Gﬂop
DiT-XL/2 models, each using a different block design—
in-context (119.4 Gﬂops), cross-attention (137.6 Gﬂops),
adaptive layer norm (adaLN, 118.6 Gﬂops) or adaLN-zero
(118.6 Gﬂops). We measure FID over the course of training.
Figure 5 shows the results. The adaLN-Zero block yields
lower FID than both cross-attention and in-context condi-
tioning while being the most compute-efﬁcient. At 400K
training iterations, the FID achieved with the adaLN-Zero
model is nearly half that of the in-context model, demon-
strating that the conditioning mechanism critically affects
model quality. Initialization is also important—adaLN-
Zero, which initializes each DiT block as the identity func-
tion, signiﬁcantly outperforms vanilla adaLN. For the rest
of the paper, all models will use adaLN-Zero DiT blocks.
6
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Perfect! Now I have the parameter information. Let me search for more specific parameter counts:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools search_pdf_content`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	 "pattern": "675M|675 M|parameter count"
}

```

```json output_result
Search ID: fc04fca2
Pattern: 675M|675 M|parameter count
Total matches: 3
Page: 1/1
Results per page: 10

Match 1:
  Page: 3
  Text: "parameter count"
  Context: ...ing architecture
complexity in the image generation literature, it is fairly
common practice to use [parameter count]s. In general, pa-
rameter counts can be poor proxies for the complexity of
image models since they ...

Match 2:
  Page: 4
  Text: "parameter count"
  Context: ...ugh it has a signiﬁcant impact on
Gﬂops, note that changing phas no meaningful impact on
downstream [parameter count]s.
We addp= 2;4;8to the DiT design space.
DiT block design. Following patchify, the input tokens
are...

Match 3:
  Page: 8
  Text: "parameter count"
  Context: ...imately ﬁxed.
DiT Gﬂops are critical to improving performance. The
results of Figure 6 suggest that [parameter count]s do not
uniquely determine the quality of a DiT model. As model
size is held constant and patch siz...


```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Let me check the appendix or other sections for parameter counts:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	 "start_page": 15,
	 "end_page": 17
}

```

```json output_result
PDF total pages: 25
Extracted page range: 15-17

=== Page 15 ===
0 100K 200K 300K 400K 500K 600K 700K 800K 900K 1M
Training Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20S/8 S/4 S/2
0 100K 200K 300K 400K 500K 600K 700K 800K 900K 1M
Training Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20B/8 B/4 B/2
0 100K 200K 300K 400K 500K 600K 700K 800K 900K 1M
Training Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20L/8 L/4 L/2
0 100K 200K 300K 400K 500K 600K 700K 800K 900K 1M
Training Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20XL/8 XL/4 XL/2
0 0.25M 0.50M 0.75M 1.00M 1.25M 1.50M 1.75M 2.00M 2.25M 2.50M 2.75M 3.00M 3.25M 3.50M 3.75M 4.00M 4.25M 4.50M 4.75M 5.00M 5.25M 5.50M 5.75M 6.00M 6.25M 6.50M 6.75M 7.00M
Training Iterations0.130.140.150.160.170.180.190.200.21Training Loss0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K0.140.150.160.170.180.190.20XL/2 (256x256) XL/2 (512x512)Figure 13. Training loss curves for all DiT models. We plot the loss over training for all DiT models (the sum of the noise prediction
mean-squared error and DKL). We also highlight early training behavior. Note that scaled-up DiT models exhibit lower training losses.
15

=== Page 16 ===
DiT-XL/2 512512samples, classiﬁer-free guidance scale = 4.0
Figure 14. Uncurated 512512DiT-XL/2 samples.
Classiﬁer-free guidance scale = 4.0
Class label = “arctic wolf” (270)
Figure 15. Uncurated 512512DiT-XL/2 samples.
Classiﬁer-free guidance scale = 4.0
Class label = “volcano” (980)
16

=== Page 17 ===
DiT-XL/2 512512samples, classiﬁer-free guidance scale = 4.0
Figure 16. Uncurated 512512DiT-XL/2 samples.
Classiﬁer-free guidance scale = 4.0
Class label = “husky” (250)
Figure 17. Uncurated 512512DiT-XL/2 samples.
Classiﬁer-free guidance scale = 4.0
Class label = “sulphur-crested cockatoo” (89)
17
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Let me search more specifically for the parameter count in the appendix or other sections:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools search_pdf_content`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	 "pattern": "675|Parameter|Param|M param"
}

```

```json output_result
Search ID: b7af18d3
Pattern: 675|Parameter|Param|M param
Total matches: 27
Page: 1/3
Results per page: 10

Match 1:
  Page: 2
  Text: "parameter"
  Context: ...] and masked generative mod-
els [4, 14]; the former has shown excellent scaling behavior
up to 20B [parameter]s [62]. Finally, transformers have been
explored in DDPMs to synthesize non-spatial data; e.g., to
g...

Match 2:
  Page: 3
  Text: "parameter"
  Context: ...ing architecture
complexity in the image generation literature, it is fairly
common practice to use [parameter] counts. In general, pa-
rameter counts can be poor proxies for the complexity of
image models since...

Match 3:
  Page: 3
  Text: "parameter"
  Context: ...ta x0:q(xtjx0) =
N(xt;ptx0;(1 t)I), where constants tare hyperpa-
rameters. By applying the re[parameter]ization trick, we can
samplext=ptx0+p1 tt, wheretN(0;I).
Diffusion models are trained to lear...

Match 4:
  Page: 3
  Text: "parameter"
  Context: ...ndp
are Gaussian,DKLcan be evaluated with the mean and co-
variance of the two distributions. By re[parameter]izing as
a noise prediction network , the model can be trained us-
ing simple mean-squared error...

Match 5:
  Page: 3
  Text: "parameter"
  Context: ...ned, new images can
be sampled by initializing xtmax N (0;I)and sampling
xt 1p(xt 1jxt)via the re[parameter]ization trick.
3...

Match 6:
  Page: 4
  Text: "parameter"
  Context: ...to all input tokens.
The number of tokens Tcreated by patchify is determined
by the patch size hyper[parameter] p. As shown in Figure 4,
halvingpwill quadruple T, and thus at least quadruple total
transformer Gﬂ...

Match 7:
  Page: 4
  Text: "m param"
  Context: ...hough it has a signiﬁcant impact on
Gﬂops, note that changing phas no meaningful impact on
downstrea[m param]eter counts.
We addp= 2;4;8to the DiT design space.
DiT block design. Following patchify, the input ...

Match 8:
  Page: 5
  Text: "parameter"
  Context: ...blocks with adaptive layer
norm (adaLN). Rather than directly learn dimension-
wise scale and shift [parameter]s and, we regress
them from the sum of the embedding vectors of tand
c. Of the three block designs...

Match 9:
  Page: 5
  Text: "parameter"
  Context: ...ock which does the same. In addi-
tion to regressing and, we also regress dimension-
wise scaling [parameter]s that are applied immediately
prior to any residual connections within the DiT block.Model Layers ...

Match 10:
  Page: 6
  Text: "parameter"
  Context: ...ng with a decay of 0.9999. All results
reported use the EMA model. We use identical training hy-
per[parameter]s across all DiT model sizes and patch sizes.
Our training hyperparameters are almost entirely retai...

Use search_pdf_next_page, search_pdf_prev_page, or search_pdf_go_page with search_id 'b7af18d3' to navigate.
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Let me check if there's a table with parameter information by searching for "675M":
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools search_pdf_content`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	 "pattern": "675M"
}

```

```json output_result
No matches found for pattern: 675M
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Let me check the appendix for detailed model information:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2212.09748v2.pdf",
	 "start_page": 18,
	 "end_page": 25
}

```

```json output_result
PDF total pages: 25
Extracted page range: 18-25

=== Page 18 ===
DiT-XL/2 512512samples, classiﬁer-free guidance scale = 4.0
Figure 18. Uncurated 512512DiT-XL/2 samples.
Classiﬁer-free guidance scale = 4.0
Class label = “cliff drop-off” (972)
Figure 19. Uncurated 512512DiT-XL/2 samples.
Classiﬁer-free guidance scale = 4.0
Class label = “balloon” (417)
18

=== Page 19 ===
DiT-XL/2 512512samples, classiﬁer-free guidance scale = 4.0
Figure 20. Uncurated 512512DiT-XL/2 samples.
Classiﬁer-free guidance scale = 4.0
Class label = “lion” (291)
Figure 21. Uncurated 512512DiT-XL/2 samples.
Classiﬁer-free guidance scale = 4.0
Class label = “otter” (360)
19

=== Page 20 ===
DiT-XL/2 512512samples, classiﬁer-free guidance scale = 2.0
Figure 22. Uncurated 512512DiT-XL/2 samples.
Classiﬁer-free guidance scale = 2.0
Class label = “red panda” (387)
Figure 23. Uncurated 512512DiT-XL/2 samples.
Classiﬁer-free guidance scale = 2.0
Class label = “panda” (388)
20

=== Page 21 ===
DiT-XL/2 512512samples, classiﬁer-free guidance scale = 1.5
Figure 24. Uncurated 512512DiT-XL/2 samples.
Classiﬁer-free guidance scale = 1.5
Class label = “coral reef” (973)
Figure 25. Uncurated 512512DiT-XL/2 samples.
Classiﬁer-free guidance scale = 1.5
Class label = “macaw” (88)
21

=== Page 22 ===
DiT-XL/2 256256samples, classiﬁer-free guidance scale = 4.0
Figure 26. Uncurated 256256DiT-XL/2 samples.
Classiﬁer-free guidance scale = 4.0
Class label = “macaw” (88)
Figure 27. Uncurated 256256DiT-XL/2 samples.
Classiﬁer-free guidance scale = 4.0
Class label = “dog sled” (537)
22

=== Page 23 ===
DiT-XL/2 256256samples, classiﬁer-free guidance scale = 4.0
Figure 28. Uncurated 256256DiT-XL/2 samples.
Classiﬁer-free guidance scale = 4.0
Class label = “arctic fox” (279)
Figure 29. Uncurated 256256DiT-XL/2 samples.
Classiﬁer-free guidance scale = 4.0
Class label = “loggerhead sea turtle” (33)
23

=== Page 24 ===
DiT-XL/2 256256samples, classiﬁer-free guidance scale = 2.0
Figure 30. Uncurated 256256DiT-XL/2 samples.
Classiﬁer-free guidance scale = 2.0
Class label = “golden retriever” (207)
Figure 31. Uncurated 256256DiT-XL/2 samples.
Classiﬁer-free guidance scale = 2.0
Class label = “lake shore” (975)
24

=== Page 25 ===
DiT-XL/2 256256samples, classiﬁer-free guidance scale = 1.5
Figure 32. Uncurated 256256DiT-XL/2 samples.
Classiﬁer-free guidance scale = 1.5
Class label = “space shuttle” (812)
Figure 33. Uncurated 256256DiT-XL/2 samples.
Classiﬁer-free guidance scale = 1.5
Class label = “ice cream” (928)
25
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Based on the information I found, I can see that the paper doesn't explicitly state the parameter count for DiT-XL/2. However, I can estimate it from the model configuration. Let me now examine the second paper:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools get_pdf_info`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2406.06525v1.pdf"
}

```

```json output_result
PDF file information:
Total pages: 26
Title: 
Author: 
Creator: LaTeX with hyperref
Creation date: D:20240611014653Z

```

</Expandable>
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2406.06525v1.pdf",
	 "start_page": 1,
	 "end_page": 3
}

```

```json output_result
PDF total pages: 26
Extracted page range: 1-3

=== Page 1 ===
Autoregressive Model Beats Diffusion: Llama for
Scalable Image Generation
Peize Sun1Yi Jiang2†Shoufa Chen1Shilong Zhang1Bingyue Peng2
Ping Luo1∗Zehuan Yuan2∗
1The University of Hong Kong2ByteDance
Codes and models: https://github.com/FoundationVision/LlamaGen
Figure 1: Image generation with vanilla autoregressive models . We show samples from our
class-conditional image (top row) and text-conditional image (bottom row) generation models.
Abstract
We introduce LlamaGen, a new family of image generation models that apply origi-
nal “next-token prediction” paradigm of large language models to visual generation
domain. It is an affirmative answer to whether vanilla autoregressive models, e.g.,
Llama, without inductive biases on visual signals can achieve state-of-the-art image
generation performance if scaling properly. We reexamine design spaces of image
tokenizers, scalability properties of image generation models, and their training
data quality. The outcome of this exploration consists of: (1) An image tokenizer
with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook
usage of 97% on ImageNet benchmark. (2) A series of class-conditional image
generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on
ImageNet 256 ×256 benchmarks, outperforming the popular diffusion models such
as LDM, DiT. (3) A text-conditional image generation model with 775M parame-
ters, from two-stage training on LAION-COCO and high aesthetics quality images,
demonstrating competitive performance of visual quality and text alignment. (4)
We verify the effectiveness of LLM serving frameworks in optimizing the inference
speed of image generation models and achieve 326% - 414% speedup. We release
all models and codes to facilitate open-source community of visual generation and
multimodal foundation models.
∗: Corresponding authors, †: project leadarXiv:2406.06525v1  [cs.CV]  10 Jun 2024

=== Page 2 ===
1 Introduction
Built upon autoregressive models, large language models (LLMs) [Vaswani et al. 2017; Devlin et al.
2018; Radford et al. 2018; Raffel et al. 2020; Radford et al. 2019; Brown et al. 2020; Zhang et al.
2022] generate the text by predicting the next token in a sequence. This “next-token prediction”
paradigm presents unprecedented capabilities in solving language tasks in a human-like conversational
manner [Ouyang et al. 2022; OpenAI 2022, 2023b; Google 2023; Anthropic 2023; Workshop et al.
2022; Touvron et al. 2023a,b; Bai et al. 2023a; Yang et al. 2023; Team 2023; Bi et al. 2024] and
incredible scalability [Kaplan et al. 2020; Henighan et al. 2020; Hoffmann et al. 2022; Wei et al. 2022;
Alabdulmohsin et al. 2022; Chowdhery et al. 2023; Anil et al. 2023], demonstrating a promising path
toward general-purpose artificial intelligence models.
Witnessed the scalability of autoregressive models on large language models, pioneering works
attempt to explore autoregressive models in image generation, for example, VQV AE [Van Den Oord
et al. 2017; Razavi et al. 2019], VQGAN [Esser et al. 2021; Lee et al. 2022], DALL-E [Ramesh et al.
2021], Parti [Yu et al. 2021, 2022]. They introduce image tokenizers to convert continuous images to
discrete tokens, and apply autoregressive models to generate image tokens in the way of next-token
prediction. They demonstrate strong performance among their contemporaries [Brock et al. 2018;
Ho et al. 2020; Dhariwal & Nichol 2021] in the year before 2022. However, their open-source
communities are not well developed, which largely limits their further improvements.
At the same period, another image generation method, diffusion models [Song & Ermon 2019; Ho
et al. 2020; Song et al. 2020; Dhariwal & Nichol 2021; Nichol et al. 2021; Lu et al. 2022a; Ho
et al. 2022a; Ho & Salimans 2022; Rombach et al. 2022; Ramesh et al. 2022; Saharia et al. 2022;
Rombach et al. 2022] develop rapidly. Along with their open-source communities, they dominate
the field of visual generation up to today. However, diffusion models share distinct paradigms with
autoregressive language models, which poses a huge challenge to building a unified model between
language and vision.
In this work, we are committed to pushing the envelope of autoregressive models on image generation
further: continuing its research methodology and contributing to open-source community. Reviewing
the literature on image generation in the year before 2024, we identify three keys to existing advanced
models [Peebles & Xie 2023; Podell et al. 2023; Xue et al. 2023; Chen et al. 2023b,c; Betker et al.
2023; Li et al. 2024; Esser et al. 2024]: 1) well-designed image compressors, 2) scalable image
generation models and 3) high-quality training data. Motivated by this, we reexamine the designs of
image tokenizers (image compressors for autoregressive models), the scalability properties of image
generation models, and the effects of training data.
Towards a potential unified model between language and vision, our design is reducing the inductive
biases on visual signals and adopting the same architecture as LLM. This belongs to a different re-
search philosophy with recent works [Chang et al. 2022; Yu et al. 2023b; Tian et al. 2024] that modify
the architectures under the guidance of vision-oriented designs. For example, MaskGIT [Chang et al.
2022], MAGVIT [Yu et al. 2023a,b] adopt the masked image modeling strategy, V AR [Tian et al.
2024] uses hierarchical multi-scale property. Although they have succeeded in achieving leading
image generation performance, and even better than diffusion models, it is still not clear whether
the original language model architectures are capable of this. Instead, our work reveals that vanilla
autoregressive models that apply the exactly same “next-token prediction” as language models are
also able to achieve state-of-the-art image generation performance. As a bonus, we can leverage the
techniques [Dao et al. 2022; Rasley et al. 2020; Shoeybi et al. 2019; Zhao et al. 2023; Kwon et al.
2023; Chen et al. 2023a; Dettmers 2022] developed in LLM community to optimize the training
recipes and inference speeds of our models.
In summary, our contributions to the community include:
1.Image tokenizer: An image tokenizer with downsample ratio of 16, achieves reconstruction
quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark. With the downsample
ratio of 8, our tokenizer is competitive or even better than continuous V AE [Rombach et al.
2022; Podell et al. 2023; OpenAI 2023a] used in diffusion models. This shows that discrete
representation in image tokenizers is no longer the bottleneck of the image reconstruction.
2.Scalable image generation model: A series of class-conditional image generation models,
ranging from 111M to 3.1B parameters, are developed based on Llama architecture [Touvron
2

=== Page 3 ===
et al. 2023a,b]. The largest model realizes 2.18 FID on ImageNet 256 ×256 benchmarks,
outperforming the popular diffusion models such as LDM [Rombach et al. 2022], DiT [Peebles
& Xie 2023]. This shows that vanilla autoregressive models without inductive biases on visual
signals can serve as the basis of image generation systems.
3.Hiqh-quality training data: A text-conditional image generation model with 775M parameters,
is firstly trained on a 50M subset of LAION-COCO [LAION 2022] and then fine-tuned on
10M internal high aesthetics quality images. It demonstrates competitive performance of visual
quality and text alignment.
4.Optimized inference speed: We adopt vLLM [Kwon et al. 2023], one of the most popular
LLM serving frameworks, to optimize the inference speed of our image generation models, and
remarkable 326% - 414% speedup is achieved.
We release all models and codes to facilitate the open-source community of visual generation and
multimodal foundation models. It is worth noticing that our released models are still behind state-of-
the-art visual generation models based on diffusion models [Alpha-VLLM 2024; Esser et al. 2024;
Brooks et al. 2024]. When more training data and computation resources are available in the future,
large-scale AR-based visual generation models, e.g., above 7B parameters, will be explored.
2 Autoregressive Models for Image Generation
2.1 Overview
Firstly, image pixels x∈RH×W×3are quantized into q∈Qh×wdiscrete tokens by the image
tokenizer [Van Den Oord et al. 2017; Esser et al. 2021; Yu et al. 2021], where h=H/p,w=W/p ,
pis downsample ratio of the image tokenizer, q(i,j)is indices of the image codebook. Then, these
image tokens are reshaped to a sequence of h·wtokens in raster scan ordering and used to train
Transformer [Vaswani et al. 2017]-based autoregressive models.
During image generation, image tokens (q1, q2, . . . , q h·w)are generated by autoregressive mod-
els [Radford et al. 2018, 2019; Brown et al. 2020; Touvron et al. 2023a] in the way of next-token
predictionQh·w
t=1p(qt|q<t, c), where cis class label embedding or text embedding. Finally, these
image tokens are converted to image pixels by the image tokenizer decoder.
2.2 Image Tokenizer
Quantized-Autoencoder architecture. We use the same architecture as VQGAN [Esser et al.
2021], encoder-quantizer-decoder. The encoder and the decoder are ConvNet with downsample ratio
p. The quantizer contains a codebook Z∈RK×CwithKlearnable vectors. The encoder projects
image pixels xto the feature map f. The quantization process maps each vector f(i,j)in the feature
map to the code index q(i,j)of its nearest vector z(i,j)in the codebook. During decoding, the code
index q(i,j)is remapped to the feature vector z(i,j)and the decoder converts these feature vectors
back to the image pixels ˆx.
The codebook has critical effects on image tokenization performance. Following [Yu et al. 2021], we
useℓ2-normalization to codebook vectors, low codebook vector dimension C, and large codebook
sizeK. These designs significantly improve reconstruction quality and codebook usage. More details
will be discussed in experiments.
Training losses. Since quantization is a non-differentiable operation, a straight-through gradient
estimator [Bengio et al. 2013] is used to preserve the gradient from the decoder to the encoder
z=sg[z−f] +f,sg[·]is stop-gradient operation. For codebook learning, LVQ=∥sg[f]−z∥2
2+
β∥f−sg[z]∥2
2, where the second term is commitment loss [Van Den Oord et al. 2017] to force feature
vectors extracted from the encoder to be close to codebook vectors, βis commitment loss weight.
For simplicity, we don’t add entropy loss [Yu et al. 2023a; Chang et al. 2022] in codebook learning.
For image reconstruction training, LAE=ℓ2(x,ˆx)+LP(x,ˆx)+λGLG(ˆx), where ℓ2is a reconstruction
loss on image pixels, LP(·)is a perceptual loss from LPIPS [Zhang et al. 2018], LG(·)is an adversarial
loss from a PatchGAN [Isola et al. 2017] discriminator trained at the same time with the image
tokenizer, and λGis adversarial loss weight.
3
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me search for the experimental results on ImageNet 256x256:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools search_pdf_content`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2406.06525v1.pdf",
	 "pattern": "ImageNet.*256.*FID|FID.*ImageNet.*256|Inception Score"
}

```

```json output_result
Search ID: db5fd17f
Pattern: ImageNet.*256.*FID|FID.*ImageNet.*256|Inception Score
Total matches: 4
Page: 1/1
Results per page: 10

Match 1:
  Page: 3
  Text: "FID on ImageNet 256 ×256"
  Context: ...et al. 2023a,b]. The largest model realizes 2.18 [FID on ImageNet 256 ×256] benchmarks,
outperforming the popular diffusion models such as LDM [Rombach et al. 2022], DiT [Peeb...

Match 2:
  Page: 6
  Text: "Inception Score"
  Context: ...cs. We use Fréchet inception distance (FID) [Heusel et al. 2017] as the main
metric. We also report [Inception Score] (IS) [Salimans et al. 2016], sFID [Nash et al. 2021] and
Precision/Recall [Kynkäänniemi et al. 2019...

Match 3:
  Page: 7
  Text: "Inception Score"
  Context: ...r-free guidance
 (b) top-k sampling
Figure 3: The effect of sampling configuration. We show FID and [Inception Score] of 256 ×256
ImageNet benchmark over different sampling configurations. The model is LlamaGen-L, and...

Match 4:
  Page: 8
  Text: "inception score"
  Context: ...n class-conditional ImageNet 256 ×256 benchmark . Metrics include
Fréchet inception distance (FID), [inception score] (IS), precision and recall. “ ↓” or “↑” indicate lower
or higher values are better. “-re” means usi...


```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Let me look at the results table:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools read_pdf_pages`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2406.06525v1.pdf",
	 "start_page": 8,
	 "end_page": 9
}

```

```json output_result
PDF total pages: 26
Extracted page range: 8-9

=== Page 8 ===
Type Model #Para. FID↓ IS↑ Precision ↑Recall↑
GANBigGAN [Brock et al. 2018] 112M 6.95 224.5 0.89 0.38
GigaGAN [Kang et al. 2023] 569M 3.45 225.5 0.84 0.61
StyleGan-XL [Sauer et al. 2022] 166M 2.30 265.1 0.78 0.53
DiffusionADM [Dhariwal & Nichol 2021] 554M 10.94 101.0 0.69 0.63
CDM [Ho et al. 2022b] − 4.88 158.7 − −
LDM-4 [Rombach et al. 2022] 400M 3.60 247.7 − −
DiT-XL/2 [Peebles & Xie 2023] 675M 2.27 278.2 0.83 0.57
Mask.MaskGIT [Chang et al. 2022] 227M 6.18 182.1 0.80 0.51
MaskGIT-re [Chang et al. 2022] 227M 4.02 355.6 − −
ARVQGAN [Esser et al. 2021] 227M 18.65 80.4 0.78 0.26
VQGAN [Esser et al. 2021] 1.4B 15.78 74.3 − −
VQGAN-re [Esser et al. 2021] 1.4B 5.20 280.3 − −
ViT-VQGAN [Yu et al. 2021] 1.7B 4.17 175.1 − −
ViT-VQGAN-re [Yu et al. 2021] 1.7B 3.04 227.4 − −
RQTran. [Lee et al. 2022] 3.8B 7.55 134.0 − −
RQTran.-re [Lee et al. 2022] 3.8B 3.80 323.7 − −
ARLlamaGen-B (cfg=2.00) 111M 5.46 193.61 0.83 0.45
LlamaGen-L (cfg=2.00) 343M 3.07 256.06 0.83 0.52
LlamaGen-XL (cfg=1.75) 775M 2.62 244.08 0.80 0.57
LlamaGen-XXL (cfg=1.75) 1.4B 2.34 253.90 0.80 0.59
LlamaGen-3B (cfg=1.65) 3.1B 2.18 263.33 0.81 0.58
LlamaGen-3B (cfg=1.75) 3.1B 2.32 280.10 0.82 0.56
LlamaGen-3B (cfg=2.00) 3.1B 2.81 311.59 0.84 0.54
Table 6: Model comparisons on class-conditional ImageNet 256 ×256 benchmark . Metrics include
Fréchet inception distance (FID), inception score (IS), precision and recall. “ ↓” or “↑” indicate lower
or higher values are better. “-re” means using rejection sampling. “cfg” means using classifier-free
guidance. More detailed results are in Appendix.
Effect of classifier-free guidance (CFG). First, as shown in Figure 2, using classifier-free guidance
can significantly enhance the visual quality across all model sizes. Moreover, Figure 3a illustrates
that the model achieves optimal FID at CFG = 2.0 and further increasing CFG would deteriorate FID,
which is consistent with previous findings [Dhariwal & Nichol 2021]. Additionally, the increment in
CFG results in a trade-off between diversity and fidelity, as evidenced by increased precision and
decreased recall, demonstrated in Table 10.
Effect of top-k sampling. As shown in Figure 3b, a small top-k value is not beneficial for FID
and IS. Increasing top-k continuously improves FID but decreases IS, which trades off fidelity for
diversity. We observe a similar trend when changing the parameter of top-p and temperature in
sampling. Since FID is our main metric, we use maximum value as the default top-k value, which is
the whole codebook size.
Comparisons with other image generation methods. In Table 6, we compare with popular image
generation models, including GAN [Brock et al. 2018; Kang et al. 2023; Sauer et al. 2022], Diffusion
models [Dhariwal & Nichol 2021; Ho et al. 2022b; Rombach et al. 2022; Peebles & Xie 2023],
and masked-prediction models [Chang et al. 2022]. Our models exhibit competitive performance
in all metrics of FID, IS, Precision and Recall. Notably, our 3B model outperforms the popular
diffusion models LDM [Rombach et al. 2022], DiT [Peebles & Xie 2023]. This shows that vanilla
autoregressive models can serve as the basis of advanced image generation systems.
When comparing with autoregressive models [Esser et al. 2021; Yu et al. 2021; Lee et al. 2022], our
model outperforms all previous models at different levels of model parameters. This benefits from
better designs of image tokenizers and better scalability of image generation models. We hope our
simple and effective implementation will serve as a solid baseline and help facilitate future research
in autoregressive models for image generations.
8

=== Page 9 ===
A furry, black bear standing in a rocky, weedy, area in the wild.
A kitchen that is in the process of having the floors done
A cutting board topped with bread, meat and vegetables.
Stage I
Stage II
a big purple bus parked in a parking spot
Figure 4: Visualization of two-stage training of text-conditional image generation models.
Comparisons of generated images by models after stage I training and stage II training. The text
prompts are from COCOPrompts.
3.3 Text-conditional Image Generation
Training setup. We adopt a two-stage training strategy. In stage I, the model is trained on a
50M subset of LAION-COCO [LAION 2022] with the image resolution 256 ×256. In Stage II,
the model is fine-tuned on 10M internal high aesthetic quality images with the image resolution
512×512. Examples of training data are shown in the Appendix. The maximum length of text token
embedding is set to 120, and left padding is used to enable batch processing. The text condition
embedding dropout for classifier-free guidance is 0.1. All models are trained with similar settings:
model parameters of 775M, base learning rate of 10−4per 256 batch size, AdamW optimizer with
β1= 0.9,β2= 0.95, decay = 0.05, gradient clipping of 1.0.
Precomputing image codes and text embeddings. We use pre-trained FLAN-T5 XL [Chung et al.
2024] to precompute text embedding of the image captions. For image code, we only extract image
codes of the original image center crop in text-conditional models training.
Fine-tune image tokenizer. Before two-stage training for text-conditional image generation models,
we first fine-tune the image tokenizer on the joint of 50M LAION-COCO and 10M internal high
aesthetic quality data.
Visualizations. In Figure 4, we select text prompts from COCOPrompts [Lin et al. 2014] to generate
images using models after stage I training and stage II training. After stage I training, the model
captures the text-image alignment, while its ability to represent image details is not clear. Stage II
training improves the visual aesthetic quality by a significant margin. We explain this improvement
comes from two aspects: high aesthetic quality images shift the domain, and high image resolution
brings better visual details. We notice that further increasing the image resolution to 1024 ×1024
could bring better visual quality, and we leave it for future research.
More visualizations on PartiPrompts [Yu et al. 2022] are in Appendix. PartiPrompts have more longer
captions than COCOPrompts, and our model demonstrates competitive performance in text-image
alignment for long caption image generation tasks.
Limitation. Due to the training data and model parameters, our text-conditional models have
several limitations, such as text rendering errors, counting errors, and common misconceptions.
These problems are promising to be mitigated when more training data and computation resources
are available in the future.
9
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Perfect! Now I have all the information I need. Let me also check if there are parameter counts mentioned for the DiT model. Let me search for "675M" in the LlamaGen paper to confirm:
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools search_pdf_content`

<Expandable title="Details">
```json arguments
{
	"pdf_file_path": "/workspace/dumps/workspace/2406.06525v1.pdf",
	 "pattern": "675M"
}

```

```json output_result
Search ID: 170917dd
Pattern: 675M
Total matches: 1
Page: 1/1
Results per page: 10

Match 1:
  Page: 8
  Text: "675M"
  Context: ...22b] − 4.88 158.7 − −
LDM-4 [Rombach et al. 2022] 400M 3.60 247.7 − −
DiT-XL/2 [Peebles & Xie 2023] [675M] 2.27 278.2 0.83 0.57
Mask.MaskGIT [Chang et al. 2022] 227M 6.18 182.1 0.80 0.51
MaskGIT-re [Chang e...


```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Excellent! Now I have all the information needed to create the LaTeX table. Based on the papers I analyzed:

1. **DiT-XL/2** from "Scalable Diffusion Models with Transformers":
   - Model: DiT-XL/2 (with classifier-free guidance cfg=1.50)
   - Method Category: Diffusion
   - Parameters: 675M
   - FID-50K: 2.27
   - Inception Score: 278.24

2. **LlamaGen-3B** from "Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation":
   - Model: LlamaGen-3B (with cfg=1.65 - best FID result)
   - Method Category: AR (Autoregressive)
   - Parameters: 3.1B
   - FID-50K: 2.18
   - Inception Score: 263.33

Now let me create the LaTeX table ordered by FID in descending order (higher FID first):
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="-4 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M25.6686 26.0962C25.1812 26.2401 24.4656 26.2563 23.6984 26.145C22.875 26.0256 22.0351 25.7739 21.2096 25.403C22.6817 25.1888 23.8237 25.2548 24.8005 25.6009C25.0319 25.6829 25.412 25.9021 25.6686 26.0962ZM17.4552 24.7459C17.3953 24.7622 17.3363 24.7776 17.2776 24.7939C16.8815 24.9017 16.4961 25.0069 16.1247 25.1005L15.6239 25.2275C14.6165 25.4824 13.5865 25.7428 12.5692 26.0529C12.9558 25.1206 13.315 24.178 13.6667 23.2564C13.9271 22.5742 14.193 21.8773 14.468 21.1894C14.6075 21.4198 14.7531 21.6503 14.9046 21.8814C15.5948 22.9326 16.4624 23.9045 17.4552 24.7459ZM14.8927 14.2326C14.958 15.383 14.7098 16.4897 14.3457 17.5514C13.8972 16.2386 13.6882 14.7889 14.2489 13.6185C14.3927 13.3185 14.5105 13.1581 14.5869 13.0744C14.7049 13.2566 14.8601 13.6642 14.8927 14.2326ZM9.63347 28.8054C9.38148 29.2562 9.12426 29.6782 8.86063 30.0767C8.22442 31.0355 7.18393 32.0621 6.64941 32.0621C6.59681 32.0621 6.53316 32.0536 6.44015 31.9554C6.38028 31.8926 6.37069 31.8476 6.37359 31.7862C6.39161 31.4337 6.85867 30.8059 7.53527 30.2238C8.14939 29.6957 8.84352 29.2262 9.63347 28.8054ZM27.3706 26.1461C27.2889 24.9719 25.3123 24.2186 25.2928 24.2116C24.5287 23.9407 23.6986 23.8091 22.7552 23.8091C21.7453 23.8091 20.6565 23.9552 19.2582 24.2819C18.014 23.3999 16.9392 22.2957 16.1362 21.0733C15.7816 20.5332 15.4628 19.9941 15.1849 19.4675C15.8633 17.8454 16.4742 16.1013 16.3632 14.1479C16.2737 12.5816 15.5674 11.5295 14.6069 11.5295C13.948 11.5295 13.3807 12.0175 12.9194 12.9813C12.0965 14.6987 12.3128 16.8962 13.562 19.5184C13.1121 20.5751 12.6941 21.6706 12.2895 22.7311C11.7861 24.0498 11.2674 25.4103 10.6828 26.7045C9.04334 27.3532 7.69648 28.1399 6.57402 29.1057C5.8387 29.7373 4.95223 30.7028 4.90163 31.7107C4.87693 32.1854 5.03969 32.6207 5.37044 32.9695C5.72183 33.3398 6.16329 33.5348 6.6487 33.5354C8.25189 33.5354 9.79489 31.3327 10.0876 30.8909C10.6767 30.0029 11.2281 29.0124 11.7684 27.8699C13.1292 27.3781 14.5794 27.011 15.985 26.6562L16.4884 26.5283C16.8668 26.4321 17.2601 26.3257 17.6635 26.2153C18.0904 26.0999 18.5296 25.9802 18.976 25.8665C20.4193 26.7844 21.9714 27.3831 23.4851 27.6028C24.7601 27.7883 25.8924 27.6807 26.6589 27.2811C27.3486 26.9219 27.3866 26.3676 27.3706 26.1461ZM30.4755 36.2428C30.4755 38.3932 28.5802 38.5258 28.1978 38.5301H3.74486C1.60224 38.5301 1.47322 36.6218 1.46913 36.2428L1.46884 3.75642C1.46884 1.6039 3.36763 1.4734 3.74457 1.46908H20.263L20.2718 1.4778V7.92396C20.2718 9.21763 21.0539 11.6669 24.0158 11.6669H30.4203L30.4753 11.7218L30.4755 36.2428ZM28.9572 10.1976H24.0169C21.8749 10.1976 21.7453 8.29969 21.7424 7.92417V2.95307L28.9572 10.1976ZM31.9447 36.2428V11.1157L21.7424 0.871022V0.823357H21.6936L20.8742 0H3.74491C2.44954 0 0 0.785336 0 3.75711V36.2435C0 37.5427 0.782956 40 3.74491 40H28.2001C29.4952 39.9997 31.9447 39.2143 31.9447 36.2428Z" fill="#EB5757"/></svg> `pdf-tools search_pdf_content`

<Expandable title="Details">
```json arguments
workspace/survey.tex
\begin{table}[!ht]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        Model & Class & \#Param & FID-50K & Inception Score \\
        \midrule
        DiT-XL/2 & Diffusion & 675M & 2.27 & 278.24 \\
        LlamaGen-3B & AR & 3.1B & 2.18 & 263.33 \\
        \bottomrule
    \end{tabular}
\end{table}

```

```json output_result
Successfully wrote to /workspace/dumps/workspace/survey.tex
```

</Expandable>
</div>

<div className="thinking-box">
🧐`Agent`

Perfect! I have successfully analyzed the papers in your workspace and created a LaTeX table summarizing the experimental results on ImageNet 256×256. Here's what I found:

## Summary of Results

From the two papers in your workspace, I extracted the following models with their best FID results on ImageNet 256×256:

1. **DiT-XL/2** (from "Scalable Diffusion Models with Transformers")
   - Method Category: Diffusion
   - Parameters: 675M
   - FID-50K: 2.27
   - Inception Score: 278.24

2. **LlamaGen-3B** (from "Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation")
   - Method Category: AR (Autoregressive)
   - Parameters: 3.1B  
   - FID-50K: 2.18
   - Inception Score: 263.33

The table has been saved to `survey.tex` in your workspace, ordered by FID in descending order as requested. The LlamaGen-3B model achieves the lowest FID (2.18), while DiT-XL/2 achieves a slightly higher FID (2.27) but with a higher Inception Score (278.24).

Both models represent state-of-the-art performance on the ImageNet 256×256 benchmark, with the autoregressive LlamaGen model slightly outperforming the diffusion-based DiT model in terms of FID.
</div>

<div className="result-box">
<svg width="14px" height="14px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M861.588238 240.133873v-65.792823c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.631049H314.659414v-65.631049c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.792823c-36.317212 0.868255-65.631049 30.539428-65.63105 67.061417v543.745565c0 37.06772 30.155471 67.223191 67.223191 67.223191h696.886045c37.06772 0 67.223191-30.155471 67.223191-67.223191V307.19529c-0.001024-36.52199-29.315885-66.193162-65.633097-67.061417z m-109.385765-65.792823c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m-546.928824 0c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m678.191947 676.600829c0 12.935767-10.532708 23.468476-23.468476 23.468475H163.111076c-12.935767 0-23.468476-10.532708-23.468476-23.468475V307.19529c0-12.402323 9.677764-22.593054 21.877358-23.415233v65.577807c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.631049h393.789368v65.631049c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.577807c12.19857 0.82218 21.877358 11.012911 21.877358 23.415233v543.746589z" fill="#22C67F" /><path d="M706.719439 478.272194l-48.01715-44.741741-182.28128 195.621482-111.468348-122.615387-48.563905 44.148911 159.469116 172.685427z" fill="#74E8AE" /></svg> `claim_done `

<Expandable title="Details">
```json arguments
{}

```

```json output_result
you have claimed the task is done!
```

</Expandable>
</div>

</Accordion>

</AccordionGroup>
