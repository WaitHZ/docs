---
title: "Paper Checker"
description: "Audit and fix all citation and cross-reference commands across LaTeX files."
---

<Card>
<div className="tools-container">
<div className="mcp-servers-container">
<div className="mcp-servers-title">
MCP Servers
</div>
<div className="mcp-servers-grid">
<div className="mcp-server-item">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="20px" height="20px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#0277bd" d="M19.847,41.956c-5.629-0.002-11.259,0.024-16.888-0.013c-2.855-0.019-3.374-0.7-2.731-3.525 c2.178-9.58,4.427-19.143,6.557-28.734C7.356,7.112,8.588,5.975,11.312,6C22.57,6.106,33.829,6.034,45.088,6.046 c2.824,0.003,3.298,0.614,2.664,3.511c-2.058,9.406-4.129,18.809-6.236,28.203c-0.789,3.516-1.697,4.187-5.353,4.195 C30.724,41.966,25.285,41.958,19.847,41.956z"/><path fill="#fafafa" d="M25.057 23.922c-.608-.687-1.114-1.267-1.531-1.732-2.43-2.728-4.656-5.27-7.063-7.869-1.102-1.189-1.453-2.344-.13-3.518 1.307-1.16 2.592-1.058 3.791.277 3.34 3.717 6.676 7.438 10.071 11.104 1.268 1.369.972 2.3-.424 3.315-5.359 3.895-10.687 7.833-16.01 11.778-1.196.887-2.337 1.109-3.304-.201-1.066-1.445-.08-2.305 1.026-3.114 3.955-2.893 7.903-5.798 11.834-8.725C23.865 24.83 24.595 24.267 25.057 23.922zM21.75 37C20.625 37 20 36 20 35s.625-2 1.75-2c4.224 0 6.112 0 9.5 0 1.125 0 1.75 1 1.75 2s-.625 2-1.75 2C29.125 37 25 37 21.75 37z"/></svg>
<span className="mcp-server-name">terminal</span>
</div>
<div className="mcp-server-item">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="20px" height="20px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg>
<span className="mcp-server-name">filesystem</span>
</div>
</div>
</div>
<div className="local-tools-container">
<div className="mcp-servers-title">
Local Tools
</div>
<div className="local-tools-grid">
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 100 100" enable-background="new 0 0 100 100" id="Layer_1" version="1.1" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style={{margin: 0, padding: 0, display: 'inline-block'}}><g><g><rect clip-rule="evenodd" fill="#F2F2F2" fill-rule="evenodd" height="83.437" width="67.025" x="9.012" y="7.604"/><path d="M77.454,92.458H7.595V6.187h69.859V92.458z M10.429,89.624H74.62V9.021H10.429V89.624z"/></g><g><rect clip-rule="evenodd" fill="#FF7C24" fill-rule="evenodd" height="10.481" width="17.952" x="46.695" y="34.866"/></g><g><rect height="2.834" width="19.463" x="20.504" y="35.575"/></g><g><rect height="2.834" width="15.561" x="20.718" y="42.508"/></g><g><rect height="2.833" width="15.562" x="20.813" y="49.514"/></g><g><rect height="2.833" width="27.128" x="20.718" y="56.753"/></g><g><rect height="2.833" width="23.51" x="20.718" y="63.688"/></g><g><rect clip-rule="evenodd" fill="#39B6CC" fill-rule="evenodd" height="9.004" width="44.095" x="20.553" y="19.064"/></g><g><rect height="2.833" width="26.272" x="20.718" y="70.32"/></g><g><rect height="2.834" width="32.8" x="20.718" y="77.253"/></g><g><rect height="2.834" width="3.235" x="38.304" y="42.508"/></g><g><path clip-rule="evenodd" d="M77.931,71.902l4.287,4.427l-6.644,6.437l-4.309-4.457    C74.147,76.998,76.504,74.726,77.931,71.902L77.931,71.902z" fill="#F2F2F2" fill-rule="evenodd"/><path d="M75.542,84.77l-6.692-6.92l1.828-0.831c2.579-1.174,4.706-3.218,5.989-5.756l0.897-1.776l6.656,6.874L75.542,84.77z     M73.584,78.669l2.023,2.091l4.605-4.463l-2.007-2.074C76.994,76.012,75.414,77.531,73.584,78.669z"/></g><g><polygon clip-rule="evenodd" fill="#39B6CC" fill-rule="evenodd" points="83.267,75.319 91.984,84.338 83.247,92.779     74.535,83.761   "/><path d="M83.213,94.783L72.531,83.726l10.771-10.41l10.687,11.056L83.213,94.783z M76.538,83.794l6.744,6.981l6.698-6.472    l-6.748-6.981L76.538,83.794z"/></g><g><path clip-rule="evenodd" d="M66.124,50.799c7.742,0,14.018,6.276,14.018,14.019    s-6.275,14.019-14.018,14.019c-7.743,0-14.019-6.276-14.019-14.019S58.381,50.799,66.124,50.799L66.124,50.799z" fill="#F2F2F2" fill-rule="evenodd"/><path d="M66.124,80.253c-8.511,0-15.435-6.924-15.435-15.435s6.924-15.435,15.435-15.435S81.56,56.307,81.56,64.818    S74.635,80.253,66.124,80.253z M66.124,52.216c-6.949,0-12.601,5.653-12.601,12.602s5.651,12.601,12.601,12.601    c6.948,0,12.602-5.652,12.602-12.601S73.072,52.216,66.124,52.216z"/></g><g><rect height="2.833" width="10.313" x="39.902" y="49.514"/></g><g><path d="M76.404,65.586H73.57c0-0.636-0.068-1.255-0.205-1.84c-0.043-0.186-0.096-0.385-0.169-0.63l2.717-0.808    c0.091,0.304,0.158,0.559,0.215,0.801C76.31,63.901,76.404,64.735,76.404,65.586z M72.438,61.433    c-1.489-2.5-4.203-4.058-7.084-4.061l0.004-2.834c3.871,0.005,7.518,2.091,9.516,5.445L72.438,61.433z"/></g></g></svg>
<span className="local-tool-name">history</span>
</div>
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M861.588238 240.133873v-65.792823c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.631049H314.659414v-65.631049c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.792823c-36.317212 0.868255-65.631049 30.539428-65.63105 67.061417v543.745565c0 37.06772 30.155471 67.223191 67.223191 67.223191h696.886045c37.06772 0 67.223191-30.155471 67.223191-67.223191V307.19529c-0.001024-36.52199-29.315885-66.193162-65.633097-67.061417z m-109.385765-65.792823c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m-546.928824 0c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m678.191947 676.600829c0 12.935767-10.532708 23.468476-23.468476 23.468475H163.111076c-12.935767 0-23.468476-10.532708-23.468476-23.468475V307.19529c0-12.402323 9.677764-22.593054 21.877358-23.415233v65.577807c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.631049h393.789368v65.631049c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.577807c12.19857 0.82218 21.877358 11.012911 21.877358 23.415233v543.746589z" fill="#22C67F" /><path d="M706.719439 478.272194l-48.01715-44.741741-182.28128 195.621482-111.468348-122.615387-48.563905 44.148911 159.469116 172.685427z" fill="#74E8AE" /></svg>
<span className="local-tool-name">claim_done</span>
</div>
<div className="local-tool-item">
<svg width="20px" height="20px" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fillRule="evenodd" clipRule="evenodd" d="M13.0164 2C10.8193 2 9.03825 3.72453 9.03825 5.85185V8.51852H15.9235V9.25926H5.97814C3.78107 9.25926 2 10.9838 2 13.1111L2 18.8889C2 21.0162 3.78107 22.7407 5.97814 22.7407H8.27322V19.4815C8.27322 17.3542 10.0543 15.6296 12.2514 15.6296H19.5956C21.4547 15.6296 22.9617 14.1704 22.9617 12.3704V5.85185C22.9617 3.72453 21.1807 2 18.9836 2H13.0164ZM12.0984 6.74074C12.8589 6.74074 13.4754 6.14378 13.4754 5.40741C13.4754 4.67103 12.8589 4.07407 12.0984 4.07407C11.3378 4.07407 10.7213 4.67103 10.7213 5.40741C10.7213 6.14378 11.3378 6.74074 12.0984 6.74074Z" fill="#327EBD"/><path fillRule="evenodd" clipRule="evenodd" d="M18.9834 30C21.1805 30 22.9616 28.2755 22.9616 26.1482V23.4815L16.0763 23.4815L16.0763 22.7408L26.0217 22.7408C28.2188 22.7408 29.9998 21.0162 29.9998 18.8889V13.1111C29.9998 10.9838 28.2188 9.25928 26.0217 9.25928L23.7266 9.25928V12.5185C23.7266 14.6459 21.9455 16.3704 19.7485 16.3704L12.4042 16.3704C10.5451 16.3704 9.03809 17.8296 9.03809 19.6296L9.03809 26.1482C9.03809 28.2755 10.8192 30 13.0162 30H18.9834ZM19.9015 25.2593C19.1409 25.2593 18.5244 25.8562 18.5244 26.5926C18.5244 27.329 19.1409 27.9259 19.9015 27.9259C20.662 27.9259 21.2785 27.329 21.2785 26.5926C21.2785 25.8562 20.662 25.2593 19.9015 25.2593Z" fill="#FFDA4B"/></svg>
<span className="local-tool-name">python_execute</span>
</div>
<div className="local-tool-item">
<Icon icon="bars-progress" size={20} color="#4286f6" />
<span className="local-tool-name">manage_context</span>
</div>
<div className="local-tool-item">
<Icon icon="filter-list" size={20} color="#4286f6" />
<span className="local-tool-name">handle_overlong_tool_outputs</span>
</div>
</div>
</div>
</div>
</Card>



## Instruction

Help me check my LaTeX paper draft for citation and reference issues. I have several .tex files and a .bib bibliography file. Please verify that all \cite\{\}, \label\{\}, \ref\{\} and \autoref\{\} commands work properly and cross-reference correctly across all files. If any citations or references are broken, misnamed or even incorrect, help me fix them to use the correct names from the .bib file and LaTeX labels.

## Initial State

### Local Workspace

<div className="file-tree">
<a href="https://github.com/hkust-nlp/mcpbench_dev/tree/finalpool_dev/tasks/finalpool/paper-checker/initial_workspace">workspace</a>/
├── figures/
├── appendix/
├── sections/
├── colm2024_conference.bbl
├── colm2024_conference.bib
├── ...
└── natbib.sty
</div>

## Model Trajectory

<AccordionGroup>
<Accordion title="claude4-sonnet">

<Columns cols={3}>
<Card title="Task Completion" icon="x">
Failed
</Card>
<Card title="Tool Calls" icon="wrench">
23
</Card>
<Card title="Turns" icon="arrows-rotate">
24
</Card>
</Columns>

<div className="thinking-box">
🧐`Agent`

I'll help you check your LaTeX paper draft for citation and reference issues. Let me start by exploring your workspace to find the .tex and .bib files.
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem list_directory`

```json
{
	"path": "/workspace/dumps/workspace"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
[DIR] my_paper
```
</summary>
```json
[DIR] my_paper
```
</details>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem directory_tree`

```json
{
	"path": "/workspace/dumps/workspace"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
[
```
</summary>
```json
[
  {
    "name": "my_paper",
    "type": "directory",
    "children": [
      {
        "name": "README.md",
        "type": "file"
      },
      {
        "name": "appendix",
        "type": "directory",
        "children": [
          {
            "name": "embodied.tex",
            "type": "file"
          },
          {
            "name": "learning-effort.tex",
            "type": "file"
          }
        ]
      },
      {
        "name": "colm2024_conference.bbl",
        "type": "file"
      },
      {
        "name": "colm2024_conference.bib",
        "type": "file"
      },
      {
        "name": "colm2024_conference.bst",
        "type": "file"
      },
      {
        "name": "colm2024_conference.sty",
        "type": "file"
      },
      {
        "name": "colm2024_conference.tex",
        "type": "file"
      },
      {
        "name": "fancyhdr.sty",
        "type": "file"
      },
      {
        "name": "figures",
        "type": "directory",
        "children": [
          {
            "name": "icon",
            "type": "directory",
            "children": [
              {
                "name": "book-icon.png",
                "type": "file"
              },
              {
                "name": "brain-icon.png",
                "type": "file"
              },
              {
                "name": "media-icon.png",
                "type": "file"
              },
              {
                "name": "work-icon.png",
                "type": "file"
              },
              {
                "name": "world-icon.png",
                "type": "file"
              }
            ]
          }
        ]
      },
      {
        "name": "math_commands.tex",
        "type": "file"
      },
      {
        "name": "natbib.sty",
        "type": "file"
      },
      {
        "name": "sections",
        "type": "directory",
        "children": [
          {
            "name": "1_introduction.tex",
            "type": "file"
          },
          {
            "name": "2_scenarios.tex",
            "type": "file"
          },
          {
            "name": "3_advanced.tex",
            "type": "file"
          },
          {
            "name": "4_evaluation.tex",
            "type": "file"
          },
          {
            "name": "5_tradeoff.tex",
            "type": "file"
          },
          {
            "name": "conclusion.tex",
            "type": "file"
          }
        ]
      }
    ]
  }
]
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Great! I can see your LaTeX paper structure. Let me read all the relevant files to analyze the citations and references. I'll start with the main .tex files and the bibliography file.
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_multiple_files`

```json
{
	"paths": ["/workspace/dumps/workspace/my_paper/colm2024_conference.tex",
	"/workspace/dumps/workspace/my_paper/colm2024_conference.bib",
	"/workspace/dumps/workspace/my_paper/math_commands.tex"]
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
/workspace/dumps/workspace/my_paper/colm2024_conference.tex:
```
</summary>
```json
/workspace/dumps/workspace/my_paper/colm2024_conference.tex:

\documentclass{article} % For LaTeX2e
\usepackage{colm2024_conference}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amsthm}
\usepackage{xcolor,colortbl}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{fdsymbol}
\usepackage{color, colortbl}
\urlstyle{same}
\newtheorem{definition}{Definition}
% variables
\newcommand{\kn}{\raisebox{-0.9mm}{\includegraphics[width=4mm]{./figures/icon/book-icon.png}}}
\newcommand{\work}{\raisebox{-0.8mm}{\includegraphics[width=4mm]{./figures/icon/work-icon.png}}}
\newcommand{\world}{\raisebox{-0.8mm}{\includegraphics[width=4mm]{./figures/icon/world-icon.png}}}
\newcommand{\modal}{\raisebox{-0.8mm}{\includegraphics[width=4mm]{./figures/icon/media-icon.png}}}
\newcommand{\nn}{\raisebox{-0.8mm}{\includegraphics[width=4mm]{./figures/icon/brain-icon.png}}}
% colors
\definecolor{ao}{rgb}{0.0, 0.5, 0.0}
\definecolor{ballblue}{rgb}{0.13, 0.67, 0.8}
\definecolor{darklavender}{rgb}{0.45, 0.31, 0.59}
\definecolor{chromeyellow}{rgb}{1.0, 0.65, 0.0}
\definecolor{candypink}{rgb}{0.89, 0.44, 0.48}
\definecolor{blue(ncs)}{rgb}{0.258, 0.520, 0.953}
% author comment
\usepackage{color-edits}
\addauthor{gn}{magenta}
\addauthor{zw}{orange}
\addauthor{df}{cyan}
\addauthor{hz}{blue}
\addauthor{zc}{violet}


\title{What Are Tools Anyway? \\A Survey from the Language Model Perspective}
% In the Eyes of Language Models
% From A Language Model Perspective
% A Survey of Tools for LMs

% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{Zora Zhiruo Wang$^{\spadesuit}$ \quad Zhoujun Cheng$^{\vardiamondsuit}$ \quad Hao Zhu$^{\spadesuit}$ \quad
{\bf  Daniel Fried$^{\spadesuit}$} \quad {\bf Graham Neubig}$^{\spadesuit}$ \\
$^{\spadesuit}$Carnegie Mellon University \quad $^{\vardiamondsuit}$Shanghai Jiao Tong University}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\colmfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Language models (LMs) are powerful yet mostly for text generation tasks. Tools have substantially enhanced their performance for tasks that require complex skills.
However, many works adopt the term ``tool'' in different ways, raising the question: \textit{What is a tool anyway?} Subsequently, \textit{where and how do tools help LMs?}
In this survey, we provide a unified definition of tools as external programs used by LMs, and perform a systematic review of LM tooling scenarios and approaches.
Grounded on this review, we empirically study the efficiency of various tooling methods by measuring their required compute and performance gains on various benchmarks, and highlight some challenges and potential future research in the field.% \footnote{\url{https://github.com/zorazrw/awesome-tool-llm}}
\end{abstract}

% ############
\input{sections/1_introduction}
\input{sections/2_scenarios}
\input{sections/3_advanced}
\input{sections/4_evaluation}
\input{sections/5_tradeoff}
\input{sections/conclusion}



\bibliography{colm2024_conference}
\bibliographystyle{colm2024_conference}

\clearpage
\appendix
\input{appendix/learning-effort}
% \input{appendix/embodied}

\end{document}


---
/workspace/dumps/workspace/my_paper/colm2024_conference.bib:
@inproceedings{komeili-etal-2022-internet,
  title={Internet-Augmented Dialogue Generation},
  author={Komeili, Mojtaba and Shuster, Kurt and Weston, Jason},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year={2022},
  url={https://aclanthology.org/2022.acl-long.579},
}

@article{lazaridou2022internetaugmented,
  title={Internet-augmented language models through few-shot prompting for open-domain question answering}, 
  author={Angeliki Lazaridou and Elena Gribovskaya and Wojciech Stokowiec and Nikolai Grigorev},
  year={2022},
  journal={arXiv preprint arXiv:2203.05115},
  url={https://arxiv.org/abs/2203.05115}
}

@article{thoppilan2022lamda,
  title={LaMDA: Language Models for Dialog Applications}, 
  author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Vincent Zhao and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Pranesh Srinivasan and Laichee Man and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
  year={2022},
  journal={arXiv preprint arXiv:2201.08239},
  url={https://arxiv.org/abs/2201.08239}
}

@inproceedings{yao2023react,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik R Narasimhan and Yuan Cao},
  booktitle={The Eleventh International Conference on Learning Representations },
  year={2023},
  url={https://openreview.net/forum?id=WE_vluYUL-X}
}

@article{parisi2022talm,
  title={TALM: Tool Augmented Language Models}, 
  author={Aaron Parisi and Yao Zhao and Noah Fiedel},
  year={2022},
  journal={arXiv preprint arXiv:2205.12255},
  url={https://arxiv.org/abs/2205.12255}
}

@article{schick2023toolformer,
  title={Toolformer: Language Models Can Teach Themselves to Use Tools}, 
  author={Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
  year={2023},
  journal={arXiv preprint arXiv:2302.04761},
  url={https://arxiv.org/abs/2302.04761}
}

@article{paranjape2023art,
  title={ART: Automatic multi-step reasoning and tool-use for large language models}, 
  author={Bhargavi Paranjape and Scott Lundberg and Sameer Singh and Hannaneh Hajishirzi and Luke Zettlemoyer and Marco Tulio Ribeiro},
  year={2023},
  journal={arXiv preprint arXiv:2303.09014},
  url={https://arxiv.org/abs/2303.09014}
}

@article{nakano2022webgpt,
  title={WebGPT: Browser-assisted question-answering with human feedback}, 
  author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
  year={2022},
  journal={arXiv preprint arXiv:2112.09332},
  url={https://arxiv.org/abs/2112.09332}
}

@inproceedings{wang-etal-2022-super,
    title={Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},
    author={Wang, Yizhong  and Mishra, Swaroop  and Alipoormolabashi, Pegah  and Kordi, Yeganeh  and Mirzaei, Amirreza  and Naik, Atharva  and Ashok, Arjun  and Dhanasekaran, Arut Selvan  and Arunkumar, Anjana  and Stap, David  and Pathak, Eshaan  and Karamanolakis, Giannis  and Lai, Haizhi  and Purohit, Ishan  and Mondal, Ishani  and Anderson, Jacob  and Kuznia, Kirby  and Doshi, Krima  and Pal, Kuntal Kumar  and Patel, Maitreya  and Moradshahi, Mehrad  and Parmar, Mihir  and Purohit, Mirali  and Varshney, Neeraj  and Kaza, Phani Rohitha  and Verma, Pulkit  and Puri, Ravsehaj Singh  and Karia, Rushang  and Doshi, Savan  and Sampat, Shailaja Keyur  and Mishra, Siddhartha  and Reddy A, Sujan  and Patro, Sumanta  and Dixit, Tanay  and Shen, Xudong},
    booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
    year={2022},
    url={https://aclanthology.org/2022.emnlp-main.340},
}

@inproceedings{li-etal-2023-api,
    title={API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs},
    author={Li, Minghao  and Zhao, Yingxiu  and Yu, Bowen  and Song, Feifan  and Li, Hangyu  and Yu, Haiyang  and Li, Zhoujun  and Huang, Fei  and Li, Yongbin},
    booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
    year={2023},
    url={https://aclanthology.org/2023.emnlp-main.187},
}

@article{liang2023taskmatrixai,
  title={TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs}, 
  author={Yaobo Liang and Chenfei Wu and Ting Song and Wenshan Wu and Yan Xia and Yu Liu and Yang Ou and Shuai Lu and Lei Ji and Shaoguang Mao and Yun Wang and Linjun Shou and Ming Gong and Nan Duan},
  year={2023},
  journal={arXiv preprint arXiv:2303.16434},
  url={https://arxiv.org/abs/2303.16434}
}

@inproceedings{hao2023toolkengpt,
  title={ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings},
  author={Shibo Hao and Tianyang Liu and Zhen Wang and Zhiting Hu},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=BHXsb69bSx}
}

@inproceedings{Baker2020Emergent,
  title={Emergent Tool Use From Multi-Agent Autocurricula},
  author={Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkxpxJBKwS}
}

@article{wolfson2020break,
  author={Wolfson, Tomer and Geva, Mor and Gupta, Ankit and Gardner, Matt and Goldberg, Yoav and Deutch, Daniel and Berant, Jonathan},
  title={Break It Down: A Question Understanding Benchmark},
  journal={Transactions of the Association for Computational Linguistics},
  year={2020},
  url={https://doi.org/10.1162/tacl\_a\_00309},
}

@inproceedings{zettlemoyer2007online,
  title={Online learning of relaxed CCG grammars for parsing to logical form},
  author={Zettlemoyer, Luke and Collins, Michael},
  booktitle={Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},
  pages={678--687},
  year={2007}
}

@inproceedings{liang2011learning,
  title={Learning minimal abstractions},
  author={Liang, Percy and Tripp, Omer and Naik, Mayur},
  booktitle={Proceedings of the 38th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
  pages={31--42},
  year={2011}
}

@article{artzi2013weakly,
  author={Artzi, Yoav and Zettlemoyer, Luke},
  title={Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions},
  journal={Transactions of the Association for Computational Linguistics},
  year={2013},
  url={https://doi.org/10.1162/tacl\_a\_00209},
}

@article{zhang2023toolcoder,
  title={ToolCoder: Teach Code Generation Models to use API search tools}, 
  author={Kechi Zhang and Huangzhao Zhang and Ge Li and Jia Li and Zhuo Li and Zhi Jin},
  year={2023},
  journal={arXiv preprint arXiv:2305.04032},
  url={https://arxiv.org/abs/2305.04032}
}

@article{wang2024trove,
  title={TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks}, 
  author={Zhiruo Wang and Daniel Fried and Graham Neubig},
  year={2024},
  journal={arXiv preprint arXiv:2401.12869},
  url={https://arxiv.org/abs/2401.12869}
}

@inproceedings{gao2023pal,
  title={Pal: Program-aided language models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={International Conference on Machine Learning},
  pages={10764--10799},
  year={2023},
  organization={PMLR}
}

@article{chen2023program,
  title={Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks},
  author={Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},
  journal={Transactions on Machine Learning Research},
  year={2023},
  url={https://openreview.net/forum?id=YfZ4ZPt8zd},
}

@inproceedings{cheng2023binding,
  title={Binding Language Models in Symbolic Languages},
  author={Zhoujun Cheng and Tianbao Xie and Peng Shi and Chengzu Li and Rahul Nadkarni and Yushi Hu and Caiming Xiong and Dragomir Radev and Mari Ostendorf and Luke Zettlemoyer and Noah A. Smith and Tao Yu},
  booktitle={The Eleventh International Conference on Learning Representations },
  year={2023},
  url={https://openreview.net/forum?id=lH1PV42cbF}
}

@inproceedings{cao2023api,
  title={API-Assisted Code Generation for Question Answering on Varied Table Structures},
  author={Cao, Yihan and Chen, Shuyi and Liu, Ryan and Wang, Zhiruo and Fried, Daniel},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://aclanthology.org/2023.emnlp-main.897},
}


@inproceedings{shi2017world,
  title={World of Bits: An Open-Domain Platform for Web-Based Agents},
  author={Tianlin Shi and Andrej Karpathy and Linxi Fan and Jonathan Hernandez and Percy Liang},
  booktitle={Proceedings of the 34th International Conference on Machine Learning},
  year={2017},
  series={Proceedings of Machine Learning Research},
  publisher={PMLR},
  url={https://proceedings.mlr.press/v70/shi17a.html},
}

@inproceedings{zheran2018reinforcement,
  title={Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration},
  author={Evan Zheran Liu and Kelvin Guu and Panupong Pasupat and Percy Liang},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=ryTp3f-0-},
}

@article{zhou2023webarena,
  title={Webarena: A realistic web environment for building autonomous agents},
  author={Zhou, Shuyan and Xu, Frank F and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Bisk, Yonatan and Fried, Daniel and Alon, Uri and others},
  journal={arXiv preprint arXiv:2307.13854},
  year={2023}
}

@inproceedings{wang2023voyager,
  title={Voyager: An Open-Ended Embodied Agent with Large Language Models},
  author={Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar},
  booktitle={NeurIPS 2023 Foundation Models for Decision Making Workshop},
  year={2023},
  url={https://openreview.net/forum?id=P8E4Br72j3}
}

@book{shumaker2011animal,
  title={Animal tool behavior: the use and manufacture of tools by animals},
  author={Shumaker, Robert W and Walkup, Kristina R and Beck, Benjamin B},
  year={2011},
  publisher={JHU Press}
}

@article{tang2023toolalpaca,
  title={ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases},
  author={Qiaoyu Tang and Ziliang Deng and Hongyu Lin and Xianpei Han and Qiao Liang and Boxi Cao and Le Sun},
  journal={arXiv preprint arXiv:2306.05301},
  year={2023}
}

@inproceedings{qin2023toolllm,
  title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs},
  author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Lauren Hong and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=dHng2O0Jjr}
}

@article{huang2024metatool,
  title={MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use},
  author={Yue Huang and Jiawen Shi and Yuan Li and Chenrui Fan and Siyuan Wu and Qihui Zhang and Yixin Liu and Pan Zhou and Yao Wan and Neil Zhenqiang Gong and Lichao Sun},
  journal={arXiv preprint arXiv:2310.03128},
  year={2024}
}

@article{cai2023large,
  title={Large Language Models as Tool Makers}, 
  author={Tianle Cai and Xuezhi Wang and Tengyu Ma and Xinyun Chen and Denny Zhou},
  year={2023},
  journal={arXiv preprint arXiv:2305.17126},
  url={https://arxiv.org/pdf/2305.17126}
}

@article{qian2023creator,
    title={CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation}, 
    author={Cheng Qian and Chi Han and Yi R. Fung and Yujia Qin and Zhiyuan Liu and Heng Ji},
    year={2023},
    journal={arXiv preprint arXiv:2305.14318},
    url={https://arxiv.org/pdf/2305.14318}
}

@article{yuan2023craft,
  title={CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets},
  author={Lifan Yuan and Yangyi Chen and Xingyao Wang and Yi R. Fung and Hao Peng and Heng Ji},
  journal={arXiv preprint arXiv:2309.17428},
  year={2023}
}

@article{srivastava2023beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={BIG-bench authors},
  journal={Transactions on Machine Learning Research},
  year={2023},
  url={https://openreview.net/forum?id=uyTL5Bvosj}
}

@article{stengeleskin2024regal,
  title={ReGAL: Refactoring Programs to Discover Generalizable Abstractions},
  author={Elias Stengel-Eskin and Archiki Prasad and Mohit Bansal},
  journal={arXiv preprint arXiv:2401.16467},
  year={2024}
}

@article{shin2019program,
  title={Program Synthesis and Semantic Parsing with Learned Code Idioms},
  author={Richard Shin and Miltiadis Allamanis and Marc Brockschmidt and Oleksandr Polozov},
  journal={arXiv preprint arXiv:1906.10816},
  year={2019}
}

@article{ellis2023dreamcoder,
  title={DreamCoder: growing generalizable, interpretable knowledge with wake--sleep Bayesian program learning},
  author={Ellis, Kevin and Wong, Lionel and Nye, Maxwell and Sable-Meyer, Mathias and Cary, Luc and Anaya Pozo, Lore and Hewitt, Luke and Solar-Lezama, Armando and Tenenbaum, Joshua B},
  journal={Philosophical Transactions of the Royal Society A},
  year={2023},
}

@inproceedings{pmlr-v139-wong21a,
  title={Leveraging Language to Learn Program Abstractions and Search Heuristics},
  author={Wong, Catherine and Ellis, Kevin M and Tenenbaum, Joshua and Andreas, Jacob},
  booktitle={Proceedings of the 38th International Conference on Machine Learning},
  year={2021},
  series={Proceedings of Machine Learning Research},
  url={https://proceedings.mlr.press/v139/wong21a.html}
}

@article{bowers2023top,
  title={Top-Down Synthesis for Library Learning},
  author={Bowers, Matthew and Olausson, Theo X. and Wong, Lionel and Grand, Gabriel and Tenenbaum, Joshua B. and Ellis, Kevin and Solar-Lezama, Armando},
  year={2023},
  journal={Proc. ACM Program. Lang.},
  publisher={Association for Computing Machinery},
  url={https://doi.org/10.1145/3571234}
}

@article{grand2023lilo,
  title={LILO: Learning Interpretable Libraries by Compressing and Documenting Code},
  author={Grand, Gabriel and Wong, Lionel and Bowers, Matthew and Olausson, Theo X and Liu, Muxin and Tenenbaum, Joshua B and Andreas, Jacob},
  journal={arXiv preprint arXiv:2310.19791},
  year={2023}
}

@article{gupta2022visual,
    title={Visual Programming: Compositional visual reasoning without training}, 
    author={Tanmay Gupta and Aniruddha Kembhavi},
    year={2022},
    journal={arXiv preprint arXiv:2211.11559},
    url={https://arxiv.org/pdf/2211.11559}
}

@article{suris2023vipergpt,
  title={Vipergpt: Visual inference via python execution for reasoning},
  author={Sur{\'\i}s, D{\'\i}dac and Menon, Sachit and Vondrick, Carl},
  journal={arXiv preprint arXiv:2303.08128},
  year={2023}
}

@inproceedings{yang2023gpttools,
  title={{GPT}4Tools: Teaching Large Language Model to Use Tools via Self-instruction},
  author={Rui Yang and Lin Song and Yanwei Li and Sijie Zhao and Yixiao Ge and Xiu Li and Ying Shan},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=cwjh8lqmOL}
}

@inproceedings{liu2023language,
  title={Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment},
  author={Hao Liu and Wilson Yan and Pieter Abbeel},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=mlxRLIy7kc}
}

@inproceedings{yu2023spae,
  title={{SPAE}: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen {LLM}s},
  author={Lijun Yu and Yong Cheng and Zhiruo Wang and Vivek Kumar and Wolfgang Macherey and Yanping Huang and David A Ross and Irfan Essa and Yonatan Bisk and Ming-Hsuan Yang and Kevin Patrick Murphy and Alexander G Hauptmann and Lu Jiang},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=CXPUg86A1D}
}

@article{yang2023mmreact,
  title={MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action},
  author={Zhengyuan Yang and Linjie Li and Jianfeng Wang and Kevin Lin and Ehsan Azarnasab and Faisal Ahmed and Zicheng Liu and Ce Liu and Michael Zeng and Lijuan Wang},
  journal={arXiv preprint arXiv:2303.11381},
  year={2023}
}

@article{gao2023assistgpt,
  title={AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn},
  author={Difei Gao and Lei Ji and Luowei Zhou and Kevin Qinghong Lin and Joya Chen and Zihan Fan and Mike Zheng Shou},
  journal={arXiv preprint arXiv:2306.08640},
  year={2023}
}

@article{xu2023tool,
  title={On the Tool Manipulation Capability of Open-source Large Language Models},
  author={Xu, Qiantong and Hong, Fenglu and Li, Bo and Hu, Changran and Chen, Zhengyu and Zhang, Jian},
  journal={arXiv preprint arXiv:2305.16504},
  year={2023}
}

@article{wang2023mint,
  title={Mint: Evaluating llms in multi-turn interaction with tools and language feedback},
  author={Wang, Xingyao and Wang, Zihan and Liu, Jiateng and Chen, Yangyi and Yuan, Lifan and Peng, Hao and Ji, Heng},
  journal={arXiv preprint arXiv:2309.10691},
  year={2023}
}

@article{mialon2023augmented,
  title={Augmented Language Models: a Survey},
  author={Gr{\'e}goire Mialon and Roberto Dessi and Maria Lomeli and Christoforos Nalmpantis and Ramakanth Pasunuru and Roberta Raileanu and Baptiste Roziere and Timo Schick and Jane Dwivedi-Yu and Asli Celikyilmaz and Edouard Grave and Yann LeCun and Thomas Scialom},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  url={https://openreview.net/forum?id=jh7wH2AzKK}
}

@article{qin2023tool,
  title={Tool Learning with Foundation Models},
  author={Yujia Qin and Shengding Hu and Yankai Lin and Weize Chen and Ning Ding and Ganqu Cui and Zheni Zeng and Yufei Huang and Chaojun Xiao and Chi Han and Yi Ren Fung and Yusheng Su and Huadong Wang and Cheng Qian and Runchu Tian and Kunlun Zhu and Shihao Liang and Xingyu Shen and Bokai Xu and Zhen Zhang and Yining Ye and Bowen Li and Ziwei Tang and Jing Yi and Yuzhang Zhu and Zhenning Dai and Lan Yan and Xin Cong and Yaxi Lu and Weilin Zhao and Yuxiang Huang and Junxi Yan and Xu Han and Xian Sun and Dahai Li and Jason Phang and Cheng Yang and Tongshuang Wu and Heng Ji and Zhiyuan Liu and Maosong Sun},
  journal={arXiv preprint arXiv:2304.08354},
  year={2023}
}

@inproceedings{gao2021simcse,
  title={SimCSE: Simple Contrastive Learning of Sentence Embeddings},
  author={Gao, Tianyu  and Yao, Xingcheng  and Chen, Danqi},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  year={2021},
  url={https://aclanthology.org/2021.emnlp-main.552}
}

@inproceedings{zhou2023docprompting,
  title={DocPrompting: Generating Code by Retrieving the Docs},
  author={Shuyan Zhou and Uri Alon and Frank F. Xu and Zhiruo Wang and Zhengbao Jiang and Graham Neubig},
  booktitle={The Eleventh International Conference on Learning Representations },
  year={2023},
  url={https://openreview.net/forum?id=ZTCxT2t2Ru}
}

@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@inproceedings{chen2017reading,
  title={Reading Wikipedia to Answer Open-Domain Questions},
  author={Chen, Danqi  and Fisch, Adam  and Weston, Jason  and Bordes, Antoine},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year={2017},
  url={https://aclanthology.org/P17-1171}
}

@inproceedings{brown2020language,
  title = {Language Models are Few-Shot Learners},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  year = {2020}
}

@article{hsieh2023tool,
  title={Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models}, 
  author={Cheng-Yu Hsieh and Si-An Chen and Chun-Liang Li and Yasuhisa Fujii and Alexander Ratner and Chen-Yu Lee and Ranjay Krishna and Tomas Pfister},
  journal={arXiv preprint arXiv:2308.00675},
  year={2023}
}

@inproceedings{lu2023chameleon,
  title={Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models},
  author={Pan Lu and Baolin Peng and Hao Cheng and Michel Galley and Kai-Wei Chang and Ying Nian Wu and Song-Chun Zhu and Jianfeng Gao},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=HtqnVSCj3q}
}

@inproceedings{shen2023hugginggpt,
  title={HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face},
  author={Yongliang Shen and Kaitao Song and Xu Tan and Dongsheng Li and Weiming Lu and Yueting Zhuang},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=yHdTscY6Ci}
}

@article{zhuang2023toolqa,
  title={ToolQA: A Dataset for LLM Question Answering with External Tools},
  author={Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},
  journal={arXiv preprint arXiv:2306.13304},
  year={2023}
}

@article{patil2023gorilla,
  title={Gorilla: Large language model connected with massive apis},
  author={Patil, Shishir G and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2305.15334},
  year={2023}
}

@article{shen2023taskbench,
  title={Taskbench: Benchmarking large language models for task automation},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Zhang, Wenqi and Ren, Kan and Yuan, Siyu and Lu, Weiming and Li, Dongsheng and Zhuang, Yueting},
  journal={arXiv preprint arXiv:2311.18760},
  year={2023}
}

@inproceedings{wenzek2020ccnet,
  title={CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data},
  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\'a}n, Francisco and Joulin, Armand and Grave, Edouard},
  booktitle={Proceedings of the Twelfth Language Resources and Evaluation Conference},
  year={2020},
  url={https://aclanthology.org/2020.lrec-1.494}
}

@misc{wang2021gpt-j,
  author={Wang, Ben and Komatsuzaki, Aran},
  title={GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model},
  url={https://github.com/kingoflolz/mesh-transformer-jax},
  year=2021
}

@inproceedings{zelle1996learning,
  title={Learning to parse database queries using inductive logic programming},
  author={Zelle, John M and Mooney, Raymond J},
  booktitle={Proceedings of the national conference on artificial intelligence},
  pages={1050--1055},
  year={1996}
}

@article{zettlemoyer2012learning,
  title={Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars},
  author={Zettlemoyer, Luke S and Collins, Michael},
  journal={arXiv preprint arXiv:1207.1420},
  year={2012}
}

@inproceedings{berant2013semantic,
  title={Semantic parsing on freebase from question-answer pairs},
  author={Berant, Jonathan and Chou, Andrew and Frostig, Roy and Liang, Percy},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1533--1544},
  year={2013}
}

@article{liang2013lambda,
  title={Lambda dependency-based compositional semantics},
  author={Liang, Percy},
  journal={arXiv preprint arXiv:1309.4408},
  year={2013}
}

@article{yu2018spider,
  title={Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task},
  author={Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and others},
  journal={arXiv preprint arXiv:1809.08887},
  year={2018}
}

@inproceedings{xie2022unifiedskg,
  title={UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models},
  author={Xie, Tianbao  and Wu, Chen Henry  and Shi, Peng  and Zhong, Ruiqi  and Scholak, Torsten  and Yasunaga, Michihiro  and Wu, Chien-Sheng  and Zhong, Ming  and Yin, Pengcheng  and Wang, Sida I.  and Zhong, Victor  and Wang, Bailin  and Li, Chengzu  and Boyle, Connor  and Ni, Ansong  and Yao, Ziyu  and Radev, Dragomir  and Xiong, Caiming  and Kong, Lingpeng  and Zhang, Rui  and Smith, Noah A.  and Zettlemoyer, Luke  and Yu, Tao},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  year={2022},
  url={https://aclanthology.org/2022.emnlp-main.39}
}

@article{wang2024executable,
  title={Executable Code Actions Elicit Better LLM Agents},
  author={Xingyao Wang and Yangyi Chen and Lifan Yuan and Yizhe Zhang and Yunzhu Li and Hao Peng and Heng Ji},
  journal={arXiv preprint arXiv:2402.01030},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@misc{hf-models,
  title={Huggingface Models},
  author={Hugging Face},
  url={https://huggingface.co/models},
}

@misc{torchhub,
  title={PyTorch Hub},
  author={The Linux Foundation},
  url={https://pytorch.org/hub}
}

@misc{openai-emb,
  title={New embeddings models and API updates},
  author={OpenAI},
  url={https://openai.com/blog/new-embedding-models-and-api-updates}
}

@misc{tensorflow2015,
  title={TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems},
  url={https://www.tensorflow.org/},
  author={Mart\'{i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and riol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  year={2015},
}

@misc{public-apis,
  title={Public APIs},
  author={public-apis.org},
  url={https://github.com/public-apis}
}

@book{beauchamp2011oxford,
  title={The Oxford handbook of animal ethics},
  author={Beauchamp, Tom L and Frey, Raymond Gillespie},
  year={2011},
  page={232},
  publisher={Oxford University Press, USA}
}


@inproceedings{ahn2022saycan,
    title={Do As I Can and Not As I Say: Grounding Language in Robotic Affordances},
    author={Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Reyes and Pierre Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan and Andy Zeng},
    booktitle={arXiv preprint arXiv:2204.01691},
    year={2022}
}

@inproceedings{shridhar2020alfred,
  title={Alfred: A benchmark for interpreting grounded instructions for everyday tasks},
  author={Shridhar, Mohit and Thomason, Jesse and Gordon, Daniel and Bisk, Yonatan and Han, Winson and Mottaghi, Roozbeh and Zettlemoyer, Luke and Fox, Dieter},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10740--10749},
  year={2020}
}

@article{kolve2017ai2,
  title={Ai2-thor: An interactive 3d environment for visual ai},
  author={Kolve, Eric and Mottaghi, Roozbeh and Han, Winson and VanderBilt, Eli and Weihs, Luca and Herrasti, Alvaro and Deitke, Matt and Ehsani, Kiana and Gordon, Daniel and Zhu, Yuke and others},
  journal={arXiv preprint arXiv:1712.05474},
  year={2017}
}

@article{baker2019emergent,
  title={Emergent tool use from multi-agent autocurricula},
  author={Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
  journal={arXiv preprint arXiv:1909.07528},
  year={2019}
}

@inproceedings{asai2023retrieval,
  title={Retrieval-based Language Models and Applications},
  author={Asai, Akari  and Min, Sewon  and Zhong, Zexuan  and Chen, Danqi},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts)},
  year={2023},
  publisher={Association for Computational Linguistics},
  url={https://aclanthology.org/2023.acl-tutorials.6},
}

@inproceedings{singh2022progprompt,
  title={ProgPrompt: Generating Situated Robot Task Plans using Large Language Models},
  author={Ishika Singh and Valts Blukis and Arsalan Mousavian and Ankit Goyal and Danfei Xu and Jonathan Tremblay and Dieter Fox and Jesse Thomason and Animesh Garg},
  booktitle={Workshop on Language and Robotics at CoRL 2022},
  year={2022},
  url={https://openreview.net/forum?id=3K4-U_5cRw}
}

@inproceedings{liang2023code,
  title={Code as policies: Language model programs for embodied control},
  author={Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={9493--9500},
  year={2023},
  organization={IEEE}
}

@inproceedings{viswanathan2023prompt2model,
  title={Prompt2Model: Generating Deployable Models from Natural Language Instructions},
  author={Viswanathan, Vijay  and Zhao, Chenyang  and Bertsch, Amanda  and Wu, Tongshuang  and Neubig, Graham},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  year={2023},
  publisher={Association for Computational Linguistics},
  url={https://aclanthology.org/2023.emnlp-demo.38},
}

@misc{yasar2022software,
  title={Software testing},
  author={Yasar, Kinza},
  url={https://www.techtarget.com/whatis/definition/software-testing},
  year=2022
}

@article{ehsan2022restful,
  title={RESTful API Testing Methodologies: Rationale, Challenges, and Solution Directions},
  author={Ehsan, Adeel and Abuhaliqa, Mohammed Ahmad M. E. and Catal, Cagatay and Mishra, Deepti},
  journal={Applied Sciences},
  year={2022},
  url={https://www.mdpi.com/2076-3417/12/9/4369},
}

@inproceedings{sharma2018automated,
  title={Automated API testing},
  author={Sharma, Abhinav and Revathi, M and others},
  booktitle={2018 3rd International Conference on Inventive Computation Technologies (ICICT)},
  pages={788--791},
  year={2018},
  organization={IEEE}
}

@article{barbir2007challenges,
  title={Challenges of testing web services and security in SOA implementations},
  author={Barbir, Abbie and Hobbs, Chris and Bertino, Elisa and Hirsch, Frederick and Martino, Lorenzo},
  journal={Test and Analysis of Web Services},
  pages={395--440},
  year={2007},
  publisher={Springer}
}

@inproceedings{liu2024agentbench,
  title={AgentBench: Evaluating {LLM}s as Agents},
  author={Xiao Liu and Hao Yu and Hanchen Zhang and Yifan Xu and Xuanyu Lei and Hanyu Lai and Yu Gu and Hangliang Ding and Kaiwen Men and Kejuan Yang and Shudan Zhang and Xiang Deng and Aohan Zeng and Zhengxiao Du and Chenhui Zhang and Sheng Shen and Tianjun Zhang and Yu Su and Huan Sun and Minlie Huang and Yuxiao Dong and Jie Tang},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=zAdUB0aCTQ}
}

@article{boiko2023autonomous,
  title={Autonomous chemical research with large language models},
  author={Boiko, Daniil A and MacKnight, Robert and Kline, Ben and Gomes, Gabe},
  journal={Nature},
  volume={624},
  number={7992},
  pages={570--578},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{yu2023language,
  title={Language to Rewards for Robotic Skill Synthesis},
  author={Yu, Wenhao and Gileadi, Nimrod and Fu, Chuyuan and Kirmani, Sean and Lee, Kuang-Huei and Arenas, Montse Gonzalez and Chiang, Hao-Tien Lewis and Erez, Tom and Hasenclever, Leonard and Humplik, Jan and others},
  journal={arXiv preprint arXiv:2306.08647},
  year={2023}
}

@book{von1995cognitive,
  title={What is cognitive science?},
  author={Von Eckardt, Barbara},
  year={1995},
  publisher={MIT press}
}

@book{russell2010artificial,
  title={Artificial intelligence a modern approach},
  author={Russell, Stuart J and Norvig, Peter},
  year={2010},
  publisher={London}
}

@article{ye2024tooleyes,
  publtype={informal},
  author={Junjie Ye and Guanyu Li and Songyang Gao and Caishuang Huang and Yilong Wu and Sixian Li and Xiaoran Fan and Shihan Dou and Qi Zhang and Tao Gui and Xuanjing Huang},
  title={ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2401.00741},
  url={https://doi.org/10.48550/arXiv.2401.00741},
}

@article{xi2023rise,
  title={The rise and potential of large language model based agents: A survey},
  author={Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and others},
  journal={arXiv preprint arXiv:2309.07864},
  year={2023}
}

@article{hendrycks2021measuring,
    title={Measuring Mathematical Problem Solving With the MATH Dataset}, 
    author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
    year={2021},
    journal={arXiv preprint arXiv:2103.03874},
    url={https://arxiv.org/pdf/2103.03874}
}

@article{kwiatkowski2019natural,
  title={Natural Questions: A Benchmark for Question Answering Research},
  author={Kwiatkowski, Tom  and Palomaki, Jennimaria  and Redfield, Olivia  and Collins, Michael  and Parikh, Ankur  and Alberti, Chris  and Epstein, Danielle  and Polosukhin, Illia  and Devlin, Jacob  and Lee, Kenton  and Toutanova, Kristina  and Jones, Llion  and Kelcey, Matthew  and Chang, Ming-Wei  and Dai, Andrew M.  and Uszkoreit, Jakob  and Le, Quoc  and Petrov, Slav},
  journal={Transactions of the Association for Computational Linguistics},
  year={2019},
  url={https://aclanthology.org/Q19-1026},
}

@inproceedings{joshi2017triviaqa,
  title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar  and Choi, Eunsol  and Weld, Daniel  and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year={2017},
  url={https://aclanthology.org/P17-1147}
}

@article{lu2023dynamic,
  title={Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning}, 
  author={Pan Lu and Liang Qiu and Kai-Wei Chang and Ying Nian Wu and Song-Chun Zhu and Tanmay Rajpurohit and Peter Clark and Ashwin Kalyan},
  year={2023},
  journal={arXiv preprint arXiv:2209.14610},
  url={https://arxiv.org/pdf/2209.14610}
}

@inproceedings{pasupat-liang-2015-compositional,
  title = "Compositional Semantic Parsing on Semi-Structured Tables",
  author = "Pasupat, Panupong  and Liang, Percy",
  booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  month = jul,
  year = "2015",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P15-1142",
}

@inproceedings{cheng-etal-2022-hitab,
  title = {HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation},
  author = "Cheng, Zhoujun  and Dong, Haoyu  and Wang, Zhiruo  and Jia, Ran  and Guo, Jiaqi  and Gao, Yan  and Han, Shi  and Lou, Jian-Guang  and Zhang, Dongmei",
  booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = may,
  year = "2022",
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6700--6709},
  year={2019}
}

@inproceedings{suhr2019corpus,
  title = "A Corpus for Reasoning about Natural Language Grounded in Photographs",
  author = "Suhr, Alane  and Zhou, Stephanie  and Zhang, Ally  and Zhang, Iris  and Bai, Huajun  and Artzi, Yoav",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  year = "2019",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P19-1644"
}

@misc{eleti2023function,
  title={Function Calling and Other API Updates},
  author={Eleti, Atty and Harris, Jeff and Kilpatrick, Logan},
  url={https://openai.com/blog/function-calling-and-other-api-updates},
  year=2023
}

@misc{SFRAIResearch2024,
  title={SFR-Embedding-Mistral:Enhance Text Retrieval with Transfer Learning},
  author={Meng, Rui and Liu, Ye and Joty, Shafiq Rayhan and Xiong, Caiming and Zhou, Yingbo and Yavuz, Semih},
  howpublished={Salesforce AI Research Blog},
  year={2024},
  url={https://blog.salesforceairesearch.com/sfr-embedded-mistral/}
}

@misc{rogers2001encapsulation,
  title={Encapsulation is not information hiding},
  author={Rogers, Wm. Paul},
  howpublished={JavaWorld},
  year={2001},
  url={https://www.infoworld.com/article/2075271/encapsulation-is-not-information-hiding.html}
}

@inproceedings{zhou2024webarena,
  title={WebArena: A Realistic Web Environment for Building Autonomous Agents},
  author={Shuyan Zhou and Frank F. Xu and Hao Zhu and Xuhui Zhou and Robert Lo and Abishek Sridhar and Xianyi Cheng and Tianyue Ou and Yonatan Bisk and Daniel Fried and Uri Alon and Graham Neubig},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=oKn9c6ytLx}
}

@inproceedings{webshop2022yao,
  title={WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents},
  author={Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  booktitle={Advances in Neural Information Processing Systems},
  publisher={Curran Associates, Inc.},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/file/82ad13ec01f9fe44c01cb91814fd7b8c-Paper-Conference.pdf},
   year={2022}
}

@article{sumers2024cognitive,
  title={Cognitive Architectures for Language Agents},
  author={Theodore Sumers and Shunyu Yao and Karthik Narasimhan and Thomas Griffiths},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2024},
  url={https://openreview.net/forum?id=1i6ZCvflQJ},
  note={Survey Certification}
}

@inproceedings{robinson-etal-2023-chatgpt,
    title = "{C}hat{GPT} {MT}: Competitive for High- (but Not Low-) Resource Languages",
    author = "Robinson, Nathaniel  and
      Ogayo, Perez  and
      Mortensen, David R.  and
      Neubig, Graham",
    editor = "Koehn, Philipp  and
      Haddow, Barry  and
      Kocmi, Tom  and
      Monz, Christof",
    booktitle = "Proceedings of the Eighth Conference on Machine Translation",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.wmt-1.40",
    doi = "10.18653/v1/2023.wmt-1.40",
    pages = "392--418",
    abstract = "Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs{'} MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world{'}s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1{\%} of languages we covered. Our analysis reveals that a language{'}s resource level is the most important feature in determining ChatGPT{'}s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.",
}

@inproceedings{yin-neubig-2017-syntactic,
    title = "A Syntactic Neural Model for General-Purpose Code Generation",
    author = "Yin, Pengcheng  and
      Neubig, Graham",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1041",
    doi = "10.18653/v1/P17-1041",
    pages = "440--450",
    abstract = "We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.",
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{balog2016deepcoder,
  title={Deepcoder: Learning to write programs},
  author={Balog, Matej and Gaunt, Alexander L and Brockschmidt, Marc and Nowozin, Sebastian and Tarlow, Daniel},
  journal={arXiv preprint arXiv:1611.01989},
  year={2016}
}

@inproceedings{wang2023execution,
    title = "Execution-Based Evaluation for Open-Domain Code Generation",
    author = "Wang, Zhiruo  and Zhou, Shuyan  and Fried, Daniel  and Neubig, Graham",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.89",
    pages = "1271--1290",
}

---
/workspace/dumps/workspace/my_paper/math_commands.tex:
%%%%% NEW MATH DEFINITIONS %%%%%

\usepackage{amsmath,amsfonts,bm}

% Mark sections of captions for referring to divisions of figures
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

% Highlight a newly defined term
\newcommand{\newterm}[1]{{\bf #1}}


% Figure reference, lower-case.
\def\figref#1{figure~\ref{#1}}
% Figure reference, capital. For start of sentence
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
% Section reference, lower-case.
\def\secref#1{section~\ref{#1}}
% Section reference, capital.
\def\Secref#1{Section~\ref{#1}}
% Reference to two sections.
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
% Reference to three sections.
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
% Reference to an equation, lower-case.
\def\eqref#1{equation~\ref{#1}}
% Reference to an equation, upper case
\def\Eqref#1{Equation~\ref{#1}}
% A raw reference to an equation---avoid using if possible
\def\plaineqref#1{\ref{#1}}
% Reference to a chapter, lower-case.
\def\chapref#1{chapter~\ref{#1}}
% Reference to an equation, upper case.
\def\Chapref#1{Chapter~\ref{#1}}
% Reference to a range of chapters
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
% Reference to an algorithm, lower-case.
\def\algref#1{algorithm~\ref{#1}}
% Reference to an algorithm, upper case.
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
% Reference to a part, lower case
\def\partref#1{part~\ref{#1}}
% Reference to a part, upper case
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


% Random variables
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
% rm is already a command, just don't name any random variables m
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

% Random vectors
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

% Elements of random vectors
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

% Random matrices
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

% Elements of random matrices
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

% Vectors
\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

% Elements of vectors
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

% Matrix
\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

% Tensor
\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


% Graph
\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

% Sets
\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
% Don't use a set called E, because this would be the same as our symbol
% for expectation.
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

% Entries of a matrix
\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

% entries of a tensor
% Same font as tensor, without \bm wrapper
\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

% The true underlying data generating distribution
\newcommand{\pdata}{p_{\rm{data}}}
% The empirical distribution defined by the training set
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
% The model distribution
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
% Stochastic autoencoder distributions
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} % Laplace distribution

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
% Wolfram Mathworld says $L^2$ is for function spaces and $\ell^2$ is for vectors
% But then they seem to use $L^2$ for vectors throughout the site, and so does
% wikipedia.
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} % See usage in notation.tex. Chosen to match Daphne's book.

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak


```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me read all the section files to analyze the citations and references:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_multiple_files`

```json
{
	"paths": ["/workspace/dumps/workspace/my_paper/sections/1_introduction.tex",
	"/workspace/dumps/workspace/my_paper/sections/2_scenarios.tex",
	"/workspace/dumps/workspace/my_paper/sections/3_advanced.tex",
	"/workspace/dumps/workspace/my_paper/sections/4_evaluation.tex",
	"/workspace/dumps/workspace/my_paper/sections/5_tradeoff.tex",
	"/workspace/dumps/workspace/my_paper/sections/conclusion.tex"]
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
/workspace/dumps/workspace/my_paper/sections/1_introduction.tex:
```
</summary>
```json
/workspace/dumps/workspace/my_paper/sections/1_introduction.tex:
\section{Introduction}
\label{sec:intro}

Language Models (LMs) have become increasingly effective in solving text-generation tasks, by taking in natural language (NL) instructions from users and outputting NL responses, such as answering the ``What is the capital of the US?'' with ``Washington D.C.''. However, LMs often struggle to perform tasks that require complex skills (e.g., math or complex reasoning), and are fundamentally unable to solve other tasks that require access to information not included in their training data (e.g., the current weather or date).

\begin{wrapfigure}[13]{r}{0.42\textwidth}
\vspace{-4mm}    
\includegraphics[width=0.39\textwidth]{./figures/definition.pdf}
\vspace{-2mm}
\caption{Illustration of tools extending and facilitating LM task-solving.}
\label{fig:def}
\end{wrapfigure}

To solve this problem, researchers and practitioners are turning to LMs enhanced with \emph{tools}, which help \textit{facilitate} the task-solving process of LMs, or \textit{extend} LMs with new abilities that the LM does not possess otherwise \citep{qin2023tool,mialon2023augmented}.
For example, a \texttt{calculator} tool may be used to facilitate mathematical calculations, or a \texttt{get\_time()} tool could be used to obtain the current time, which is not available purely through the LM's parameters. 
Inspired by the tools used by humans \citep{shumaker2011animal}, some works introduce application-specific \texttt{software} as tools, such as using a \texttt{search engine} to obtain knowledge \citep{lazaridou2022internetaugmented,komeili-etal-2022-internet}, using a \texttt{translator} to process unknown languages \citep{schick2023toolformer}, or using a \texttt{SQL engine} to query databases \citep{hao2023toolkengpt,zhuang2023toolqa}.
With the development of numerous application programming interfaces (APIs) on the web, many works collect \texttt{API}s as tools to access world data in real-time \citep{balog2016deepcoder,xu2023tool,qin2023toolllm} via multiple modalities \citep{tang2023toolalpaca}, even performing professional activities such as financial analysis \citep{li-etal-2023-api} and digital marketing \citep{huang2024metatool}.
Instead of using black-box APIs with unseen implementations, other works use locally-crafted \texttt{function}s to query over structured tables \citep{wang2024executable,cao2023api} or images \citep{suris2023vipergpt}, where the function tools can be created by human \citep{gupta2022visual} or model experts \citep{wang2023voyager,cai2023large,wang2024trove}.

% issue, motivation; our diff from existing surveys
However, despite this broad and burgeoning area of tool use in LMs, existing surveys only cover certain tool categories such as software \citep{mialon2023augmented} or APIs \citep{qin2023tool}. 
In this paper, we (1) provide a unified view of tool use across a broad range of scenarios, (2) empirically analyze the cost efficiency of tooling methods, to give practical guidance on when and how one should use tools, and (3) offer concrete suggestions for evaluations.

We start with proposing \textit{a unified definition} of tools and explain \textit{why tools help task-solving} (\S\ref{sec:background}).
We first introduce the \emph{basic tool-use paradigm} (\S\ref{sec:basic-paradigm}) and study a variety of tool-using scenarios by enumerating \textit{which tools exist} and \textit{to which tasks they apply} (\S\ref{sec:scenarios}). 
Next, we study advanced approaches for \textit{complex tool usage} and even \textit{make new tools} if they are unavailable for the task (\S\ref{sec:methods}). 
We then summarize existing testbeds and evaluation metrics across LM tooling works, and highlight several missing aspects with concrete metric suggestions (\S\ref{sec:good-tool}).
Lastly, grounding on our empirical analysis about \textit{when tools are effective}, we identify the most efficient tooling approaches and the tasks that benefit most from tools (\S\ref{sub:trade-off}). 



% ##################
\section{Background}
\label{sec:background}

\subsection{What are tools?}
% how existing works define tools
Because LMs are products of the digital world, tools employed by LMs are often computer \textbf{programs} that are executable in corresponding environments, e.g., Python programs are executable in Python environments. 
Referring back to human-used tools, \citet{shumaker2011animal} defines animal tool use as \textit{``the external employment of an unattached or manipulable attached environmental object to alter more efficiently the form, position, or condition of another object.''}
% two properties of tools
Similar to this definition of physical tools, LM-used program tools should also be \textbf{external} to the employer (i.e., the LM) and are part of the environment. 
In the meantime, instead of arbitrary program snippets, a tool is a \textbf{function} (e.g., \texttt{plus\_one}), meaning that it can be applied to other objects (e.g., data) and yield an output (e.g. \texttt{plus\_one}$(1) \rightarrow 2$).

Existing definitions of LM-used tools touch on some of these aspects. 
\citet{qin2023tool} make an intuitive appeal to the similarity to human tool use, but do not define what entails a tool.
\citet{mialon2023augmented} define \textit{a tool} as \textit{``an external module that is typically called using a rule or a special token and whose output is included in the augmented LM's context.''}
We argue for a somewhat more broad definition than this, which encompasses a wide variety of more recent works on tool usage:

\begin{definition}
  \label{def:tools-lm}
  An LM-used tool is a function interface to a computer program that runs \textit{externally} to the LM, where the LM generates the function calls and input arguments in order to use the tool.\footnote{Under our definition, tool functions can be implemented by any means, including symbolic computation or neural networks --- the functions only require a programmatic interface.}
\end{definition} 


\subsection{Why are tools helpful?}
Tools can help task-solving in different ways, depending on the functionality of individual tools. We summarize their functions into three major categories: perception, action, and computation. A tool may belong to one or more of these three categories.

\noindent \textbf{Perception} \quad
Perception tools provide or collect information from the environment. An example is using a \texttt{get\_time()} API to obtain the current time, which is not included in the LM's parametric knowledge learned from training.

\noindent \textbf{Action} \quad
Action tools can exert actions on the environment and change its state. For example, \texttt{turn\_left()} can shift the direction of an embodied agent, or executing \texttt{make\_post(website, post)} can change the content on a \texttt{website}.

\noindent \textbf{Computation} \quad
Computation tools do not necessarily perceive or modify the external environment, but use programs to tackle complex computational tasks. For example, a \texttt{calculator} is a computation tool for mathematical calculation.
Note that the computation also includes more general acts of computing beyond numerical calculation. Therefore, a \texttt{translator} is also a computation tool that can be used to translate between languages.

Note that many tools can fall into multiple categories.
For instance, a \texttt{search engine} is a tool that can perform both computation and perception.
As computation, it measures document similarity and selects relevant ones, but it also perceives the environment (i.e., the web) and fetches data (i.e., returned documents) from it.
In a similar spirit, \textsc{SQL} queries can be used as computation tools (e.g., \texttt{SELECT SQRT(16) / 10 AS result}), perception tools for viewing data (e.g., \texttt{SELECT name FROM data}), action tools to modify data (e.g., \texttt{INSERT INTO data VALUES name}), or all of the above (e.g., \texttt{INSERT INTO counts (grp\_id, grp\_cnt) SELECT grp\_id, COUNT(*) FROM data GROUP BY grp\_id}).

\subsection{Tools and ``Agents''} 
There has recently been a burgeoning of work on LM-powered agents \citep{xi2023rise,sumers2024cognitive}.
\citet{russell2010artificial} define agents as \textit{``anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.''}
According to this definition, agents are programs that use perception tools to perceive the situated environment, or action tools to interact with the environment.
Models that only use computation tools and do not interact with their environments through perception or action tools arguably do not fall under the category of ``agents'' according to this definition.



% ###################################
\section{The basic tool use paradigm}
\label{sec:basic-paradigm}
% figure start
\begin{wrapfigure}[16]{r}{0.42\textwidth}
\vspace{-5mm}
\includegraphics[width=0.40\textwidth]{./figures/call-api.pdf}
\vspace{-2mm}
\caption{The basic tool use paradigm. LM calls \raisebox{0.3mm}{\colorbox{violet!13}{\texttt{check\_weather}}} tool by generating text tokens. This call triggers the server to execute the call and return the output \raisebox{0.3mm}{\colorbox{yellow!33}{sunny}}, using which the LM replaces the API call tokens in the response to the user.}
\label{fig:call-api}
\end{wrapfigure}
% figure end

First, in this section, we show an illustrative example of a basic tool-use paradigm introduced by Toolformer \citep{schick2023toolformer}, which many tool-related works adopt (\autoref{fig:call-api}). Assuming an LM communicates with users mainly in natural language, upon receiving a user query such as ``How is the weather today?'', the LM then proceeds to generate either text or tool calls. In the example, starts with generating a few tokens of text ``It is ...''. When the LM needs to seek external tools to complete the task, e.g., get real-time weather information, it generates tokens of the tool name and corresponding input arguments enclosed with \texttt{(}parentheses\texttt{)} to construct a complete tool calling expression. 
This completed expression will trigger a shift from text-generation mode to tool-execution mode. The server hosting the tool will execute the expression and return the execution result to the LM. 

Taking the example in \autoref{fig:call-api-v0}, the LM sends the \texttt{check\_weather()} call to the weather server and receives the output ``sunny''. The returned result replaces the tool call in the LM-generated tokens (e.g., from ``It is \texttt{check\_weather()}'' to ``It is sunny''), which is used for subsequent steps of generation.
Accordingly, the LM shifts back to the text generation mode and continues to finish the response by generating new text tokens, e.g., adding `today.', and finally returning the response to the user.

In order for LMs to use this basic paradigm of using tools, current works mainly leverage inference-time prompting and training-time learning methods.

\noindent \textbf{Inference-time prompting} \quad
Leveraging the ability of LMs to learn in-context \citep{}, many works provide tool information through a prompt and expect LMs to acquire abilities to use these tools from input contexts.
This is achieved by providing instructions about the task, example pairs of queries and solutions that use tools \citep{gupta2022visual,lu2023chameleon,paranjape2023art,shen2023hugginggpt,yang2023mmreact}, and/or documentation of the tools' functionality \citep{hsieh2023tool}.


\noindent \textbf{Learning by training} \quad
Beyond learning tools from test-time contexts, LMs can learn from examples that use these tools during training.
LMs can simply be trained to generate tool-using solutions, where the examples can be manually annotated by humans \citep{li-etal-2023-api}, synthesized by larger teacher LMs \citep{tang2023toolalpaca,qin2023toolllm,huang2024metatool}, or bootstrapped by the test-time LM itself \citep{schick2023toolformer}.


---
/workspace/dumps/workspace/my_paper/sections/2_scenarios.tex:

\textbf{\begin{table}[t]
\vspace{-5mm}
\small
\begin{center}
    \begin{tabular}{ll}
    \toprule
    \multicolumn{1}{c}{\textbf{Category}} & \multicolumn{1}{c}{\textbf{Example Tools}} \\
    \midrule
    \multirow{3}{*}{\kn ~~Knowledge access} & {\texttt{sql\_executor(query:~str) -> answer:~any}} \\
    {} & {\texttt{search\_engine(query:~str) -> document:~str}} \\
    {} & {\texttt{retriever(query:~str) -> document:~str}} \\
    \midrule
    \multirow{3}{*}{\work ~~Computation activities} & {\texttt{calculator(formula:~str) -> value:~int | float}} \\
    {} & {\texttt{python\_interpreter(program:~str) -> result:~any}} \\
    {} & {\texttt{worksheet.insert\_row(row:~list, index:~int) -> None}} \\
    \midrule
    \multirow{4}{*}{\world ~~Interaction w/ the world} & \texttt{get\_weather(city\_name:~str) -> weather:~str} \\
    {} & \texttt{get\_location(ip:~str) -> location:~str} \\
    {} & \texttt{calendar.fetch\_events(date:~str) -> events:~list} \\
    {} & \texttt{email.verify(address:~str) -> result:~bool} \\
    \midrule
    \multirow{3}{*}{\modal ~~Non-textual modalities} & \texttt{cat\_image.delete(image\_id:~str) -> None} \\
    {} & \texttt{spotify.play\_music(name:~str) -> None} \\
    {} & \texttt{visual\_qa(query:~str, image:~Image) -> answer:~str} \\
    \midrule
    \multirow{2}{*}{\nn ~~Special-skilled LMs} & {\texttt{QA(question:~str) -> answer:~str}} \\
    {} & \texttt{translation(text:~str, language:~str) -> text:~str} \\
    \bottomrule
    \end{tabular}
\end{center}
\vspace{-2mm}
\caption{Exemplar tools for each category.}
\vspace{-15mm}
\label{tab:example-tools}
\end{table}}


\section{Scenarios where tools are useful}
\label{sec:scenarios}

While LMs may easily learn to do many tasks to high accuracy without tools, many other tasks greatly benefit from tool use.
In this section, we study a broad range of scenarios where tools have been used to assist agents. We discuss tasks where human-created, application-specific tools can improve their performance or other positive aspects (\S\ref{sub:app-spec}), as well as scenarios where tools may not be as useful (\S\ref{sub:not-useful}). 



\subsection{Utilizing existing tools for specific applications}
\label{sub:app-spec}

While it is difficult to exhaustively enumerate every scenario where tools could be useful, we summarize some major categories of tool use in \autoref{tab:1} and below.
Note that a tool may fall into one or more categories.


\noindent \textbf{\raisebox{-1mm}{\includegraphics[width=4mm]{./figures/icon/book-icon.png}} Knowledge access} \quad
% unstructured text & structured kb/kg
LMs store limited knowledge during training due to both limits in (i) the data that they are trained on and (ii) the ability of LMs to accurately memorize and utilize all of the data that they see at training time.
Several varieties of tools can be used to alleviate this issue.
\texttt{SQL} and \texttt{SPARL} executors can provide access to data in structured knowledge bases \citep{thoppilan2022lamda,parisi2022talm,hao2023toolkengpt} or knowledge graphs \citep{zhuang2023toolqa}.
An \texttt{search engine} tool  over the Internet \citep{yao2023react,schick2023toolformer,paranjape2023art} can enable LMs to access more up-to-date information \citep{komeili-etal-2022-internet,lazaridou2022internetaugmented}.
More generally, retrieval-augmented generation systems \citep{asai2023retrieval} can be seen as using a \texttt{retriever} tool \citep{mialon2023augmented}.


\noindent \textbf{\raisebox{-0.5mm}{\includegraphics[width=4mm]{./figures/icon/work-icon.png}} Computation activities} \quad
% math
Complex computing activities such as math calculations are known to be challenging for neural LMs \citep{schick2023toolformer}. While even a \texttt{calculator} can enhance LMs' numeracy abilities \citep{parisi2022talm,hao2023toolkengpt}, more generic \texttt{Python} programs are also employed to aid reasoning tasks \citep{gao2023pal,chen2023program,wang2023mint}.
% business tools
For more complex professional jobs, business tools are also applied, such as using \texttt{worksheet} to manipulate Google Sheets \citep{xu2023tool}, or even tools for financial, medical, education, or advertising domains \citep{tang2023toolalpaca,huang2024metatool}.


\noindent \textbf{\raisebox{-0.5mm}{\includegraphics[width=4mm]{./figures/icon/world-icon.png}} Interaction with the world} \quad
LMs without tools are fundamentally unable to perceive and act in the world around them, necessitating tool use where such perception and action is necessary. For instance, LMs can access real-time information such as weather \citep{xu2023tool,tang2023toolalpaca}, or positional knowledge such as location \citep{qin2023toolllm}. On the other hand, LMs can manipulate real-world information such as managing calendars \citep{schick2023toolformer} and emails \citep{qin2023toolllm}.
In addition to web-based activities, LMs can engage in physical activities in embodied environments, such as fishing with rods or mining with axes in the Minecraft world \citep{wang2023voyager}; further propagate to the real-world tasks to perform cooking \citep{singh2022progprompt,shridhar2020alfred}, plotting \citep{liang2023code}, and even conducting chemical research \citep{boiko2023autonomous}.


\noindent \textbf{\raisebox{-0.5mm}{\includegraphics[width=4mm]{./figures/icon/media-icon.png}} Non-textual modalities} \quad
While many LMs only consume and generate texts, some works bring in access to visual \citep{gupta2022visual,suris2023vipergpt}, audio \citep{yang2023mmreact,gao2023assistgpt}, or other modalities. For example, LMs can access images with \texttt{cat\_image} APIs \citep{xu2023tool,tang2023toolalpaca} or songs \citep{huang2024metatool} provided by \texttt{spotify}, even answer questions about them \citep{gupta2022visual,gao2023assistgpt}.


\noindent \textbf{\raisebox{-0.5mm}{\includegraphics[width=4mm]{./figures/icon/brain-icon.png}} Accessing specialized LMs} \quad
Some works propose to use specialized LMs as tools, essentially using the main LM as a task planner to dispatch requests to other LMs.
\citet{schick2023toolformer} propose QA models to fill in factoid details in responses, \citet{thoppilan2022lamda,schick2023toolformer,paranjape2023art} use machine translation models to assist multilingual tasks. Beyond specific tasks, some works adopt multiple neural models from Hugginface or similar platforms \citep{patil2023gorilla,shen2023hugginggpt}, or further fine-tune them on various data \citep{viswanathan2023prompt2model}.
Compared to the base LM, these tool models mainly vary in their specialized skills, and may or may not have substantial architectural differences from the base LMs.


\subsection{Where are tools \emph{not} useful?}
\label{sub:not-useful}

Despite the fact that tools can be helpful under many scenarios discussed above, it is also important to note scenarios where tools are arguably not very helpful.
Some examples of tasks where tools have not (yet) been used to great effect include machine translation, summarization, and sentiment analysis (among others).
These are tasks that are not easy to perform using non-ML methods (c.f.~solving math problems or accessing databases, which can be done using a calculator or SQL), and can be performed with high accuracy by a powerful LM alone.
One intuitive reason is that the tools currently leveraged for these tasks are \textit{neural networks} and have limited advantages over the base LM.
Imagine if we leverage tools on these tasks, the tools would mostly generally be another neural LM with specialized skills, e.g., an LM specifically trained on many summarization datasets to perform this task. However, this special-skilled neural LM may not have significant architectural differences from the base tool-using LM, or be smaller in size or training tokens hence having inferior language modeling abilities in general.
In comparison, the base LM capable of solution planning and tool management, usually are more powerful (e.g., GPT-4) and can achieve reasonable performance on a wide variety of tasks, perhaps even outperforming special-purpose LMs \citep{robinson-etal-2023-chatgpt}.


---
/workspace/dumps/workspace/my_paper/sections/3_advanced.tex:
\section{Advanced tool-use methods}
\label{sec:methods}

Given this understanding of the basic tooling paradigm and the scenarios in which tools are useful, we now discuss more advanced approaches for tools.
Concretely, we study multi-tool selection and usage (\S\ref{sub:tool-select}), complex tooling under programmatic contexts (\S\ref{sub:tools-and-programs}), and creation of tools when they are not available a-priori (\S\ref{sub:make-tool}). 


% #########################
\subsection{Complex tool selection and usage}
\label{sub:tool-select}

Depending on the number of tools available, the system may include an implicit or explicit tool selection module.
If tools are already \textit{designated} for the task \citep{lazaridou2022internetaugmented,thoppilan2022lamda}, then no tool selection is needed.
If \textit{a small number} (e.g., 5--10) of tools are available, metadata and use cases of these tools can be provided as input contexts along with the user query \citep{schick2023toolformer,paranjape2023art}, and LMs can directly select tools from contexts via a standard generation process.
If the toolbox size \textit{further grows} (e.g., to hundreds), fitting all tools into model inputs is not feasible. Thus an extra retrieval step is often incorporated: a retriever model short-lists the most relevant tools and feeds their metadata to the solution-generation LM. Specifically, \citet{zhou2023docprompting,qin2023toolllm} train retriever models that map NL intents to tool documentation.
\citet{yuan2023craft} ask LMs to write hypothetical descriptions and use the SimCSE retriever \citep{} to find similar tools. More easily, one can directly use off-the-shelf embeddings \citep{SFRAIResearch2024,openai-emb} or training-free sparse retrievers \citep{robertson2009probabilistic}.


% \noindent \textbf{Complex, multi-tool usage} \quad
For complex queries that require multiple tools to solve, the common approach so far is to break down the task and tackle each step sequentially \citep{paranjape2023art} by selecting and using tools with intermediate contexts.
However, this sequential multi-turn paradigm may not be reflective of more complex or realistic usage of the involved tools. For example, a user may prefer \textit{nested} function calls \texttt{check\_weather(get\_local\_time(`Pittsburgh'))} to allow information hiding or encapsulation \citep{rogers2001encapsulation}, \textit{parallel} calls to reduce round trips with the API \citep{eleti2023function}, or \textit{iterative} calls \texttt{buy\_ticket(event)} in a loop until it returns \texttt{True} to indicate a successful transaction. 


% ################################
\subsection{Tools in programmatic contexts}
\label{sub:tools-and-programs}

Unlike text-based tasks where tools are auxiliary modules to extend LM abilities, on programmatic tasks, where code LMs can solve the problem by generating programs, tools can be seen as compositions of basic functions.
In this part, we discuss tools in programmatic tasks for domain-specific (\S\ref{}) and general-purpose problems (\S\ref{}).


\noindent \textbf{Focus on varied tools} \quad
Depending on the tasks of interest, existing works focus on different types of tools under programmatic contexts. With the increasing complexity of these tools and presumably a decreasing familiarity of LMs about them, there are works that adopt (i) \textcolor{candypink}{\textit{built-in functions}} of a programming language (PL) to augment LMs in symbolic reasoning, (ii) \textcolor{ao!80}{\textit{external libraries}} in pre-designed packages to tackle complex open-domain coding queries \citep{wang2023execution}, and (iii) \textcolor{blue(ncs)}{\textit{utility functions}} unseen at training time to solve specific tasks.

\begin{figure}[ht]
\centering
\vspace{-1mm}
    \includegraphics[width=\textwidth]{./figures/codelm-tools.pdf}
\vspace{-6mm}
\caption{Relative to what is considered as the base LM or base actions, tools can refer to built-in functions, external libraries, or task-specific utility functions (from left to right).}
\vspace{-1mm}
\label{fig:codelm-tools}
\end{figure}



% ############
\subsubsection{Domain-specific semantic parsing}
\label{sub:domain-spec}

NL-to-code generation systems have been studied for many years on special-domain tasks such as querying databases \citep{zelle1996learning,zettlemoyer2012learning} or knowledge graphs \citep{berant2013semantic}. Code produced by these systems is often domain-specific logical forms (DSL) manually designed by experts, such as lambda expressions \citep{liang2013lambda} or SQL queries \citep{yu2018spider}, and more recently, the QDMR grammar \citep{wolfson2020break} as an extension to SQL.
In addition to knowledge-oriented tasks, many agentic tasks adopt DSL to operate in corresponding environments, such as \texttt{click} or \texttt{type} in web navigation \citep{zheran2018reinforcement,webshop2022yao,zhou2024webarena}, \texttt{placeItem} and \texttt{killMob} in the embodied Minecraft world \citep{wang2023voyager}, or \texttt{set\_joint\_target} for robot dogs \citep{yu2023language}.
Because DSLs are often specific enough to the target problems, most works directly use these \textcolor{candypink}{built-in actions}.
Yet still, for complex task queries, solution programs written in basic DSL actions alone can be hard to interpret or cumbersome to use, e.g., it is hard to tell that the lambda expression \texttt{(fold xs ($\lambda$ (n x) (+ 1 n)) 0)} is to calculate the length of \texttt{xs}.




% ############
\subsubsection{General-purpose code generation}
\label{sub:general-codegen}
Recent code generation systems have expanded from using DSL to more general-purpose PLs such as Python or Java \citep{yin-neubig-2017-syntactic,chen2021evaluating}. These languages enable more programming flexibility and readily apply to versatile scenarios. 
As we have introduced using \textcolor{candypink}{built-in actions} as tools in \S\ref{sub:app-spec}, we discuss more on two other
common categories of tools for code LMs, namely \textcolor{ao!90}{\textit{external libraries}} and task-specific \textcolor{blue(ncs)}{\textit{utility functions}}.


\noindent \textbf{External libraries} \quad
From the usage of PLs, built-in functions are internal to whichever environment, whereas third-party libraries lie externally and need to be imported to tackle specific contexts, such as \autoref{} (middle). Aligning with this conception, \citet{zhang2023toolcoder} use Python libraries such as \texttt{matplotlib} to plot figures and \texttt{pandas} to manage data.


\noindent \textbf{Utility functions} \quad
For more task-specific applications,
expert-crafted utility functions, usually unseen at training time, are incorporated as tools. E.g., in \autoref{fig:codelm-tools} (right), the highlighted \texttt{locate\_objects} function is designed by human experts \citep{gupta2022visual,suris2023vipergpt} to load neural models and perform post-processing to obtain the detected box region. In a similar spirit, \citet{cheng2023binding} use GPT as a tool to query world facts external to the tabular contents, \citet{cao2023api} further design macro operation APIs to support advanced tabular operations.
However, because human tool curation requires expertise and effort, some works explore using LMs to automatically create tools instead.



% ################################

\begin{wrapfigure}[8]{r}{0.40\textwidth}
\vspace{-6mm}
\includegraphics[width=0.39\textwidth]{./figures/make-tool.pdf}
\vspace{-2mm}
\caption{LM makes tools when no tools readily apply to the task.}
% \vspace{-1mm}
\label{fig:make-tool}
\end{wrapfigure}

\subsection{Tool creation and reuse}
\label{sub:make-tool}


While one can readily use tools for tasks equipped with pre-designed tools, for tasks that do not have readily-applicable, human-created tools, some works explore using LMs to make tools and use them.

\noindent \textbf{Domain-specific library abstraction} \quad
Works that use DSLs often compose frequently-used-together actions as shortcut tools. For example, \citet{ellis2023dreamcoder} learn function abstractions such as \texttt{length} and \texttt{count\_to} from lambda primitives (e.g., \texttt{$0$}, \texttt{+}) for the list processing task. 
\citet{pmlr-v139-wong21a,bowers2023top} similarly build functions bottom-up from a large corpus of DSL programs. More recently, \citet{grand2023lilo} use LLMs to abstract libraries with auto-documentation. 
Further for agentic tasks, \citet{zheran2018reinforcement} learn common workflows to guide web navigation, such as composing the basic $\{$\texttt{click}, \texttt{like}$\}$ actions to form a higher-level login action \texttt{click(like(`login'))}.

\noindent \textbf{General-purpose tool making} \quad
Nonetheless, on general-purpose PLs, running the DSL-oriented methods above may expand their search space and limit their scalability. Instead, recent works often leverage LMs' procedural knowledge to alleviate the search issue. 
To start, \citet{wang2023voyager} designs an automatic learning curriculum in Minecraft to make and use Java program tools.
LATM \citep{} use LMs to build, verify, and use Python tools on BigBench \citep{srivastava2023beyond} tasks, where however, all examples require the same single tool hence have limited difficulty.
CREATOR \citep{qian2023creator} extend tool-making to harder tasks such as math and table world problems, and improves task success by creating tools yet repetitively for individual examples, thus CRAFT \citep{yuan2023craft} add heuristic-based training to craft less repetitive tools.
Towards more efficient pipelines, ReGAL \citep{stengeleskin2024regal} learns from refactoring a smaller number of programs, while TroVE \citep{wang2024trove} purely relies on inference-time execution signal and induces reusable tools on-the-fly.


---
/workspace/dumps/workspace/my_paper/sections/4_evaluation.tex:
\section{How to evaluate tool use?}
\label{sec:good-tool}

In this section, we study existing LM-tooling benchmarks (\S\ref{sub:testbeds}) and their evaluation metrics (\S\ref{sub:eval-metrics}), lastly, we discuss the missing yet important evaluation aspects of tools (\S\ref{sub:desired-properties}).

% ############
\subsection{Testbeds for evaluating tools}
\label{sub:testbeds}
LM tool use can be evaluated on (i) repurposed existing datasets that can additionally benefit from tools (\S\ref{sub:existing-dataset}), and (ii) newly crafted benchmarks that necessitate tool use (\S\ref{sub:api-benchmark}).

% ############
\subsubsection{Repurposed Existing Datasets}
\label{sub:existing-dataset}
Many tasks are solvable by using LMs, yet often with great difficulty or inefficiency. Therefore, some works use tool-augmented LMs as an alternative approach to solve these tasks. 

Many of these datasets require \textbf{reasoning}. Starting from when questions are expressed in NL, such as complex reasoning with the Big-bench \citep{srivastava2023beyond} dataset, mathematical problems with the MATH \citep{hendrycks2021measuring} dataset, and reasoning over world knowledge to answer questions in NaturalQuestions \citep{kwiatkowski2019natural} and TriviaQA \citep{joshi2017triviaqa} datasets.
Beyond free-form texts, datasets that require reasoning over \textbf{structured data} can also benefit from tools. These tasks include table-based QA with tabular math world problems in TabMWP \citep{lu2023dynamic}, Wikipedia tables in WTQ \citep{pasupat-liang-2015-compositional}, and complex-structured tables in HiTab \citep{cheng-etal-2022-hitab}.
Beyond the text modality, datasets that require reasoning over \textbf{other modalities} also benefit from modality-extending tools, e.g., answering questions about an image with the GQA \citep{hudson2019gqa} dataset, or image pairs with the NLVR2 dataset \citep{suhr2019corpus}.

Because tool use is proposed as an alternative method to solve these datasets, evaluations of these tool-augmented systems follow the standard evaluation process for individual datasets. Concretely, almost all tasks are measured by answer exact match, either in textual or numerical formats.
Note that, to obtain the final answers for lexical matching evaluations, all tool-calling expressions \textbf{need to be executed}, and the execution outputs are incorporated into the final answers produced by the tool-augmented systems, as introduced in \S\ref{sec:basic-paradigm}.

% ############
\subsubsection{Aggregated API Benchmarks}
\label{sub:api-benchmark}
Existing benchmarks can only benefit from a limited set of tools, yet there are far more tools we can utilize to perform versatile tasks in the real world, particularly the API tools created by human developers spread on the web. 
Therefore, many recent works aggregate API tools from various web sources and create benchmarks for using these APIs, as shown in \autoref{tab:api-benchmarks}.

\begin{table}[ht]
\small
\vspace{-1mm}
\begin{center}
    \begin{tabular}{l|lllc}
    \toprule
    \multicolumn{1}{c|}{\textbf{Benchmark}} & \multicolumn{1}{c}{\textbf{Tool Source}} & \multicolumn{1}{c}{\textbf{Example Curation}} & \textbf{Domain (\S\ref{sub:app-spec})} & \multicolumn{1}{c}{\textbf{Executable}} \\ 
    \midrule
    {\hyperlink{cite.xu2023tool}{ToolBench$_1$}} & {existing dataset} & {adopted, human annotated} & {\work, \world} & {\ding{51}} \\
    {\hyperlink{cite.qin2023toolllm}{ToolBench$_2$}} & {RapidAPI} & {model synthesized} & {\work, \world} & {\ding{51}} \\
    {\hyperlink{cite.zhuang2023toolqa}{ToolQA}} & {existing dataset} & {model synthesized} & {\work, \kn} & {\ding{51}} \\
    {\hyperlink{cite.tang2023toolalpaca}{ToolAlpaca}} & {PublicAPIs} & {model synthesized} & {\kn, \work, \world, \modal} & {\ding{55}} \\
    {\hyperlink{cite.li-etal-2023-api}{API-Bank}} & {PublicAPIs} & {human annotated} & {\work, \world} & {\ding{51}} \\
    {\hyperlink{cite.huang2024metatool}{MetaTool}} & {OpenAI Plugins} & {model synthesized} & {\work, \world, \modal} & {\ding{55}} \\
    % \midrule
    {\hyperlink{cite.patil2023gorilla}{Gorilla}} & {HF, Torch, TF} & {model synthesized} & {\nn} & {\ding{55}} \\
    {\hyperlink{cite.shen2023hugginggpt}{HuggingGPT}} & {HF} & {human annotated} & {\nn} & {$~~$\ding{55}$^{*}$}\\
    {\hyperlink{cite.shen2023taskbench}{Task Bench}} & {HF, PublicAPIs} & {model synthesized} & {\nn, \modal, \world} & {\ding{55}} \\
    \bottomrule
    \end{tabular}
\end{center}
\vspace{-3mm}
\caption{Benchmarks of providing aggregated APIs to LMs as tools. 
HF is short for HuggingFace. `\ding{55}$^{*}$' means that: though tools employed by HuggingGPT are executable, it does not evaluate the execution output due to the cost of hosting and inferencing.}
\vspace{-1mm}
\label{tab:api-benchmarks}
\end{table}



\noindent \textbf{Tool sources} \quad
Tools are mainly aggregated from existing datasets or public APIs.
While \citet{xu2023tool,zhuang2023toolqa} adopt existing datasets and propose alternative methods via tool augmentation, these benchmarks are limited in domains. Several works scrape more APIs from online sources such as Public APIs \citep{tang2023toolalpaca}, RESTful APIs \citep{tang2023toolalpaca}, or the OpenAI plugin list \citep{huang2024metatool}. Beyond human-crafted APIs \citep{li-etal-2023-api}, neural models from ML platforms can be similarly presented in an API format \citep{patil2023gorilla,shen2023hugginggpt,shen2023taskbench}. 
Nonetheless, as tools are collected from heterogeneous sources, it is challenging to select the best benchmark or unify all these varied benchmarks.


\noindent \textbf{Example curation} \quad
Examples can be adopted from existing datasets, annotated by humans, or synthesized by LMs.
While most examples adopted from existing datasets are human annotated \citep{xu2023tool}, only \citet{li-etal-2023-api} do so for scraped APIs, by surveying 500 people and creating 314 dialogues manually.
Most other works prompt GPT models to synthesize examples \citep{qin2023toolllm,tang2023toolalpaca,shen2023taskbench,zhuang2023toolqa, huang2024metatool}, however, leading to issues of \textit{naturalness} and \textit{executability}.

\textbf{First}, LMs are often asked to create examples, even tool outputs in \citet{tang2023toolalpaca}, given a heuristically selected set of tools. This approach leads to potential issues in two-fold: (i) the selected tools may not be used together in practice, and (ii) the synthesized examples may not reflect the \textit{natural use cases} of these tools.
\textbf{Second}, 5 out of 9 benchmarks in \autoref{tab:api-benchmarks} do not support tool execution, to alleviate the cost of hosting multiple APIs, especially when they may fail or produce unstable outputs. For example, the weather returned by the \texttt{check\_weather} API may change over time. This un-executability causes \textit{issues in evaluation}. Instead of matching final execution results using lexical- \citep{li-etal-2023-api} or neural-based metrics \citep{tang2023toolalpaca, qin2023toolllm}, works with unexecutable tools resort to pseudo matching of API calling expressions with lexical~\citep{tang2023toolalpaca,shen2023hugginggpt,huang2024metatool} and syntactical \citep{patil2023gorilla, shen2023taskbench} means.



% #################################
\subsection{What metrics are measured now?}
\label{sub:eval-metrics}

\noindent \textbf{Task completion} \quad
Tools are used to assist task solving. Most works that allow tool execution evaluate the task completion score to quantify the effectiveness of utilizing tools.

\noindent \textbf{Tool selection} \quad
For datasets with execution issues \citep{huang2024metatool,shen2023taskbench}, another common metric is the accuracy of selecting the correct tools. This helps disentangle incorrect tool selection errors from inaccurate tool usage errors. 
Despite that tool selection mainly serves as a proxy for evaluating task completion when having unexecutable tools, it can be seen as a measure of LM planning abilities --- the process of breaking down a task into multiple steps and selecting tools to complete individual steps.

\noindent \textbf{Tool reusability} \quad
While tool reusability is often deemed important in took-making literature \citep{cai2023large,yuan2023craft}, only \citet{wang2024trove} evaluates tool reusability by the size of induced toolboxes over a fixed number of examples. As its literal meaning, reusable tools can be (re)used to solve multiple examples hence having more generic functionalities. Adopting a reusable tool is more efficient than using multiple specific tools, and facilitates human verification in both speed and accuracy dimensions \citep{wang2024trove}.


% #################################
\subsection{What properties are missing?}
\label{sub:desired-properties}

\noindent \textbf{Efficiency of tool integration} \quad
As demonstrated by our empirical study (\S\ref{sub:trade-off}), the benefits brought by the tools come with the cost of additional computation, especially for teaching LMs to use tools via training or prompting. In addition to performance gain, reporting the computation overhead can enable fairer comparisons between different approaches.

\noindent \textbf{Quality of tools} \quad
While existing works mostly focus on how tools improve task accuracy, the \textit{performance of tools} themselves is also important. Tool performance can cover multiple aspects such as completing the call quickly, requiring less computation, and not putting users at risk or failing unexpectedly. 
One way to measure these aspects is to conduct API testing \citep{yasar2022software,ehsan2022restful} on their runtime, memory usage, and success rate.

\noindent \textbf{Reliability of unstable tools} \quad
Particularly for tools that involve \textit{neural models} or \textit{randomized components}, their output quality may be unstable and unpredictable. For example, the \texttt{VQA} tool \citep{gupta2022visual} may answer some questions correctly but others incorrectly.
It is important to \textit{be aware of} this uncertainty in contrast to stable, rule-based tools such as a \texttt{calculator}, further alleviate this instability and guarantee more predictable outputs.

\noindent \textbf{Reproducible testing} \quad
Many tools interact with the real world and may return different results at different times. For example, \texttt{check\_weather} may return ``sunny'' today but ``cloudy'' tomorrow. This irreproducible behavior poses great challenges to creating \textit{static evaluation} benchmarks with reference answers. % For instance, the answer to ``How's the weather today?'' should not be a fixed ``sunny'', because the correct answer may change according to the specific time of evaluation.
While some works alleviate this by evaluating API calls without executing them, a more rigorous method could be \textit{parallel testing} \citep{sharma2018automated} --- executing the model-generated program and the reference program in parallel, and measuring if their final outputs match.

\noindent \textbf{Safe usage} \quad
Most systems may only opt to use tools if they are trusted to be secure \citep{barbir2007challenges}.
At the very least, users favor tools that can be easily understood and verified. Further, systems may need to enforce mutual authentication and ensure data integrity \citep{ehsan2022restful}.
Yet there are more security threats and methods beyond the discussion here. We encourage readers to peruse the referenced works above for thorough studies.


---
/workspace/dumps/workspace/my_paper/sections/5_tradeoff.tex:
\section{Trade-offs in tool usage}
\label{sub:trade-off}

Leveraging tools often brings better performance, however, should we always use tools? More concretely, is the performance gain from using tools worthy of the computation cost spent for LMs to learn to use tools, or the inference cost at test time?
Existing works mainly focus on task accuracy, but a more nuanced picture emerges when we take other factors into account.
We empirically study the performance gain and learning cost of various methods on their experimented datasets in \autoref{tab:api-benchmarks}, using which we discover more efficient (i.e., achieve greater gains with less compute) methods and tasks that benefit more from tools.

\begin{table}[ht]
\vspace{-3mm}
\small
\begin{center}
\resizebox{0.92\linewidth}{!}{
    \begin{tabular}{llcrcrr}
    \toprule
    \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Type}}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Task}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{$\Delta$ Perf.}}} & \multirow{2}{*}{\textbf{\# Params (B)}} & \multicolumn{2}{c}{\textbf{\# Tokens (M)}} \\
    {} & {} & {} & {} & {} & {train} & {test} \\
    \midrule
    \multirowcell{9}{tool\\use} & \multirow{5}{*}{ToolFormer} & \color{blue} {cloze} & {+ 14.7} & {6.7} & {642.1} & {269.0} \\
    {} & {} & \color{ao} {math} & {+ 30.4} & {6.7} & {3864.2} & {421.0} \\
    {} & {} & \color{chromeyellow} {QA} & {+ 5.8} & {6.7} & {1101.2} & {189.0} \\
    {} & {} & \color{red} {multilingual} & \colorbox{red!27}{- 0.2}  & {6.7} & {606.0} & {274.0} \\
    {} & {} & \color{ballblue} {temporal} & {+ 13.0} & {6.7} & {508.8} & {202.0} \\
    \cmidrule{2-7}
    {} & {API-Bank} & {API} & {+ 24.4} & {7} & \textbf{190414.6} & {0.0} \\
    \cmidrule{2-7}
    {} & {ToolAlpaca} & {API} & {+ 45.2} & {7} & \textbf{241889.3} & {0.0} \\
    \cmidrule{2-7}
    {} & \multirow{2}{*}{Chameleon} & {science} & {+ 2.6} & {-} & {0.0} & {88.3} \\
    {} & {} & \color{darklavender} {table} & {+ 1.9} & {-} & {0.0} & {325.9} \\
    \midrule
    \midrule
    \multirowcell{7}{tool\\making} & {LATM} & {BigBench} & {+ 29.1} & {-} & {28.5} & {4720.0} \\
    \cmidrule{2-7}
    {} & \multirow{2}{*}{CREATOR} & \color{ao} {math} & {+ 4.5} & {-} & {0.0} & {5113.6} \\
    {} & {} & \color{darklavender} {table} & \colorbox{chromeyellow!20}{+ 0.0} & {-} & {0.0} & \textbf{6827.6} \\
    \cmidrule{2-7}
    {} & \multirow{2}{*}{CRAFT} & \color{ao} {math} & {+ 13.2} & {-} & {4126.6} & {4098.5} \\
    {} & {} & \color{darklavender} {table} & {+ 17.2} & {-} & {2750.6} & {5018.2} \\
    \cmidrule{2-7}
    {} & \multirow{2}{*}{TroVE} & \color{ao} {math} & {+ 21.0} & {-} & {0.0} & {1825.2} \\
    {} & {} & \color{darklavender} {table} & {+ 12.0} & {-} & {0.0} & {1358.8} \\
    \bottomrule
    \end{tabular}
    }
\end{center}
\vspace{-2mm}
\caption{Computation cost (number of tokens in $M$ and parameters in $B$) of tooling methods and their performance gain on experimented datasets. To fairly compare costs on datasets with different sizes, we report the average number of tokens spent on a testing example.}
\vspace{-3mm}
\label{tab:compute-cost}
\end{table}



For each work and each dataset they experimented with,\footnote{ We did not measure some works due to insufficient resources.} we evaluate the performance gain after LM learned or made tools to solve tasks, compared to the baseline LM with no prior exposure to tool-related information.
We also quantify the computation cost of their tooling approaches during the token-consuming training and inference processes. For works using models with known sizes, we report both (i) the number of tokens in input prompts and outputs, and (ii) the parameters in experimented models to achieve corresponding performance improvements. For methods using the size-unknown GPT-4 model, which are also comparable w.r.t. to model size since they use the same GPT-4 model, we only report the number of tokens processed.
We elaborate more on computation details in \S\ref{app:learning-effort}.





\noindent \textbf{What tasks benefit the most from tools?} \quad
In general, tasks that cover multiple domains experience the highest increase, such as the ToolAlpaca benchmark in tool-using and the BigBench dataset in tool-making scenarios. Nonetheless, substantial gains may be expected
\begin{wrapfigure}[12]{r}{0.33\textwidth}
\vspace{-3mm}
\includegraphics[width=0.31\textwidth]{./figures/toolformer-cost.pdf}
\vspace{-3mm}
\caption{Compute \& performance gain with ToolFormer.}
\vspace{-2mm}
\label{fig:toolformer-cost}
\end{wrapfigure}
on API benchmarks (i.e., API-Bank and ToolAlpaca), because all examples are synthesized use cases for designated tools (\S\ref{sub:app-spec}), no-tool baselines are deprived of necessary components (i.e., tools) to solve the task, therefore achieving much lower accuracy.


On existing benchmarks, the ToolFormer method is the most efficient on MATH problems, showing the highest $30.4$ increase with little computation ($0.17$ MB). While other tasks improve less, multilingual tasks even degrade by $-0.2$ points, despite using a similar amount of compute. This variance across tasks aligns with expectations: using a \texttt{calculator} tool greatly improves the arithmetic ability of probabilistic LMs, which are not naturally suitable for symbolic calculations; however, LMs are originally built to solve language tasks such as machine translation (MT), so assigning the MT task to another (usually smaller) LM may not bring substantial improvements.


\begin{wrapfigure}[14]{r}{0.33\textwidth}
\vspace{-2mm}
\includegraphics[width=0.32\textwidth]{./figures/tool-make-cost.pdf}
\vspace{-3mm}
\caption{Comparing different tool-making methods.}
\vspace{-1mm}
\label{fig:tool-make-cost}
\end{wrapfigure}
\noindent \textbf{What methods are efficient in tool-making?} \quad
While it is hard to conduct fair comparisons for many works experimenting on different datasets, in tool-making scenarios (\autoref{fig:tool-make-cost}), the three methods (Creator, CRAFT, \textsc{TroVE}) experiment on the same MATH and TabMWP datasets, thus enabling fair comparisons in both cost and performance dimensions.
\textsc{TroVE} appears to be the most efficient method in general, costing only $1.2$--$1.4$K tokens while improving the performance by $12.0$--$21.0$ points in accuracy. 
In contrast, CREATOR and CRAFT are less efficient, costing $3.8$--$6.0$ times of compute, yet achieve only minimal ($0.0$--$4.5$\%) or comparable ($4.1$--$5.0$\%) accuracy increases.


\noindent \textbf{Training-time vs inference-time cost} \quad
Training-time and inference-time costs may not be equally important to many practitioners, since inference may be run many times but training often only needs to be done once.\footnote{Another measure of the inference process is latency, which also heavily depends on implementation or hardware choices. We do not report latency since these methods are implemented differently.} If we only consider inference-time cost in \autoref{tab:compute-cost}, the efficiency ranking of tooling methods changes. On one hand, tool-making method rankings roughly remain the same, except that CRAFT requires less compute than CREATOR on both tasks after getting rid of the training cost.
On the other hand, however, the ranking among tool-using methods drastically changes: ToolFormer requires more compute than API-Bank and ToolAlpaca when considering only inference-time cost. We conjecture this is mainly due to differences in baseline setups: ToolFormer adds in-context examples than the CoT baseline, API-Bank and ToolAlpaca use the same prompt for baseline and fine-tuned LMs with varied abilities to utilize tools presented in the prompt.
In general, if the user has sufficient budgets for training but higher demands on inference-time efficiency, the training approaches proposed by API-Bank and ToolAlpaca could be more suitable.


---
/workspace/dumps/workspace/my_paper/sections/conclusion.tex:
\section{Final Remarks}
Our survey provides definitions for LM-used tools and systematic summaries of existing approaches. While our empirical analysis guides when (on what tasks) and how (use what methods) should one use tools, we hope readers can more clearly understand the scenarios and techniques of LM tooling, from basic paradigm to advanced settings, and across LMs speaking natural and programming languages. 
% \dfcomment{I feel like the `what methods' part was comparatively underexplored -- and rightly so, since methods are still pretty new and haven't been thoroughly compared. Maybe we should instead focus, here, on the `how-to-evaluate' aspects which we were more concrete on?}
% Further, we point out missing aspects of tool evaluation and suggest more comprehensive measurements for efficiency, performance, reliability, and safety.
We believe tools can greatly extend and facilitate LM abilities, and hope our work elicits more discussions and research developments in %this direction.
(i) proposing benchmarks with natural use cases and executable tools, (ii) utilizing comprehensive evaluation metrics proposed in \S\ref{sec:good-tool}, and (iii) exploring more challenging and realistic scenarios for tool-using and tool-making techniques.
% (i) proposing better benchmarks, methods, and evaluation metrics, as well as (ii) advanced topics empowered by tools such as more capable agents and their interactions with humans or the world.
% \dfcomment{could consider broader directions for future work here enabled by your perspective, e.g. what other sorts of things might be implemented as tools that haven't been already? (interaction with people? reads and writes from an external memory?)}
% \gncomment{I agree with Daniel's comment above. I think that more insight into future directions or missing pieces would be nice here. Right now it's basically ``better evaluation'' and ``better methods'' which isn't super-insightful -- those are always the things that we can do :)}


% \subsubsection*{Author Contributions}
% If you'd like to, you may include a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
We thank Saujas Vaduguru, Sherry Tongshuang Wu, Jiawei Liu, Shihao Liang, Pengfei Liu for the helpful discussions. Zora Zhiruo Wang is supported by the Teng Family Presidential Fellowship. Hao Zhu is supported by NSF EAGER Award \#2141751.

```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me read the appendix files:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_multiple_files`

```json
{
	"paths": ["/workspace/dumps/workspace/my_paper/appendix/learning-effort.tex",
	"/workspace/dumps/workspace/my_paper/appendix/embodied.tex"]
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
/workspace/dumps/workspace/my_paper/appendix/learning-effort.tex:
```
</summary>
```json
/workspace/dumps/workspace/my_paper/appendix/learning-effort.tex:
\section{Detailed computation process for tooling trade-offs}
\label{app:learning-effort}

For each method measured in \S\ref{sub:trade-off}, we describe the detailed processes in estimating their computation cost and performance improvement. 
For open-source models, we estimate cost $C = 6ND$, where $N$ is the number of tokens and $D$ is the parameter size (\autoref{fig:overall-cost}, left). Because the parameter size $D$ of closed-source GPT is unknown, we only measure the number of extra tokens $N$ per example (\autoref{fig:overall-cost}, right). 


\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{./figures/compute-cost.pdf}
\vspace{-6mm}
\caption{Computation cost of different approaches using open-source (left) and closed-source (right) models, and their performance gain on experimented datasets. We use different colors to represent tasks and different shapes to represent methods.}
% \vspace{-2mm}
\label{fig:overall-cost}
\end{figure}

% \textbf{\begin{table}[ht]
% \small
% \begin{center}
%     \begin{tabular}{lllll}
%     \toprule
%     \multicolumn{1}{c}{\textbf{Work}} & \multicolumn{1}{c}{\textbf{Baseline}} & \multicolumn{1}{c}{\textbf{Tool-Use LM}} & \multicolumn{1}{c}{\textbf{Compute}} & \multicolumn{1}{c}{\textbf{Type}} \\
%     \midrule
%     {API-Bank} & {Alpaca} & {Lynx} & {3-epoch train} & {tool-use, train \& test} \\
%     {ToolAlpaca} & {Vicuna} & {ToolAlpaca} & {3-epoch train} & {tool-use, train \& test} \\
%     {Toolformer} & {GPT-J} & {Toolformer} & {25$k$ examples train} & {tool-use, train \& test} \\
%     {LATM} & {CoT} & {LATM} & {train, verify, test} & {tool-make, train \& test} \\
%     {CRAFT} & {PoT} & {CRAFT} & {train, verify, test} & {tool-make, train \& test} \\
%     % \midrule
%     {Chameleon} & {CoT/PoT} & {Chameleon} & {few-shot, verify} & {tool-use, test} \\
%     {CREATOR} & {PoT} & {CREATOR} & {abstract, decide, rectify} & {tool-make, test} \\
%     {TroVE} & {Primitive} & {TroVE} & {3-way generation} & {tool-make, test} \\
%     \bottomrule
%     \end{tabular}
% \end{center}
% \caption{Details for measuring computation cost and task performance.}
% \label{tab:est-compute-cost}
% \end{table}}


\subsection{Methods using known-sized models}

For methods using models whose parameter sizes are known, we estimate the computation cost by the FLOPs during any additional modules such as training and inference with additional context. In general, the computation cost is majorly affected by (1) the number of tokens processed, and (2) the parameter size of models.



\noindent \textbf{API-Bank \citep{li-etal-2023-api}} \quad
This work trains the Lynx model that uses tools to solve problems in the proposed API-Bank dataset. The Lynx model is initialized by Alpaca 7B parameters, and trained on the API-Bank training set with 3 epochs. Therefore, we adopt the Alpaca 7B as the baseline and Lynx as the tool-using model, where the 3-epoch training is the additional computation cost introduced to enable tool use.
We calculate the total number of tokens involved in the training process, including the example i/o and additional instructions. 
Because the baseline and proposed method use the same prompt at inference time, no additional computation is required.
Regarding task performance, we adopt the total correctness across all evaluation systems, as reported in Table 3. We report the difference between the fine-tuned Lynx-7B and the zero-shot Alpaca-7B.


\noindent \textbf{ToolAlpaca \citep{tang2023toolalpaca}} \quad
This work proposes the ToolAlpaca dataset and trains Vicuna models to use tools. The baseline models are Vicuna-7B and Vicuna-13B models. The trained tool-using models are called ToolAlpaca-7B and ToolAlpaca-13B models. All ToolAlpaca models are trained on the training split for 3 epochs, so we estimate the cost during this training process for 7B and 13B models, respectively.
We adopt the `overall' results reported in Table 3, on examples with both simulated tools and real-world APIs, and report their average results. We measure the performance gain by the difference between the ToolAlpaca-7/13B and Vicuna-7/13B. 


\noindent \textbf{Toolformer \citep{schick2023toolformer}} \quad
This work integrates five tools --- question answering system, calculator, Wikipedia search, machine translation system, and calendar --- respectively for five tasks transformed from a subset of CCNet \citep{wenzek2020ccnet}.
Starting with GPT-J models \citep{wang2021gpt-j} as the no-tool baseline, they train on 25$k$ model-synthesized examples for each tool and obtain the Toolformer models, causing a total of 1$M$ FLOPs for each task. At inference time, they add special instruction and in-context examples to prompt tool using, resulting in extra compute.
Because each task contains multiple datasets, we report the average results to represent the general task performance.




% ######################
\subsection{Models with unknown size}

While many of the works use GPT-3.5 or GPT-4 models that do not release their parameter size, we estimate the cost by using the number of tokens processed in extra modules.

\noindent \textbf{Chameleon \citep{lu2023chameleon}} \quad 
This work proposes to take a tool-augmented approach to improve on two existing datasets --- ScienceQA and TabMWP. Because all experiments use ChatGPT and GPT-4 models, whose parameter sizes are unknown, we only examine results with (the better) GPT-4 model to fairly compare with other methods using GPT-4. 
Specifically for the ScienceQA dataset, we adopt the Chain-of-Thought (CoT) baseline reported in the paper, and report task accuracy as in the \textsc{All} column in Table 3. We calculate the difference in number of tokens between the proposed Chameleon methods against the CoT baseline.
For the TabMWP dataset, we adopt the Program-of-Thought (PoT) baseline and similarly calculate the token number difference using the provided results.\footnote{\url{https://github.com/lupantech/chameleon-llm}} We adopt numbers in the \textsc{All} column in Table 4 as the TabMWP accuracy. 


\noindent \textbf{LATM \citep{cai2023large}} \quad
This work proposes to use LMs to make tools for individual tasks in BigBench. Compared to the chain-of-thought (CoT) baseline, the proposed LATM method integrates training, validation, and inference stages to make tools and solve questions. We estimate the compute cost by the additional number of tokens used for LATM than for CoT.
We measure each method by averaging its accuracy across all six selected tasks.

\noindent \textbf{CRAFT \citep{yuan2023craft}} \quad
This work uses LMs to make tools for math, table, and image reasoning tasks. We calculate the number of tokens used during training and inference, using its released code and data.\footnote{\url{https://github.com/lifan-yuan/CRAFT}}
CRAFT similarly implements CoT as the baseline, and proposes further training, verification, and finally testing in the CRAFT method. 
We report its task accuracy on the representative datasets from each task --- MATH, TabMWP, and GQA --- to enable fairer comparison with other works having overlapping datasets.


\noindent \textbf{CREATOR \citep{qian2023creator}} \quad
As a prior work for CRAFT, CREATOR similarly tests on MATH and table tasks, but designs its methods differently. In addition to CoT, this work implements a stronger program-oriented baseline called Program-of-Thought (PoT). We also adopt PoT as the main baseline without tool making or using. The CREATOR method operates at test time, with multiple steps through tool making, solution generation, verification, rectification, etc. We calculate the difference in number of tokens between the CREATOR approach and the baseline PoT setting.
We adopt the task accuracy reported in Table 2 (MATH) and Table 3 (TabMWP) from the original paper.

\noindent \textbf{TroVE \citep{wang2024trove}} \quad
TroVE also induces tools without training supervision. This work adopts the primitive baseline, a presumably stronger version of PoT yet without much textual explanation. The main implementation change in TroVE is the three-mode generation and multi-candidate sampling. We calculate the additional tokens used in TroVE compared to the primitive baseline.
The dataset reports task accuracy, solution complexity, and toolbox size, we only adopt the task accuracy to fairly compare with other works.


% \begin{figure}[ht]
% \centering
% % \vspace{-1mm}
% \includegraphics[width=\textwidth]{./figures/inference-cost.pdf}
% \vspace{-6mm}
% \caption{Performance gain versus inference-time computation cost.}
% \vspace{-3mm}
% \label{fig:inference-cost}
% \end{figure}


% \begin{table}[ht]
% \small
% \begin{center}
% \resizebox{0.92\linewidth}{!}{
%     \begin{tabular}{ll}
%     \toprule
%     \multicolumn{1}{c}{\textbf{Missing aspects}} & \multicolumn{1}{c}{\textbf{Potential metrics}} \\
%     \midrule
%     {Efficiency of tool integration} & {computation overhead} \\
%     {Quality of tools} & {runtime and memory usage of tools} \\
%     {Reliability of unstable tools} & {success rate of tools} \\
%     {Reproducible testing} & {execution correctness w.r.t. canonical trajectory} \\
%     {Safe usage} & {visibility, data integrity, and other meta-data about tools} \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{center}
% \caption{Missing evaluation aspects and potential metrics.}
% \label{tab:missing-eval}
% \end{table}

---
/workspace/dumps/workspace/my_paper/appendix/embodied.tex:
\section{Tools with embodied agents}
\label{sec:embodied}

\zwcomment{merge related works to earlier sections and remove this section?}

In previous sections, we discuss LM-used tools in NLP task environments. In the last part of this survey, we broaden our discussion about tools to the agent-environment interaction setup. 
Specifically, we regard \emph{agents} as the policies that interact with environments based on the perceived environmental state;\footnote{We are specifically interested in LM-based agents, which use LM as a component.} and \emph{environments} are the collections of objects and their relationships.\footnote{In NLP tasks, an environment contains example contexts and the collection of tool environments.}
While most objects can be purposed as tools, e.g., brushes can be used to paint, brooms can reach high objects, % magnifiers can help read books,
% books can be used to hit nails into blocks, terminal can be used to execute a drawing program, 
the following definition captures the essence of tools:

\begin{definition}[Tools]
\label{def:tools_broad}
Tools are the objects in the environments that are not directly part of the goals of the agents, but are used by the agents in order to achieve the goals.
\end{definition}

For example, when drawing a picture, $\{$brush, paint, canvas$\}$ are all used, but only the \texttt{brush} is regarded as tools, since they are not part of the final product --- a picture.
% \zwcomment{we could align what figure 4 draws to the examples we made below. it also helps that you could discuss the example by referring to the illustration}
% For example, when making a hamburger, stove, toaster, pans, breads, patties, tomatoes, and cabbages are all used, but only stove, toasters, and pans are regarded as tools, since they are not part of final product, a hamburger. 
It should be noted that an object can be a tool for one task but not a tool for another, e.g. a \texttt{calculator} is a tool when finding the root of an equation, but it is not a tool when a virtual agent is launching the calculator app.\footnote{The definition of tools used by humans and non-human animals has been evolving in the past decades. Def. \ref{def:tools_broad} is similar to \citet{beauchamp2011oxford}, which defines tools as inanimate objects ``causing a change in the environment, thereby facilitating one's achievement of a target goal''.}


\begin{wrapfigure}[18]{r}{0.43\textwidth}
\vspace{-4mm}
\footnotesize
    \includegraphics[width=0.43\textwidth]{figures/tool-survey-paradigm.pdf}
\vspace{-6mm}
\caption{The action-observation feedback loop: an agent interacts with an environment by acting (with tools) on other objects in the environment, and getting feedback reward from it.}
\label{fig:agent-env}
\end{wrapfigure}



The approaches for using tools under Def. \ref{def:tools-lm} can also be applied to tools under Def. \ref{def:tools_broad}, by implementing external functions composed of low-level manipulation policies. 
These composed functions are often \textit{rule-based methods} or \textit{magic functions} provided by the simulation environments (similar to works providing expert-designed APIs to LMs in \S\ref{sub:app-spec}). For example, \citet{shridhar2020alfred} employ human-crafted APIs in the AI2THOR simulator \citep{kolve2017ai2} to use microwaves and fridges to cool and heat objects.  
\citet{wang2023voyager} use JavaScript APIs in Mineflayer\footnote{\url{https://github.com/PrismarineJS/mineflayer}} to manipulate Minecraft tools such as mining with axes. \citet{liu2024agentbench} hand-craft the \texttt{search} tool as Python functions to query data over the web, and \citet{boiko2023autonomous} similarly for conducting chemical research.

However, in the real world, not all high-level manipulation policies are readily available. For example, in \citet{ahn2022saycan}, % although picking up sponges can be learned as a policy, 
wiping tables with a sponge is too complex to be implemented as an action, hence is actually executed by humans.
% although picking up sponge is a learned subpolicy, wiping tables with a sponge is executed by humans. 
Nonetheless, instead of adopting readily available policies, these compositions can also be learned via behavior cloning and reinforcement learning.
For instance, \citet{baker2019emergent} show that tool use can be learned without expert demonstrations or rewarding signals, and \citet{yu2023language} feed NL inputs to robots to learn skills such as picking up apples.
% More recently, \citet{ahn2022saycan} also learn policies to pick up objects such as sponges.

Training and evaluating LMs and LM-based agents on their tooling abilities in the real world remain an open challenge.
% Training and evaluating LMs' and LM-based agents' ability to use tools in the real world remains an open question. 
Here we highlight two future directions: (1) \textit{building more complex environments}: with sufficient objects for versatile tool use and tool make, (2) \textit{harness LM procedural knowledge}: LMs equip strong procedural knowledge via extensive training, which can be utilized to improve planning and navigation in complex, realistic environments. 


```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now I'll create a Python script to systematically analyze all citations and references in the LaTeX files:
</div>

<div className="tool-call-box">
<svg width="14px" height="14px" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fillRule="evenodd" clipRule="evenodd" d="M13.0164 2C10.8193 2 9.03825 3.72453 9.03825 5.85185V8.51852H15.9235V9.25926H5.97814C3.78107 9.25926 2 10.9838 2 13.1111L2 18.8889C2 21.0162 3.78107 22.7407 5.97814 22.7407H8.27322V19.4815C8.27322 17.3542 10.0543 15.6296 12.2514 15.6296H19.5956C21.4547 15.6296 22.9617 14.1704 22.9617 12.3704V5.85185C22.9617 3.72453 21.1807 2 18.9836 2H13.0164ZM12.0984 6.74074C12.8589 6.74074 13.4754 6.14378 13.4754 5.40741C13.4754 4.67103 12.8589 4.07407 12.0984 4.07407C11.3378 4.07407 10.7213 4.67103 10.7213 5.40741C10.7213 6.14378 11.3378 6.74074 12.0984 6.74074Z" fill="#327EBD"/><path fillRule="evenodd" clipRule="evenodd" d="M18.9834 30C21.1805 30 22.9616 28.2755 22.9616 26.1482V23.4815L16.0763 23.4815L16.0763 22.7408L26.0217 22.7408C28.2188 22.7408 29.9998 21.0162 29.9998 18.8889V13.1111C29.9998 10.9838 28.2188 9.25928 26.0217 9.25928L23.7266 9.25928V12.5185C23.7266 14.6459 21.9455 16.3704 19.7485 16.3704L12.4042 16.3704C10.5451 16.3704 9.03809 17.8296 9.03809 19.6296L9.03809 26.1482C9.03809 28.2755 10.8192 30 13.0162 30H18.9834ZM19.9015 25.2593C19.1409 25.2593 18.5244 25.8562 18.5244 26.5926C18.5244 27.329 19.1409 27.9259 19.9015 27.9259C20.662 27.9259 21.2785 27.329 21.2785 26.5926C21.2785 25.8562 20.662 25.2593 19.9015 25.2593Z" fill="#FFDA4B"/></svg> `python-execute`

```python 
import re
import os
from collections import defaultdict, Counter

def extract_citations_and_references(tex_content):
    """Extract all \cite{}, \citep{}, \citet{}, \label{}, \ref{}, \autoref{} commands"""
    
    # Pattern for citations
    cite_patterns = [
        r'\\cite\{([^}]+)\}',
        r'\\citep\{([^}]+)\}', 
        r'\\citet\{([^}]+)\}',
        r'\\citealp\{([^}]+)\}',
        r'\\citealt\{([^}]+)\}',
    ]
    
    # Pattern for labels and references
    label_pattern = r'\\label\{([^}]+)\}'
    ref_patterns = [
        r'\\ref\{([^}]+)\}',
        r'\\autoref\{([^}]+)\}',
        r'\\eqref\{([^}]+)\}',
        r'\\figref\{([^}]+)\}',
        r'\\Figref\{([^}]+)\}',
        r'\\secref\{([^}]+)\}',
        r'\\Secref\{([^}]+)\}',
    ]
    
    citations = []
    labels = []
    references = []
    
    # Extract citations
    for pattern in cite_patterns:
        matches = re.findall(pattern, tex_content)
        for match in matches:
            # Handle multiple citations in one command
            cites = [c.strip() for c in match.split(',')]
            citations.extend(cites)
    
    # Extract labels
    labels = re.findall(label_pattern, tex_content)
    
    # Extract references
    for pattern in ref_patterns:
        matches = re.findall(pattern, tex_content)
        references.extend(matches)
    
    return citations, labels, references

def extract_bib_keys(bib_content):
    """Extract all bibliography keys from .bib file"""
    pattern = r'@[^{]+\{([^,]+),'
    return re.findall(pattern, bib_content)

def analyze_citations_and_refs():
    """Analyze all citations and references in the LaTeX project"""
    
    # Read all tex files
    tex_files = [
        '/workspace/dumps/workspace/my_paper/colm2024_conference.tex',
        '/workspace/dumps/workspace/my_paper/math_commands.tex',
        '/workspace/dumps/workspace/my_paper/sections/1_introduction.tex',
        '/workspace/dumps/workspace/my_paper/sections/2_scenarios.tex', 
        '/workspace/dumps/workspace/my_paper/sections/3_advanced.tex',
        '/workspace/dumps/workspace/my_paper/sections/4_evaluation.tex',
        '/workspace/dumps/workspace/my_paper/sections/5_tradeoff.tex',
        '/workspace/dumps/workspace/my_paper/sections/conclusion.tex',
        '/workspace/dumps/workspace/my_paper/appendix/learning-effort.tex',
        '/workspace/dumps/workspace/my_paper/appendix/embodied.tex'
    ]
    
    # Read bibliography file
    with open('/workspace/dumps/workspace/my_paper/colm2024_conference.bib', 'r') as f:
        bib_content = f.read()
    
    # Extract bibliography keys
    bib_keys = set(extract_bib_keys(bib_content))
    
    all_citations = []
    all_labels = []
    all_references = []
    
    # Process each tex file
    for tex_file in tex_files:
        try:
            with open(tex_file, 'r') as f:
                content = f.read()
            
            citations, labels, references = extract_citations_and_references(content)
            all_citations.extend(citations)
            all_labels.extend(labels)
            all_references.extend(references)
            
            print(f"\n=== {os.path.basename(tex_file)} ===")
            print(f"Citations: {len(citations)}")
            print(f"Labels: {len(labels)}")
            print(f"References: {len(references)}")
            
        except FileNotFoundError:
            print(f"File not found: {tex_file}")
    
    # Check for issues
    print("\n" + "="*60)
    print("CITATION AND REFERENCE ANALYSIS")
    print("="*60)
    
    # 1. Check for undefined citations
    undefined_citations = []
    for cite in set(all_citations):
        if cite not in bib_keys:
            undefined_citations.append(cite)
    
    if undefined_citations:
        print(f"\n❌ UNDEFINED CITATIONS ({len(undefined_citations)}):")
        for cite in sorted(undefined_citations):
            print(f"  - {cite}")
    else:
        print("\n✅ All citations are defined in the bibliography")
    
    # 2. Check for unused bibliography entries
    unused_bib = []
    for bib_key in bib_keys:
        if bib_key not in all_citations:
            unused_bib.append(bib_key)
    
    if unused_bib:
        print(f"\n⚠️  UNUSED BIBLIOGRAPHY ENTRIES ({len(unused_bib)}):")
        for bib in sorted(unused_bib):
            print(f"  - {bib}")
    else:
        print("\n✅ All bibliography entries are used")
    
    # 3. Check for undefined references
    undefined_refs = []
    label_set = set(all_labels)
    for ref in set(all_references):
        if ref not in label_set:
            undefined_refs.append(ref)
    
    if undefined_refs:
        print(f"\n❌ UNDEFINED REFERENCES ({len(undefined_refs)}):")
        for ref in sorted(undefined_refs):
            print(f"  - {ref}")
    else:
        print("\n✅ All references are defined")
    
    # 4. Check for unused labels
    unused_labels = []
    ref_set = set(all_references)
    for label in set(all_labels):
        if label not in ref_set:
            unused_labels.append(label)
    
    if unused_labels:
        print(f"\n⚠️  UNUSED LABELS ({len(unused_labels)}):")
        for label in sorted(unused_labels):
            print(f"  - {label}")
    else:
        print("\n✅ All labels are used")
    
    # 5. Summary statistics
    print(f"\n📊 SUMMARY STATISTICS:")
    print(f"  Total citations used: {len(all_citations)} (unique: {len(set(all_citations))})")
    print(f"  Total bibliography entries: {len(bib_keys)}")
    print(f"  Total labels defined: {len(all_labels)} (unique: {len(set(all_labels))})")
    print(f"  Total references used: {len(all_references)} (unique: {len(set(all_references))})")
    
    return {
        'undefined_citations': undefined_citations,
        'unused_bib': unused_bib, 
        'undefined_refs': undefined_refs,
        'unused_labels': unused_labels,
        'all_citations': all_citations,
        'all_labels': all_labels,
        'all_references': all_references,
        'bib_keys': list(bib_keys)
    }

# Run the analysis
results = analyze_citations_and_refs()
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
=== STDOUT ===
```
</summary>
```json
=== STDOUT ===

=== colm2024_conference.tex ===
Citations: 0
Labels: 0
References: 0

=== math_commands.tex ===
Citations: 0
Labels: 0
References: 32

=== 1_introduction.tex ===
Citations: 41
Labels: 6
References: 8

=== 2_scenarios.tex ===
Citations: 47
Labels: 4
References: 3

=== 3_advanced.tex ===
Citations: 43
Labels: 8
References: 5

=== 4_evaluation.tex ===
Citations: 46
Labels: 7
References: 10

=== 5_tradeoff.tex ===
Citations: 0
Labels: 4
References: 5

=== conclusion.tex ===
Citations: 0
Labels: 0
References: 1

=== learning-effort.tex ===
Citations: 10
Labels: 5
References: 3

=== embodied.tex ===
Citations: 10
Labels: 3
References: 4

============================================================
CITATION AND REFERENCE ANALYSIS
============================================================

✅ All citations are defined in the bibliography

⚠️  UNUSED BIBLIOGRAPHY ENTRIES (24):
  - Baker2020Emergent
  - artzi2013weakly
  - brown2020language
  - chen2017reading
  - gao2021simcse
  - hf-models
  - liang2011learning
  - liang2023taskmatrixai
  - liu2023language
  - nakano2022webgpt
  - ouyang2022training
  - public-apis
  - shi2017world
  - shin2019program
  - tensorflow2015
  - torchhub
  - von1995cognitive
  - wang-etal-2022-super
  - xie2022unifiedskg
  - yang2023gpttools
  - ye2024tooleyes
  - yu2023spae
  - zettlemoyer2007online
  - zhou2023webarena

❌ UNDEFINED REFERENCES (6):
  - #1
  - #2
  - #3
  - #4
  - fig:call-api-v0
  - tab:1

⚠️  UNUSED LABELS (12):
  - fig:agent-env
  - fig:def
  - fig:inference-cost
  - fig:make-tool
  - fig:toolformer-cost
  - sec:embodied
  - sec:intro
  - sub:domain-spec
  - sub:general-codegen
  - tab:est-compute-cost
  - tab:example-tools
  - tab:missing-eval

📊 SUMMARY STATISTICS:
  Total citations used: 197 (unique: 93)
  Total bibliography entries: 117
  Total labels defined: 37 (unique: 37)
  Total references used: 71 (unique: 31)
=== STDERR ===
/workspace/dumps/workspace/./.python_tmp/cd8c7666-ee2f-446e-96a4-b80523e8ca01.py:6: SyntaxWarning: invalid escape sequence '\c'
  """Extract all \cite{}, \citep{}, \citet{}, \label{}, \ref{}, \autoref{} commands"""
=== EXECUTION INFO ===
Return code: 0
Execution time: 0.055 seconds
Timeout limit: 30 seconds
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Great! I found several issues. Now let me look more closely at the specific problems and fix them:
</div>

<div className="tool-call-box">
<svg width="14px" height="14px" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fillRule="evenodd" clipRule="evenodd" d="M13.0164 2C10.8193 2 9.03825 3.72453 9.03825 5.85185V8.51852H15.9235V9.25926H5.97814C3.78107 9.25926 2 10.9838 2 13.1111L2 18.8889C2 21.0162 3.78107 22.7407 5.97814 22.7407H8.27322V19.4815C8.27322 17.3542 10.0543 15.6296 12.2514 15.6296H19.5956C21.4547 15.6296 22.9617 14.1704 22.9617 12.3704V5.85185C22.9617 3.72453 21.1807 2 18.9836 2H13.0164ZM12.0984 6.74074C12.8589 6.74074 13.4754 6.14378 13.4754 5.40741C13.4754 4.67103 12.8589 4.07407 12.0984 4.07407C11.3378 4.07407 10.7213 4.67103 10.7213 5.40741C10.7213 6.14378 11.3378 6.74074 12.0984 6.74074Z" fill="#327EBD"/><path fillRule="evenodd" clipRule="evenodd" d="M18.9834 30C21.1805 30 22.9616 28.2755 22.9616 26.1482V23.4815L16.0763 23.4815L16.0763 22.7408L26.0217 22.7408C28.2188 22.7408 29.9998 21.0162 29.9998 18.8889V13.1111C29.9998 10.9838 28.2188 9.25928 26.0217 9.25928L23.7266 9.25928V12.5185C23.7266 14.6459 21.9455 16.3704 19.7485 16.3704L12.4042 16.3704C10.5451 16.3704 9.03809 17.8296 9.03809 19.6296L9.03809 26.1482C9.03809 28.2755 10.8192 30 13.0162 30H18.9834ZM19.9015 25.2593C19.1409 25.2593 18.5244 25.8562 18.5244 26.5926C18.5244 27.329 19.1409 27.9259 19.9015 27.9259C20.662 27.9259 21.2785 27.329 21.2785 26.5926C21.2785 25.8562 20.662 25.2593 19.9015 25.2593Z" fill="#FFDA4B"/></svg> `python-execute`

```python 
# Let's look for the specific undefined references to understand what they should be
import re

def find_undefined_ref_contexts():
    """Find the context around undefined references to understand what they should be"""
    
    tex_files = [
        '/workspace/dumps/workspace/my_paper/sections/1_introduction.tex',
        '/workspace/dumps/workspace/my_paper/sections/2_scenarios.tex', 
        '/workspace/dumps/workspace/my_paper/sections/3_advanced.tex',
        '/workspace/dumps/workspace/my_paper/sections/4_evaluation.tex',
        '/workspace/dumps/workspace/my_paper/sections/5_tradeoff.tex',
        '/workspace/dumps/workspace/my_paper/sections/conclusion.tex',
        '/workspace/dumps/workspace/my_paper/appendix/learning-effort.tex',
        '/workspace/dumps/workspace/my_paper/appendix/embodied.tex'
    ]
    
    undefined_refs = ['#1', '#2', '#3', '#4', 'fig:call-api-v0', 'tab:1']
    
    for tex_file in tex_files:
        try:
            with open(tex_file, 'r') as f:
                content = f.read()
            
            for ref in undefined_refs:
                # Look for this reference in the content
                pattern = rf'\\(?:ref|autoref|figref|Figref|secref|Secref|eqref)\{{{re.escape(ref)}\}}'
                matches = re.finditer(pattern, content)
                
                for match in matches:
                    start = max(0, match.start() - 100)
                    end = min(len(content), match.end() + 100)
                    context = content[start:end].replace('\n', ' ')
                    
                    print(f"\n❌ Found undefined reference '{ref}' in {tex_file}:")
                    print(f"Context: ...{context}...")
                    
        except FileNotFoundError:
            continue

find_undefined_ref_contexts()
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
=== STDOUT ===
```
</summary>
```json
=== STDOUT ===

❌ Found undefined reference 'fig:call-api-v0' in /workspace/dumps/workspace/my_paper/sections/1_introduction.tex:
Context: ...tool will execute the expression and return the execution result to the LM.   Taking the example in \autoref{fig:call-api-v0}, the LM sends the \texttt{check\_weather()} call to the weather server and receives the output ``su...

❌ Found undefined reference 'tab:1' in /workspace/dumps/workspace/my_paper/sections/2_scenarios.tex:
Context: ...erate every scenario where tools could be useful, we summarize some major categories of tool use in \autoref{tab:1} and below. Note that a tool may fall into one or more categories.   \noindent \textbf{\raisebox{-1m...
=== EXECUTION INFO ===
Return code: 0
Execution time: 0.054 seconds
Timeout limit: 30 seconds
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me check what figures and tables actually exist to understand what the correct references should be:
</div>

<div className="tool-call-box">
<svg width="14px" height="14px" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fillRule="evenodd" clipRule="evenodd" d="M13.0164 2C10.8193 2 9.03825 3.72453 9.03825 5.85185V8.51852H15.9235V9.25926H5.97814C3.78107 9.25926 2 10.9838 2 13.1111L2 18.8889C2 21.0162 3.78107 22.7407 5.97814 22.7407H8.27322V19.4815C8.27322 17.3542 10.0543 15.6296 12.2514 15.6296H19.5956C21.4547 15.6296 22.9617 14.1704 22.9617 12.3704V5.85185C22.9617 3.72453 21.1807 2 18.9836 2H13.0164ZM12.0984 6.74074C12.8589 6.74074 13.4754 6.14378 13.4754 5.40741C13.4754 4.67103 12.8589 4.07407 12.0984 4.07407C11.3378 4.07407 10.7213 4.67103 10.7213 5.40741C10.7213 6.14378 11.3378 6.74074 12.0984 6.74074Z" fill="#327EBD"/><path fillRule="evenodd" clipRule="evenodd" d="M18.9834 30C21.1805 30 22.9616 28.2755 22.9616 26.1482V23.4815L16.0763 23.4815L16.0763 22.7408L26.0217 22.7408C28.2188 22.7408 29.9998 21.0162 29.9998 18.8889V13.1111C29.9998 10.9838 28.2188 9.25928 26.0217 9.25928L23.7266 9.25928V12.5185C23.7266 14.6459 21.9455 16.3704 19.7485 16.3704L12.4042 16.3704C10.5451 16.3704 9.03809 17.8296 9.03809 19.6296L9.03809 26.1482C9.03809 28.2755 10.8192 30 13.0162 30H18.9834ZM19.9015 25.2593C19.1409 25.2593 18.5244 25.8562 18.5244 26.5926C18.5244 27.329 19.1409 27.9259 19.9015 27.9259C20.662 27.9259 21.2785 27.329 21.2785 26.5926C21.2785 25.8562 20.662 25.2593 19.9015 25.2593Z" fill="#FFDA4B"/></svg> `python-execute`

```python 
# Let's find all actual labels and see what the correct references should be
import re

def find_all_labels_and_context():
    """Find all labels and their context to understand what they refer to"""
    
    tex_files = [
        '/workspace/dumps/workspace/my_paper/colm2024_conference.tex',
        '/workspace/dumps/workspace/my_paper/sections/1_introduction.tex',
        '/workspace/dumps/workspace/my_paper/sections/2_scenarios.tex', 
        '/workspace/dumps/workspace/my_paper/sections/3_advanced.tex',
        '/workspace/dumps/workspace/my_paper/sections/4_evaluation.tex',
        '/workspace/dumps/workspace/my_paper/sections/5_tradeoff.tex',
        '/workspace/dumps/workspace/my_paper/sections/conclusion.tex',
        '/workspace/dumps/workspace/my_paper/appendix/learning-effort.tex',
        '/workspace/dumps/workspace/my_paper/appendix/embodied.tex'
    ]
    
    print("=== ALL LABELS FOUND ===")
    
    for tex_file in tex_files:
        try:
            with open(tex_file, 'r') as f:
                content = f.read()
            
            # Find labels with context
            label_pattern = r'\\label\{([^}]+)\}'
            matches = re.finditer(label_pattern, content)
            
            for match in matches:
                label = match.group(1)
                start = max(0, match.start() - 150)
                end = min(len(content), match.end() + 50)
                context = content[start:end].replace('\n', ' ')
                
                print(f"\n📍 Label: {label}")
                print(f"   File: {tex_file}")
                print(f"   Context: ...{context}...")
                
        except FileNotFoundError:
            continue

find_all_labels_and_context()
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
=== STDOUT ===
```
</summary>
```json
=== STDOUT ===
=== ALL LABELS FOUND ===

📍 Label: sec:intro
   File: /workspace/dumps/workspace/my_paper/sections/1_introduction.tex
   Context: ...\section{Introduction} \label{sec:intro}  Language Models (LMs) have become increasingly e...

📍 Label: fig:def
   File: /workspace/dumps/workspace/my_paper/sections/1_introduction.tex
   Context: ...udegraphics[width=0.39\textwidth]{./figures/definition.pdf} \vspace{-2mm} \caption{Illustration of tools extending and facilitating LM task-solving.} \label{fig:def} \end{wrapfigure}  To solve this problem, research...

📍 Label: sec:background
   File: /workspace/dumps/workspace/my_paper/sections/1_introduction.tex
   Context: ...e most efficient tooling approaches and the tasks that benefit most from tools (\S\ref{sub:trade-off}).     % ################## \section{Background} \label{sec:background}  \subsection{What are tools?} % how existing work...

📍 Label: def:tools-lm
   File: /workspace/dumps/workspace/my_paper/sections/1_introduction.tex
   Context: ...} We argue for a somewhat more broad definition than this, which encompasses a wide variety of more recent works on tool usage:  \begin{definition}   \label{def:tools-lm}   An LM-used tool is a function interface to a co...

📍 Label: sec:basic-paradigm
   File: /workspace/dumps/workspace/my_paper/sections/1_introduction.tex
   Context: ...ot fall under the category of ``agents'' according to this definition.    % ################################### \section{The basic tool use paradigm} \label{sec:basic-paradigm} % figure start \begin{wrapfigure}[16]{r}{0.42\tex...

📍 Label: fig:call-api
   File: /workspace/dumps/workspace/my_paper/sections/1_introduction.tex
   Context: ...ll and return the output \raisebox{0.3mm}{\colorbox{yellow!33}{sunny}}, using which the LM replaces the API call tokens in the response to the user.} \label{fig:call-api} \end{wrapfigure} % figure end  First, in this sec...

📍 Label: tab:example-tools
   File: /workspace/dumps/workspace/my_paper/sections/2_scenarios.tex
   Context: ...nguage:~str) -> text:~str} \\     \bottomrule     \end{tabular} \end{center} \vspace{-2mm} \caption{Exemplar tools for each category.} \vspace{-15mm} \label{tab:example-tools} \end{table}}   \section{Scenarios where tools are...

📍 Label: sec:scenarios
   File: /workspace/dumps/workspace/my_paper/sections/2_scenarios.tex
   Context: ...{-2mm} \caption{Exemplar tools for each category.} \vspace{-15mm} \label{tab:example-tools} \end{table}}   \section{Scenarios where tools are useful} \label{sec:scenarios}  While LMs may easily learn to do many tasks to h...

📍 Label: sub:app-spec
   File: /workspace/dumps/workspace/my_paper/sections/2_scenarios.tex
   Context: ..., as well as scenarios where tools may not be as useful (\S\ref{sub:not-useful}).     \subsection{Utilizing existing tools for specific applications} \label{sub:app-spec}  While it is difficult to exhaustively enumerate ...

📍 Label: sub:not-useful
   File: /workspace/dumps/workspace/my_paper/sections/2_scenarios.tex
   Context: ...pecialized skills, and may or may not have substantial architectural differences from the base LMs.   \subsection{Where are tools \emph{not} useful?} \label{sub:not-useful}  Despite the fact that tools can be helpful under...

📍 Label: sec:methods
   File: /workspace/dumps/workspace/my_paper/sections/3_advanced.tex
   Context: ...\section{Advanced tool-use methods} \label{sec:methods}  Given this understanding of the basic tooling pa...

📍 Label: sub:tool-select
   File: /workspace/dumps/workspace/my_paper/sections/3_advanced.tex
   Context: ...n of tools when they are not available a-priori (\S\ref{sub:make-tool}).    % ######################### \subsection{Complex tool selection and usage} \label{sub:tool-select}  Depending on the number of tools available, the ...

📍 Label: sub:tools-and-programs
   File: /workspace/dumps/workspace/my_paper/sections/3_advanced.tex
   Context: ...ntil it returns \texttt{True} to indicate a successful transaction.    % ################################ \subsection{Tools in programmatic contexts} \label{sub:tools-and-programs}  Unlike text-based tasks where tools are auxiliar...

📍 Label: fig:codelm-tools
   File: /workspace/dumps/workspace/my_paper/sections/3_advanced.tex
   Context: ...M or base actions, tools can refer to built-in functions, external libraries, or task-specific utility functions (from left to right).} \vspace{-1mm} \label{fig:codelm-tools} \end{figure}    % ############ \subsubsection{Dom...

📍 Label: sub:domain-spec
   File: /workspace/dumps/workspace/my_paper/sections/3_advanced.tex
   Context: ...nctions (from left to right).} \vspace{-1mm} \label{fig:codelm-tools} \end{figure}    % ############ \subsubsection{Domain-specific semantic parsing} \label{sub:domain-spec}  NL-to-code generation systems have been studied ...

📍 Label: sub:general-codegen
   File: /workspace/dumps/workspace/my_paper/sections/3_advanced.tex
   Context: ...{(fold xs ($\lambda$ (n x) (+ 1 n)) 0)} is to calculate the length of \texttt{xs}.     % ############ \subsubsection{General-purpose code generation} \label{sub:general-codegen} Recent code generation systems have expanded from...

📍 Label: fig:make-tool
   File: /workspace/dumps/workspace/my_paper/sections/3_advanced.tex
   Context: ...aphics[width=0.39\textwidth]{./figures/make-tool.pdf} \vspace{-2mm} \caption{LM makes tools when no tools readily apply to the task.} % \vspace{-1mm} \label{fig:make-tool} \end{wrapfigure}  \subsection{Tool creation and r...

📍 Label: sub:make-tool
   File: /workspace/dumps/workspace/my_paper/sections/3_advanced.tex
   Context: ...LM makes tools when no tools readily apply to the task.} % \vspace{-1mm} \label{fig:make-tool} \end{wrapfigure}  \subsection{Tool creation and reuse} \label{sub:make-tool}   While one can readily use tools for tasks equip...

📍 Label: sec:good-tool
   File: /workspace/dumps/workspace/my_paper/sections/4_evaluation.tex
   Context: ...\section{How to evaluate tool use?} \label{sec:good-tool}  In this section, we study existing LM-tooling be...

📍 Label: sub:testbeds
   File: /workspace/dumps/workspace/my_paper/sections/4_evaluation.tex
   Context: ...ss the missing yet important evaluation aspects of tools (\S\ref{sub:desired-properties}).  % ############ \subsection{Testbeds for evaluating tools} \label{sub:testbeds} LM tool use can be evaluated on (i) repurposed ex...

📍 Label: sub:existing-dataset
   File: /workspace/dumps/workspace/my_paper/sections/4_evaluation.tex
   Context: ...and (ii) newly crafted benchmarks that necessitate tool use (\S\ref{sub:api-benchmark}).  % ############ \subsubsection{Repurposed Existing Datasets} \label{sub:existing-dataset} Many tasks are solvable by using LMs, yet often w...

📍 Label: sub:api-benchmark
   File: /workspace/dumps/workspace/my_paper/sections/4_evaluation.tex
   Context: ...nswers produced by the tool-augmented systems, as introduced in \S\ref{sec:basic-paradigm}.  % ############ \subsubsection{Aggregated API Benchmarks} \label{sub:api-benchmark} Existing benchmarks can only benefit from a limit...

📍 Label: tab:api-benchmarks
   File: /workspace/dumps/workspace/my_paper/sections/4_evaluation.tex
   Context: ...ugh tools employed by HuggingGPT are executable, it does not evaluate the execution output due to the cost of hosting and inferencing.} \vspace{-1mm} \label{tab:api-benchmarks} \end{table}    \noindent \textbf{Tool sources} \q...

📍 Label: sub:eval-metrics
   File: /workspace/dumps/workspace/my_paper/sections/4_evaluation.tex
   Context: ...and syntactical \citep{patil2023gorilla, shen2023taskbench} means.    % ################################# \subsection{What metrics are measured now?} \label{sub:eval-metrics}  \noindent \textbf{Task completion} \quad Tools a...

📍 Label: sub:desired-properties
   File: /workspace/dumps/workspace/my_paper/sections/4_evaluation.tex
   Context: ...fication in both speed and accuracy dimensions \citep{wang2024trove}.   % ################################# \subsection{What properties are missing?} \label{sub:desired-properties}  \noindent \textbf{Efficiency of tool integration...

📍 Label: sub:trade-off
   File: /workspace/dumps/workspace/my_paper/sections/5_tradeoff.tex
   Context: ...\section{Trade-offs in tool usage} \label{sub:trade-off}  Leveraging tools often brings better performance...

📍 Label: tab:compute-cost
   File: /workspace/dumps/workspace/my_paper/sections/5_tradeoff.tex
   Context: ...datasets. To fairly compare costs on datasets with different sizes, we report the average number of tokens spent on a testing example.} \vspace{-3mm} \label{tab:compute-cost} \end{table}    For each work and each dataset the...

📍 Label: fig:toolformer-cost
   File: /workspace/dumps/workspace/my_paper/sections/5_tradeoff.tex
   Context: ...cludegraphics[width=0.31\textwidth]{./figures/toolformer-cost.pdf} \vspace{-3mm} \caption{Compute \& performance gain with ToolFormer.} \vspace{-2mm} \label{fig:toolformer-cost} \end{wrapfigure} on API benchmarks (i.e., API-Ban...

📍 Label: fig:tool-make-cost
   File: /workspace/dumps/workspace/my_paper/sections/5_tradeoff.tex
   Context: ...} \includegraphics[width=0.32\textwidth]{./figures/tool-make-cost.pdf} \vspace{-3mm} \caption{Comparing different tool-making methods.} \vspace{-1mm} \label{fig:tool-make-cost} \end{wrapfigure} \noindent \textbf{What methods a...

📍 Label: app:learning-effort
   File: /workspace/dumps/workspace/my_paper/appendix/learning-effort.tex
   Context: ...\section{Detailed computation process for tooling trade-offs} \label{app:learning-effort}  For each method measured in \S\ref{sub:trade-off...

📍 Label: fig:overall-cost
   File: /workspace/dumps/workspace/my_paper/appendix/learning-effort.tex
   Context: ...eir performance gain on experimented datasets. We use different colors to represent tasks and different shapes to represent methods.} % \vspace{-2mm} \label{fig:overall-cost} \end{figure}  % \textbf{\begin{table}[ht] % \smal...

📍 Label: tab:est-compute-cost
   File: /workspace/dumps/workspace/my_paper/appendix/learning-effort.tex
   Context: ... {tool-make, test} \\ %     \bottomrule %     \end{tabular} % \end{center} % \caption{Details for measuring computation cost and task performance.} % \label{tab:est-compute-cost} % \end{table}}   \subsection{Methods using known-...

📍 Label: fig:inference-cost
   File: /workspace/dumps/workspace/my_paper/appendix/learning-effort.tex
   Context: ...idth=\textwidth]{./figures/inference-cost.pdf} % \vspace{-6mm} % \caption{Performance gain versus inference-time computation cost.} % \vspace{-3mm} % \label{fig:inference-cost} % \end{figure}   % \begin{table}[ht] % \small % \...

📍 Label: tab:missing-eval
   File: /workspace/dumps/workspace/my_paper/appendix/learning-effort.tex
   Context: ...eta-data about tools} \\ %     \bottomrule %     \end{tabular} %     } % \end{center} % \caption{Missing evaluation aspects and potential metrics.} % \label{tab:missing-eval} % \end{table}...

📍 Label: sec:embodied
   File: /workspace/dumps/workspace/my_paper/appendix/embodied.tex
   Context: ...\section{Tools with embodied agents} \label{sec:embodied}  \zwcomment{merge related works to earlier sectio...

📍 Label: def:tools_broad
   File: /workspace/dumps/workspace/my_paper/appendix/embodied.tex
   Context: ...s into blocks, terminal can be used to execute a drawing program,  the following definition captures the essence of tools:  \begin{definition}[Tools] \label{def:tools_broad} Tools are the objects in the environments that ar...

📍 Label: fig:agent-env
   File: /workspace/dumps/workspace/my_paper/appendix/embodied.tex
   Context: ...edback loop: an agent interacts with an environment by acting (with tools) on other objects in the environment, and getting feedback reward from it.} \label{fig:agent-env} \end{wrapfigure}    The approaches for using tool...
=== EXECUTION INFO ===
Return code: 0
Execution time: 0.053 seconds
Timeout limit: 30 seconds
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now I can see the issues clearly. Let me fix them systematically:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem edit_file`

```json
{
	"path": "/workspace/dumps/workspace/my_paper/sections/1_introduction.tex",
	 "edits": [{"oldText":"Taking the example in \\autoref{fig:call-api-v0},
	 the LM sends the \\texttt{check\\_weather()} call to the weather server and receives the output ``sunny''.",
	"newText":"Taking the example in \\autoref{fig:call-api},
	 the LM sends the \\texttt{check\\_weather()} call to the weather server and receives the output ``sunny''."}]
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
diff
```
</summary>
```json
diff
Index: /workspace/dumps/workspace/my_paper/sections/1_introduction.tex
===================================================================
--- /workspace/dumps/workspace/my_paper/sections/1_introduction.tex	original
+++ /workspace/dumps/workspace/my_paper/sections/1_introduction.tex	modified
@@ -93,9 +93,9 @@
 
 First, in this section, we show an illustrative example of a basic tool-use paradigm introduced by Toolformer \citep{schick2023toolformer}, which many tool-related works adopt (\autoref{fig:call-api}). Assuming an LM communicates with users mainly in natural language, upon receiving a user query such as ``How is the weather today?'', the LM then proceeds to generate either text or tool calls. In the example, starts with generating a few tokens of text ``It is ...''. When the LM needs to seek external tools to complete the task, e.g., get real-time weather information, it generates tokens of the tool name and corresponding input arguments enclosed with \texttt{(}parentheses\texttt{)} to construct a complete tool calling expression. 
 This completed expression will trigger a shift from text-generation mode to tool-execution mode. The server hosting the tool will execute the expression and return the execution result to the LM. 
 
-Taking the example in \autoref{fig:call-api-v0}, the LM sends the \texttt{check\_weather()} call to the weather server and receives the output ``sunny''. The returned result replaces the tool call in the LM-generated tokens (e.g., from ``It is \texttt{check\_weather()}'' to ``It is sunny''), which is used for subsequent steps of generation.
+Taking the example in \autoref{fig:call-api}, the LM sends the \texttt{check\_weather()} call to the weather server and receives the output ``sunny''. The returned result replaces the tool call in the LM-generated tokens (e.g., from ``It is \texttt{check\_weather()}'' to ``It is sunny''), which is used for subsequent steps of generation.
 Accordingly, the LM shifts back to the text generation mode and continues to finish the response by generating new text tokens, e.g., adding `today.', and finally returning the response to the user.
 
 In order for LMs to use this basic paradigm of using tools, current works mainly leverage inference-time prompting and training-time learning methods.
 



```
</details>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem edit_file`

```json
{
	"path": "/workspace/dumps/workspace/my_paper/sections/2_scenarios.tex",
	 "edits": [{"oldText":"While it is difficult to exhaustively enumerate every scenario where tools could be useful,
	 we summarize some major categories of tool use in \\autoref{tab:1} and below.",
	"newText":"While it is difficult to exhaustively enumerate every scenario where tools could be useful,
	 we summarize some major categories of tool use in \\autoref{tab:example-tools} and below."}]
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
diff
```
</summary>
```json
diff
Index: /workspace/dumps/workspace/my_paper/sections/2_scenarios.tex
===================================================================
--- /workspace/dumps/workspace/my_paper/sections/2_scenarios.tex	original
+++ /workspace/dumps/workspace/my_paper/sections/2_scenarios.tex	modified
@@ -46,9 +46,9 @@
 
 \subsection{Utilizing existing tools for specific applications}
 \label{sub:app-spec}
 
-While it is difficult to exhaustively enumerate every scenario where tools could be useful, we summarize some major categories of tool use in \autoref{tab:1} and below.
+While it is difficult to exhaustively enumerate every scenario where tools could be useful, we summarize some major categories of tool use in \autoref{tab:example-tools} and below.
 Note that a tool may fall into one or more categories.
 
 
 \noindent \textbf{\raisebox{-1mm}{\includegraphics[width=4mm]{./figures/icon/book-icon.png}} Knowledge access} \quad



```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me check for some missing references that appear to be incomplete in the math_commands.tex file:
</div>

<div className="tool-call-box">
<svg width="14px" height="14px" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fillRule="evenodd" clipRule="evenodd" d="M13.0164 2C10.8193 2 9.03825 3.72453 9.03825 5.85185V8.51852H15.9235V9.25926H5.97814C3.78107 9.25926 2 10.9838 2 13.1111L2 18.8889C2 21.0162 3.78107 22.7407 5.97814 22.7407H8.27322V19.4815C8.27322 17.3542 10.0543 15.6296 12.2514 15.6296H19.5956C21.4547 15.6296 22.9617 14.1704 22.9617 12.3704V5.85185C22.9617 3.72453 21.1807 2 18.9836 2H13.0164ZM12.0984 6.74074C12.8589 6.74074 13.4754 6.14378 13.4754 5.40741C13.4754 4.67103 12.8589 4.07407 12.0984 4.07407C11.3378 4.07407 10.7213 4.67103 10.7213 5.40741C10.7213 6.14378 11.3378 6.74074 12.0984 6.74074Z" fill="#327EBD"/><path fillRule="evenodd" clipRule="evenodd" d="M18.9834 30C21.1805 30 22.9616 28.2755 22.9616 26.1482V23.4815L16.0763 23.4815L16.0763 22.7408L26.0217 22.7408C28.2188 22.7408 29.9998 21.0162 29.9998 18.8889V13.1111C29.9998 10.9838 28.2188 9.25928 26.0217 9.25928L23.7266 9.25928V12.5185C23.7266 14.6459 21.9455 16.3704 19.7485 16.3704L12.4042 16.3704C10.5451 16.3704 9.03809 17.8296 9.03809 19.6296L9.03809 26.1482C9.03809 28.2755 10.8192 30 13.0162 30H18.9834ZM19.9015 25.2593C19.1409 25.2593 18.5244 25.8562 18.5244 26.5926C18.5244 27.329 19.1409 27.9259 19.9015 27.9259C20.662 27.9259 21.2785 27.329 21.2785 26.5926C21.2785 25.8562 20.662 25.2593 19.9015 25.2593Z" fill="#FFDA4B"/></svg> `python-execute`

```python 
# Let's look at the specific undefined references in math_commands.tex
with open('/workspace/dumps/workspace/my_paper/math_commands.tex', 'r') as f:
    content = f.read()

# Find the problematic references
import re
refs = re.findall(r'\\ref\{([^}]+)\}', content)
print("References in math_commands.tex:")
for ref in refs:
    print(f"  - {ref}")

# Let's also see the context for these
for ref in ['#1', '#2', '#3', '#4']:
    pattern = rf'\\ref\{{{re.escape(ref)}\}}'
    matches = re.finditer(pattern, content)
    
    for match in matches:
        start = max(0, match.start() - 50)
        end = min(len(content), match.end() + 50)
        context = content[start:end].replace('\n', ' ')
        print(f"\nReference {ref} context: ...{context}...")
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
=== STDOUT ===
```
</summary>
```json
=== STDOUT ===
References in math_commands.tex:
  - #1
  - #1
  - #1
  - #2
  - #1
  - #2
  - #3
  - #4
  - #1
  - #1
  - #1
  - #2
  - #1
  - #2
  - #3
  - #1
  - #1
  - #1
  - #1
  - #1
  - #1
  - #2
  - #1
  - #1
  - #1
  - #2
  - #1
  - #2
  - #1
  - #1
  - #1
  - #2

Reference #1 context: ...igure reference, lower-case. \def\figref#1{figure~\ref{#1}} % Figure reference, capital. For start of senten...

Reference #1 context: ...pital. For start of sentence \def\Figref#1{Figure~\ref{#1}} \def\twofigref#1#2{figures \ref{#1} and \ref{#2}...

Reference #1 context: ...ref#1{Figure~\ref{#1}} \def\twofigref#1#2{figures \ref{#1} and \ref{#2}} \def\quadfigref#1#2#3#4{figures \re...

Reference #1 context: ...#1} and \ref{#2}} \def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}} % Section refer...

Reference #1 context: ...tion reference, lower-case. \def\secref#1{section~\ref{#1}} % Section reference, capital. \def\Secref#1{Sect...

Reference #1 context: ...Section reference, capital. \def\Secref#1{Section~\ref{#1}} % Reference to two sections. \def\twosecrefs#1#2...

Reference #1 context: ...nce to two sections. \def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}} % Reference to three sections. \def...

Reference #1 context: ...ce to three sections. \def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}} % Reference to an equatio...

Reference #1 context: ...to an equation, lower-case. \def\eqref#1{equation~\ref{#1}} % Reference to an equation, upper case \def\Eqre...

Reference #1 context: ... to an equation, upper case \def\Eqref#1{Equation~\ref{#1}} % A raw reference to an equation---avoid using i...

Reference #1 context: ...ation---avoid using if possible \def\plaineqref#1{\ref{#1}} % Reference to a chapter, lower-case. \def\chapr...

Reference #1 context: ... to a chapter, lower-case. \def\chapref#1{chapter~\ref{#1}} % Reference to an equation, upper case. \def\Cha...

Reference #1 context: ...o an equation, upper case. \def\Chapref#1{Chapter~\ref{#1}} % Reference to a range of chapters \def\rangecha...

Reference #1 context: ...a range of chapters \def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}} % Reference to an algorithm, lower-cas...

Reference #1 context: ...an algorithm, lower-case. \def\algref#1{algorithm~\ref{#1}} % Reference to an algorithm, upper case. \def\Al...

Reference #1 context: ...an algorithm, upper case. \def\Algref#1{Algorithm~\ref{#1}} \def\twoalgref#1#2{algorithms \ref{#1} and \ref{...

Reference #1 context: ...Algorithm~\ref{#1}} \def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}} \def\Twoalgref#1#2{Algorithms \ref{...

Reference #1 context: ...f{#1} and \ref{#2}} \def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}} % Reference to a part, lower case \...

Reference #1 context: ...ference to a part, lower case \def\partref#1{part~\ref{#1}} % Reference to a part, upper case \def\Partref#1...

Reference #1 context: ...ference to a part, upper case \def\Partref#1{Part~\ref{#1}} \def\twopartref#1#2{parts \ref{#1} and \ref{#2}}...

Reference #1 context: ...artref#1{Part~\ref{#1}} \def\twopartref#1#2{parts \ref{#1} and \ref{#2}}  \def\ceil#1{\lceil #1 \rceil} \def...

Reference #2 context: ...\ref{#1}} \def\twofigref#1#2{figures \ref{#1} and \ref{#2}} \def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#...

Reference #2 context: ...ef{#2}} \def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}} % Section reference, lowe...

Reference #2 context: ...ctions. \def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}} % Reference to three sections. \def\secrefs#1#2#...

Reference #2 context: ...e sections. \def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}} % Reference to an equation, lower-c...

Reference #2 context: ... chapters \def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}} % Reference to an algorithm, lower-case. \def\al...

Reference #2 context: ...f{#1}} \def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}} \def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{...

Reference #2 context: ...f{#2}} \def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}} % Reference to a part, lower case \def\partref#1...

Reference #2 context: ...~\ref{#1}} \def\twopartref#1#2{parts \ref{#1} and \ref{#2}}  \def\ceil#1{\lceil #1 \rceil} \def\floor#1{\lfl...

Reference #3 context: ...ef\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}} % Section reference, lower-case. \d...

Reference #3 context: ...def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}} % Reference to an equation, lower-case. \def\eqr...

Reference #4 context: ...#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}} % Section reference, lower-case. \def\secref#1{s...
=== EXECUTION INFO ===
Return code: 0
Execution time: 0.052 seconds
Timeout limit: 30 seconds
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

I can see that the `#1`, `#2`, etc. in math_commands.tex are actually LaTeX macro parameters, not references to be resolved. These are correct as they are. The real issues are just the two undefined references I already fixed. 

Now let me check for some other potential issues by looking at specific citations that might have typos:
</div>

<div className="tool-call-box">
<svg width="14px" height="14px" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fillRule="evenodd" clipRule="evenodd" d="M13.0164 2C10.8193 2 9.03825 3.72453 9.03825 5.85185V8.51852H15.9235V9.25926H5.97814C3.78107 9.25926 2 10.9838 2 13.1111L2 18.8889C2 21.0162 3.78107 22.7407 5.97814 22.7407H8.27322V19.4815C8.27322 17.3542 10.0543 15.6296 12.2514 15.6296H19.5956C21.4547 15.6296 22.9617 14.1704 22.9617 12.3704V5.85185C22.9617 3.72453 21.1807 2 18.9836 2H13.0164ZM12.0984 6.74074C12.8589 6.74074 13.4754 6.14378 13.4754 5.40741C13.4754 4.67103 12.8589 4.07407 12.0984 4.07407C11.3378 4.07407 10.7213 4.67103 10.7213 5.40741C10.7213 6.14378 11.3378 6.74074 12.0984 6.74074Z" fill="#327EBD"/><path fillRule="evenodd" clipRule="evenodd" d="M18.9834 30C21.1805 30 22.9616 28.2755 22.9616 26.1482V23.4815L16.0763 23.4815L16.0763 22.7408L26.0217 22.7408C28.2188 22.7408 29.9998 21.0162 29.9998 18.8889V13.1111C29.9998 10.9838 28.2188 9.25928 26.0217 9.25928L23.7266 9.25928V12.5185C23.7266 14.6459 21.9455 16.3704 19.7485 16.3704L12.4042 16.3704C10.5451 16.3704 9.03809 17.8296 9.03809 19.6296L9.03809 26.1482C9.03809 28.2755 10.8192 30 13.0162 30H18.9834ZM19.9015 25.2593C19.1409 25.2593 18.5244 25.8562 18.5244 26.5926C18.5244 27.329 19.1409 27.9259 19.9015 27.9259C20.662 27.9259 21.2785 27.329 21.2785 26.5926C21.2785 25.8562 20.662 25.2593 19.9015 25.2593Z" fill="#FFDA4B"/></svg> `python-execute`

```python 
# Let's check for any potential citation issues by looking for common patterns
import re

def check_citation_patterns():
    """Check for potential citation issues"""
    
    tex_files = [
        '/workspace/dumps/workspace/my_paper/sections/1_introduction.tex',
        '/workspace/dumps/workspace/my_paper/sections/2_scenarios.tex', 
        '/workspace/dumps/workspace/my_paper/sections/3_advanced.tex',
        '/workspace/dumps/workspace/my_paper/sections/4_evaluation.tex',
        '/workspace/dumps/workspace/my_paper/sections/5_tradeoff.tex',
        '/workspace/dumps/workspace/my_paper/sections/conclusion.tex',
        '/workspace/dumps/workspace/my_paper/appendix/learning-effort.tex',
        '/workspace/dumps/workspace/my_paper/appendix/embodied.tex'
    ]
    
    # Read bib file to get all available keys
    with open('/workspace/dumps/workspace/my_paper/colm2024_conference.bib', 'r') as f:
        bib_content = f.read()
    bib_keys = set(re.findall(r'@[^{]+\{([^,]+),', bib_content))
    
    print("=== CHECKING FOR POTENTIAL CITATION ISSUES ===")
    
    for tex_file in tex_files:
        try:
            with open(tex_file, 'r') as f:
                content = f.read()
            
            # Find all citations
            cite_patterns = [
                r'\\cite\{([^}]+)\}',
                r'\\citep\{([^}]+)\}', 
                r'\\citet\{([^}]+)\}',
            ]
            
            for pattern in cite_patterns:
                matches = re.findall(pattern, content)
                for match in matches:
                    cites = [c.strip() for c in match.split(',')]
                    for cite in cites:
                        if cite not in bib_keys:
                            print(f"❌ Undefined citation '{cite}' in {tex_file}")
            
            # Look for potential issues like empty citations or malformed ones
            empty_cites = re.findall(r'\\cite\{\s*\}', content)
            if empty_cites:
                print(f"⚠️  Empty citations found in {tex_file}")
                
        except FileNotFoundError:
            continue

check_citation_patterns()
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
=== STDOUT ===
```
</summary>
```json
=== STDOUT ===
=== CHECKING FOR POTENTIAL CITATION ISSUES ===
=== EXECUTION INFO ===
Return code: 0
Execution time: 0.052 seconds
Timeout limit: 30 seconds
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Great! No undefined citations found. Now let me check for some issues I noticed in the text - there are some missing citations that should be added:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

```json
{
	"path": "/workspace/dumps/workspace/my_paper/sections/1_introduction.tex",
	 "head": 110
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
\section{Introduction}
```
</summary>
```json
\section{Introduction}
\label{sec:intro}

Language Models (LMs) have become increasingly effective in solving text-generation tasks, by taking in natural language (NL) instructions from users and outputting NL responses, such as answering the ``What is the capital of the US?'' with ``Washington D.C.''. However, LMs often struggle to perform tasks that require complex skills (e.g., math or complex reasoning), and are fundamentally unable to solve other tasks that require access to information not included in their training data (e.g., the current weather or date).

\begin{wrapfigure}[13]{r}{0.42\textwidth}
\vspace{-4mm}    
\includegraphics[width=0.39\textwidth]{./figures/definition.pdf}
\vspace{-2mm}
\caption{Illustration of tools extending and facilitating LM task-solving.}
\label{fig:def}
\end{wrapfigure}

To solve this problem, researchers and practitioners are turning to LMs enhanced with \emph{tools}, which help \textit{facilitate} the task-solving process of LMs, or \textit{extend} LMs with new abilities that the LM does not possess otherwise \citep{qin2023tool,mialon2023augmented}.
For example, a \texttt{calculator} tool may be used to facilitate mathematical calculations, or a \texttt{get\_time()} tool could be used to obtain the current time, which is not available purely through the LM's parameters. 
Inspired by the tools used by humans \citep{shumaker2011animal}, some works introduce application-specific \texttt{software} as tools, such as using a \texttt{search engine} to obtain knowledge \citep{lazaridou2022internetaugmented,komeili-etal-2022-internet}, using a \texttt{translator} to process unknown languages \citep{schick2023toolformer}, or using a \texttt{SQL engine} to query databases \citep{hao2023toolkengpt,zhuang2023toolqa}.
With the development of numerous application programming interfaces (APIs) on the web, many works collect \texttt{API}s as tools to access world data in real-time \citep{balog2016deepcoder,xu2023tool,qin2023toolllm} via multiple modalities \citep{tang2023toolalpaca}, even performing professional activities such as financial analysis \citep{li-etal-2023-api} and digital marketing \citep{huang2024metatool}.
Instead of using black-box APIs with unseen implementations, other works use locally-crafted \texttt{function}s to query over structured tables \citep{wang2024executable,cao2023api} or images \citep{suris2023vipergpt}, where the function tools can be created by human \citep{gupta2022visual} or model experts \citep{wang2023voyager,cai2023large,wang2024trove}.

% issue, motivation; our diff from existing surveys
However, despite this broad and burgeoning area of tool use in LMs, existing surveys only cover certain tool categories such as software \citep{mialon2023augmented} or APIs \citep{qin2023tool}. 
In this paper, we (1) provide a unified view of tool use across a broad range of scenarios, (2) empirically analyze the cost efficiency of tooling methods, to give practical guidance on when and how one should use tools, and (3) offer concrete suggestions for evaluations.

We start with proposing \textit{a unified definition} of tools and explain \textit{why tools help task-solving} (\S\ref{sec:background}).
We first introduce the \emph{basic tool-use paradigm} (\S\ref{sec:basic-paradigm}) and study a variety of tool-using scenarios by enumerating \textit{which tools exist} and \textit{to which tasks they apply} (\S\ref{sec:scenarios}). 
Next, we study advanced approaches for \textit{complex tool usage} and even \textit{make new tools} if they are unavailable for the task (\S\ref{sec:methods}). 
We then summarize existing testbeds and evaluation metrics across LM tooling works, and highlight several missing aspects with concrete metric suggestions (\S\ref{sec:good-tool}).
Lastly, grounding on our empirical analysis about \textit{when tools are effective}, we identify the most efficient tooling approaches and the tasks that benefit most from tools (\S\ref{sub:trade-off}). 



% ##################
\section{Background}
\label{sec:background}

\subsection{What are tools?}
% how existing works define tools
Because LMs are products of the digital world, tools employed by LMs are often computer \textbf{programs} that are executable in corresponding environments, e.g., Python programs are executable in Python environments. 
Referring back to human-used tools, \citet{shumaker2011animal} defines animal tool use as \textit{``the external employment of an unattached or manipulable attached environmental object to alter more efficiently the form, position, or condition of another object.''}
% two properties of tools
Similar to this definition of physical tools, LM-used program tools should also be \textbf{external} to the employer (i.e., the LM) and are part of the environment. 
In the meantime, instead of arbitrary program snippets, a tool is a \textbf{function} (e.g., \texttt{plus\_one}), meaning that it can be applied to other objects (e.g., data) and yield an output (e.g. \texttt{plus\_one}$(1) \rightarrow 2$).

Existing definitions of LM-used tools touch on some of these aspects. 
\citet{qin2023tool} make an intuitive appeal to the similarity to human tool use, but do not define what entails a tool.
\citet{mialon2023augmented} define \textit{a tool} as \textit{``an external module that is typically called using a rule or a special token and whose output is included in the augmented LM's context.''}
We argue for a somewhat more broad definition than this, which encompasses a wide variety of more recent works on tool usage:

\begin{definition}
  \label{def:tools-lm}
  An LM-used tool is a function interface to a computer program that runs \textit{externally} to the LM, where the LM generates the function calls and input arguments in order to use the tool.\footnote{Under our definition, tool functions can be implemented by any means, including symbolic computation or neural networks --- the functions only require a programmatic interface.}
\end{definition} 


\subsection{Why are tools helpful?}
Tools can help task-solving in different ways, depending on the functionality of individual tools. We summarize their functions into three major categories: perception, action, and computation. A tool may belong to one or more of these three categories.

\noindent \textbf{Perception} \quad
Perception tools provide or collect information from the environment. An example is using a \texttt{get\_time()} API to obtain the current time, which is not included in the LM's parametric knowledge learned from training.

\noindent \textbf{Action} \quad
Action tools can exert actions on the environment and change its state. For example, \texttt{turn\_left()} can shift the direction of an embodied agent, or executing \texttt{make\_post(website, post)} can change the content on a \texttt{website}.

\noindent \textbf{Computation} \quad
Computation tools do not necessarily perceive or modify the external environment, but use programs to tackle complex computational tasks. For example, a \texttt{calculator} is a computation tool for mathematical calculation.
Note that the computation also includes more general acts of computing beyond numerical calculation. Therefore, a \texttt{translator} is also a computation tool that can be used to translate between languages.

Note that many tools can fall into multiple categories.
For instance, a \texttt{search engine} is a tool that can perform both computation and perception.
As computation, it measures document similarity and selects relevant ones, but it also perceives the environment (i.e., the web) and fetches data (i.e., returned documents) from it.
In a similar spirit, \textsc{SQL} queries can be used as computation tools (e.g., \texttt{SELECT SQRT(16) / 10 AS result}), perception tools for viewing data (e.g., \texttt{SELECT name FROM data}), action tools to modify data (e.g., \texttt{INSERT INTO data VALUES name}), or all of the above (e.g., \texttt{INSERT INTO counts (grp\_id, grp\_cnt) SELECT grp\_id, COUNT(*) FROM data GROUP BY grp\_id}).

\subsection{Tools and ``Agents''} 
There has recently been a burgeoning of work on LM-powered agents \citep{xi2023rise,sumers2024cognitive}.
\citet{russell2010artificial} define agents as \textit{``anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.''}
According to this definition, agents are programs that use perception tools to perceive the situated environment, or action tools to interact with the environment.
Models that only use computation tools and do not interact with their environments through perception or action tools arguably do not fall under the category of ``agents'' according to this definition.



% ###################################
\section{The basic tool use paradigm}
\label{sec:basic-paradigm}
% figure start
\begin{wrapfigure}[16]{r}{0.42\textwidth}
\vspace{-5mm}
\includegraphics[width=0.40\textwidth]{./figures/call-api.pdf}
\vspace{-2mm}
\caption{The basic tool use paradigm. LM calls \raisebox{0.3mm}{\colorbox{violet!13}{\texttt{check\_weather}}} tool by generating text tokens. This call triggers the server to execute the call and return the output \raisebox{0.3mm}{\colorbox{yellow!33}{sunny}}, using which the LM replaces the API call tokens in the response to the user.}
\label{fig:call-api}
\end{wrapfigure}
% figure end

First, in this section, we show an illustrative example of a basic tool-use paradigm introduced by Toolformer \citep{schick2023toolformer}, which many tool-related works adopt (\autoref{fig:call-api}). Assuming an LM communicates with users mainly in natural language, upon receiving a user query such as ``How is the weather today?'', the LM then proceeds to generate either text or tool calls. In the example, starts with generating a few tokens of text ``It is ...''. When the LM needs to seek external tools to complete the task, e.g., get real-time weather information, it generates tokens of the tool name and corresponding input arguments enclosed with \texttt{(}parentheses\texttt{)} to construct a complete tool calling expression. 
This completed expression will trigger a shift from text-generation mode to tool-execution mode. The server hosting the tool will execute the expression and return the execution result to the LM. 

Taking the example in \autoref{fig:call-api}, the LM sends the \texttt{check\_weather()} call to the weather server and receives the output ``sunny''. The returned result replaces the tool call in the LM-generated tokens (e.g., from ``It is \texttt{check\_weather()}'' to ``It is sunny''), which is used for subsequent steps of generation.
Accordingly, the LM shifts back to the text generation mode and continues to finish the response by generating new text tokens, e.g., adding `today.', and finally returning the response to the user.

In order for LMs to use this basic paradigm of using tools, current works mainly leverage inference-time prompting and training-time learning methods.

\noindent \textbf{Inference-time prompting} \quad
Leveraging the ability of LMs to learn in-context \citep{}, many works provide tool information through a prompt and expect LMs to acquire abilities to use these tools from input contexts.
This is achieved by providing instructions about the task, example pairs of queries and solutions that use tools \citep{gupta2022visual,lu2023chameleon,paranjape2023art,shen2023hugginggpt,yang2023mmreact}, and/or documentation of the tools' functionality \citep{hsieh2023tool}.


\noindent \textbf{Learning by training} \quad
Beyond learning tools from test-time contexts, LMs can learn from examples that use these tools during training.
LMs can simply be trained to generate tool-using solutions, where the examples can be manually annotated by humans \citep{li-etal-2023-api}, synthesized by larger teacher LMs \citep{tang2023toolalpaca,qin2023toolllm,huang2024metatool}, or bootstrapped by the test-time LM itself \citep{schick2023toolformer}.
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

I notice there's a missing citation in the "Inference-time prompting" section. Let me fix that:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem edit_file`

```json
{
	"path": "/workspace/dumps/workspace/my_paper/sections/1_introduction.tex",
	 "edits": [{"oldText":"Leveraging the ability of LMs to learn in-context \\citep{},
	 many works provide tool information through a prompt",
	"newText":"Leveraging the ability of LMs to learn in-context \\citep{brown2020language},
	 many works provide tool information through a prompt"}]
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
diff
```
</summary>
```json
diff
Index: /workspace/dumps/workspace/my_paper/sections/1_introduction.tex
===================================================================
--- /workspace/dumps/workspace/my_paper/sections/1_introduction.tex	original
+++ /workspace/dumps/workspace/my_paper/sections/1_introduction.tex	modified
@@ -99,9 +99,9 @@
 
 In order for LMs to use this basic paradigm of using tools, current works mainly leverage inference-time prompting and training-time learning methods.
 
 \noindent \textbf{Inference-time prompting} \quad
-Leveraging the ability of LMs to learn in-context \citep{}, many works provide tool information through a prompt and expect LMs to acquire abilities to use these tools from input contexts.
+Leveraging the ability of LMs to learn in-context \citep{brown2020language}, many works provide tool information through a prompt and expect LMs to acquire abilities to use these tools from input contexts.
 This is achieved by providing instructions about the task, example pairs of queries and solutions that use tools \citep{gupta2022visual,lu2023chameleon,paranjape2023art,shen2023hugginggpt,yang2023mmreact}, and/or documentation of the tools' functionality \citep{hsieh2023tool}.
 
 
 \noindent \textbf{Learning by training} \quad



```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me check for another missing citation I noticed in the sections:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

```json
{
	"path": "/workspace/dumps/workspace/my_paper/sections/3_advanced.tex",
	 "head": 50
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
\section{Advanced tool-use methods}
```
</summary>
```json
\section{Advanced tool-use methods}
\label{sec:methods}

Given this understanding of the basic tooling paradigm and the scenarios in which tools are useful, we now discuss more advanced approaches for tools.
Concretely, we study multi-tool selection and usage (\S\ref{sub:tool-select}), complex tooling under programmatic contexts (\S\ref{sub:tools-and-programs}), and creation of tools when they are not available a-priori (\S\ref{sub:make-tool}). 


% #########################
\subsection{Complex tool selection and usage}
\label{sub:tool-select}

Depending on the number of tools available, the system may include an implicit or explicit tool selection module.
If tools are already \textit{designated} for the task \citep{lazaridou2022internetaugmented,thoppilan2022lamda}, then no tool selection is needed.
If \textit{a small number} (e.g., 5--10) of tools are available, metadata and use cases of these tools can be provided as input contexts along with the user query \citep{schick2023toolformer,paranjape2023art}, and LMs can directly select tools from contexts via a standard generation process.
If the toolbox size \textit{further grows} (e.g., to hundreds), fitting all tools into model inputs is not feasible. Thus an extra retrieval step is often incorporated: a retriever model short-lists the most relevant tools and feeds their metadata to the solution-generation LM. Specifically, \citet{zhou2023docprompting,qin2023toolllm} train retriever models that map NL intents to tool documentation.
\citet{yuan2023craft} ask LMs to write hypothetical descriptions and use the SimCSE retriever \citep{} to find similar tools. More easily, one can directly use off-the-shelf embeddings \citep{SFRAIResearch2024,openai-emb} or training-free sparse retrievers \citep{robertson2009probabilistic}.


% \noindent \textbf{Complex, multi-tool usage} \quad
For complex queries that require multiple tools to solve, the common approach so far is to break down the task and tackle each step sequentially \citep{paranjape2023art} by selecting and using tools with intermediate contexts.
However, this sequential multi-turn paradigm may not be reflective of more complex or realistic usage of the involved tools. For example, a user may prefer \textit{nested} function calls \texttt{check\_weather(get\_local\_time(`Pittsburgh'))} to allow information hiding or encapsulation \citep{rogers2001encapsulation}, \textit{parallel} calls to reduce round trips with the API \citep{eleti2023function}, or \textit{iterative} calls \texttt{buy\_ticket(event)} in a loop until it returns \texttt{True} to indicate a successful transaction. 


% ################################
\subsection{Tools in programmatic contexts}
\label{sub:tools-and-programs}

Unlike text-based tasks where tools are auxiliary modules to extend LM abilities, on programmatic tasks, where code LMs can solve the problem by generating programs, tools can be seen as compositions of basic functions.
In this part, we discuss tools in programmatic tasks for domain-specific (\S\ref{}) and general-purpose problems (\S\ref{}).


\noindent \textbf{Focus on varied tools} \quad
Depending on the tasks of interest, existing works focus on different types of tools under programmatic contexts. With the increasing complexity of these tools and presumably a decreasing familiarity of LMs about them, there are works that adopt (i) \textcolor{candypink}{\textit{built-in functions}} of a programming language (PL) to augment LMs in symbolic reasoning, (ii) \textcolor{ao!80}{\textit{external libraries}} in pre-designed packages to tackle complex open-domain coding queries \citep{wang2023execution}, and (iii) \textcolor{blue(ncs)}{\textit{utility functions}} unseen at training time to solve specific tasks.

\begin{figure}[ht]
\centering
\vspace{-1mm}
    \includegraphics[width=\textwidth]{./figures/codelm-tools.pdf}
\vspace{-6mm}
\caption{Relative to what is considered as the base LM or base actions, tools can refer to built-in functions, external libraries, or task-specific utility functions (from left to right).}
\vspace{-1mm}
\label{fig:codelm-tools}
\end{figure}



% ############
\subsubsection{Domain-specific semantic parsing}
\label{sub:domain-spec}

```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

I found another missing citation. Let me fix it:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem edit_file`

```json
{
	"path": "/workspace/dumps/workspace/my_paper/sections/3_advanced.tex",
	 "edits": [{"oldText":"\\citet{yuan2023craft} ask LMs to write hypothetical descriptions and use the SimCSE retriever \\citep{} to find similar tools.",
	"newText":"\\citet{yuan2023craft} ask LMs to write hypothetical descriptions and use the SimCSE retriever \\citep{gao2021simcse} to find similar tools."}]
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
diff
```
</summary>
```json
diff
Index: /workspace/dumps/workspace/my_paper/sections/3_advanced.tex
===================================================================
--- /workspace/dumps/workspace/my_paper/sections/3_advanced.tex	original
+++ /workspace/dumps/workspace/my_paper/sections/3_advanced.tex	modified
@@ -12,9 +12,9 @@
 Depending on the number of tools available, the system may include an implicit or explicit tool selection module.
 If tools are already \textit{designated} for the task \citep{lazaridou2022internetaugmented,thoppilan2022lamda}, then no tool selection is needed.
 If \textit{a small number} (e.g., 5--10) of tools are available, metadata and use cases of these tools can be provided as input contexts along with the user query \citep{schick2023toolformer,paranjape2023art}, and LMs can directly select tools from contexts via a standard generation process.
 If the toolbox size \textit{further grows} (e.g., to hundreds), fitting all tools into model inputs is not feasible. Thus an extra retrieval step is often incorporated: a retriever model short-lists the most relevant tools and feeds their metadata to the solution-generation LM. Specifically, \citet{zhou2023docprompting,qin2023toolllm} train retriever models that map NL intents to tool documentation.
-\citet{yuan2023craft} ask LMs to write hypothetical descriptions and use the SimCSE retriever \citep{} to find similar tools. More easily, one can directly use off-the-shelf embeddings \citep{SFRAIResearch2024,openai-emb} or training-free sparse retrievers \citep{robertson2009probabilistic}.
+\citet{yuan2023craft} ask LMs to write hypothetical descriptions and use the SimCSE retriever \citep{gao2021simcse} to find similar tools. More easily, one can directly use off-the-shelf embeddings \citep{SFRAIResearch2024,openai-emb} or training-free sparse retrievers \citep{robertson2009probabilistic}.
 
 
 % \noindent \textbf{Complex, multi-tool usage} \quad
 For complex queries that require multiple tools to solve, the common approach so far is to break down the task and tackle each step sequentially \citep{paranjape2023art} by selecting and using tools with intermediate contexts.



```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me check for missing section references I noticed:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem edit_file`

```json
{
	"path": "/workspace/dumps/workspace/my_paper/sections/3_advanced.tex",
	 "edits": [{"oldText":"In this part,
	 we discuss tools in programmatic tasks for domain-specific (\\S\\ref{}) and general-purpose problems (\\S\\ref{}).",
	"newText":"In this part,
	 we discuss tools in programmatic tasks for domain-specific (\\S\\ref{sub:domain-spec}) and general-purpose problems (\\S\\ref{sub:general-codegen})."}]
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
diff
```
</summary>
```json
diff
Index: /workspace/dumps/workspace/my_paper/sections/3_advanced.tex
===================================================================
--- /workspace/dumps/workspace/my_paper/sections/3_advanced.tex	original
+++ /workspace/dumps/workspace/my_paper/sections/3_advanced.tex	modified
@@ -25,9 +25,9 @@
 \subsection{Tools in programmatic contexts}
 \label{sub:tools-and-programs}
 
 Unlike text-based tasks where tools are auxiliary modules to extend LM abilities, on programmatic tasks, where code LMs can solve the problem by generating programs, tools can be seen as compositions of basic functions.
-In this part, we discuss tools in programmatic tasks for domain-specific (\S\ref{}) and general-purpose problems (\S\ref{}).
+In this part, we discuss tools in programmatic tasks for domain-specific (\S\ref{sub:domain-spec}) and general-purpose problems (\S\ref{sub:general-codegen}).
 
 
 \noindent \textbf{Focus on varied tools} \quad
 Depending on the tasks of interest, existing works focus on different types of tools under programmatic contexts. With the increasing complexity of these tools and presumably a decreasing familiarity of LMs about them, there are works that adopt (i) \textcolor{candypink}{\textit{built-in functions}} of a programming language (PL) to augment LMs in symbolic reasoning, (ii) \textcolor{ao!80}{\textit{external libraries}} in pre-designed packages to tackle complex open-domain coding queries \citep{wang2023execution}, and (iii) \textcolor{blue(ncs)}{\textit{utility functions}} unseen at training time to solve specific tasks.



```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me look for another missing figure reference I noticed:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

```json
{
	"path": "/workspace/dumps/workspace/my_paper/sections/3_advanced.tex",
	 "head": 80
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
\section{Advanced tool-use methods}
```
</summary>
```json
\section{Advanced tool-use methods}
\label{sec:methods}

Given this understanding of the basic tooling paradigm and the scenarios in which tools are useful, we now discuss more advanced approaches for tools.
Concretely, we study multi-tool selection and usage (\S\ref{sub:tool-select}), complex tooling under programmatic contexts (\S\ref{sub:tools-and-programs}), and creation of tools when they are not available a-priori (\S\ref{sub:make-tool}). 


% #########################
\subsection{Complex tool selection and usage}
\label{sub:tool-select}

Depending on the number of tools available, the system may include an implicit or explicit tool selection module.
If tools are already \textit{designated} for the task \citep{lazaridou2022internetaugmented,thoppilan2022lamda}, then no tool selection is needed.
If \textit{a small number} (e.g., 5--10) of tools are available, metadata and use cases of these tools can be provided as input contexts along with the user query \citep{schick2023toolformer,paranjape2023art}, and LMs can directly select tools from contexts via a standard generation process.
If the toolbox size \textit{further grows} (e.g., to hundreds), fitting all tools into model inputs is not feasible. Thus an extra retrieval step is often incorporated: a retriever model short-lists the most relevant tools and feeds their metadata to the solution-generation LM. Specifically, \citet{zhou2023docprompting,qin2023toolllm} train retriever models that map NL intents to tool documentation.
\citet{yuan2023craft} ask LMs to write hypothetical descriptions and use the SimCSE retriever \citep{gao2021simcse} to find similar tools. More easily, one can directly use off-the-shelf embeddings \citep{SFRAIResearch2024,openai-emb} or training-free sparse retrievers \citep{robertson2009probabilistic}.


% \noindent \textbf{Complex, multi-tool usage} \quad
For complex queries that require multiple tools to solve, the common approach so far is to break down the task and tackle each step sequentially \citep{paranjape2023art} by selecting and using tools with intermediate contexts.
However, this sequential multi-turn paradigm may not be reflective of more complex or realistic usage of the involved tools. For example, a user may prefer \textit{nested} function calls \texttt{check\_weather(get\_local\_time(`Pittsburgh'))} to allow information hiding or encapsulation \citep{rogers2001encapsulation}, \textit{parallel} calls to reduce round trips with the API \citep{eleti2023function}, or \textit{iterative} calls \texttt{buy\_ticket(event)} in a loop until it returns \texttt{True} to indicate a successful transaction. 


% ################################
\subsection{Tools in programmatic contexts}
\label{sub:tools-and-programs}

Unlike text-based tasks where tools are auxiliary modules to extend LM abilities, on programmatic tasks, where code LMs can solve the problem by generating programs, tools can be seen as compositions of basic functions.
In this part, we discuss tools in programmatic tasks for domain-specific (\S\ref{sub:domain-spec}) and general-purpose problems (\S\ref{sub:general-codegen}).


\noindent \textbf{Focus on varied tools} \quad
Depending on the tasks of interest, existing works focus on different types of tools under programmatic contexts. With the increasing complexity of these tools and presumably a decreasing familiarity of LMs about them, there are works that adopt (i) \textcolor{candypink}{\textit{built-in functions}} of a programming language (PL) to augment LMs in symbolic reasoning, (ii) \textcolor{ao!80}{\textit{external libraries}} in pre-designed packages to tackle complex open-domain coding queries \citep{wang2023execution}, and (iii) \textcolor{blue(ncs)}{\textit{utility functions}} unseen at training time to solve specific tasks.

\begin{figure}[ht]
\centering
\vspace{-1mm}
    \includegraphics[width=\textwidth]{./figures/codelm-tools.pdf}
\vspace{-6mm}
\caption{Relative to what is considered as the base LM or base actions, tools can refer to built-in functions, external libraries, or task-specific utility functions (from left to right).}
\vspace{-1mm}
\label{fig:codelm-tools}
\end{figure}



% ############
\subsubsection{Domain-specific semantic parsing}
\label{sub:domain-spec}

NL-to-code generation systems have been studied for many years on special-domain tasks such as querying databases \citep{zelle1996learning,zettlemoyer2012learning} or knowledge graphs \citep{berant2013semantic}. Code produced by these systems is often domain-specific logical forms (DSL) manually designed by experts, such as lambda expressions \citep{liang2013lambda} or SQL queries \citep{yu2018spider}, and more recently, the QDMR grammar \citep{wolfson2020break} as an extension to SQL.
In addition to knowledge-oriented tasks, many agentic tasks adopt DSL to operate in corresponding environments, such as \texttt{click} or \texttt{type} in web navigation \citep{zheran2018reinforcement,webshop2022yao,zhou2024webarena}, \texttt{placeItem} and \texttt{killMob} in the embodied Minecraft world \citep{wang2023voyager}, or \texttt{set\_joint\_target} for robot dogs \citep{yu2023language}.
Because DSLs are often specific enough to the target problems, most works directly use these \textcolor{candypink}{built-in actions}.
Yet still, for complex task queries, solution programs written in basic DSL actions alone can be hard to interpret or cumbersome to use, e.g., it is hard to tell that the lambda expression \texttt{(fold xs ($\lambda$ (n x) (+ 1 n)) 0)} is to calculate the length of \texttt{xs}.




% ############
\subsubsection{General-purpose code generation}
\label{sub:general-codegen}
Recent code generation systems have expanded from using DSL to more general-purpose PLs such as Python or Java \citep{yin-neubig-2017-syntactic,chen2021evaluating}. These languages enable more programming flexibility and readily apply to versatile scenarios. 
As we have introduced using \textcolor{candypink}{built-in actions} as tools in \S\ref{sub:app-spec}, we discuss more on two other
common categories of tools for code LMs, namely \textcolor{ao!90}{\textit{external libraries}} and task-specific \textcolor{blue(ncs)}{\textit{utility functions}}.


\noindent \textbf{External libraries} \quad
From the usage of PLs, built-in functions are internal to whichever environment, whereas third-party libraries lie externally and need to be imported to tackle specific contexts, such as \autoref{} (middle). Aligning with this conception, \citet{zhang2023toolcoder} use Python libraries such as \texttt{matplotlib} to plot figures and \texttt{pandas} to manage data.


\noindent \textbf{Utility functions} \quad
For more task-specific applications,
expert-crafted utility functions, usually unseen at training time, are incorporated as tools. E.g., in \autoref{fig:codelm-tools} (right), the highlighted \texttt{locate\_objects} function is designed by human experts \citep{gupta2022visual,suris2023vipergpt} to load neural models and perform post-processing to obtain the detected box region. In a similar spirit, \citet{cheng2023binding} use GPT as a tool to query world facts external to the tabular contents, \citet{cao2023api} further design macro operation APIs to support advanced tabular operations.
However, because human tool curation requires expertise and effort, some works explore using LMs to automatically create tools instead.



% ################################

\begin{wrapfigure}[8]{r}{0.40\textwidth}
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

I found another missing reference. Let me fix it:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem edit_file`

```json
{
	"path": "/workspace/dumps/workspace/my_paper/sections/3_advanced.tex",
	 "edits": [{"oldText":"From the usage of PLs,
	 built-in functions are internal to whichever environment,
	 whereas third-party libraries lie externally and need to be imported to tackle specific contexts,
	 such as \\autoref{} (middle).",
	"newText":"From the usage of PLs,
	 built-in functions are internal to whichever environment,
	 whereas third-party libraries lie externally and need to be imported to tackle specific contexts,
	 such as \\autoref{fig:codelm-tools} (middle)."}]
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
diff
```
</summary>
```json
diff
Index: /workspace/dumps/workspace/my_paper/sections/3_advanced.tex
===================================================================
--- /workspace/dumps/workspace/my_paper/sections/3_advanced.tex	original
+++ /workspace/dumps/workspace/my_paper/sections/3_advanced.tex	modified
@@ -64,9 +64,9 @@
 common categories of tools for code LMs, namely \textcolor{ao!90}{\textit{external libraries}} and task-specific \textcolor{blue(ncs)}{\textit{utility functions}}.
 
 
 \noindent \textbf{External libraries} \quad
-From the usage of PLs, built-in functions are internal to whichever environment, whereas third-party libraries lie externally and need to be imported to tackle specific contexts, such as \autoref{} (middle). Aligning with this conception, \citet{zhang2023toolcoder} use Python libraries such as \texttt{matplotlib} to plot figures and \texttt{pandas} to manage data.
+From the usage of PLs, built-in functions are internal to whichever environment, whereas third-party libraries lie externally and need to be imported to tackle specific contexts, such as \autoref{fig:codelm-tools} (middle). Aligning with this conception, \citet{zhang2023toolcoder} use Python libraries such as \texttt{matplotlib} to plot figures and \texttt{pandas} to manage data.
 
 
 \noindent \textbf{Utility functions} \quad
 For more task-specific applications,



```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Now let me run the analysis again to see if I've fixed all the issues:
</div>

<div className="tool-call-box">
<svg width="14px" height="14px" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fillRule="evenodd" clipRule="evenodd" d="M13.0164 2C10.8193 2 9.03825 3.72453 9.03825 5.85185V8.51852H15.9235V9.25926H5.97814C3.78107 9.25926 2 10.9838 2 13.1111L2 18.8889C2 21.0162 3.78107 22.7407 5.97814 22.7407H8.27322V19.4815C8.27322 17.3542 10.0543 15.6296 12.2514 15.6296H19.5956C21.4547 15.6296 22.9617 14.1704 22.9617 12.3704V5.85185C22.9617 3.72453 21.1807 2 18.9836 2H13.0164ZM12.0984 6.74074C12.8589 6.74074 13.4754 6.14378 13.4754 5.40741C13.4754 4.67103 12.8589 4.07407 12.0984 4.07407C11.3378 4.07407 10.7213 4.67103 10.7213 5.40741C10.7213 6.14378 11.3378 6.74074 12.0984 6.74074Z" fill="#327EBD"/><path fillRule="evenodd" clipRule="evenodd" d="M18.9834 30C21.1805 30 22.9616 28.2755 22.9616 26.1482V23.4815L16.0763 23.4815L16.0763 22.7408L26.0217 22.7408C28.2188 22.7408 29.9998 21.0162 29.9998 18.8889V13.1111C29.9998 10.9838 28.2188 9.25928 26.0217 9.25928L23.7266 9.25928V12.5185C23.7266 14.6459 21.9455 16.3704 19.7485 16.3704L12.4042 16.3704C10.5451 16.3704 9.03809 17.8296 9.03809 19.6296L9.03809 26.1482C9.03809 28.2755 10.8192 30 13.0162 30H18.9834ZM19.9015 25.2593C19.1409 25.2593 18.5244 25.8562 18.5244 26.5926C18.5244 27.329 19.1409 27.9259 19.9015 27.9259C20.662 27.9259 21.2785 27.329 21.2785 26.5926C21.2785 25.8562 20.662 25.2593 19.9015 25.2593Z" fill="#FFDA4B"/></svg> `python-execute`

```python 
# Run the analysis again to see current status
results = analyze_citations_and_refs()
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
=== STDERR ===
```
</summary>
```json
=== STDERR ===
Traceback (most recent call last):
  File "/workspace/dumps/workspace/./.python_tmp/89ea0f46-b9cc-4691-8fa2-5f7334cdb6d8.py", line 2, in <module>
    results = analyze_citations_and_refs()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
NameError: name 'analyze_citations_and_refs' is not defined
=== EXECUTION INFO ===
Return code: 1
Execution time: 0.058 seconds
Timeout limit: 30 seconds
```
</details>
</div>

<div className="tool-call-box">
<svg width="14px" height="14px" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fillRule="evenodd" clipRule="evenodd" d="M13.0164 2C10.8193 2 9.03825 3.72453 9.03825 5.85185V8.51852H15.9235V9.25926H5.97814C3.78107 9.25926 2 10.9838 2 13.1111L2 18.8889C2 21.0162 3.78107 22.7407 5.97814 22.7407H8.27322V19.4815C8.27322 17.3542 10.0543 15.6296 12.2514 15.6296H19.5956C21.4547 15.6296 22.9617 14.1704 22.9617 12.3704V5.85185C22.9617 3.72453 21.1807 2 18.9836 2H13.0164ZM12.0984 6.74074C12.8589 6.74074 13.4754 6.14378 13.4754 5.40741C13.4754 4.67103 12.8589 4.07407 12.0984 4.07407C11.3378 4.07407 10.7213 4.67103 10.7213 5.40741C10.7213 6.14378 11.3378 6.74074 12.0984 6.74074Z" fill="#327EBD"/><path fillRule="evenodd" clipRule="evenodd" d="M18.9834 30C21.1805 30 22.9616 28.2755 22.9616 26.1482V23.4815L16.0763 23.4815L16.0763 22.7408L26.0217 22.7408C28.2188 22.7408 29.9998 21.0162 29.9998 18.8889V13.1111C29.9998 10.9838 28.2188 9.25928 26.0217 9.25928L23.7266 9.25928V12.5185C23.7266 14.6459 21.9455 16.3704 19.7485 16.3704L12.4042 16.3704C10.5451 16.3704 9.03809 17.8296 9.03809 19.6296L9.03809 26.1482C9.03809 28.2755 10.8192 30 13.0162 30H18.9834ZM19.9015 25.2593C19.1409 25.2593 18.5244 25.8562 18.5244 26.5926C18.5244 27.329 19.1409 27.9259 19.9015 27.9259C20.662 27.9259 21.2785 27.329 21.2785 26.5926C21.2785 25.8562 20.662 25.2593 19.9015 25.2593Z" fill="#FFDA4B"/></svg> `python-execute`

```python 
import re
import os
from collections import defaultdict, Counter

def extract_citations_and_references(tex_content):
    """Extract all \cite{}, \citep{}, \citet{}, \label{}, \ref{}, \autoref{} commands"""
    
    # Pattern for citations
    cite_patterns = [
        r'\\cite\{([^}]+)\}',
        r'\\citep\{([^}]+)\}', 
        r'\\citet\{([^}]+)\}',
        r'\\citealp\{([^}]+)\}',
        r'\\citealt\{([^}]+)\}',
    ]
    
    # Pattern for labels and references
    label_pattern = r'\\label\{([^}]+)\}'
    ref_patterns = [
        r'\\ref\{([^}]+)\}',
        r'\\autoref\{([^}]+)\}',
        r'\\eqref\{([^}]+)\}',
        r'\\figref\{([^}]+)\}',
        r'\\Figref\{([^}]+)\}',
        r'\\secref\{([^}]+)\}',
        r'\\Secref\{([^}]+)\}',
    ]
    
    citations = []
    labels = []
    references = []
    
    # Extract citations
    for pattern in cite_patterns:
        matches = re.findall(pattern, tex_content)
        for match in matches:
            # Handle multiple citations in one command
            cites = [c.strip() for c in match.split(',')]
            citations.extend(cites)
    
    # Extract labels
    labels = re.findall(label_pattern, tex_content)
    
    # Extract references
    for pattern in ref_patterns:
        matches = re.findall(pattern, tex_content)
        references.extend(matches)
    
    return citations, labels, references

def extract_bib_keys(bib_content):
    """Extract all bibliography keys from .bib file"""
    pattern = r'@[^{]+\{([^,]+),'
    return re.findall(pattern, bib_content)

def analyze_citations_and_refs():
    """Analyze all citations and references in the LaTeX project"""
    
    # Read all tex files
    tex_files = [
        '/workspace/dumps/workspace/my_paper/colm2024_conference.tex',
        '/workspace/dumps/workspace/my_paper/math_commands.tex',
        '/workspace/dumps/workspace/my_paper/sections/1_introduction.tex',
        '/workspace/dumps/workspace/my_paper/sections/2_scenarios.tex', 
        '/workspace/dumps/workspace/my_paper/sections/3_advanced.tex',
        '/workspace/dumps/workspace/my_paper/sections/4_evaluation.tex',
        '/workspace/dumps/workspace/my_paper/sections/5_tradeoff.tex',
        '/workspace/dumps/workspace/my_paper/sections/conclusion.tex',
        '/workspace/dumps/workspace/my_paper/appendix/learning-effort.tex',
        '/workspace/dumps/workspace/my_paper/appendix/embodied.tex'
    ]
    
    # Read bibliography file
    with open('/workspace/dumps/workspace/my_paper/colm2024_conference.bib', 'r') as f:
        bib_content = f.read()
    
    # Extract bibliography keys
    bib_keys = set(extract_bib_keys(bib_content))
    
    all_citations = []
    all_labels = []
    all_references = []
    
    # Process each tex file
    for tex_file in tex_files:
        try:
            with open(tex_file, 'r') as f:
                content = f.read()
            
            citations, labels, references = extract_citations_and_references(content)
            all_citations.extend(citations)
            all_labels.extend(labels)
            all_references.extend(references)
            
        except FileNotFoundError:
            print(f"File not found: {tex_file}")
    
    # Check for issues
    print("\n" + "="*60)
    print("CITATION AND REFERENCE ANALYSIS")
    print("="*60)
    
    # 1. Check for undefined citations
    undefined_citations = []
    for cite in set(all_citations):
        if cite not in bib_keys:
            undefined_citations.append(cite)
    
    if undefined_citations:
        print(f"\n❌ UNDEFINED CITATIONS ({len(undefined_citations)}):")
        for cite in sorted(undefined_citations):
            print(f"  - {cite}")
    else:
        print("\n✅ All citations are defined in the bibliography")
    
    # 2. Check for unused bibliography entries
    unused_bib = []
    for bib_key in bib_keys:
        if bib_key not in all_citations:
            unused_bib.append(bib_key)
    
    if unused_bib:
        print(f"\n⚠️  UNUSED BIBLIOGRAPHY ENTRIES ({len(unused_bib)}):")
        for bib in sorted(unused_bib):
            print(f"  - {bib}")
    else:
        print("\n✅ All bibliography entries are used")
    
    # 3. Check for undefined references - ignore LaTeX macro parameters
    undefined_refs = []
    label_set = set(all_labels)
    for ref in set(all_references):
        # Skip LaTeX macro parameters like #1, #2, etc.
        if not re.match(r'^#\d+$', ref) and ref not in label_set:
            undefined_refs.append(ref)
    
    if undefined_refs:
        print(f"\n❌ UNDEFINED REFERENCES ({len(undefined_refs)}):")
        for ref in sorted(undefined_refs):
            print(f"  - {ref}")
    else:
        print("\n✅ All references are defined")
    
    # 4. Check for unused labels
    unused_labels = []
    ref_set = set(all_references)
    for label in set(all_labels):
        if label not in ref_set:
            unused_labels.append(label)
    
    if unused_labels:
        print(f"\n⚠️  UNUSED LABELS ({len(unused_labels)}):")
        for label in sorted(unused_labels):
            print(f"  - {label}")
    else:
        print("\n✅ All labels are used")
    
    # 5. Summary statistics
    print(f"\n📊 SUMMARY STATISTICS:")
    print(f"  Total citations used: {len(all_citations)} (unique: {len(set(all_citations))})")
    print(f"  Total bibliography entries: {len(bib_keys)}")
    print(f"  Total labels defined: {len(all_labels)} (unique: {len(set(all_labels))})")
    print(f"  Total references used: {len(all_references)} (unique: {len(set(all_references))})")
    
    return {
        'undefined_citations': undefined_citations,
        'unused_bib': unused_bib, 
        'undefined_refs': undefined_refs,
        'unused_labels': unused_labels,
        'all_citations': all_citations,
        'all_labels': all_labels,
        'all_references': all_references,
        'bib_keys': list(bib_keys)
    }

# Run the analysis
results = analyze_citations_and_refs()
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
=== STDOUT ===
```
</summary>
```json
=== STDOUT ===

============================================================
CITATION AND REFERENCE ANALYSIS
============================================================

✅ All citations are defined in the bibliography

⚠️  UNUSED BIBLIOGRAPHY ENTRIES (22):
  - Baker2020Emergent
  - artzi2013weakly
  - chen2017reading
  - hf-models
  - liang2011learning
  - liang2023taskmatrixai
  - liu2023language
  - nakano2022webgpt
  - ouyang2022training
  - public-apis
  - shi2017world
  - shin2019program
  - tensorflow2015
  - torchhub
  - von1995cognitive
  - wang-etal-2022-super
  - xie2022unifiedskg
  - yang2023gpttools
  - ye2024tooleyes
  - yu2023spae
  - zettlemoyer2007online
  - zhou2023webarena

✅ All references are defined

⚠️  UNUSED LABELS (9):
  - fig:agent-env
  - fig:def
  - fig:inference-cost
  - fig:make-tool
  - fig:toolformer-cost
  - sec:embodied
  - sec:intro
  - tab:est-compute-cost
  - tab:missing-eval

📊 SUMMARY STATISTICS:
  Total citations used: 199 (unique: 95)
  Total bibliography entries: 117
  Total labels defined: 37 (unique: 37)
  Total references used: 74 (unique: 32)
=== STDERR ===
/workspace/dumps/workspace/./.python_tmp/d4d7dc14-498b-40ed-8239-bc8310fea856.py:6: SyntaxWarning: invalid escape sequence '\c'
  """Extract all \cite{}, \citep{}, \citet{}, \label{}, \ref{}, \autoref{} commands"""
=== EXECUTION INFO ===
Return code: 0
Execution time: 0.054 seconds
Timeout limit: 30 seconds
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

Excellent! I've successfully fixed the main issues:

✅ **Fixed Issues:**
1. **Undefined citations**: All citations are now properly defined in the bibliography
2. **Undefined references**: All references are now properly defined (the #1, #2, etc. were correctly identified as LaTeX macro parameters)

The remaining issues are minor and mostly acceptable:

⚠️ **Remaining Issues (mostly acceptable):**
1. **Unused bibliography entries (22)**: These are bibliography entries that exist but aren't cited. This is common in academic papers where authors prepare more references than they end up using.
2. **Unused labels (9)**: These are labels that are defined but not referenced. Some might be intentionally unused (like `sec:intro`, `sec:embodied`) or figures that aren't referenced in text.

Let me now create a comprehensive summary report:
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem-write_file`

```text workspace/citation_reference_check_report.md
# LaTeX Citation and Reference Check Report

## Summary

✅ **All critical issues have been fixed!**

Your LaTeX paper now has properly working citations and references. All `\cite{}`, `\label{}`, `\ref{}`, and `\autoref{}` commands are correctly cross-referenced.

## Issues Fixed

### 1. Undefined References (Fixed ✅)
- **`fig:call-api-v0`** → Fixed to **`fig:call-api`** in `sections/1_introduction.tex`
- **`tab:1`** → Fixed to **`tab:example-tools`** in `sections/2_scenarios.tex`

### 2. Missing Citations (Fixed ✅)
- **Empty citation `\citep{}`** → Fixed to **`\citep{brown2020language}`** in `sections/1_introduction.tex`
- **Empty citation `\citep{}`** → Fixed to **`\citep{gao2021simcse}`** in `sections/3_advanced.tex`

### 3. Missing Section References (Fixed ✅)
- **`\S\ref{}`** → Fixed to **`\S\ref{sub:domain-spec}`** and **`\S\ref{sub:general-codegen}`** in `sections/3_advanced.tex`
- **`\autoref{}`** → Fixed to **`\autoref{fig:codelm-tools}`** in `sections/3_advanced.tex`

## Current Status

### ✅ Citations
- **Total citations**: 199 (95 unique)
- **Bibliography entries**: 117
- **Status**: All citations properly defined ✅

### ✅ References  
- **Total references**: 74 (32 unique)
- **Labels defined**: 37
- **Status**: All references properly defined ✅

### ⚠️ Minor Issues (Acceptable)

#### Unused Bibliography Entries (22)
These are references in your `.bib` file that aren't cited. This is normal and acceptable:
- `Baker2020Emergent`, `artzi2013weakly`, `chen2017reading`, `hf-models`, `liang2011learning`, `liang2023taskmatrixai`, `liu2023language`, `nakano2022webgpt`, `ouyang2022training`, `public-apis`, `shi2017world`, `shin2019program`, `tensorflow2015`, `torchhub`, `von1995cognitive`, `wang-etal-2022-super`, `xie2022unifiedskg`, `yang2023gpttools`, `ye2024tooleyes`, `yu2023spae`, `zettlemoyer2007online`, `zhou2023webarena`

#### Unused Labels (9)
These are labels defined but not referenced. Some might be intentional:
- `fig:agent-env`, `fig:def`, `fig:inference-cost`, `fig:make-tool`, `fig:toolformer-cost`, `sec:embodied`, `sec:intro`, `tab:est-compute-cost`, `tab:missing-eval`

## Files Modified

1. **`sections/1_introduction.tex`**:
   - Fixed `\autoref{fig:call-api-v0}` → `\autoref{fig:call-api}`
   - Fixed `\citep{}` → `\citep{brown2020language}`

2. **`sections/2_scenarios.tex`**:
   - Fixed `\autoref{tab:1}` → `\autoref{tab:example-tools}`

3. **`sections/3_advanced.tex`**:
   - Fixed `\citep{}` → `\citep{gao2021simcse}`
   - Fixed `\S\ref{}` → `\S\ref{sub:domain-spec}` and `\S\ref{sub:general-codegen}`
   - Fixed `\autoref{}` → `\autoref{fig:codelm-tools}`

## Verification

Your LaTeX document should now compile without any undefined citation or reference warnings. All cross-references will work correctly when you compile the PDF.

## Recommendations

1. **Compile and test**: Run LaTeX compilation to verify everything works
2. **Consider unused entries**: You may optionally remove unused bibliography entries to clean up your `.bib` file
3. **Check unused labels**: Review if any unused labels should be referenced or can be removed

Your paper's citation and reference system is now fully functional! 🎉
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
Successfully wrote to /workspace/dumps/workspace/my_paper/citation_reference_check_report.md
```
</summary>
```json
Successfully wrote to /workspace/dumps/workspace/my_paper/citation_reference_check_report.md
```
</details>
</div>

<div className="tool-call-box">
<svg width="14px" height="14px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" style={{margin: 0, padding: 0, display: 'inline-block'}}><path d="M861.588238 240.133873v-65.792823c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.631049H314.659414v-65.631049c0-36.191275-29.439775-65.631049-65.631049-65.63105h-21.877358c-36.191275 0-65.631049 29.439775-65.631049 65.63105v65.792823c-36.317212 0.868255-65.631049 30.539428-65.63105 67.061417v543.745565c0 37.06772 30.155471 67.223191 67.223191 67.223191h696.886045c37.06772 0 67.223191-30.155471 67.223191-67.223191V307.19529c-0.001024-36.52199-29.315885-66.193162-65.633097-67.061417z m-109.385765-65.792823c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m-546.928824 0c0-12.060345 9.817012-21.877358 21.877358-21.877358h21.877358c12.060345 0 21.877358 9.817012 21.877358 21.877358v175.016814c0 12.060345-9.817012 21.877358-21.877358 21.877358h-21.877358c-12.060345 0-21.877358-9.817012-21.877358-21.877358V174.34105z m678.191947 676.600829c0 12.935767-10.532708 23.468476-23.468476 23.468475H163.111076c-12.935767 0-23.468476-10.532708-23.468476-23.468475V307.19529c0-12.402323 9.677764-22.593054 21.877358-23.415233v65.577807c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.631049h393.789368v65.631049c0 36.191275 29.439775 65.631049 65.631049 65.631049h21.877358c36.191275 0 65.631049-29.439775 65.631049-65.631049v-65.577807c12.19857 0.82218 21.877358 11.012911 21.877358 23.415233v543.746589z" fill="#22C67F" /><path d="M706.719439 478.272194l-48.01715-44.741741-182.28128 195.621482-111.468348-122.615387-48.563905 44.148911 159.469116 172.685427z" fill="#74E8AE" /></svg> `claim_done `

```json
{}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
you have claimed the task is done!
```
</summary>
```json
you have claimed the task is done!
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

## Summary

I have successfully checked and fixed all citation and reference issues in your LaTeX paper! Here's what I accomplished:

### ✅ **Issues Fixed:**

1. **Undefined References:**
   - `fig:call-api-v0` → Fixed to `fig:call-api` 
   - `tab:1` → Fixed to `tab:example-tools`

2. **Missing Citations:**
   - Empty `\citep\{\}` → Fixed to `\citep\{brown2020language\}`
   - Empty `\citep\{\}` → Fixed to `\citep\{gao2021simcse\}`

3. **Missing Section References:**
   - Empty `\S\ref\{\}` → Fixed to proper section references
   - Empty `\autoref\{\}` → Fixed to `\autoref\{fig:codelm-tools\}`

### ✅ **Current Status:**
- **All citations properly defined** (199 total, 95 unique)
- **All references properly defined** (74 total, 32 unique)
- **No undefined citations or references**

### ⚠️ **Minor Issues (Acceptable):**
- 22 unused bibliography entries (normal in academic papers)
- 9 unused labels (some may be intentional)

Your LaTeX paper will now compile without any undefined citation or reference warnings, and all cross-references will work correctly! I've also created a detailed report file (`citation_reference_check_report.md`) documenting all the changes made.
</div>

</Accordion>

<Accordion title="gemini-2.5-pro">

<Columns cols={3}>
<Card title="Task Completion" icon="x">
Failed
</Card>
<Card title="Tool Calls" icon="wrench">
7
</Card>
<Card title="Turns" icon="arrows-rotate">
8
</Card>
</Columns>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem list_directory`

```json
{
	"path":"."
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
Error: Access denied - path outside allowed directories: /workspace not in /workspace/dumps/workspac...
```
</summary>
```json
Error: Access denied - path outside allowed directories: /workspace not in /workspace/dumps/workspace
```
</details>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem list_directory`

```json
{
	"path":"/workspace/dumps/workspace"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
[DIR] my_paper
```
</summary>
```json
[DIR] my_paper
```
</details>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem list_directory`

```json
{
	"path":"/workspace/dumps/workspace/my_paper"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
[FILE] README.md
```
</summary>
```json
[FILE] README.md
[DIR] appendix
[FILE] colm2024_conference.bbl
[FILE] colm2024_conference.bib
[FILE] colm2024_conference.bst
[FILE] colm2024_conference.sty
[FILE] colm2024_conference.tex
[FILE] fancyhdr.sty
[DIR] figures
[FILE] math_commands.tex
[FILE] natbib.sty
[DIR] sections
```
</details>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem list_directory`

```json
{
	"path":"/workspace/dumps/workspace/my_paper/sections"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
[FILE] 1_introduction.tex
```
</summary>
```json
[FILE] 1_introduction.tex
[FILE] 2_scenarios.tex
[FILE] 3_advanced.tex
[FILE] 4_evaluation.tex
[FILE] 5_tradeoff.tex
[FILE] conclusion.tex
```
</details>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem list_directory`

```json
{
	"path":"/workspace/dumps/workspace/my_paper/appendix"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
[FILE] embodied.tex
```
</summary>
```json
[FILE] embodied.tex
[FILE] learning-effort.tex
```
</details>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_file`

```json
{
	"path":"/workspace/dumps/workspace/my_paper/colm2024_conference.tex"
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json

```
</summary>
```json

\documentclass{article} % For LaTeX2e
\usepackage{colm2024_conference}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amsthm}
\usepackage{xcolor,colortbl}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{fdsymbol}
\usepackage{color, colortbl}
\urlstyle{same}
\newtheorem{definition}{Definition}
% variables
\newcommand{\kn}{\raisebox{-0.9mm}{\includegraphics[width=4mm]{./figures/icon/book-icon.png}}}
\newcommand{\work}{\raisebox{-0.8mm}{\includegraphics[width=4mm]{./figures/icon/work-icon.png}}}
\newcommand{\world}{\raisebox{-0.8mm}{\includegraphics[width=4mm]{./figures/icon/world-icon.png}}}
\newcommand{\modal}{\raisebox{-0.8mm}{\includegraphics[width=4mm]{./figures/icon/media-icon.png}}}
\newcommand{\nn}{\raisebox{-0.8mm}{\includegraphics[width=4mm]{./figures/icon/brain-icon.png}}}
% colors
\definecolor{ao}{rgb}{0.0, 0.5, 0.0}
\definecolor{ballblue}{rgb}{0.13, 0.67, 0.8}
\definecolor{darklavender}{rgb}{0.45, 0.31, 0.59}
\definecolor{chromeyellow}{rgb}{1.0, 0.65, 0.0}
\definecolor{candypink}{rgb}{0.89, 0.44, 0.48}
\definecolor{blue(ncs)}{rgb}{0.258, 0.520, 0.953}
% author comment
\usepackage{color-edits}
\addauthor{gn}{magenta}
\addauthor{zw}{orange}
\addauthor{df}{cyan}
\addauthor{hz}{blue}
\addauthor{zc}{violet}


\title{What Are Tools Anyway? \\A Survey from the Language Model Perspective}
% In the Eyes of Language Models
% From A Language Model Perspective
% A Survey of Tools for LMs

% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{Zora Zhiruo Wang$^{\spadesuit}$ \quad Zhoujun Cheng$^{\vardiamondsuit}$ \quad Hao Zhu$^{\spadesuit}$ \quad
{\bf  Daniel Fried$^{\spadesuit}$} \quad {\bf Graham Neubig}$^{\spadesuit}$ \\
$^{\spadesuit}$Carnegie Mellon University \quad $^{\vardiamondsuit}$Shanghai Jiao Tong University}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\colmfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Language models (LMs) are powerful yet mostly for text generation tasks. Tools have substantially enhanced their performance for tasks that require complex skills.
However, many works adopt the term ``tool'' in different ways, raising the question: \textit{What is a tool anyway?} Subsequently, \textit{where and how do tools help LMs?}
In this survey, we provide a unified definition of tools as external programs used by LMs, and perform a systematic review of LM tooling scenarios and approaches.
Grounded on this review, we empirically study the efficiency of various tooling methods by measuring their required compute and performance gains on various benchmarks, and highlight some challenges and potential future research in the field.% \footnote{\url{https://github.com/zorazrw/awesome-tool-llm}}
\end{abstract}

% ############
\input{sections/1_introduction}
\input{sections/2_scenarios}
\input{sections/3_advanced}
\input{sections/4_evaluation}
\input{sections/5_tradeoff}
\input{sections/conclusion}



\bibliography{colm2024_conference}
\bibliographystyle{colm2024_conference}

\clearpage
\appendix
\input{appendix/learning-effort}
% \input{appendix/embodied}

\end{document}

```
</details>
</div>

<div className="tool-call-box">
<svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 48 48" width="14px" height="14px" style={{margin: 0, padding: 0, display: 'inline-block'}}><path fill="#FFA000" d="M40,12H22l-4-4H8c-2.2,0-4,1.8-4,4v8h40v-4C44,13.8,42.2,12,40,12z"/><path fill="#FFCA28" d="M40,12H8c-2.2,0-4,1.8-4,4v20c0,2.2,1.8,4,4,4h32c2.2,0,4-1.8,4-4V16C44,13.8,42.2,12,40,12z"/></svg> `filesystem read_multiple_files`

```json
{
	"paths":["/workspace/dumps/workspace/my_paper/sections/1_introduction.tex",
	"/workspace/dumps/workspace/my_paper/sections/2_scenarios.tex",
	"/workspace/dumps/workspace/my_paper/sections/3_advanced.tex",
	"/workspace/dumps/workspace/my_paper/sections/4_evaluation.tex",
	"/workspace/dumps/workspace/my_paper/sections/5_tradeoff.tex",
	"/workspace/dumps/workspace/my_paper/sections/conclusion.tex",
	"/workspace/dumps/workspace/my_paper/appendix/learning-effort.tex",
	"/workspace/dumps/workspace/my_paper/appendix/embodied.tex",
	"/workspace/dumps/workspace/my_paper/colm2024_conference.bib",
	"/workspace/dumps/workspace/my_paper/colm2024_conference.bbl"]
}
```
</div>

<div className="result-box">
🔍`tool result`
<details>
<summary className="tool-result-summary">
```json
{"type":"text","text":"/workspace/dumps/workspace/my_paper/sections/1_introduction.tex:\n\\section{I...
```
</summary>
```json
{"type":"text","text":"/workspace/dumps/workspace/my_paper/sections/1_introduction.tex:\n\\section{Introduction}\n\\label{sec:intro}\n\nLanguage Models (LMs) have become increasingly effective in solving text-generation tasks, by taking in natural language (NL) instructions from users and outputting NL responses, such as answering the ``What is the capital of the US?'' with ``Washington D.C.''. However, LMs often struggle to perform tasks that require complex skills (e.g., math or complex reasoning), and are fundamentally unable to solve other tasks that require access to information not included in their training data (e.g., the current weather or date).\n\n\\begin{wrapfigure}[13]{r}{0.42\\textwidth}\n\\vspace{-4mm}    \n\\includegraphics[width=0.39\\textwidth]{./figures/definition.pdf}\n\\vspace{-2mm}\n\\caption{Illustration of tools extending and facilitating LM task-solving.}\n\\label{fig:def}\n\\end{wrapfigure}\n\nTo solve this problem, researchers and practitioners are turning to LMs enhanced with \\emph{tools}, which help \\textit{facilitate} the task-solving process of LMs, or \\textit{extend} LMs with new abilities that the LM does not possess otherwise \\citep{qin2023tool,mialon2023augmented}.\nFor example, a \\texttt{calculator} tool may be used to facilitate mathematical calculations, or a \\texttt{get\\_time()} tool could be used to obtain the current time, which is not available purely through the LM's parameters. \nInspired by the tools used by humans \\citep{shumaker2011animal}, some works introduce application-specific \\texttt{software} as tools, such as using a \\texttt{search engine} to obtain knowledge \\citep{lazaridou2022internetaugmented,komeili-etal-2022-internet}, using a \\texttt{translator} to process unknown languages \\citep{schick2023toolformer}, or using a \\texttt{SQL engine} to query databases \\citep{hao2023toolkengpt,zhuang2023toolqa}.\nWith the development of numerous application programming interfaces (APIs) on the web, many works collect \\texttt{API}s as tools to access world data in real-time \\citep{balog2016deepcoder,xu2023tool,qin2023toolllm} via multiple modalities \\citep{tang2023toolalpaca}, even performing professional activities such as financial analysis \\citep{li-etal-2023-api} and digital marketing \\citep{huang2024metatool}.\nInstead of using black-box APIs with unseen implementations, other works use locally-crafted \\texttt{function}s to query over structured tables \\citep{wang2024executable,cao2023api} or images \\citep{suris2023vipergpt}, where the function tools can be created by human \\citep{gupta2022visual} or model experts \\citep{wang2023voyager,cai2023large,wang2024trove}.\n\n% issue, motivation; our diff from existing surveys\nHowever, despite this broad and burgeoning area of tool use in LMs, existing surveys only cover certain tool categories such as software \\citep{mialon2023augmented} or APIs \\citep{qin2023tool}. \nIn this paper, we (1) provide a unified view of tool use across a broad range of scenarios, (2) empirically analyze the cost efficiency of tooling methods, to give practical guidance on when and how one should use tools, and (3) offer concrete suggestions for evaluations.\n\nWe start with proposing \\textit{a unified definition} of tools and explain \\textit{why tools help task-solving} (\\S\\ref{sec:background}).\nWe first introduce the \\emph{basic tool-use paradigm} (\\S\\ref{sec:basic-paradigm}) and study a variety of tool-using scenarios by enumerating \\textit{which tools exist} and \\textit{to which tasks they apply} (\\S\\ref{sec:scenarios}). \nNext, we study advanced approaches for \\textit{complex tool usage} and even \\textit{make new tools} if they are unavailable for the task (\\S\\ref{sec:methods}). \nWe then summarize existing testbeds and evaluation metrics across LM tooling works, and highlight several missing aspects with concrete metric suggestions (\\S\\ref{sec:good-tool}).\nLastly, grounding on our empirical analysis about \\textit{when tools are effective}, we identify the most efficient tooling approaches and the tasks that benefit most from tools (\\S\\ref{sub:trade-off}). \n\n\n\n% ##################\n\\section{Background}\n\\label{sec:background}\n\n\\subsection{What are tools?}\n% how existing works define tools\nBecause LMs are products of the digital world, tools employed by LMs are often computer \\textbf{programs} that are executable in corresponding environments, e.g., Python programs are executable in Python environments. \nReferring back to human-used tools, \\citet{shumaker2011animal} defines animal tool use as \\textit{``the external employment of an unattached or manipulable attached environmental object to alter more efficiently the form, position, or condition of another object.''}\n% two properties of tools\nSimilar to this definition of physical tools, LM-used program tools should also be \\textbf{external} to the employer (i.e., the LM) and are part of the environment. \nIn the meantime, instead of arbitrary program snippets, a tool is a \\textbf{function} (e.g., \\texttt{plus\\_one}), meaning that it can be applied to other objects (e.g., data) and yield an output (e.g. \\texttt{plus\\_one}$(1) \\rightarrow 2$).\n\nExisting definitions of LM-used tools touch on some of these aspects. \n\\citet{qin2023tool} make an intuitive appeal to the similarity to human tool use, but do not define what entails a tool.\n\\citet{mialon2023augmented} define \\textit{a tool} as \\textit{``an external module that is typically called using a rule or a special token and whose output is included in the augmented LM's context.''}\nWe argue for a somewhat more broad definition than this, which encompasses a wide variety of more recent works on tool usage:\n\n\\begin{definition}\n  \\label{def:tools-lm}\n  An LM-used tool is a function interface to a computer program that runs \\textit{externally} to the LM, where the LM generates the function calls and input arguments in order to use the tool.\\footnote{Under our definition, tool functions can be implemented by any means, including symbolic computation or neural networks --- the functions only require a programmatic interface.}\n\\end{definition} \n\n\n\\subsection{Why are tools helpful?}\nTools can help task-solving in different ways, depending on the functionality of individual tools. We summarize their functions into three major categories: perception, action, and computation. A tool may belong to one or more of these three categories.\n\n\\noindent \\textbf{Perception} \\quad\nPerception tools provide or collect information from the environment. An example is using a \\texttt{get\\_time()} API to obtain the current time, which is not included in the LM's parametric knowledge learned from training.\n\n\\noindent \\textbf{Action} \\quad\nAction tools can exert actions on the environment and change its state. For example, \\texttt{turn\\_left()} can shift the direction of an embodied agent, or executing \\texttt{make\\_post(website, post)} can change the content on a \\texttt{website}.\n\n\\noindent \\textbf{Computation} \\quad\nComputation tools do not necessarily perceive or modify the external environment, but use programs to tackle complex computational tasks. For example, a \\texttt{calculator} is a computation tool for mathematical calculation.\nNote that the computation also includes more general acts of computing beyond numerical calculation. Therefore, a \\texttt{translator} is also a computation tool that can be used to translate between languages.\n\nNote that many tools can fall into multiple categories.\nFor instance, a \\texttt{search engine} is a tool that can perform both computation and perception.\nAs computation, it measures document similarity and selects relevant ones, but it also perceives the environment (i.e., the web) and fetches data (i.e., returned documents) from it.\nIn a similar spirit, \\textsc{SQL} queries can be used as computation tools (e.g., \\texttt{SELECT SQRT(16) / 10 AS result}), perception tools for viewing data (e.g., \\texttt{SELECT name FROM data}), action tools to modify data (e.g., \\texttt{INSERT INTO data VALUES name}), or all of the above (e.g., \\texttt{INSERT INTO counts (grp\\_id, grp\\_cnt) SELECT grp\\_id, COUNT(*) FROM data GROUP BY grp\\_id}).\n\n\\subsection{Tools and ``Agents''} \nThere has recently been a burgeoning of work on LM-powered agents \\citep{xi2023rise,sumers2024cognitive}.\n\\citet{russell2010artificial} define agents as \\textit{``anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.''}\nAccording to this definition, agents are programs that use perception tools to perceive the situated environment, or action tools to interact with the environment.\nModels that only use computation tools and do not interact with their environments through perception or action tools arguably do not fall under the category of ``agents'' according to this definition.\n\n\n\n% ###################################\n\\section{The basic tool use paradigm}\n\\label{sec:basic-paradigm}\n% figure start\n\\begin{wrapfigure}[16]{r}{0.42\\textwidth}\n\\vspace{-5mm}\n\\includegraphics[width=0.40\\textwidth]{./figures/call-api.pdf}\n\\vspace{-2mm}\n\\caption{The basic tool use paradigm. LM calls \\raisebox{0.3mm}{\\colorbox{violet!13}{\\texttt{check\\_weather}}} tool by generating text tokens. This call triggers the server to execute the call and return the output \\raisebox{0.3mm}{\\colorbox{yellow!33}{sunny}}, using which the LM replaces the API call tokens in the response to the user.}\n\\label{fig:call-api}\n\\end{wrapfigure}\n% figure end\n\nFirst, in this section, we show an illustrative example of a basic tool-use paradigm introduced by Toolformer \\citep{schick2023toolformer}, which many tool-related works adopt (\\autoref{fig:call-api}). Assuming an LM communicates with users mainly in natural language, upon receiving a user query such as ``How is the weather today?'', the LM then proceeds to generate either text or tool calls. In the example, starts with generating a few tokens of text ``It is ...''. When the LM needs to seek external tools to complete the task, e.g., get real-time weather information, it generates tokens of the tool name and corresponding input arguments enclosed with \\texttt{(}parentheses\\texttt{)} to construct a complete tool calling expression. \nThis completed expression will trigger a shift from text-generation mode to tool-execution mode. The server hosting the tool will execute the expression and return the execution result to the LM. \n\nTaking the example in \\autoref{fig:call-api-v0}, the LM sends the \\texttt{check\\_weather()} call to the weather server and receives the output ``sunny''. The returned result replaces the tool call in the LM-generated tokens (e.g., from ``It is \\texttt{check\\_weather()}'' to ``It is sunny''), which is used for subsequent steps of generation.\nAccordingly, the LM shifts back to the text generation mode and continues to finish the response by generating new text tokens, e.g., adding `today.', and finally returning the response to the user.\n\nIn order for LMs to use this basic paradigm of using tools, current works mainly leverage inference-time prompting and training-time learning methods.\n\n\\noindent \\textbf{Inference-time prompting} \\quad\nLeveraging the ability of LMs to learn in-context \\citep{}, many works provide tool information through a prompt and expect LMs to acquire abilities to use these tools from input contexts.\nThis is achieved by providing instructions about the task, example pairs of queries and solutions that use tools \\citep{gupta2022visual,lu2023chameleon,paranjape2023art,shen2023hugginggpt,yang2023mmreact}, and/or documentation of the tools' functionality \\citep{hsieh2023tool}.\n\n\n\\noindent \\textbf{Learning by training} \\quad\nBeyond learning tools from test-time contexts, LMs can learn from examples that use these tools during training.\nLMs can simply be trained to generate tool-using solutions, where the examples can be manually annotated by humans \\citep{li-etal-2023-api}, synthesized by larger teacher LMs \\citep{tang2023toolalpaca,qin2023toolllm,huang2024metatool}, or bootstrapped by the test-time LM itself \\citep{schick2023toolformer}.\n\n\n---\n/workspace/dumps/workspace/my_paper/sections/2_scenarios.tex:\n\n\\textbf{\\begin{table}[t]\n\\vspace{-5mm}\n\\small\n\\begin{center}\n    \\begin{tabular}{ll}\n    \\toprule\n    \\multicolumn{1}{c}{\\textbf{Category}} & \\multicolumn{1}{c}{\\textbf{Example Tools}} \\\\\n    \\midrule\n    \\multirow{3}{*}{\\kn ~~Knowledge access} & {\\texttt{sql\\_executor(query:~str) -> answer:~any}} \\\\\n    {} & {\\texttt{search\\_engine(query:~str) -> document:~str}} \\\\\n    {} & {\\texttt{retriever(query:~str) -> document:~str}} \\\\\n    \\midrule\n    \\multirow{3}{*}{\\work ~~Computation activities} & {\\texttt{calculator(formula:~str) -> value:~int | float}} \\\\\n    {} & {\\texttt{python\\_interpreter(program:~str) -> result:~any}} \\\\\n    {} & {\\texttt{worksheet.insert\\_row(row:~list, index:~int) -> None}} \\\\\n    \\midrule\n    \\multirow{4}{*}{\\world ~~Interaction w/ the world} & \\texttt{get\\_weather(city\\_name:~str) -> weather:~str} \\\\\n    {} & \\texttt{get\\_location(ip:~str) -> location:~str} \\\\\n    {} & \\texttt{calendar.fetch\\_events(date:~str) -> events:~list} \\\\\n    {} & \\texttt{email.verify(address:~str) -> result:~bool} \\\\\n    \\midrule\n    \\multirow{3}{*}{\\modal ~~Non-textual modalities} & \\texttt{cat\\_image.delete(image\\_id:~str) -> None} \\\\\n    {} & \\texttt{spotify.play\\_music(name:~str) -> None} \\\\\n    {} & \\texttt{visual\\_qa(query:~str, image:~Image) -> answer:~str} \\\\\n    \\midrule\n    \\multirow{2}{*}{\\nn ~~Special-skilled LMs} & {\\texttt{QA(question:~str) -> answer:~str}} \\\\\n    {} & \\texttt{translation(text:~str, language:~str) -> text:~str} \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{center}\n\\vspace{-2mm}\n\\caption{Exemplar tools for each category.}\n\\vspace{-15mm}\n\\label{tab:example-tools}\n\\end{table}}\n\n\n\\section{Scenarios where tools are useful}\n\\label{sec:scenarios}\n\nWhile LMs may easily learn to do many tasks to high accuracy without tools, many other tasks greatly benefit from tool use.\nIn this section, we study a broad range of scenarios where tools have been used to assist agents. We discuss tasks where human-created, application-specific tools can improve their performance or other positive aspects (\\S\\ref{sub:app-spec}), as well as scenarios where tools may not be as useful (\\S\\ref{sub:not-useful}). \n\n\n\n\\subsection{Utilizing existing tools for specific applications}\n\\label{sub:app-spec}\n\nWhile it is difficult to exhaustively enumerate every scenario where tools could be useful, we summarize some major categories of tool use in \\autoref{tab:1} and below.\nNote that a tool may fall into one or more categories.\n\n\n\\noindent \\textbf{\\raisebox{-1mm}{\\includegraphics[width=4mm]{./figures/icon/book-icon.png}} Knowledge access} \\quad\n% unstructured text & structured kb/kg\nLMs store limited knowledge during training due to both limits in (i) the data that they are trained on and (ii) the ability of LMs to accurately memorize and utilize all of the data that they see at training time.\nSeveral varieties of tools can be used to alleviate this issue.\n\\texttt{SQL} and \\texttt{SPARL} executors can provide access to data in structured knowledge bases \\citep{thoppilan2022lamda,parisi2022talm,hao2023toolkengpt} or knowledge graphs \\citep{zhuang2023toolqa}.\nAn \\texttt{search engine} tool  over the Internet \\citep{yao2023react,schick2023toolformer,paranjape2023art} can enable LMs to access more up-to-date information \\citep{komeili-etal-2022-internet,lazaridou2022internetaugmented}.\nMore generally, retrieval-augmented generation systems \\citep{asai2023retrieval} can be seen as using a \\texttt{retriever} tool \\citep{mialon2023augmented}.\n\n\n\\noindent \\textbf{\\raisebox{-0.5mm}{\\includegraphics[width=4mm]{./figures/icon/work-icon.png}} Computation activities} \\quad\n% math\nComplex computing activities such as math calculations are known to be challenging for neural LMs \\citep{schick2023toolformer}. While even a \\texttt{calculator} can enhance LMs' numeracy abilities \\citep{parisi2022talm,hao2023toolkengpt}, more generic \\texttt{Python} programs are also employed to aid reasoning tasks \\citep{gao2023pal,chen2023program,wang2023mint}.\n% business tools\nFor more complex professional jobs, business tools are also applied, such as using \\texttt{worksheet} to manipulate Google Sheets \\citep{xu2023tool}, or even tools for financial, medical, education, or advertising domains \\citep{tang2023toolalpaca,huang2024metatool}.\n\n\n\\noindent \\textbf{\\raisebox{-0.5mm}{\\includegraphics[width=4mm]{./figures/icon/world-icon.png}} Interaction with the world} \\quad\nLMs without tools are fundamentally unable to perceive and act in the world around them, necessitating tool use where such perception and action is necessary. For instance, LMs can access real-time information such as weather \\citep{xu2023tool,tang2023toolalpaca}, or positional knowledge such as location \\citep{qin2023toolllm}. On the other hand, LMs can manipulate real-world information such as managing calendars \\citep{schick2023toolformer} and emails \\citep{qin2023toolllm}.\nIn addition to web-based activities, LMs can engage in physical activities in embodied environments, such as fishing with rods or mining with axes in the Minecraft world \\citep{wang2023voyager}; further propagate to the real-world tasks to perform cooking \\citep{singh2022progprompt,shridhar2020alfred}, plotting \\citep{liang2023code}, and even conducting chemical research \\citep{boiko2023autonomous}.\n\n\n\\noindent \\textbf{\\raisebox{-0.5mm}{\\includegraphics[width=4mm]{./figures/icon/media-icon.png}} Non-textual modalities} \\quad\nWhile many LMs only consume and generate texts, some works bring in access to visual \\citep{gupta2022visual,suris2023vipergpt}, audio \\citep{yang2023mmreact,gao2023assistgpt}, or other modalities. For example, LMs can access images with \\texttt{cat\\_image} APIs \\citep{xu2023tool,tang2023toolalpaca} or songs \\citep{huang2024metatool} provided by \\texttt{spotify}, even answer questions about them \\citep{gupta2022visual,gao2023assistgpt}.\n\n\n\\noindent \\textbf{\\raisebox{-0.5mm}{\\includegraphics[width=4mm]{./figures/icon/brain-icon.png}} Accessing specialized LMs} \\quad\nSome works propose to use specialized LMs as tools, essentially using the main LM as a task planner to dispatch requests to other LMs.\n\\citet{schick2023toolformer} propose QA models to fill in factoid details in responses, \\citet{thoppilan2022lamda,schick2023toolformer,paranjape2023art} use machine translation models to assist multilingual tasks. Beyond specific tasks, some works adopt multiple neural models from Hugginface or similar platforms \\citep{patil2023gorilla,shen2023hugginggpt}, or further fine-tune them on various data \\citep{viswanathan2023prompt2model}.\nCompared to the base LM, these tool models mainly vary in their specialized skills, and may or may not have substantial architectural differences from the base LMs.\n\n\n\\subsection{Where are tools \\emph{not} useful?}\n\\label{sub:not-useful}\n\nDespite the fact that tools can be helpful under many scenarios discussed above, it is also important to note scenarios where tools are arguably not very helpful.\nSome examples of tasks where tools have not (yet) been used to great effect include machine translation, summarization, and sentiment analysis (among others).\nThese are tasks that are not easy to perform using non-ML methods (c.f.~solving math problems or accessing databases, which can be done using a calculator or SQL), and can be performed with high accuracy by a powerful LM alone.\nOne intuitive reason is that the tools currently leveraged for these tasks are \\textit{neural networks} and have limited advantages over the base LM.\nImagine if we leverage tools on these tasks, the tools would mostly generally be another neural LM with specialized skills, e.g., an LM specifically trained on many summarization datasets to perform this task. However, this special-skilled neural LM may not have significant architectural differences from the base tool-using LM, or be smaller in size or training tokens hence having inferior language modeling abilities in general.\nIn comparison, the base LM capable of solution planning and tool management, usually are more powerful (e.g., GPT-4) and can achieve reasonable performance on a wide variety of tasks, perhaps even outperforming special-purpose LMs \\citep{robinson-etal-2023-chatgpt}.\n\n\n---\n/workspace/dumps/workspace/my_paper/sections/3_advanced.tex:\n\\section{Advanced tool-use methods}\n\\label{sec:methods}\n\nGiven this understanding of the basic tooling paradigm and the scenarios in which tools are useful, we now discuss more advanced approaches for tools.\nConcretely, we study multi-tool selection and usage (\\S\\ref{sub:tool-select}), complex tooling under programmatic contexts (\\S\\ref{sub:tools-and-programs}), and creation of tools when they are not available a-priori (\\S\\ref{sub:make-tool}). \n\n\n% #########################\n\\subsection{Complex tool selection and usage}\n\\label{sub:tool-select}\n\nDepending on the number of tools available, the system may include an implicit or explicit tool selection module.\nIf tools are already \\textit{designated} for the task \\citep{lazaridou2022internetaugmented,thoppilan2022lamda}, then no tool selection is needed.\nIf \\textit{a small number} (e.g., 5--10) of tools are available, metadata and use cases of these tools can be provided as input contexts along with the user query \\citep{schick2023toolformer,paranjape2023art}, and LMs can directly select tools from contexts via a standard generation process.\nIf the toolbox size \\textit{further grows} (e.g., to hundreds), fitting all tools into model inputs is not feasible. Thus an extra retrieval step is often incorporated: a retriever model short-lists the most relevant tools and feeds their metadata to the solution-generation LM. Specifically, \\citet{zhou2023docprompting,qin2023toolllm} train retriever models that map NL intents to tool documentation.\n\\citet{yuan2023craft} ask LMs to write hypothetical descriptions and use the SimCSE retriever \\citep{} to find similar tools. More easily, one can directly use off-the-shelf embeddings \\citep{SFRAIResearch2024,openai-emb} or training-free sparse retrievers \\citep{robertson2009probabilistic}.\n\n\n% \\noindent \\textbf{Complex, multi-tool usage} \\quad\nFor complex queries that require multiple tools to solve, the common approach so far is to break down the task and tackle each step sequentially \\citep{paranjape2023art} by selecting and using tools with intermediate contexts.\nHowever, this sequential multi-turn paradigm may not be reflective of more complex or realistic usage of the involved tools. For example, a user may prefer \\textit{nested} function calls \\texttt{check\\_weather(get\\_local\\_time(`Pittsburgh'))} to allow information hiding or encapsulation \\citep{rogers2001encapsulation}, \\textit{parallel} calls to reduce round trips with the API \\citep{eleti2023function}, or \\textit{iterative} calls \\texttt{buy\\_ticket(event)} in a loop until it returns \\texttt{True} to indicate a successful transaction. \n\n\n% ################################\n\\subsection{Tools in programmatic contexts}\n\\label{sub:tools-and-programs}\n\nUnlike text-based tasks where tools are auxiliary modules to extend LM abilities, on programmatic tasks, where code LMs can solve the problem by generating programs, tools can be seen as compositions of basic functions.\nIn this part, we discuss tools in programmatic tasks for domain-specific (\\S\\ref{}) and general-purpose problems (\\S\\ref{}).\n\n\n\\noindent \\textbf{Focus on varied tools} \\quad\nDepending on the tasks of interest, existing works focus on different types of tools under programmatic contexts. With the increasing complexity of these tools and presumably a decreasing familiarity of LMs about them, there are works that adopt (i) \\textcolor{candypink}{\\textit{built-in functions}} of a programming language (PL) to augment LMs in symbolic reasoning, (ii) \\textcolor{ao!80}{\\textit{external libraries}} in pre-designed packages to tackle complex open-domain coding queries \\citep{wang2023execution}, and (iii) \\textcolor{blue(ncs)}{\\textit{utility functions}} unseen at training time to solve specific tasks.\n\n\\begin{figure}[ht]\n\\centering\n\\vspace{-1mm}\n    \\includegraphics[width=\\textwidth]{./figures/codelm-tools.pdf}\n\\vspace{-6mm}\n\\caption{Relative to what is considered as the base LM or base actions, tools can refer to built-in functions, external libraries, or task-specific utility functions (from left to right).}\n\\vspace{-1mm}\n\\label{fig:codelm-tools}\n\\end{figure}\n\n\n\n% ############\n\\subsubsection{Domain-specific semantic parsing}\n\\label{sub:domain-spec}\n\nNL-to-code generation systems have been studied for many years on special-domain tasks such as querying databases \\citep{zelle1996learning,zettlemoyer2012learning} or knowledge graphs \\citep{berant2013semantic}. Code produced by these systems is often domain-specific logical forms (DSL) manually designed by experts, such as lambda expressions \\citep{liang2013lambda} or SQL queries \\citep{yu2018spider}, and more recently, the QDMR grammar \\citep{wolfson2020break} as an extension to SQL.\nIn addition to knowledge-oriented tasks, many agentic tasks adopt DSL to operate in corresponding environments, such as \\texttt{click} or \\texttt{type} in web navigation \\citep{zheran2018reinforcement,webshop2022yao,zhou2024webarena}, \\texttt{placeItem} and \\texttt{killMob} in the embodied Minecraft world \\citep{wang2023voyager}, or \\texttt{set\\_joint\\_target} for robot dogs \\citep{yu2023language}.\nBecause DSLs are often specific enough to the target problems, most works directly use these \\textcolor{candypink}{built-in actions}.\nYet still, for complex task queries, solution programs written in basic DSL actions alone can be hard to interpret or cumbersome to use, e.g., it is hard to tell that the lambda expression \\texttt{(fold xs ($\\lambda$ (n x) (+ 1 n)) 0)} is to calculate the length of \\texttt{xs}.\n\n\n\n\n% ############\n\\subsubsection{General-purpose code generation}\n\\label{sub:general-codegen}\nRecent code generation systems have expanded from using DSL to more general-purpose PLs such as Python or Java \\citep{yin-neubig-2017-syntactic,chen2021evaluating}. These languages enable more programming flexibility and readily apply to versatile scenarios. \nAs we have introduced using \\textcolor{candypink}{built-in actions} as tools in \\S\\ref{sub:app-spec}, we discuss more on two other\ncommon categories of tools for code LMs, namely \\textcolor{ao!90}{\\textit{external libraries}} and task-specific \\textcolor{blue(ncs)}{\\textit{utility functions}}.\n\n\n\\noindent \\textbf{External libraries} \\quad\nFrom the usage of PLs, built-in functions are internal to whichever environment, whereas third-party libraries lie externally and need to be imported to tackle specific contexts, such as \\autoref{} (middle). Aligning with this conception, \\citet{zhang2023toolcoder} use Python libraries such as \\texttt{matplotlib} to plot figures and \\texttt{pandas} to manage data.\n\n\n\\noindent \\textbf{Utility functions} \\quad\nFor more task-specific applications,\nexpert-crafted utility functions, usually unseen at training time, are incorporated as tools. E.g., in \\autoref{fig:codelm-tools} (right), the highlighted \\texttt{locate\\_objects} function is designed by human experts \\citep{gupta2022visual,suris2023vipergpt} to load neural models and perform post-processing to obtain the detected box region. In a similar spirit, \\citet{cheng2023binding} use GPT as a tool to query world facts external to the tabular contents, \\citet{cao2023api} further design macro operation APIs to support advanced tabular operations.\nHowever, because human tool curation requires expertise and effort, some works explore using LMs to automatically create tools instead.\n\n\n\n% ################################\n\n\\begin{wrapfigure}[8]{r}{0.40\\textwidth}\n\\vspace{-6mm}\n\\includegraphics[width=0.39\\textwidth]{./figures/make-tool.pdf}\n\\vspace{-2mm}\n\\caption{LM makes tools when no tools readily apply to the task.}\n% \\vspace{-1mm}\n\\label{fig:make-tool}\n\\end{wrapfigure}\n\n\\subsection{Tool creation and reuse}\n\\label{sub:make-tool}\n\n\nWhile one can readily use tools for tasks equipped with pre-designed tools, for tasks that do not have readily-applicable, human-created tools, some works explore using LMs to make tools and use them.\n\n\\noindent \\textbf{Domain-specific library abstraction} \\quad\nWorks that use DSLs often compose frequently-used-together actions as shortcut tools. For example, \\citet{ellis2023dreamcoder} learn function abstractions such as \\texttt{length} and \\texttt{count\\_to} from lambda primitives (e.g., \\texttt{$0$}, \\texttt{+}) for the list processing task. \n\\citet{pmlr-v139-wong21a,bowers2023top} similarly build functions bottom-up from a large corpus of DSL programs. More recently, \\citet{grand2023lilo} use LLMs to abstract libraries with auto-documentation. \nFurther for agentic tasks, \\citet{zheran2018reinforcement} learn common workflows to guide web navigation, such as composing the basic $\\{$\\texttt{click}, \\texttt{like}$\\}$ actions to form a higher-level login action \\texttt{click(like(`login'))}.\n\n\\noindent \\textbf{General-purpose tool making} \\quad\nNonetheless, on general-purpose PLs, running the DSL-oriented methods above may expand their search space and limit their scalability. Instead, recent works often leverage LMs' procedural knowledge to alleviate the search issue. \nTo start, \\citet{wang2023voyager} designs an automatic learning curriculum in Minecraft to make and use Java program tools.\nLATM \\citep{} use LMs to build, verify, and use Python tools on BigBench \\citep{srivastava2023beyond} tasks, where however, all examples require the same single tool hence have limited difficulty.\nCREATOR \\citep{qian2023creator} extend tool-making to harder tasks such as math and table world problems, and improves task success by creating tools yet repetitively for individual examples, thus CRAFT \\citep{yuan2023craft} add heuristic-based training to craft less repetitive tools.\nTowards more efficient pipelines, ReGAL \\citep{stengeleskin2024regal} learns from refactoring a smaller number of programs, while TroVE \\citep{wang2024trove} purely relies on inference-time execution signal and induces reusable tools on-the-fly.\n\n\n---\n/workspace/dumps/workspace/my_paper/sections/4_evaluation.tex:\n\\section{How to evaluate tool use?}\n\\label{sec:good-tool}\n\nIn this section, we study existing LM-tooling benchmarks (\\S\\ref{sub:testbeds}) and their evaluation metrics (\\S\\ref{sub:eval-metrics}), lastly, we discuss the missing yet important evaluation aspects of tools (\\S\\ref{sub:desired-properties}).\n\n% ############\n\\subsection{Testbeds for evaluating tools}\n\\label{sub:testbeds}\nLM tool use can be evaluated on (i) repurposed existing datasets that can additionally benefit from tools (\\S\\ref{sub:existing-dataset}), and (ii) newly crafted benchmarks that necessitate tool use (\\S\\ref{sub:api-benchmark}).\n\n% ############\n\\subsubsection{Repurposed Existing Datasets}\n\\label{sub:existing-dataset}\nMany tasks are solvable by using LMs, yet often with great difficulty or inefficiency. Therefore, some works use tool-augmented LMs as an alternative approach to solve these tasks. \n\nMany of these datasets require \\textbf{reasoning}. Starting from when questions are expressed in NL, such as complex reasoning with the Big-bench \\citep{srivastava2023beyond} dataset, mathematical problems with the MATH \\citep{hendrycks2021measuring} dataset, and reasoning over world knowledge to answer questions in NaturalQuestions \\citep{kwiatkowski2019natural} and TriviaQA \\citep{joshi2017triviaqa} datasets.\nBeyond free-form texts, datasets that require reasoning over \\textbf{structured data} can also benefit from tools. These tasks include table-based QA with tabular math world problems in TabMWP \\citep{lu2023dynamic}, Wikipedia tables in WTQ \\citep{pasupat-liang-2015-compositional}, and complex-structured tables in HiTab \\citep{cheng-etal-2022-hitab}.\nBeyond the text modality, datasets that require reasoning over \\textbf{other modalities} also benefit from modality-extending tools, e.g., answering questions about an image with the GQA \\citep{hudson2019gqa} dataset, or image pairs with the NLVR2 dataset \\citep{suhr2019corpus}.\n\nBecause tool use is proposed as an alternative method to solve these datasets, evaluations of these tool-augmented systems follow the standard evaluation process for individual datasets. Concretely, almost all tasks are measured by answer exact match, either in textual or numerical formats.\nNote that, to obtain the final answers for lexical matching evaluations, all tool-calling expressions \\textbf{need to be executed}, and the execution outputs are incorporated into the final answers produced by the tool-augmented systems, as introduced in \\S\\ref{sec:basic-paradigm}.\n\n% ############\n\\subsubsection{Aggregated API Benchmarks}\n\\label{sub:api-benchmark}\nExisting benchmarks can only benefit from a limited set of tools, yet there are far more tools we can utilize to perform versatile tasks in the real world, particularly the API tools created by human developers spread on the web. \nTherefore, many recent works aggregate API tools from various web sources and create benchmarks for using these APIs, as shown in \\autoref{tab:api-benchmarks}.\n\n\\begin{table}[ht]\n\\small\n\\vspace{-1mm}\n\\begin{center}\n    \\begin{tabular}{l|lllc}\n    \\toprule\n    \\multicolumn{1}{c|}{\\textbf{Benchmark}} & \\multicolumn{1}{c}{\\textbf{Tool Source}} & \\multicolumn{1}{c}{\\textbf{Example Curation}} & \\textbf{Domain (\\S\\ref{sub:app-spec})} & \\multicolumn{1}{c}{\\textbf{Executable}} \\\\ \n    \\midrule\n    {\\hyperlink{cite.xu2023tool}{ToolBench$_1$}} & {existing dataset} & {adopted, human annotated} & {\\work, \\world} & {\\ding{51}} \\\\\n    {\\hyperlink{cite.qin2023toolllm}{ToolBench$_2$}} & {RapidAPI} & {model synthesized} & {\\work, \\world} & {\\ding{51}} \\\\\n    {\\hyperlink{cite.zhuang2023toolqa}{ToolQA}} & {existing dataset} & {model synthesized} & {\\work, \\kn} & {\\ding{51}} \\\\\n    {\\hyperlink{cite.tang2023toolalpaca}{ToolAlpaca}} & {PublicAPIs} & {model synthesized} & {\\kn, \\work, \\world, \\modal} & {\\ding{55}} \\\\\n    {\\hyperlink{cite.li-etal-2023-api}{API-Bank}} & {PublicAPIs} & {human annotated} & {\\work, \\world} & {\\ding{51}} \\\\\n    {\\hyperlink{cite.huang2024metatool}{MetaTool}} & {OpenAI Plugins} & {model synthesized} & {\\work, \\world, \\modal} & {\\ding{55}} \\\\\n    % \\midrule\n    {\\hyperlink{cite.patil2023gorilla}{Gorilla}} & {HF, Torch, TF} & {model synthesized} & {\\nn} & {\\ding{55}} \\\\\n    {\\hyperlink{cite.shen2023hugginggpt}{HuggingGPT}} & {HF} & {human annotated} & {\\nn} & {$~~$\\ding{55}$^{*}$}\\\\\n    {\\hyperlink{cite.shen2023taskbench}{Task Bench}} & {HF, PublicAPIs} & {model synthesized} & {\\nn, \\modal, \\world} & {\\ding{55}} \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{center}\n\\vspace{-3mm}\n\\caption{Benchmarks of providing aggregated APIs to LMs as tools. \nHF is short for HuggingFace. `\\ding{55}$^{*}$' means that: though tools employed by HuggingGPT are executable, it does not evaluate the execution output due to the cost of hosting and inferencing.}\n\\vspace{-1mm}\n\\label{tab:api-benchmarks}\n\\end{table}\n\n\n\n\\noindent \\textbf{Tool sources} \\quad\nTools are mainly aggregated from existing datasets or public APIs.\nWhile \\citet{xu2023tool,zhuang2023toolqa} adopt existing datasets and propose alternative methods via tool augmentation, these benchmarks are limited in domains. Several works scrape more APIs from online sources such as Public APIs \\citep{tang2023toolalpaca}, RESTful APIs \\citep{tang2023toolalpaca}, or the OpenAI plugin list \\citep{huang2024metatool}. Beyond human-crafted APIs \\citep{li-etal-2023-api}, neural models from ML platforms can be similarly presented in an API format \\citep{patil2023gorilla,shen2023hugginggpt,shen2023taskbench}. \nNonetheless, as tools are collected from heterogeneous sources, it is challenging to select the best benchmark or unify all these varied benchmarks.\n\n\n\\noindent \\textbf{Example curation} \\quad\nExamples can be adopted from existing datasets, annotated by humans, or synthesized by LMs.\nWhile most examples adopted from existing datasets are human annotated \\citep{xu2023tool}, only \\citet{li-etal-2023-api} do so for scraped APIs, by surveying 500 people and creating 314 dialogues manually.\nMost other works prompt GPT models to synthesize examples \\citep{qin2023toolllm,tang2023toolalpaca,shen2023taskbench,zhuang2023toolqa, huang2024metatool}, however, leading to issues of \\textit{naturalness} and \\textit{executability}.\n\n\\textbf{First}, LMs are often asked to create examples, even tool outputs in \\citet{tang2023toolalpaca}, given a heuristically selected set of tools. This approach leads to potential issues in two-fold: (i) the selected tools may not be used together in practice, and (ii) the synthesized examples may not reflect the \\textit{natural use cases} of these tools.\n\\textbf{Second}, 5 out of 9 benchmarks in \\autoref{tab:api-benchmarks} do not support tool execution, to alleviate the cost of hosting multiple APIs, especially when they may fail or produce unstable outputs. For example, the weather returned by the \\texttt{check\\_weather} API may change over time. This un-executability causes \\textit{issues in evaluation}. Instead of matching final execution results using lexical- \\citep{li-etal-2023-api} or neural-based metrics \\citep{tang2023toolalpaca, qin2023toolllm}, works with unexecutable tools resort to pseudo matching of API calling expressions with lexical~\\citep{tang2023toolalpaca,shen2023hugginggpt,huang2024metatool} and syntactical \\citep{patil2023gorilla, shen2023taskbench} means.\n\n\n\n% #################################\n\\subsection{What metrics are measured now?}\n\\label{sub:eval-metrics}\n\n\\noindent \\textbf{Task completion} \\quad\nTools are used to assist task solving. Most works that allow tool execution evaluate the task completion score to quantify the effectiveness of utilizing tools.\n\n\\noindent \\textbf{Tool selection} \\quad\nFor datasets with execution issues \\citep{huang2024metatool,shen2023taskbench}, another common metric is the accuracy of selecting the correct tools. This helps disentangle incorrect tool selection errors from inaccurate tool usage errors. \nDespite that tool selection mainly serves as a proxy for evaluating task completion when having unexecutable tools, it can be seen as a measure of LM planning abilities --- the process of breaking down a task into multiple steps and selecting tools to complete individual steps.\n\n\\noindent \\textbf{Tool reusability} \\quad\nWhile tool reusability is often deemed important in took-making literature \\citep{cai2023large,yuan2023craft}, only \\citet{wang2024trove} evaluates tool reusability by the size of induced toolboxes over a fixed number of examples. As its literal meaning, reusable tools can be (re)used to solve multiple examples hence having more generic functionalities. Adopting a reusable tool is more efficient than using multiple specific tools, and facilitates human verification in both speed and accuracy dimensions \\citep{wang2024trove}.\n\n\n% #################################\n\\subsection{What properties are missing?}\n\\label{sub:desired-properties}\n\n\\noindent \\textbf{Efficiency of tool integration} \\quad\nAs demonstrated by our empirical study (\\S\\ref{sub:trade-off}), the benefits brought by the tools come with the cost of additional computation, especially for teaching LMs to use tools via training or prompting. In addition to performance gain, reporting the computation overhead can enable fairer comparisons between different approaches.\n\n\\noindent \\textbf{Quality of tools} \\quad\nWhile existing works mostly focus on how tools improve task accuracy, the \\textit{performance of tools} themselves is also important. Tool performance can cover multiple aspects such as completing the call quickly, requiring less computation, and not putting users at risk or failing unexpectedly. \nOne way to measure these aspects is to conduct API testing \\citep{yasar2022software,ehsan2022restful} on their runtime, memory usage, and success rate.\n\n\\noindent \\textbf{Reliability of unstable tools} \\quad\nParticularly for tools that involve \\textit{neural models} or \\textit{randomized components}, their output quality may be unstable and unpredictable. For example, the \\texttt{VQA} tool \\citep{gupta2022visual} may answer some questions correctly but others incorrectly.\nIt is important to \\textit{be aware of} this uncertainty in contrast to stable, rule-based tools such as a \\texttt{calculator}, further alleviate this instability and guarantee more predictable outputs.\n\n\\noindent \\textbf{Reproducible testing} \\quad\nMany tools interact with the real world and may return different results at different times. For example, \\texttt{check\\_weather} may return ``sunny'' today but ``cloudy'' tomorrow. This irreproducible behavior poses great challenges to creating \\textit{static evaluation} benchmarks with reference answers. % For instance, the answer to ``How's the weather today?'' should not be a fixed ``sunny'', because the correct answer may change according to the specific time of evaluation.\nWhile some works alleviate this by evaluating API calls without executing them, a more rigorous method could be \\textit{parallel testing} \\citep{sharma2018automated} --- executing the model-generated program and the reference program in parallel, and measuring if their final outputs match.\n\n\\noindent \\textbf{Safe usage} \\quad\nMost systems may only opt to use tools if they are trusted to be secure \\citep{barbir2007challenges}.\nAt the very least, users favor tools that can be easily understood and verified. Further, systems may need to enforce mutual authentication and ensure data integrity \\citep{ehsan2022restful}.\nYet there are more security threats and methods beyond the discussion here. We encourage readers to peruse the referenced works above for thorough studies.\n\n\n---\n/workspace/dumps/workspace/my_paper/sections/5_tradeoff.tex:\n\\section{Trade-offs in tool usage}\n\\label{sub:trade-off}\n\nLeveraging tools often brings better performance, however, should we always use tools? More concretely, is the performance gain from using tools worthy of the computation cost spent for LMs to learn to use tools, or the inference cost at test time?\nExisting works mainly focus on task accuracy, but a more nuanced picture emerges when we take other factors into account.\nWe empirically study the performance gain and learning cost of various methods on their experimented datasets in \\autoref{tab:api-benchmarks}, using which we discover more efficient (i.e., achieve greater gains with less compute) methods and tasks that benefit more from tools.\n\n\\begin{table}[ht]\n\\vspace{-3mm}\n\\small\n\\begin{center}\n\\resizebox{0.92\\linewidth}{!}{\n    \\begin{tabular}{llcrcrr}\n    \\toprule\n    \\multicolumn{1}{c}{\\multirow{2}{*}{\\textbf{Type}}} & \\multirow{2}{*}{\\textbf{Method}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\textbf{Task}}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\textbf{$\\Delta$ Perf.}}} & \\multirow{2}{*}{\\textbf{\\# Params (B)}} & \\multicolumn{2}{c}{\\textbf{\\# Tokens (M)}} \\\\\n    {} & {} & {} & {} & {} & {train} & {test} \\\\\n    \\midrule\n    \\multirowcell{9}{tool\\\\use} & \\multirow{5}{*}{ToolFormer} & \\color{blue} {cloze} & {+ 14.7} & {6.7} & {642.1} & {269.0} \\\\\n    {} & {} & \\color{ao} {math} & {+ 30.4} & {6.7} & {3864.2} & {421.0} \\\\\n    {} & {} & \\color{chromeyellow} {QA} & {+ 5.8} & {6.7} & {1101.2} & {189.0} \\\\\n    {} & {} & \\color{red} {multilingual} & \\colorbox{red!27}{- 0.2}  & {6.7} & {606.0} & {274.0} \\\\\n    {} & {} & \\color{ballblue} {temporal} & {+ 13.0} & {6.7} & {508.8} & {202.0} \\\\\n    \\cmidrule{2-7}\n    {} & {API-Bank} & {API} & {+ 24.4} & {7} & \\textbf{190414.6} & {0.0} \\\\\n    \\cmidrule{2-7}\n    {} & {ToolAlpaca} & {API} & {+ 45.2} & {7} & \\textbf{241889.3} & {0.0} \\\\\n    \\cmidrule{2-7}\n    {} & \\multirow{2}{*}{Chameleon} & {science} & {+ 2.6} & {-} & {0.0} & {88.3} \\\\\n    {} & {} & \\color{darklavender} {table} & {+ 1.9} & {-} & {0.0} & {325.9} \\\\\n    \\midrule\n    \\midrule\n    \\multirowcell{7}{tool\\\\making} & {LATM} & {BigBench} & {+ 29.1} & {-} & {28.5} & {4720.0} \\\\\n    \\cmidrule{2-7}\n    {} & \\multirow{2}{*}{CREATOR} & \\color{ao} {math} & {+ 4.5} & {-} & {0.0} & {5113.6} \\\\\n    {} & {} & \\color{darklavender} {table} & \\colorbox{chromeyellow!20}{+ 0.0} & {-} & {0.0} & \\textbf{6827.6} \\\\\n    \\cmidrule{2-7}\n    {} & \\multirow{2}{*}{CRAFT} & \\color{ao} {math} & {+ 13.2} & {-} & {4126.6} & {4098.5} \\\\\n    {} & {} & \\color{darklavender} {table} & {+ 17.2} & {-} & {2750.6} & {5018.2} \\\\\n    \\cmidrule{2-7}\n    {} & \\multirow{2}{*}{TroVE} & \\color{ao} {math} & {+ 21.0} & {-} & {0.0} & {1825.2} \\\\\n    {} & {} & \\color{darklavender} {table} & {+ 12.0} & {-} & {0.0} & {1358.8} \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n\\end{center}\n\\vspace{-2mm}\n\\caption{Computation cost (number of tokens in $M$ and parameters in $B$) of tooling methods and their performance gain on experimented datasets. To fairly compare costs on datasets with different sizes, we report the average number of tokens spent on a testing example.}\n\\vspace{-3mm}\n\\label{tab:compute-cost}\n\\end{table}\n\n\n\nFor each work and each dataset they experimented with,\\footnote{ We did not measure some works due to insufficient resources.} we evaluate the performance gain after LM learned or made tools to solve tasks, compared to the baseline LM with no prior exposure to tool-related information.\nWe also quantify the computation cost of their tooling approaches during the token-consuming training and inference processes. For works using models with known sizes, we report both (i) the number of tokens in input prompts and outputs, and (ii) the parameters in experimented models to achieve corresponding performance improvements. For methods using the size-unknown GPT-4 model, which are also comparable w.r.t. to model size since they use the same GPT-4 model, we only report the number of tokens processed.\nWe elaborate more on computation details in \\S\\ref{app:learning-effort}.\n\n\n\n\n\n\\noindent \\textbf{What tasks benefit the most from tools?} \\quad\nIn general, tasks that cover multiple domains experience the highest increase, such as the ToolAlpaca benchmark in tool-using and the BigBench dataset in tool-making scenarios. Nonetheless, substantial gains may be expected\n\\begin{wrapfigure}[12]{r}{0.33\\textwidth}\n\\vspace{-3mm}\n\\includegraphics[width=0.31\\textwidth]{./figures/toolformer-cost.pdf}\n\\vspace{-3mm}\n\\caption{Compute \\& performance gain with ToolFormer.}\n\\vspace{-2mm}\n\\label{fig:toolformer-cost}\n\\end{wrapfigure}\non API benchmarks (i.e., API-Bank and ToolAlpaca), because all examples are synthesized use cases for designated tools (\\S\\ref{sub:app-spec}), no-tool baselines are deprived of necessary components (i.e., tools) to solve the task, therefore achieving much lower accuracy.\n\n\nOn existing benchmarks, the ToolFormer method is the most efficient on MATH problems, showing the highest $30.4$ increase with little computation ($0.17$ MB). While other tasks improve less, multilingual tasks even degrade by $-0.2$ points, despite using a similar amount of compute. This variance across tasks aligns with expectations: using a \\texttt{calculator} tool greatly improves the arithmetic ability of probabilistic LMs, which are not naturally suitable for symbolic calculations; however, LMs are originally built to solve language tasks such as machine translation (MT), so assigning the MT task to another (usually smaller) LM may not bring substantial improvements.\n\n\n\\begin{wrapfigure}[14]{r}{0.33\\textwidth}\n\\vspace{-2mm}\n\\includegraphics[width=0.32\\textwidth]{./figures/tool-make-cost.pdf}\n\\vspace{-3mm}\n\\caption{Comparing different tool-making methods.}\n\\vspace{-1mm}\n\\label{fig:tool-make-cost}\n\\end{wrapfigure}\n\\noindent \\textbf{What methods are efficient in tool-making?} \\quad\nWhile it is hard to conduct fair comparisons for many works experimenting on different datasets, in tool-making scenarios (\\autoref{fig:tool-make-cost}), the three methods (Creator, CRAFT, \\textsc{TroVE}) experiment on the same MATH and TabMWP datasets, thus enabling fair comparisons in both cost and performance dimensions.\n\\textsc{TroVE} appears to be the most efficient method in general, costing only $1.2$--$1.4$K tokens while improving the performance by $12.0$--$21.0$ points in accuracy. \nIn contrast, CREATOR and CRAFT are less efficient, costing $3.8$--$6.0$ times of compute, yet achieve only minimal ($0.0$--$4.5$\\%) or comparable ($4.1$--$5.0$\\%) accuracy increases.\n\n\n\\noindent \\textbf{Training-time vs inference-time cost} \\quad\nTraining-time and inference-time costs may not be equally important to many practitioners, since inference may be run many times but training often only needs to be done once.\\footnote{Another measure of the inference process is latency, which also heavily depends on implementation or hardware choices. We do not report latency since these methods are implemented differently.} If we only consider inference-time cost in \\autoref{tab:compute-cost}, the efficiency ranking of tooling methods changes. On one hand, tool-making method rankings roughly remain the same, except that CRAFT requires less compute than CREATOR on both tasks after getting rid of the training cost.\nOn the other hand, however, the ranking among tool-using methods drastically changes: ToolFormer requires more compute than API-Bank and ToolAlpaca when considering only inference-time cost. We conjecture this is mainly due to differences in baseline setups: ToolFormer adds in-context examples than the CoT baseline, API-Bank and ToolAlpaca use the same prompt for baseline and fine-tuned LMs with varied abilities to utilize tools presented in the prompt.\nIn general, if the user has sufficient budgets for training but higher demands on inference-time efficiency, the training approaches proposed by API-Bank and ToolAlpaca could be more suitable.\n\n\n---\n/workspace/dumps/workspace/my_paper/sections/conclusion.tex:\n\\section{Final Remarks}\nOur survey provides definitions for LM-used tools and systematic summaries of existing approaches. While our empirical analysis guides when (on what tasks) and how (use what methods) should one use tools, we hope readers can more clearly understand the scenarios and techniques of LM tooling, from basic paradigm to advanced settings, and across LMs speaking natural and programming languages. \n% \\dfcomment{I feel like the `what methods' part was comparatively underexplored -- and rightly so, since methods are still pretty new and haven't been thoroughly compared. Maybe we should instead focus, here, on the `how-to-evaluate' aspects which we were more concrete on?}\n% Further, we point out missing aspects of tool evaluation and suggest more comprehensive measurements for efficiency, performance, reliability, and safety.\nWe believe tools can greatly extend and facilitate LM abilities, and hope our work elicits more discussions and research developments in %this direction.\n(i) proposing benchmarks with natural use cases and executable tools, (ii) utilizing comprehensive evaluation metrics proposed in \\S\\ref{sec:good-tool}, and (iii) exploring more challenging and realistic scenarios for tool-using and tool-making techniques.\n% (i) proposing better benchmarks, methods, and evaluation metrics, as well as (ii) advanced topics empowered by tools such as more capable agents and their interactions with humans or the world.\n% \\dfcomment{could consider broader directions for future work here enabled by your perspective, e.g. what other sorts of things might be implemented as tools that haven't been already? (interaction with people? reads and writes from an external memory?)}\n% \\gncomment{I agree with Daniel's comment above. I think that more insight into future directions or missing pieces would be nice here. Right now it's basically ``better evaluation'' and ``better methods'' which isn't super-insightful -- those are always the things that we can do :)}\n\n\n% \\subsubsection*{Author Contributions}\n% If you'd like to, you may include a section for author contributions as is done\n% in many journals. This is optional and at the discretion of the authors.\n\n\\subsubsection*{Acknowledgments}\nWe thank Saujas Vaduguru, Sherry Tongshuang Wu, Jiawei Liu, Shihao Liang, Pengfei Liu for the helpful discussions. Zora Zhiruo Wang is supported by the Teng Family Presidential Fellowship. Hao Zhu is supported by NSF EAGER Award \\#2141751.\n\n---\n/workspace/dumps/workspace/my_paper/appendix/learning-effort.tex:\n\\section{Detailed computation process for tooling trade-offs}\n\\label{app:learning-effort}\n\nFor each method measured in \\S\\ref{sub:trade-off}, we describe the detailed processes in estimating their computation cost and performance improvement. \nFor open-source models, we estimate cost $C = 6ND$, where $N$ is the number of tokens and $D$ is the parameter size (\\autoref{fig:overall-cost}, left). Because the parameter size $D$ of closed-source GPT is unknown, we only measure the number of extra tokens $N$ per example (\\autoref{fig:overall-cost}, right). \n\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=\\textwidth]{./figures/compute-cost.pdf}\n\\vspace{-6mm}\n\\caption{Computation cost of different approaches using open-source (left) and closed-source (right) models, and their performance gain on experimented datasets. We use different colors to represent tasks and different shapes to represent methods.}\n% \\vspace{-2mm}\n\\label{fig:overall-cost}\n\\end{figure}\n\n% \\textbf{\\begin{table}[ht]\n% \\small\n% \\begin{center}\n%     \\begin{tabular}{lllll}\n%     \\toprule\n%     \\multicolumn{1}{c}{\\textbf{Work}} & \\multicolumn{1}{c}{\\textbf{Baseline}} & \\multicolumn{1}{c}{\\textbf{Tool-Use LM}} & \\multicolumn{1}{c}{\\textbf{Compute}} & \\multicolumn{1}{c}{\\textbf{Type}} \\\\\n%     \\midrule\n%     {API-Bank} & {Alpaca} & {Lynx} & {3-epoch train} & {tool-use, train \\& test} \\\\\n%     {ToolAlpaca} & {Vicuna} & {ToolAlpaca} & {3-epoch train} & {tool-use, train \\& test} \\\\\n%     {Toolformer} & {GPT-J} & {Toolformer} & {25$k$ examples train} & {tool-use, train \\& test} \\\\\n%     {LATM} & {CoT} & {LATM} & {train, verify, test} & {tool-make, train \\& test} \\\\\n%     {CRAFT} & {PoT} & {CRAFT} & {train, verify, test} & {tool-make, train \\& test} \\\\\n%     % \\midrule\n%     {Chameleon} & {CoT/PoT} & {Chameleon} & {few-shot, verify} & {tool-use, test} \\\\\n%     {CREATOR} & {PoT} & {CREATOR} & {abstract, decide, rectify} & {tool-make, test} \\\\\n%     {TroVE} & {Primitive} & {TroVE} & {3-way generation} & {tool-make, test} \\\\\n%     \\bottomrule\n%     \\end{tabular}\n% \\end{center}\n% \\caption{Details for measuring computation cost and task performance.}\n% \\label{tab:est-compute-cost}\n% \\end{table}}\n\n\n\\subsection{Methods using known-sized models}\n\nFor methods using models whose parameter sizes are known, we estimate the computation cost by the FLOPs during any additional modules such as training and inference with additional context. In general, the computation cost is majorly affected by (1) the number of tokens processed, and (2) the parameter size of models.\n\n\n\n\\noindent \\textbf{API-Bank \\citep{li-etal-2023-api}} \\quad\nThis work trains the Lynx model that uses tools to solve problems in the proposed API-Bank dataset. The Lynx model is initialized by Alpaca 7B parameters, and trained on the API-Bank training set with 3 epochs. Therefore, we adopt the Alpaca 7B as the baseline and Lynx as the tool-using model, where the 3-epoch training is the additional computation cost introduced to enable tool use.\nWe calculate the total number of tokens involved in the training process, including the example i/o and additional instructions. \nBecause the baseline and proposed method use the same prompt at inference time, no additional computation is required.\nRegarding task performance, we adopt the total correctness across all evaluation systems, as reported in Table 3. We report the difference between the fine-tuned Lynx-7B and the zero-shot Alpaca-7B.\n\n\n\\noindent \\textbf{ToolAlpaca \\citep{tang2023toolalpaca}} \\quad\nThis work proposes the ToolAlpaca dataset and trains Vicuna models to use tools. The baseline models are Vicuna-7B and Vicuna-13B models. The trained tool-using models are called ToolAlpaca-7B and ToolAlpaca-13B models. All ToolAlpaca models are trained on the training split for 3 epochs, so we estimate the cost during this training process for 7B and 13B models, respectively.\nWe adopt the `overall' results reported in Table 3, on examples with both simulated tools and real-world APIs, and report their average results. We measure the performance gain by the difference between the ToolAlpaca-7/13B and Vicuna-7/13B. \n\n\n\\noindent \\textbf{Toolformer \\citep{schick2023toolformer}} \\quad\nThis work integrates five tools --- question answering system, calculator, Wikipedia search, machine translation system, and calendar --- respectively for five tasks transformed from a subset of CCNet \\citep{wenzek2020ccnet}.\nStarting with GPT-J models \\citep{wang2021gpt-j} as the no-tool baseline, they train on 25$k$ model-synthesized examples for each tool and obtain the Toolformer models, causing a total of 1$M$ FLOPs for each task. At inference time, they add special instruction and in-context examples to prompt tool using, resulting in extra compute.\nBecause each task contains multiple datasets, we report the average results to represent the general task performance.\n\n\n\n\n% ######################\n\\subsection{Models with unknown size}\n\nWhile many of the works use GPT-3.5 or GPT-4 models that do not release their parameter size, we estimate the cost by using the number of tokens processed in extra modules.\n\n\\noindent \\textbf{Chameleon \\citep{lu2023chameleon}} \\quad \nThis work proposes to take a tool-augmented approach to improve on two existing datasets --- ScienceQA and TabMWP. Because all experiments use ChatGPT and GPT-4 models, whose parameter sizes are unknown, we only examine results with (the better) GPT-4 model to fairly compare with other methods using GPT-4. \nSpecifically for the ScienceQA dataset, we adopt the Chain-of-Thought (CoT) baseline reported in the paper, and report task accuracy as in the \\textsc{All} column in Table 3. We calculate the difference in number of tokens between the proposed Chameleon methods against the CoT baseline.\nFor the TabMWP dataset, we adopt the Program-of-Thought (PoT) baseline and similarly calculate the token number difference using the provided results.\\footnote{\\url{https://github.com/lupantech/chameleon-llm}} We adopt numbers in the \\textsc{All} column in Table 4 as the TabMWP accuracy. \n\n\n\\noindent \\textbf{LATM \\citep{cai2023large}} \\quad\nThis work proposes to use LMs to make tools for individual tasks in BigBench. Compared to the chain-of-thought (CoT) baseline, the proposed LATM method integrates training, validation, and inference stages to make tools and solve questions. We estimate the compute cost by the additional number of tokens used for LATM than for CoT.\nWe measure each method by averaging its accuracy across all six selected tasks.\n\n\\noindent \\textbf{CRAFT \\citep{yuan2023craft}} \\quad\nThis work uses LMs to make tools for math, table, and image reasoning tasks. We calculate the number of tokens used during training and inference, using its released code and data.\\footnote{\\url{https://github.com/lifan-yuan/CRAFT}}\nCRAFT similarly implements CoT as the baseline, and proposes further training, verification, and finally testing in the CRAFT method. \nWe report its task accuracy on the representative datasets from each task --- MATH, TabMWP, and GQA --- to enable fairer comparison with other works having overlapping datasets.\n\n\n\\noindent \\textbf{CREATOR \\citep{qian2023creator}} \\quad\nAs a prior work for CRAFT, CREATOR similarly tests on MATH and table tasks, but designs its methods differently. In addition to CoT, this work implements a stronger program-oriented baseline called Program-of-Thought (PoT). We also adopt PoT as the main baseline without tool making or using. The CREATOR method operates at test time, with multiple steps through tool making, solution generation, verification, rectification, etc. We calculate the difference in number of tokens between the CREATOR approach and the baseline PoT setting.\nWe adopt the task accuracy reported in Table 2 (MATH) and Table 3 (TabMWP) from the original paper.\n\n\\noindent \\textbf{TroVE \\citep{wang2024trove}} \\quad\nTroVE also induces tools without training supervision. This work adopts the primitive baseline, a presumably stronger version of PoT yet without much textual explanation. The main implementation change in TroVE is the three-mode generation and multi-candidate sampling. We calculate the additional tokens used in TroVE compared to the primitive baseline.\nThe dataset reports task accuracy, solution complexity, and toolbox size, we only adopt the task accuracy to fairly compare with other works.\n\n\n% \\begin{figure}[ht]\n% \\centering\n% % \\vspace{-1mm}\n% \\includegraphics[width=\\textwidth]{./figures/inference-cost.pdf}\n% \\vspace{-6mm}\n% \\caption{Performance gain versus inference-time computation cost.}\n% \\vspace{-3mm}\n% \\label{fig:inference-cost}\n% \\end{figure}\n\n\n% \\begin{table}[ht]\n% \\small\n% \\begin{center}\n% \\resizebox{0.92\\linewidth}{!}{\n%     \\begin{tabular}{ll}\n%     \\toprule\n%     \\multicolumn{1}{c}{\\textbf{Missing aspects}} & \\multicolumn{1}{c}{\\textbf{Potential metrics}} \\\\\n%     \\midrule\n%     {Efficiency of tool integration} & {computation overhead} \\\\\n%     {Quality of tools} & {runtime and memory usage of tools} \\\\\n%     {Reliability of unstable tools} & {success rate of tools} \\\\\n%     {Reproducible testing} & {execution correctness w.r.t. canonical trajectory} \\\\\n%     {Safe usage} & {visibility, data integrity, and other meta-data about tools} \\\\\n%     \\bottomrule\n%     \\end{tabular}\n%     }\n% \\end{center}\n% \\caption{Missing evaluation aspects and potential metrics.}\n% \\label{tab:missing-eval}\n% \\end{table}\n\n---\n/workspace/dumps/workspace/my_paper/appendix/embodied.tex:\n\\section{Tools with embodied agents}\n\\label{sec:embodied}\n\n\\zwcomment{merge related works to earlier sections and remove this section?}\n\nIn previous sections, we discuss LM-used tools in NLP task environments. In the last part of this survey, we broaden our discussion about tools to the agent-environment interaction setup. \nSpecifically, we regard \\emph{agents} as the policies that interact with environments based on the perceived environmental state;\\footnote{We are specifically interested in LM-based agents, which use LM as a component.} and \\emph{environments} are the collections of objects and their relationships.\\footnote{In NLP tasks, an environment contains example contexts and the collection of tool environments.}\nWhile most objects can be purposed as tools, e.g., brushes can be used to paint, brooms can reach high objects, % magnifiers can help read books,\n% books can be used to hit nails into blocks, terminal can be used to execute a drawing program, \nthe following definition captures the essence of tools:\n\n\\begin{definition}[Tools]\n\\label{def:tools_broad}\nTools are the objects in the environments that are not directly part of the goals of the agents, but are used by the agents in order to achieve the goals.\n\\end{definition}\n\nFor example, when drawing a picture, $\\{$brush, paint, canvas$\\}$ are all used, but only the \\texttt{brush} is regarded as tools, since they are not part of the final product --- a picture.\n% \\zwcomment{we could align what figure 4 draws to the examples we made below. it also helps that you could discuss the example by referring to the illustration}\n% For example, when making a hamburger, stove, toaster, pans, breads, patties, tomatoes, and cabbages are all used, but only stove, toasters, and pans are regarded as tools, since they are not part of final product, a hamburger. \nIt should be noted that an object can be a tool for one task but not a tool for another, e.g. a \\texttt{calculator} is a tool when finding the root of an equation, but it is not a tool when a virtual agent is launching the calculator app.\\footnote{The definition of tools used by humans and non-human animals has been evolving in the past decades. Def. \\ref{def:tools_broad} is similar to \\citet{beauchamp2011oxford}, which defines tools as inanimate objects ``causing a change in the environment, thereby facilitating one's achievement of a target goal''.}\n\n\n\\begin{wrapfigure}[18]{r}{0.43\\textwidth}\n\\vspace{-4mm}\n\\footnotesize\n    \\includegraphics[width=0.43\\textwidth]{figures/tool-survey-paradigm.pdf}\n\\vspace{-6mm}\n\\caption{The action-observation feedback loop: an agent interacts with an environment by acting (with tools) on other objects in the environment, and getting feedback reward from it.}\n\\label{fig:agent-env}\n\\end{wrapfigure}\n\n\n\nThe approaches for using tools under Def. \\ref{def:tools-lm} can also be applied to tools under Def. \\ref{def:tools_broad}, by implementing external functions composed of low-level manipulation policies. \nThese composed functions are often \\textit{rule-based methods} or \\textit{magic functions} provided by the simulation environments (similar to works providing expert-designed APIs to LMs in \\S\\ref{sub:app-spec}). For example, \\citet{shridhar2020alfred} employ human-crafted APIs in the AI2THOR simulator \\citep{kolve2017ai2} to use microwaves and fridges to cool and heat objects.  \n\\citet{wang2023voyager} use JavaScript APIs in Mineflayer\\footnote{\\url{https://github.com/PrismarineJS/mineflayer}} to manipulate Minecraft tools such as mining with axes. \\citet{liu2024agentbench} hand-craft the \\texttt{search} tool as Python functions to query data over the web, and \\citet{boiko2023autonomous} similarly for conducting chemical research.\n\nHowever, in the real world, not all high-level manipulation policies are readily available. For example, in \\citet{ahn2022saycan}, % although picking up sponges can be learned as a policy, \nwiping tables with a sponge is too complex to be implemented as an action, hence is actually executed by humans.\n% although picking up sponge is a learned subpolicy, wiping tables with a sponge is executed by humans. \nNonetheless, instead of adopting readily available policies, these compositions can also be learned via behavior cloning and reinforcement learning.\nFor instance, \\citet{baker2019emergent} show that tool use can be learned without expert demonstrations or rewarding signals, and \\citet{yu2023language} feed NL inputs to robots to learn skills such as picking up apples.\n% More recently, \\citet{ahn2022saycan} also learn policies to pick up objects such as sponges.\n\nTraining and evaluating LMs and LM-based agents on their tooling abilities in the real world remain an open challenge.\n% Training and evaluating LMs' and LM-based agents' ability to use tools in the real world remains an open question. \nHere we highlight two future directions: (1) \\textit{building more complex environments}: with sufficient objects for versatile tool use and tool make, (2) \\textit{harness LM procedural knowledge}: LMs equip strong procedural knowledge via extensive training, which can be utilized to improve planning and navigation in complex, realistic environments. \n\n\n---\n/workspace/dumps/workspace/my_paper/colm2024_conference.bib:\n@inproceedings{komeili-etal-2022-internet,\n  title={Internet-Augmented Dialogue Generation},\n  author={Komeili, Mojtaba and Shuster, Kurt and Weston, Jason},\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  year={2022},\n  url={https://aclanthology.org/2022.acl-long.579},\n}\n\n@article{lazaridou2022internetaugmented,\n  title={Internet-augmented language models through few-shot prompting for open-domain question answering}, \n  author={Angeliki Lazaridou and Elena Gribovskaya and Wojciech Stokowiec and Nikolai Grigorev},\n  year={2022},\n  journal={arXiv preprint arXiv:2203.05115},\n  url={https://arxiv.org/abs/2203.05115}\n}\n\n@article{thoppilan2022lamda,\n  title={LaMDA: Language Models for Dialog Applications}, \n  author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Vincent Zhao and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Pranesh Srinivasan and Laichee Man and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},\n  year={2022},\n  journal={arXiv preprint arXiv:2201.08239},\n  url={https://arxiv.org/abs/2201.08239}\n}\n\n@inproceedings{yao2023react,\n  title={ReAct: Synergizing Reasoning and Acting in Language Models},\n  author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik R Narasimhan and Yuan Cao},\n  booktitle={The Eleventh International Conference on Learning Representations },\n  year={2023},\n  url={https://openreview.net/forum?id=WE_vluYUL-X}\n}\n\n@article{parisi2022talm,\n  title={TALM: Tool Augmented Language Models}, \n  author={Aaron Parisi and Yao Zhao and Noah Fiedel},\n  year={2022},\n  journal={arXiv preprint arXiv:2205.12255},\n  url={https://arxiv.org/abs/2205.12255}\n}\n\n@article{schick2023toolformer,\n  title={Toolformer: Language Models Can Teach Themselves to Use Tools}, \n  author={Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},\n  year={2023},\n  journal={arXiv preprint arXiv:2302.04761},\n  url={https://arxiv.org/abs/2302.04761}\n}\n\n@article{paranjape2023art,\n  title={ART: Automatic multi-step reasoning and tool-use for large language models}, \n  author={Bhargavi Paranjape and Scott Lundberg and Sameer Singh and Hannaneh Hajishirzi and Luke Zettlemoyer and Marco Tulio Ribeiro},\n  year={2023},\n  journal={arXiv preprint arXiv:2303.09014},\n  url={https://arxiv.org/abs/2303.09014}\n}\n\n@article{nakano2022webgpt,\n  title={WebGPT: Browser-assisted question-answering with human feedback}, \n  author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},\n  year={2022},\n  journal={arXiv preprint arXiv:2112.09332},\n  url={https://arxiv.org/abs/2112.09332}\n}\n\n@inproceedings{wang-etal-2022-super,\n    title={Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},\n    author={Wang, Yizhong  and Mishra, Swaroop  and Alipoormolabashi, Pegah  and Kordi, Yeganeh  and Mirzaei, Amirreza  and Naik, Atharva  and Ashok, Arjun  and Dhanasekaran, Arut Selvan  and Arunkumar, Anjana  and Stap, David  and Pathak, Eshaan  and Karamanolakis, Giannis  and Lai, Haizhi  and Purohit, Ishan  and Mondal, Ishani  and Anderson, Jacob  and Kuznia, Kirby  and Doshi, Krima  and Pal, Kuntal Kumar  and Patel, Maitreya  and Moradshahi, Mehrad  and Parmar, Mihir  and Purohit, Mirali  and Varshney, Neeraj  and Kaza, Phani Rohitha  and Verma, Pulkit  and Puri, Ravsehaj Singh  and Karia, Rushang  and Doshi, Savan  and Sampat, Shailaja Keyur  and Mishra, Siddhartha  and Reddy A, Sujan  and Patro, Sumanta  and Dixit, Tanay  and Shen, Xudong},\n    booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},\n    year={2022},\n    url={https://aclanthology.org/2022.emnlp-main.340},\n}\n\n@inproceedings{li-etal-2023-api,\n    title={API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs},\n    author={Li, Minghao  and Zhao, Yingxiu  and Yu, Bowen  and Song, Feifan  and Li, Hangyu  and Yu, Haiyang  and Li, Zhoujun  and Huang, Fei  and Li, Yongbin},\n    booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},\n    year={2023},\n    url={https://aclanthology.org/2023.emnlp-main.187},\n}\n\n@article{liang2023taskmatrixai,\n  title={TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs}, \n  author={Yaobo Liang and Chenfei Wu and Ting Song and Wenshan Wu and Yan Xia and Yu Liu and Yang Ou and Shuai Lu and Lei Ji and Shaoguang Mao and Yun Wang and Linjun Shou and Ming Gong and Nan Duan},\n  year={2023},\n  journal={arXiv preprint arXiv:2303.16434},\n  url={https://arxiv.org/abs/2303.16434}\n}\n\n@inproceedings{hao2023toolkengpt,\n  title={ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings},\n  author={Shibo Hao and Tianyang Liu and Zhen Wang and Zhiting Hu},\n  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n  year={2023},\n  url={https://openreview.net/forum?id=BHXsb69bSx}\n}\n\n@inproceedings{Baker2020Emergent,\n  title={Emergent Tool Use From Multi-Agent Autocurricula},\n  author={Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},\n  booktitle={International Conference on Learning Representations},\n  year={2020},\n  url={https://openreview.net/forum?id=SkxpxJBKwS}\n}\n\n@article{wolfson2020break,\n  author={Wolfson, Tomer and Geva, Mor and Gupta, Ankit and Gardner, Matt and Goldberg, Yoav and Deutch, Daniel and Berant, Jonathan},\n  title={Break It Down: A Question Understanding Benchmark},\n  journal={Transactions of the Association for Computational Linguistics},\n  year={2020},\n  url={https://doi.org/10.1162/tacl\\_a\\_00309},\n}\n\n@inproceedings{zettlemoyer2007online,\n  title={Online learning of relaxed CCG grammars for parsing to logical form},\n  author={Zettlemoyer, Luke and Collins, Michael},\n  booktitle={Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},\n  pages={678--687},\n  year={2007}\n}\n\n@inproceedings{liang2011learning,\n  title={Learning minimal abstractions},\n  author={Liang, Percy and Tripp, Omer and Naik, Mayur},\n  booktitle={Proceedings of the 38th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages},\n  pages={31--42},\n  year={2011}\n}\n\n@article{artzi2013weakly,\n  author={Artzi, Yoav and Zettlemoyer, Luke},\n  title={Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions},\n  journal={Transactions of the Association for Computational Linguistics},\n  year={2013},\n  url={https://doi.org/10.1162/tacl\\_a\\_00209},\n}\n\n@article{zhang2023toolcoder,\n  title={ToolCoder: Teach Code Generation Models to use API search tools}, \n  author={Kechi Zhang and Huangzhao Zhang and Ge Li and Jia Li and Zhuo Li and Zhi Jin},\n  year={2023},\n  journal={arXiv preprint arXiv:2305.04032},\n  url={https://arxiv.org/abs/2305.04032}\n}\n\n@article{wang2024trove,\n  title={TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks}, \n  author={Zhiruo Wang and Daniel Fried and Graham Neubig},\n  year={2024},\n  journal={arXiv preprint arXiv:2401.12869},\n  url={https://arxiv.org/abs/2401.12869}\n}\n\n@inproceedings{gao2023pal,\n  title={Pal: Program-aided language models},\n  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},\n  booktitle={International Conference on Machine Learning},\n  pages={10764--10799},\n  year={2023},\n  organization={PMLR}\n}\n\n@article{chen2023program,\n  title={Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks},\n  author={Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},\n  journal={Transactions on Machine Learning Research},\n  year={2023},\n  url={https://openreview.net/forum?id=YfZ4ZPt8zd},\n}\n\n@inproceedings{cheng2023binding,\n  title={Binding Language Models in Symbolic Languages},\n  author={Zhoujun Cheng and Tianbao Xie and Peng Shi and Chengzu Li and Rahul Nadkarni and Yushi Hu and Caiming Xiong and Dragomir Radev and Mari Ostendorf and Luke Zettlemoyer and Noah A. Smith and Tao Yu},\n  booktitle={The Eleventh International Conference on Learning Representations },\n  year={2023},\n  url={https://openreview.net/forum?id=lH1PV42cbF}\n}\n\n@inproceedings{cao2023api,\n  title={API-Assisted Code Generation for Question Answering on Varied Table Structures},\n  author={Cao, Yihan and Chen, Shuyi and Liu, Ryan and Wang, Zhiruo and Fried, Daniel},\n  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},\n  year={2023},\n  url={https://aclanthology.org/2023.emnlp-main.897},\n}\n\n\n@inproceedings{shi2017world,\n  title={World of Bits: An Open-Domain Platform for Web-Based Agents},\n  author={Tianlin Shi and Andrej Karpathy and Linxi Fan and Jonathan Hernandez and Percy Liang},\n  booktitle={Proceedings of the 34th International Conference on Machine Learning},\n  year={2017},\n  series={Proceedings of Machine Learning Research},\n  publisher={PMLR},\n  url={https://proceedings.mlr.press/v70/shi17a.html},\n}\n\n@inproceedings{zheran2018reinforcement,\n  title={Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration},\n  author={Evan Zheran Liu and Kelvin Guu and Panupong Pasupat and Percy Liang},\n  booktitle={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryTp3f-0-},\n}\n\n@article{zhou2023webarena,\n  title={Webarena: A realistic web environment for building autonomous agents},\n  author={Zhou, Shuyan and Xu, Frank F and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Bisk, Yonatan and Fried, Daniel and Alon, Uri and others},\n  journal={arXiv preprint arXiv:2307.13854},\n  year={2023}\n}\n\n@inproceedings{wang2023voyager,\n  title={Voyager: An Open-Ended Embodied Agent with Large Language Models},\n  author={Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar},\n  booktitle={NeurIPS 2023 Foundation Models for Decision Making Workshop},\n  year={2023},\n  url={https://openreview.net/forum?id=P8E4Br72j3}\n}\n\n@book{shumaker2011animal,\n  title={Animal tool behavior: the use and manufacture of tools by animals},\n  author={Shumaker, Robert W and Walkup, Kristina R and Beck, Benjamin B},\n  year={2011},\n  publisher={JHU Press}\n}\n\n@article{tang2023toolalpaca,\n  title={ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases},\n  author={Qiaoyu Tang and Ziliang Deng and Hongyu Lin and Xianpei Han and Qiao Liang and Boxi Cao and Le Sun},\n  journal={arXiv preprint arXiv:2306.05301},\n  year={2023}\n}\n\n@inproceedings{qin2023toolllm,\n  title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs},\n  author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Lauren Hong and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},\n  booktitle={The Twelfth International Conference on Learning Representations},\n  year={2024},\n  url={https://openreview.net/forum?id=dHng2O0Jjr}\n}\n\n@article{huang2024metatool,\n  title={MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use},\n  author={Yue Huang and Jiawen Shi and Yuan Li and Chenrui Fan and Siyuan Wu and Qihui Zhang and Yixin Liu and Pan Zhou and Yao Wan and Neil Zhenqiang Gong and Lichao Sun},\n  journal={arXiv preprint arXiv:2310.03128},\n  year={2024}\n}\n\n@article{cai2023large,\n  title={Large Language Models as Tool Makers}, \n  author={Tianle Cai and Xuezhi Wang and Tengyu Ma and Xinyun Chen and Denny Zhou},\n  year={2023},\n  journal={arXiv preprint arXiv:2305.17126},\n  url={https://arxiv.org/pdf/2305.17126}\n}\n\n@article{qian2023creator,\n    title={CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation}, \n    author={Cheng Qian and Chi Han and Yi R. Fung and Yujia Qin and Zhiyuan Liu and Heng Ji},\n    year={2023},\n    journal={arXiv preprint arXiv:2305.14318},\n    url={https://arxiv.org/pdf/2305.14318}\n}\n\n@article{yuan2023craft,\n  title={CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets},\n  author={Lifan Yuan and Yangyi Chen and Xingyao Wang and Yi R. Fung and Hao Peng and Heng Ji},\n  journal={arXiv preprint arXiv:2309.17428},\n  year={2023}\n}\n\n@article{srivastava2023beyond,\n  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},\n  author={BIG-bench authors},\n  journal={Transactions on Machine Learning Research},\n  year={2023},\n  url={https://openreview.net/forum?id=uyTL5Bvosj}\n}\n\n@article{stengeleskin2024regal,\n  title={ReGAL: Refactoring Programs to Discover Generalizable Abstractions},\n  author={Elias Stengel-Eskin and Archiki Prasad and Mohit Bansal},\n  journal={arXiv preprint arXiv:2401.16467},\n  year={2024}\n}\n\n@article{shin2019program,\n  title={Program Synthesis and Semantic Parsing with Learned Code Idioms},\n  author={Richard Shin and Miltiadis Allamanis and Marc Brockschmidt and Oleksandr Polozov},\n  journal={arXiv preprint arXiv:1906.10816},\n  year={2019}\n}\n\n@article{ellis2023dreamcoder,\n  title={DreamCoder: growing generalizable, interpretable knowledge with wake--sleep Bayesian program learning},\n  author={Ellis, Kevin and Wong, Lionel and Nye, Maxwell and Sable-Meyer, Mathias and Cary, Luc and Anaya Pozo, Lore and Hewitt, Luke and Solar-Lezama, Armando and Tenenbaum, Joshua B},\n  journal={Philosophical Transactions of the Royal Society A},\n  year={2023},\n}\n\n@inproceedings{pmlr-v139-wong21a,\n  title={Leveraging Language to Learn Program Abstractions and Search Heuristics},\n  author={Wong, Catherine and Ellis, Kevin M and Tenenbaum, Joshua and Andreas, Jacob},\n  booktitle={Proceedings of the 38th International Conference on Machine Learning},\n  year={2021},\n  series={Proceedings of Machine Learning Research},\n  url={https://proceedings.mlr.press/v139/wong21a.html}\n}\n\n@article{bowers2023top,\n  title={Top-Down Synthesis for Library Learning},\n  author={Bowers, Matthew and Olausson, Theo X. and Wong, Lionel and Grand, Gabriel and Tenenbaum, Joshua B. and Ellis, Kevin and Solar-Lezama, Armando},\n  year={2023},\n  journal={Proc. ACM Program. Lang.},\n  publisher={Association for Computing Machinery},\n  url={https://doi.org/10.1145/3571234}\n}\n\n@article{grand2023lilo,\n  title={LILO: Learning Interpretable Libraries by Compressing and Documenting Code},\n  author={Grand, Gabriel and Wong, Lionel and Bowers, Matthew and Olausson, Theo X and Liu, Muxin and Tenenbaum, Joshua B and Andreas, Jacob},\n  journal={arXiv preprint arXiv:2310.19791},\n  year={2023}\n}\n\n@article{gupta2022visual,\n    title={Visual Programming: Compositional visual reasoning without training}, \n    author={Tanmay Gupta and Aniruddha Kembhavi},\n    year={2022},\n    journal={arXiv preprint arXiv:2211.11559},\n    url={https://arxiv.org/pdf/2211.11559}\n}\n\n@article{suris2023vipergpt,\n  title={Vipergpt: Visual inference via python execution for reasoning},\n  author={Sur{\\'\\i}s, D{\\'\\i}dac and Menon, Sachit and Vondrick, Carl},\n  journal={arXiv preprint arXiv:2303.08128},\n  year={2023}\n}\n\n@inproceedings{yang2023gpttools,\n  title={{GPT}4Tools: Teaching Large Language Model to Use Tools via Self-instruction},\n  author={Rui Yang and Lin Song and Yanwei Li and Sijie Zhao and Yixiao Ge and Xiu Li and Ying Shan},\n  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n  year={2023},\n  url={https://openreview.net/forum?id=cwjh8lqmOL}\n}\n\n@inproceedings{liu2023language,\n  title={Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment},\n  author={Hao Liu and Wilson Yan and Pieter Abbeel},\n  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n  year={2023},\n  url={https://openreview.net/forum?id=mlxRLIy7kc}\n}\n\n@inproceedings{yu2023spae,\n  title={{SPAE}: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen {LLM}s},\n  author={Lijun Yu and Yong Cheng and Zhiruo Wang and Vivek Kumar and Wolfgang Macherey and Yanping Huang and David A Ross and Irfan Essa and Yonatan Bisk and Ming-Hsuan Yang and Kevin Patrick Murphy and Alexander G Hauptmann and Lu Jiang},\n  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n  year={2023},\n  url={https://openreview.net/forum?id=CXPUg86A1D}\n}\n\n@article{yang2023mmreact,\n  title={MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action},\n  author={Zhengyuan Yang and Linjie Li and Jianfeng Wang and Kevin Lin and Ehsan Azarnasab and Faisal Ahmed and Zicheng Liu and Ce Liu and Michael Zeng and Lijuan Wang},\n  journal={arXiv preprint arXiv:2303.11381},\n  year={2023}\n}\n\n@article{gao2023assistgpt,\n  title={AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn},\n  author={Difei Gao and Lei Ji and Luowei Zhou and Kevin Qinghong Lin and Joya Chen and Zihan Fan and Mike Zheng Shou},\n  journal={arXiv preprint arXiv:2306.08640},\n  year={2023}\n}\n\n@article{xu2023tool,\n  title={On the Tool Manipulation Capability of Open-source Large Language Models},\n  author={Xu, Qiantong and Hong, Fenglu and Li, Bo and Hu, Changran and Chen, Zhengyu and Zhang, Jian},\n  journal={arXiv preprint arXiv:2305.16504},\n  year={2023}\n}\n\n@article{wang2023mint,\n  title={Mint: Evaluating llms in multi-turn interaction with tools and language feedback},\n  author={Wang, Xingyao and Wang, Zihan and Liu, Jiateng and Chen, Yangyi and Yuan, Lifan and Peng, Hao and Ji, Heng},\n  journal={arXiv preprint arXiv:2309.10691},\n  year={2023}\n}\n\n@article{mialon2023augmented,\n  title={Augmented Language Models: a Survey},\n  author={Gr{\\'e}goire Mialon and Roberto Dessi and Maria Lomeli and Christoforos Nalmpantis and Ramakanth Pasunuru and Roberta Raileanu and Baptiste Roziere and Timo Schick and Jane Dwivedi-Yu and Asli Celikyilmaz and Edouard Grave and Yann LeCun and Thomas Scialom},\n  journal={Transactions on Machine Learning Research},\n  issn={2835-8856},\n  year={2023},\n  url={https://openreview.net/forum?id=jh7wH2AzKK}\n}\n\n@article{qin2023tool,\n  title={Tool Learning with Foundation Models},\n  author={Yujia Qin and Shengding Hu and Yankai Lin and Weize Chen and Ning Ding and Ganqu Cui and Zheni Zeng and Yufei Huang and Chaojun Xiao and Chi Han and Yi Ren Fung and Yusheng Su and Huadong Wang and Cheng Qian and Runchu Tian and Kunlun Zhu and Shihao Liang and Xingyu Shen and Bokai Xu and Zhen Zhang and Yining Ye and Bowen Li and Ziwei Tang and Jing Yi and Yuzhang Zhu and Zhenning Dai and Lan Yan and Xin Cong and Yaxi Lu and Weilin Zhao and Yuxiang Huang and Junxi Yan and Xu Han and Xian Sun and Dahai Li and Jason Phang and Cheng Yang and Tongshuang Wu and Heng Ji and Zhiyuan Liu and Maosong Sun},\n  journal={arXiv preprint arXiv:2304.08354},\n  year={2023}\n}\n\n@inproceedings{gao2021simcse,\n  title={SimCSE: Simple Contrastive Learning of Sentence Embeddings},\n  author={Gao, Tianyu  and Yao, Xingcheng  and Chen, Danqi},\n  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  year={2021},\n  url={https://aclanthology.org/2021.emnlp-main.552}\n}\n\n@inproceedings{zhou2023docprompting,\n  title={DocPrompting: Generating Code by Retrieving the Docs},\n  author={Shuyan Zhou and Uri Alon and Frank F. Xu and Zhiruo Wang and Zhengbao Jiang and Graham Neubig},\n  booktitle={The Eleventh International Conference on Learning Representations },\n  year={2023},\n  url={https://openreview.net/forum?id=ZTCxT2t2Ru}\n}\n\n@article{robertson2009probabilistic,\n  title={The probabilistic relevance framework: BM25 and beyond},\n  author={Robertson, Stephen and Zaragoza, Hugo and others},\n  journal={Foundations and Trends{\\textregistered} in Information Retrieval},\n  volume={3},\n  number={4},\n  pages={333--389},\n  year={2009},\n  publisher={Now Publishers, Inc.}\n}\n\n@inproceedings{chen2017reading,\n  title={Reading Wikipedia to Answer Open-Domain Questions},\n  author={Chen, Danqi  and Fisch, Adam  and Weston, Jason  and Bordes, Antoine},\n  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  year={2017},\n  url={https://aclanthology.org/P17-1171}\n}\n\n@inproceedings{brown2020language,\n  title = {Language Models are Few-Shot Learners},\n  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},\n  booktitle = {Advances in Neural Information Processing Systems},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},\n  year = {2020}\n}\n\n@article{hsieh2023tool,\n  title={Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models}, \n  author={Cheng-Yu Hsieh and Si-An Chen and Chun-Liang Li and Yasuhisa Fujii and Alexander Ratner and Chen-Yu Lee and Ranjay Krishna and Tomas Pfister},\n  journal={arXiv preprint arXiv:2308.00675},\n  year={2023}\n}\n\n@inproceedings{lu2023chameleon,\n  title={Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models},\n  author={Pan Lu and Baolin Peng and Hao Cheng and Michel Galley and Kai-Wei Chang and Ying Nian Wu and Song-Chun Zhu and Jianfeng Gao},\n  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n  year={2023},\n  url={https://openreview.net/forum?id=HtqnVSCj3q}\n}\n\n@inproceedings{shen2023hugginggpt,\n  title={HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face},\n  author={Yongliang Shen and Kaitao Song and Xu Tan and Dongsheng Li and Weiming Lu and Yueting Zhuang},\n  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n  year={2023},\n  url={https://openreview.net/forum?id=yHdTscY6Ci}\n}\n\n@article{zhuang2023toolqa,\n  title={ToolQA: A Dataset for LLM Question Answering with External Tools},\n  author={Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},\n  journal={arXiv preprint arXiv:2306.13304},\n  year={2023}\n}\n\n@article{patil2023gorilla,\n  title={Gorilla: Large language model connected with massive apis},\n  author={Patil, Shishir G and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E},\n  journal={arXiv preprint arXiv:2305.15334},\n  year={2023}\n}\n\n@article{shen2023taskbench,\n  title={Taskbench: Benchmarking large language models for task automation},\n  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Zhang, Wenqi and Ren, Kan and Yuan, Siyu and Lu, Weiming and Li, Dongsheng and Zhuang, Yueting},\n  journal={arXiv preprint arXiv:2311.18760},\n  year={2023}\n}\n\n@inproceedings{wenzek2020ccnet,\n  title={CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data},\n  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\\'a}n, Francisco and Joulin, Armand and Grave, Edouard},\n  booktitle={Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  year={2020},\n  url={https://aclanthology.org/2020.lrec-1.494}\n}\n\n@misc{wang2021gpt-j,\n  author={Wang, Ben and Komatsuzaki, Aran},\n  title={GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model},\n  url={https://github.com/kingoflolz/mesh-transformer-jax},\n  year=2021\n}\n\n@inproceedings{zelle1996learning,\n  title={Learning to parse database queries using inductive logic programming},\n  author={Zelle, John M and Mooney, Raymond J},\n  booktitle={Proceedings of the national conference on artificial intelligence},\n  pages={1050--1055},\n  year={1996}\n}\n\n@article{zettlemoyer2012learning,\n  title={Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars},\n  author={Zettlemoyer, Luke S and Collins, Michael},\n  journal={arXiv preprint arXiv:1207.1420},\n  year={2012}\n}\n\n@inproceedings{berant2013semantic,\n  title={Semantic parsing on freebase from question-answer pairs},\n  author={Berant, Jonathan and Chou, Andrew and Frostig, Roy and Liang, Percy},\n  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},\n  pages={1533--1544},\n  year={2013}\n}\n\n@article{liang2013lambda,\n  title={Lambda dependency-based compositional semantics},\n  author={Liang, Percy},\n  journal={arXiv preprint arXiv:1309.4408},\n  year={2013}\n}\n\n@article{yu2018spider,\n  title={Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task},\n  author={Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and others},\n  journal={arXiv preprint arXiv:1809.08887},\n  year={2018}\n}\n\n@inproceedings{xie2022unifiedskg,\n  title={UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models},\n  author={Xie, Tianbao  and Wu, Chen Henry  and Shi, Peng  and Zhong, Ruiqi  and Scholak, Torsten  and Yasunaga, Michihiro  and Wu, Chien-Sheng  and Zhong, Ming  and Yin, Pengcheng  and Wang, Sida I.  and Zhong, Victor  and Wang, Bailin  and Li, Chengzu  and Boyle, Connor  and Ni, Ansong  and Yao, Ziyu  and Radev, Dragomir  and Xiong, Caiming  and Kong, Lingpeng  and Zhang, Rui  and Smith, Noah A.  and Zettlemoyer, Luke  and Yu, Tao},\n  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},\n  year={2022},\n  url={https://aclanthology.org/2022.emnlp-main.39}\n}\n\n@article{wang2024executable,\n  title={Executable Code Actions Elicit Better LLM Agents},\n  author={Xingyao Wang and Yangyi Chen and Lifan Yuan and Yizhe Zhang and Yunzhu Li and Hao Peng and Heng Ji},\n  journal={arXiv preprint arXiv:2402.01030},\n  year={2024}\n}\n\n@article{ouyang2022training,\n  title={Training language models to follow instructions with human feedback},\n  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},\n  journal={Advances in Neural Information Processing Systems},\n  volume={35},\n  pages={27730--27744},\n  year={2022}\n}\n\n@misc{hf-models,\n  title={Huggingface Models},\n  author={Hugging Face},\n  url={https://huggingface.co/models},\n}\n\n@misc{torchhub,\n  title={PyTorch Hub},\n  author={The Linux Foundation},\n  url={https://pytorch.org/hub}\n}\n\n@misc{openai-emb,\n  title={New embeddings models and API updates},\n  author={OpenAI},\n  url={https://openai.com/blog/new-embedding-models-and-api-updates}\n}\n\n@misc{tensorflow2015,\n  title={TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems},\n  url={https://www.tensorflow.org/},\n  author={Mart\\'{i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\\'{e}gas and riol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},\n  year={2015},\n}\n\n@misc{public-apis,\n  title={Public APIs},\n  author={public-apis.org},\n  url={https://github.com/public-apis}\n}\n\n@book{beauchamp2011oxford,\n  title={The Oxford handbook of animal ethics},\n  author={Beauchamp, Tom L and Frey, Raymond Gillespie},\n  year={2011},\n  page={232},\n  publisher={Oxford University Press, USA}\n}\n\n\n@inproceedings{ahn2022saycan,\n    title={Do As I Can and Not As I Say: Grounding Language in Robotic Affordances},\n    author={Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Re ...

(The output of the tool call (shortuuid identifier: JFjxDZUJCT6PssApebfk4Y) is too long! Only the first 100000 characters are shown here. The original output length is 154923 characters. The full output has been saved to the file /workspace/dumps/workspace/.overlong_tool_outputs/JFjxDZUJCT6PssApebfk4Y.json. Please check this file carefully, as it may be very long!)
```
</details>
</div>

<div className="thinking-box">
🧐`Agent`

I have carefully reviewed your LaTeX paper draft and identified several issues with citations and references. I will now correct them.

First, I will address the broken cross-references. In `sections/1_introduction.tex`, the reference to `fig:call-api-v0` is incorrect because the label is `fig:call-api`. I will correct this reference. In `sections/2_scenarios.tex`, the reference `\autoref\{tab:1\}` is pointing to a non-existent label. Based on the context, it should refer to `tab:example-tools`. I will update this reference as well. Finally, in `sections/3_advanced.tex`, there are two empty `\ref` and `\autoref` commands. I will fill them with the correct labels `sub:domain-spec`, `sub:general-codegen`, and `fig:codelm-tools` based on the surrounding text.
</div>

</Accordion>

</AccordionGroup>
