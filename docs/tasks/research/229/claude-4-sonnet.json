{
    "pass": false,
    "messages": [
        {
            "role": "user",
            "content": "Only edit the TODO part at the end of the appendix within my paper Latex project, without affecting any other content of the paper to help me do so: Add a new section at the end of the appendix to display the Simple Prompt and the Complex Prompt we actually used, in a box style exactly the same in M-STAR (https://arxiv.org/pdf/2412.17451). You can find these needed prompts in our codebase. You only need to replace the text part of the box without modifying any style, color or fontsize etc, except that you must use '\\\\' for new lines, please just set the names of the boxes as `Simple Prompt` and `Complex Prompt`. You also need to handle proper package import and consistent color definition if necessary (this is the only edit you can make besides the TODO part). Do not change the file names in my paper project."
        },
        {
            "role": "assistant",
            "content": "I'll help you add the Simple Prompt and Complex Prompt sections to your LaTeX paper. Let me start by exploring your workspace to understand the structure of your project.",
            "tool_calls": [
                {
                    "id": "toolu_0198PthuCunLks2nASV2Qjhi",
                    "type": "function",
                    "function": {
                        "name": "filesystem-list_allowed_directories",
                        "arguments": ""
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Allowed directories:\\n/workspace/dumps/workspace\",\"annotations\":null}",
            "tool_call_id": "toolu_0198PthuCunLks2nASV2Qjhi"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01H4V2ETA3ECXM9hLixK8jdi",
                    "type": "function",
                    "function": {
                        "name": "filesystem-list_directory",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"[DIR] simpleRL-reason\\n[DIR] simplerlcolm25\",\"annotations\":null}",
            "tool_call_id": "toolu_01H4V2ETA3ECXM9hLixK8jdi"
        },
        {
            "role": "assistant",
            "content": "Let me explore both directories to find your LaTeX project:",
            "tool_calls": [
                {
                    "id": "toolu_013Mafcj7tGNq7qqwCFGQwgf",
                    "type": "function",
                    "function": {
                        "name": "filesystem-directory_tree",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"[\\n  {\\n    \\\"name\\\": \\\"simpleRL-reason\\\",\\n    \\\"type\\\": \\\"directory\\\",\\n    \\\"children\\\": [\\n      {\\n        \\\"name\\\": \\\".gitignore\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\".readthedocs.yaml\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\".style.yapf\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"LICENSE\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Notice.txt\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"README.md\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"assets\\\",\\n        \\\"type\\\": \\\"directory\\\",\\n        \\\"children\\\": [\\n          {\\n            \\\"name\\\": \\\"plot_figure1_v2.3_token_length_vs_steps.png\\\",\\n            \\\"type\\\": \\\"file\\\"\\n          }\\n        ]\\n      },\\n      {\\n        \\\"name\\\": \\\"docker\\\",\\n        \\\"type\\\": \\\"directory\\\",\\n        \\\"children\\\": [\\n          {\\n            \\\"name\\\": \\\"Dockerfile.ngc.vllm\\\",\\n            \\\"type\\\": \\\"file\\\"\\n          },\\n          {\\n            \\\"name\\\": \\\"Dockerfile.vemlp.vllm.te\\\",\\n            \\\"type\\\": \\\"file\\\"\\n          }\\n        ]\\n      },\\n      {\\n        \\\"name\\\": \\\"docs\\\",\\n        \\\"type\\\": \\\"directory\\\",\\n        \\\"children\\\": [\\n          {\\n            \\\"name\\\": \\\"Makefile\\\",\\n            \\\"type\\\": \\\"file\\\"\\n          },\\n          {\\n            \\\"name\\\": \\\"README.md\\\",\\n            \\\"type\\\": \\\"file\\\"\\n          },\\n          {\\n            \\\"name\\\": \\\"_static\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"logo.png\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"advance\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"dpo_extension.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"fsdp_extension.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"megatron_extension.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"placement.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"conf.py\\\",\\n            \\\"type\\\": \\\"file\\\"\\n          },\\n          {\\n            \\\"name\\\": \\\"examples\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"config.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"gsm8k_example.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"ppo_code_architecture.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"experiment\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"ppo.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"faq\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"faq.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"index.rst\\\",\\n            \\\"type\\\": \\\"file\\\"\\n          },\\n          {\\n            \\\"name\\\": \\\"perf\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"perf_tuning.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"preparation\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"prepare_data.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"reward_function.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"requirements-docs.txt\\\",\\n            \\\"type\\\": \\\"file\\\"\\n          },\\n          {\\n            \\\"name\\\": \\\"start\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"install.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"quickstart.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"workers\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"fsdp_workers.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"megatron_workers.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"ray_trainer.rst\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          }\\n        ]\\n      },\\n      {\\n        \\\"name\\\": \\\"eval_math_nodes.sh\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"examples\\\",\\n        \\\"type\\\": \\\"directory\\\",\\n        \\\"children\\\": [\\n          {\\n            \\\"name\\\": \\\"simplelr_math_eval\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"LICENSE\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"README.md\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"data\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"aime24\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"amc23\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"aqua\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"asdiv\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"carp_en\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"demo.json\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"cmath\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"cn_middle_school\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"college_math\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"eval_rm_maj_example\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"math_cot_100.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"gaokao2023en\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"gaokao2024_I\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"gaokao2024_II\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"gaokao2024_mix\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"gaokao_math_cloze\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"gaokao_math_qa\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"gsm8k\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"train.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"math\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"train.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"math500\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"mawps\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"addsub.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"multiarith.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"singleeq.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"singleop.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"minerva_math\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"README.md\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"mmlu_stem\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"olympiadbench\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.json\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"sat_math\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"svamp\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"tabmwp\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test.jsonl\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"data_loader.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"evaluate.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"examples.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"grader.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"latex2sympy\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\".coveragerc\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\".gitignore\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"LICENSE.txt\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"PS.g4\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"README.md\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"antlr-4.11.1-complete.jar\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"asciimath_printer.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"description.txt\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"dev-requirements.in\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"dev-requirements.txt\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"gen\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"PS.interp\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"PS.tokens\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"PSLexer.interp\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"PSLexer.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"PSLexer.tokens\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"PSListener.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"PSParser.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"__init__.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"icon.png\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"latex2sympy2.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"requirements.in\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"requirements.txt\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"sandbox\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"linalg_equations.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"linalg_span.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"matrix.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"matrix_placeholders.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"sandbox.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"sandbox_equality.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"sectan.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"vector.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"scripts\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"compile.sh\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"coverage-ci.sh\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"coverage.sh\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"pre-commit\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"pre-push\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"publish.sh\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"setup-hooks.sh\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"setup.sh\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"test.sh\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"setup.cfg\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"setup.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"tests\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"__init__.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"abs_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"all_bad_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"all_good_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"atom_expr_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"binomial_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"ceil_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"complex_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"context.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"exp_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"floor_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"gcd_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"greek_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"grouping_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"lcm_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"left_right_cdot_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"linalg_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"max_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"min_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"mod_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"overline_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"pi_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"trig_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"variable_test.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"math_eval.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"math_utils.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"model_utils.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"parser.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"python_executor.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"rm_maj_eval.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"sh\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"collect_results.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"convert_and_evaluate_gpu_nodes.sh\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"eval.sh\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"trajectory.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"utils.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"visualizer.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          }\\n        ]\\n      },\\n      {\\n        \\\"name\\\": \\\"install.sh\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"launch_gradio.sh\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"patches\\\",\\n        \\\"type\\\": \\\"directory\\\",\\n        \\\"children\\\": [\\n          {\\n            \\\"name\\\": \\\"megatron_v4.patch\\\",\\n            \\\"type\\\": \\\"file\\\"\\n          }\\n        ]\\n      },\\n      {\\n        \\\"name\\\": \\\"pyproject.toml\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"requirements.txt\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"scripts\\\",\\n        \\\"type\\\": \\\"directory\\\",\\n        \\\"children\\\": [\\n          {\\n            \\\"name\\\": \\\"format.sh\\\",\\n            \\\"type\\\": \\\"file\\\"\\n          }\\n        ]\\n      },\\n      {\\n        \\\"name\\\": \\\"setup.py\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"tests\\\",\\n        \\\"type\\\": \\\"directory\\\",\\n        \\\"children\\\": [\\n          {\\n            \\\"name\\\": \\\"__init__.py\\\",\\n            \\\"type\\\": \\\"file\\\"\\n          },\\n          {\\n            \\\"name\\\": \\\"checkpoint\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"test_fsdp_ckpt.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"e2e\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"__init__.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"arithmetic_sequence\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"data\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"create_dataset.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"test.parquet\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"train.parquet\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"model\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"config.json\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"create_model_tokenizer.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"generation_config.json\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"model.safetensors\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"tokenizer_config.json\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"rl\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"README.md\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"main_trainer.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"check_results.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"envs\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"digit_completion\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"__init__.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"task.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"tokenizer.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"run_deepseek_megatron.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"run_qwen_gsm8k_function_rm.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"run_qwen_gsm8k_function_rm_grpo.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"run_qwen_gsm8k_function_rm_no_rmpad.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"run_qwen_gsm8k_model_rm.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"run_qwen_gsm8k_model_rm_liger_kernel.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"run_qwen_gsm8k_model_rm_no_rmpad.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"run_qwen_gsm8k_model_rm_seq_balance.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"run_qwen_gsm8k_model_rm_ulysses.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"run_ray_trainer.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"run_ray_trainer_rmpad.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"gpu_utility\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"test_memory_buffers.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_ops.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_torch_functional.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"model\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"test_transformer.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_transformers_ulysses.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"ray\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"check_worker_alive\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"main.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"detached_worker\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"README.md\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"client.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"run.sh\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"server.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"test_check_worker_alive.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_colocated_workers.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_data_transfer.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_driverfunc_to_worker.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_high_level_scheduling_api.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_ray_local_envs.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_rvdz.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_worker_group_basics.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_worker_group_torch.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"rollout\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"run_fsdp_vllm.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_vllm_hf_loader.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"sanity\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"check_license.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_import.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"sft\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"run_sft.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"run_sft_qwen05_peft.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"run_sft_qwen05_sp2_liger.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"run_sft_sp_loss_match.sh\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"test_sp_loss_match.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"utility\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"test_tensor_dict_utilities.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"verl\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"utils\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"dataset\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"test_rl_dataset.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"test_rm_dataset.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"test_sft_dataset.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  }\\n                ]\\n              }\\n            ]\\n          }\\n        ]\\n      },\\n      {\\n        \\\"name\\\": \\\"train_grpo_math_tune_ray.sh\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"verl\\\",\\n        \\\"type\\\": \\\"directory\\\",\\n        \\\"children\\\": [\\n          {\\n            \\\"name\\\": \\\"__init__.py\\\",\\n            \\\"type\\\": \\\"file\\\"\\n          },\\n          {\\n            \\\"name\\\": \\\"models\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"README.md\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"__init__.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"llama\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"megatron\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"__init__.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"checkpoint_utils\\\",\\n                        \\\"type\\\": \\\"directory\\\",\\n                        \\\"children\\\": [\\n                          {\\n                            \\\"name\\\": \\\"__init__.py\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                          },\\n                          {\\n                            \\\"name\\\": \\\"llama_loader.py\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                          },\\n                          {\\n                            \\\"name\\\": \\\"llama_saver.py\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                          }\\n                        ]\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"layers\\\",\\n                        \\\"type\\\": \\\"directory\\\",\\n                        \\\"children\\\": [\\n                          {\\n                            \\\"name\\\": \\\"__init__.py\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                          },\\n                          {\\n                            \\\"name\\\": \\\"parallel_attention.py\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                          },\\n                          {\\n                            \\\"name\\\": \\\"parallel_decoder.py\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                          },\\n                          {\\n                            \\\"name\\\": \\\"parallel_linear.py\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                          },\\n                          {\\n                            \\\"name\\\": \\\"parallel_mlp.py\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                          },\\n                          {\\n                            \\\"name\\\": \\\"parallel_rmsnorm.py\\\",\\n                            \\\"type\\\": \\\"file\\\"\\n                          }\\n                        ]\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"modeling_llama_megatron.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"registry.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"transformers\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"llama.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"monkey_patch.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"qwen2.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"weight_loader_registry.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"protocol.py\\\",\\n            \\\"type\\\": \\\"file\\\"\\n          },\\n          {\\n            \\\"name\\\": \\\"single_controller\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"__init__.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"base\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"decorator.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"megatron\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"__init__.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"worker.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"worker_group.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"register_center\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"__init__.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"ray.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"worker.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"worker_group.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"ray\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"base.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"megatron.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"third_party\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"__init__.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"vllm\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"vllm_v_0_3_1\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"__init__.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"arg_utils.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"config.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"llm.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"llm_engine_sp.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"model_loader.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"model_runner.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"parallel_state.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"tokenizer.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"weight_loaders.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"worker.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"vllm_v_0_4_2\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"__init__.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"arg_utils.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"config.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"dtensor_weight_loaders.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"hf_weight_loader.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"llm.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"llm_engine_sp.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"megatron_weight_loaders.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"model_loader.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"model_runner.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"parallel_state.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"spmd_gpu_executor.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"tokenizer.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"worker.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"vllm_v_0_5_4\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"__init__.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"arg_utils.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"config.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"dtensor_weight_loaders.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"hf_weight_loader.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"llm.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"llm_engine_sp.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"megatron_weight_loaders.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"model_loader.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"model_runner.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"parallel_state.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"spmd_gpu_executor.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"tokenizer.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"worker.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"vllm_v_0_6_3\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"__init__.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"arg_utils.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"config.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"dtensor_weight_loaders.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"hf_weight_loader.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"llm.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"llm_engine_sp.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"megatron_weight_loaders.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"model_loader.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"model_runner.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"parallel_state.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"spmd_gpu_executor.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"tokenizer.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"worker.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  }\\n                ]\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"trainer\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"__init__.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"config\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"evaluation.yaml\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"generation.yaml\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"ppo_megatron_trainer.yaml\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"ppo_trainer.yaml\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"sft_trainer.yaml\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"fsdp_sft_trainer.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"main_eval.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"main_generation.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"main_ppo.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"ppo\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"core_algos.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"ray_trainer.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"runtime_env.yaml\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"utils\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"__init__.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"checkpoint\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"checkpoint_manager.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"fsdp_checkpoint_manager.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"config.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"dataset\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"README.md\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"rl_dataset.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"rm_dataset.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"sft_dataset.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"debug\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"performance.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"trajectory_tracker.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"distributed.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"flops_counter.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"fs.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"fsdp_utils.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"hdfs_io.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"import_utils.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"logger\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"aggregate_logger.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"logging_utils.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"megatron\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"memory.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"optimizer.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"optimizer_config.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"pipeline_parallel.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"sequence_parallel.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"tensor_parallel.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"megatron_utils.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"memory_buffer.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"model.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"py_functional.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"ray_utils.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"rendezvous\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"ray_backend.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"reward_score\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"deepseek_r1.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"gsm8k.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"hf_math_verify copy.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"hf_math_verify.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"kk.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"math.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"qwen_math_eval_toolkit\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"examples.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"grader.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"parser.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"utils.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"simplelr_math.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"seqlen_balancing.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"tokenizer.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"torch_dtypes.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"torch_functional.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"tracking.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"ulysses.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"version\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"version\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              }\\n            ]\\n          },\\n          {\\n            \\\"name\\\": \\\"workers\\\",\\n            \\\"type\\\": \\\"directory\\\",\\n            \\\"children\\\": [\\n              {\\n                \\\"name\\\": \\\"__init__.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"actor\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"base.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"dp_actor.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"megatron_actor.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"critic\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"base.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"dp_critic.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"megatron_critic.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"fsdp_workers.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"megatron_workers.py\\\",\\n                \\\"type\\\": \\\"file\\\"\\n              },\\n              {\\n                \\\"name\\\": \\\"reward_model\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"base.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"megatron\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"__init__.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"reward_model.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"rollout\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"base.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"hf_rollout.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"naive\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"__init__.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"naive_rollout.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"tokenizer.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"vllm_rollout\\\",\\n                    \\\"type\\\": \\\"directory\\\",\\n                    \\\"children\\\": [\\n                      {\\n                        \\\"name\\\": \\\"__init__.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      },\\n                      {\\n                        \\\"name\\\": \\\"vllm_rollout.py\\\",\\n                        \\\"type\\\": \\\"file\\\"\\n                      }\\n                    ]\\n                  }\\n                ]\\n              },\\n              {\\n                \\\"name\\\": \\\"sharding_manager\\\",\\n                \\\"type\\\": \\\"directory\\\",\\n                \\\"children\\\": [\\n                  {\\n                    \\\"name\\\": \\\"__init__.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"base.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"fsdp_ulysses.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"fsdp_vllm.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  },\\n                  {\\n                    \\\"name\\\": \\\"megatron_vllm.py\\\",\\n                    \\\"type\\\": \\\"file\\\"\\n                  }\\n                ]\\n              }\\n            ]\\n          }\\n        ]\\n      }\\n    ]\\n  },\\n  {\\n    \\\"name\\\": \\\"simplerlcolm25\\\",\\n    \\\"type\\\": \\\"directory\\\",\\n    \\\"children\\\": [\\n      {\\n        \\\"name\\\": \\\"00README.json\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Appendix.tex\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"Zero_Result.tex\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"colm2025_conference.bib\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"colm2025_conference.bst\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"colm2025_conference.sty\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"fancyhdr.sty\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"introduction.tex\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"main.bbl\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"main.tex\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      },\\n      {\\n        \\\"name\\\": \\\"natbib.sty\\\",\\n        \\\"type\\\": \\\"file\\\"\\n      }\\n    ]\\n  }\\n]\",\"annotations\":null}",
            "tool_call_id": "toolu_013Mafcj7tGNq7qqwCFGQwgf"
        },
        {
            "role": "assistant",
            "content": "Perfect! I can see that the LaTeX project is in the `simplerlcolm25` directory. Let me first examine the main LaTeX file and the appendix to understand the structure:",
            "tool_calls": [
                {
                    "id": "toolu_019Hvm38znSRehhAsxMe8Q8L",
                    "type": "function",
                    "function": {
                        "name": "filesystem-read_file",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace/simplerlcolm25/main.tex\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"\\n\\\\documentclass{article} % For LaTeX2e\\n\\\\PassOptionsToPackage{table,xcdraw,dvipsnames}{xcolor}\\n\\\\usepackage[preprint]{colm2025_conference}\\n\\n\\\\usepackage{microtype}\\n\\\\usepackage{hyperref}\\n\\\\usepackage{url}\\n\\\\usepackage{booktabs}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{lineno}\\n\\\\usepackage{graphicx}\\n% \\\\usepackage{color, colortbl}\\n\\\\usepackage{subfigure}\\n\\\\usepackage{wrapfig}\\n\\\\usepackage{xspace}\\n\\\\usepackage[normalem]{ulem}\\n\\\\usepackage{enumitem}\\n\\n% \\\\usepackage[dvipsnames]{xcolor}    % colors\\n\\n\\\\definecolor{darkblue}{rgb}{0,0,0.5}\\n\\\\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}\\n\\\\definecolor{lightblue}{RGB}{221,235,247}\\n\\\\newcommand{\\\\ourmethod}{SimpleRL-Zoo\\\\xspace}\\n\\\\usepackage[most]{tcolorbox}\\n\\n\\\\title{SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild}\\n% \\\\title{SimpleRL: On Emerging Reasoning with Reinforcement Learning for Diverse Open Models -- Insights and Practices}\\n\\n% Authors must not appear in the submitted version. They should be hidden\\n% as long as the \\\\colmfinalcopy macro remains commented out below.\\n% Non-anonymous submissions will be rejected without review.\\n\\n\\\\author{Weihao Zeng\\\\thanks{Equal Contribution. Correspondence to Weihao Zeng (wzengak@connect.ust.hk), Yuzhen Huang (yhuanghj@cse.ust.hk), and Junxian He (junxianh@cse.ust.hk).}\\\\hspace{4pt}$^1$ \\\\quad Yuzhen Huang$^{*1}$ \\\\quad Qian Liu$^{*2}$ \\\\quad\\nWei Liu$^1$ \\\\quad\\nKeqing He$^3$ \\\\\\\\ \\\\textbf{Zejun Ma$^2$ \\\\quad Junxian He$^1$}\\\\\\\\\\n$^1$HKUST \\\\quad $^2$TikTok \\\\quad $^3$Meituan\\\\\\\\\\n\\\\href{https://github.com/hkust-nlp/simpleRL-reason}{https://github.com/hkust-nlp/simpleRL-reason}\\n% Pittsburgh, PA 15213, USA \\\\\\\\\\n% \\\\texttt{\\\\{hippo,brain,jen\\\\}@cs.cranberry-lemon.edu} \\\\\\\\\\n% \\\\And\\n% Ji Q. Ren \\\\& Yevgeny LeNet \\\\\\\\\\n% Department of Computational Neuroscience \\\\\\\\\\n% University of the Witwatersrand \\\\\\\\\\n% Joburg, South Africa \\\\\\\\\\n% \\\\texttt{\\\\{robot,net\\\\}@wits.ac.za} \\\\\\\\\\n% \\\\AND\\n% Coauthor \\\\\\\\\\n% Affiliation \\\\\\\\\\n% Address \\\\\\\\\\n% \\\\texttt{email}\\n}\\n\\n% The \\\\author macro works with any number of authors. There are two commands\\n% used to separate the names and addresses of multiple authors: \\\\And and \\\\AND.\\n%\\n% Using \\\\And between authors leaves it to \\\\LaTeX{} to determine where to break\\n% the lines. Using \\\\AND forces a linebreak at that point. So, if \\\\LaTeX{}\\n% puts 3 of 4 authors names on the first line, and the last on the second\\n% line, try using \\\\AND instead of \\\\And before the third author name.\\n\\n\\\\newcommand{\\\\fix}{\\\\marginpar{FIX}}\\n\\\\newcommand{\\\\new}{\\\\marginpar{NEW}}\\n\\n\\n\\\\newcommand{\\\\wz}[1]{\\\\textcolor{brown}{\\\\bf\\\\small [#1 --WZ]}}\\n\\\\newcommand{\\\\yh}[1]{\\\\textcolor{orange}{\\\\bf\\\\small [#1 --YH]}}\\n\\\\newcommand{\\\\jh}[1]{\\\\textcolor{magenta}{\\\\bf\\\\small [#1 --JH]}}\\n\\\\newcommand{\\\\jhc}[2]{\\\\bgroup\\\\textcolor{magenta}{\\\\sout{#1} #2}\\\\egroup}\\n\\\\newcommand{\\\\qian}[1]{\\\\textcolor{blue}{\\\\bf\\\\small [#1 --Qian]}}\\n\\n\\n\\\\newcommand{\\\\up}[1]{\\\\textcolor{OliveGreen}{\\\\small \\\\ $\\\\uparrow${#1}}}\\n\\\\newcommand{\\\\down}[1]{\\\\textcolor{Maroon}{\\\\small \\\\ $\\\\downarrow${#1}}}\\n\\\\usepackage{arydshln}\\n\\\\begin{document}\\n\\n\\\\ifcolmsubmission\\n\\\\linenumbers\\n\\\\fi\\n\\n\\\\maketitle\\n\\n\\\\begin{abstract}\\n% Advanced large language model has achieved leading performance\\n% Long chain-of-thought (CoT) reasoning, with high-level reasoning mechanisms such as reflection and verification, has been shown to be a powerful approach for scaling test-time computation and achieving remarkable performance in extremely complex reasoning tasks.\\n\\n% Advanced models can perform long chain-of-thought (CoT) reasoning on complex tasks to achieve enhanced performance. \\nDeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models\u2014a paradigm referred to as \\\\emph{zero RL training}. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities.\\nIn this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. \\n% can several critical questions remain unanswered: (1) Can smaller, various base models exhibit similar emergent reasoning with limited, simple data? (2) Does increased CoT length always lead to increased cognitive phenomena such as self-reflection (i.e., the``aha moment'')? (3) What are the essential designs that enable the emergence of long CoT? In this work, we take mathematical reasoning as an example and try to answer these questions. We conduct extensive zero-training experiments across various base models, spanning different families and sizes, including LLama3.1-8B, Mistral-7B/24B, DeepSeekMath-7B, and Qwen2.5-0.5B/1.5B/7B/14B/32B. To keep the recipe simple, we restrict our training data to the original GSM8K and MATH datasets. \\nLeveraging several key design strategies\u2014such as adjusting format reward and controlling query difficulty\u2014we achieve substantial improvements in both reasoning accuracy and response length across most settings.\\nHowever, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the ``aha moment\\\"). Notably, we observe the ``aha moment'' for the first time in small models not from the Qwen family.\\n% Through comprehensive analyses of the results, we uncover several intriguing findings. For example, increased CoT length does not always correspond to the ``aha moment'', and different base models exhibit distinct behaviors during training. \\n% We also compare our approach to traditional RL pipelines that follow a traditional SFT stage, demonstrating that such common practices can slow down or even inhibit the emergence of long CoT reasoning. \\nWe share the key designs that enable successful zero RL training, along with our findings and practices. \\nTo facilitate further research, we open-source the code, models, and analysis tools.\\n% \\\\qian{I think we can mention somewhere that we employ the same data and the same hyper-parameter to make it work on every model, and it would be really impressive.}\\n% \\\\footnote{A preliminary result of this paper based on Qwen2.5-Math-7B was reported in a \\\\href{https://hkust-nlp.notion.site/simplerl-reason?pvs=4}{blog} released concurrently with DeepSeek-R1~\\\\citep{guo2025deepseek}.\\n% \\\\jh{the deepseek r1 citation seems weird, I think other people cite it where the first author is DeepSeekAI?}\\n% \\\\jh{this footnote needs to be removed in the submission}}  \\n\\n% Advanced models can perform long chain-of-thought (CoT) reasoning on complex tasks by leveraging high-level mechanisms like reflection. \\n% Recently, open-source flagship long-CoT models such as DeepSeek-R1 have introduced a simple yet effective rule-based reward design, using pure reinforcement learning (RL) directly from base models without any supervised fine-tuning (SFT)\u2014a process known as ``zero training\\\". However, most recent efforts have concentrated on a limited series of models and have not thoroughly analyzed the critical underlying factors contributing to the success of such seemingly simple designs. In this paper, we investigates the mechanisms and challenges associated with ``zero training\\\" in LLM, focusing on the emergence of high-level reasoning mechanisms during RL training. We explore how reasoning behaviors evolve across different series of models ranging in size from 1.5B to 32B parameters, and analyze the critical factors contributing to successful ``zero training\\\". Using limited open-source datasets like GSM8K and MATH, we demonstrate significant performance improvements and increased response lengths, highlighting the emergence of reflection-like reasoning across different models. Through fine-grained monitoring, we uncover key challenges that affect both training stability and overall performance.\\n% %Through find-grained monitoring, we identify key challenges, including weak zero-shot capabilities, rigid formatting constraints, and misalignment between data difficulty and model capabilities. These factors impact both training stability and performance.  \\n% Furthermore, we reveal the surprising limitations of traditional SFT methods, which, while effective for short-term performance gains, hinder model exploration and constrain the development of advanced reasoning abilities. Our findings offer essential insights into RL training dynamics and suggest directions for scalable zero-training in the future.\\n\\\\end{abstract}\\n\\n\\n% 1. The first comprehensive experiment to perform zero RL training on different base models (apart from the Qwen base model) and conduct in-depth analysis, including analysis of length and performance, using comprehensive metrics (including pass@k) to analyze length changes, and whether reflection is produced (in this experimental setting, we need to align most of the settings). **TODO:** We need to quickly align key settings, including whether to remove formatting, the difficulty level of prompts used, and finalize the experiments.\\n% 2. Analysis of key tricks affecting zero experiment effectiveness, including stop ids, reward design (with or without format), prompt difficulty design, and other key hyperparameters such as batch size/KL coefficient/temperature. **TODO:** We need to select 2-3 representative models for analysis experiments\\n% 3. Comprehensive analysis of why base models may or may not produce emergence, including patterns of emergence in base models, pass@K factors. Establish connections between base models and the aha momentTODO:\\n% 4. Explain why traditional SFT experiments don't work? For example, why might using short CoT for SFT be harmful? (Expected conclusion: using short CoT for SFT suppresses emergence) TODO: Also conduct SFT on the representative models from point 2 for analysis experiments\\n\\n\\n\\\\input{introduction}\\n\\n\\\\input{Zero_Result}\\n\\n%\\\\section{Dynamics of Emergence in Zero Training}\\n\\n%  \\\\begin{wrapfigure}{r}{0.48\\\\columnwidth}\\n%     \\\\centering\\n%     \\\\includegraphics[width=\\\\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\\\\vspace{-10pt}\\n%     \\\\caption{Comparison of accuracy and response length with and without format rewards.}\\n%     \\\\label{fig:w&wo_format_reward}\\n%     \\\\vspace{-20pt}\\n% \\\\end{wrapfigure}\\n\\n\\n\\\\section{Key Factors Shaping Zero RL Training}\\nIn this section, we identify key factors that influence stability and performance during zero RL training, particularly when dealing with early-stage or weaker models.  \\nFirst, we explore how an over-reliance on format rewards restricts exploration. Next, we analyze how data difficulty impacts exploratory behavior, illustrating how exposure to varying levels of difficulty shapes the exploration dynamics. We also discuss the impact of exploration-related hyperparameters in Appendix~\\\\ref{sec:impact_explore_hyper}.\\n\\n\\n%  \\\\begin{wrapfigure}{r}{0.48\\\\columnwidth}\\n%     \\\\centering\\n%     \\\\includegraphics[width=\\\\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\\\\vspace{-10pt}\\n%     \\\\caption{Comparison of accuracy and response length with and without format rewards.}\\n%     \\\\label{fig:w&wo_format_reward}\\n%     \\\\vspace{-20pt}\\n% \\\\end{wrapfigure}\\n\\n\\n\\n%\\\\subsection{Inherent Limitations of Zero-Shot Capabilities}\\n\\n\\n% \u611f\u89c9\u53ef\u4ee5\u5305\u542b\u4e24\u4e2a\u5c0f\u90e8\u5206\uff0c\u4e00\u4e2a\u662fprompt\u7684\u5f71\u54cd\uff0c\u53e6\u4e00\u4e2a\u662fstop id\u7684\u5b9e\u9a8c\\n\\n% \u5bf9\u4e8eprompt\u7684\u5f71\u54cd\uff0c\u53ef\u4ee5\u76f4\u63a5\u57280.5b\u4e0a\u505a\\n% \u5bf9\u4e8estop id\u7684\u5f71\u54cd\uff0c\u5219\u5728mistral\u4e0a\u505a (\u53d6\u6d88)\\n% \\\\begin{figure*}[!t]\\n%     \\\\centering\\n%     % \\\\subfigure[Pass@1 on GSM8K]{\\n%     %     \\\\includegraphics[width=0.23\\\\textwidth]{fig/gsm8k_pass@1_math_only_v2.pdf}\\n%     % }\\n%     \\\\subfigure[Qwen-2.5-7B]{\\n%         \\\\includegraphics[width=0.4\\\\textwidth]{fig/plot_figure5_v1-3_w&wo_format_reward-qwen-2.5-7b.pdf}\\n%         \\\\label{fig:w&wo_format_reward-qwen-2.5-7b}\\n%     }\\n%     \\\\subfigure[Llama-3.1-8B]{\\n%         \\\\includegraphics[width=0.4\\\\textwidth]{fig/plot_figure5_v1-4_w&wo_format_reward-llama-3.1-8b.pdf}\\n%         \\\\label{fig:w&wo_format_reward-llama-3.1-8b}\\n%     }\\\\vspace{-10pt}\\n%     \\\\caption{Comparison of accuracy and response length with and without format rewards.}\\n%     \\\\label{fig:w&wo_format_reward}\\n%     \\\\vspace{-10pt}\\n% \\\\end{figure*}\\n\\n% \\\\begin{figure*}[!t]\\n%     \\\\centering\\n%     \\\\includegraphics[width=\\\\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\\\\vspace{-10pt}\\n%      \\\\caption{Comparison of accuracy and response length with and without format rewards.}\\n%    \\\\label{fig:w&wo_format_reward}\\n% \\\\end{figure*}\\n\\n\\n% \\\\begin{wrapfigure}{r}{0.48\\\\columnwidth}\\n%     \\\\centering\\n%     \\\\includegraphics[width=\\\\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\\\\vspace{-10pt}\\n%     \\\\caption{Comparison of accuracy and response length with and without format rewards.}\\n%     \\\\label{fig:w&wo_format_reward}\\n%     \\\\vspace{-10pt}\\n% \\\\end{wrapfigure}\\n\\n% \\\\begin{wrapfigure}{r}{0.48\\\\columnwidth}\\n%     \\\\centering\\n%     \\\\subfigure[Qwen-2.5-7B]{\\n%         \\\\includegraphics[width=0.21\\\\textwidth]{fig/plot_figure5_v1-3_w&wo_format_reward-qwen-2.5-7b.pdf}\\n%         \\\\label{fig:w&wo_format_reward-qwen-2.5-7b}\\n%     }\\n%     \\\\subfigure[Llama-3.1-8B]{\\n%         \\\\includegraphics[width=0.21\\\\textwidth]{fig/plot_figure5_v1-4_w&wo_format_reward-llama-3.1-8b.pdf}\\n%         \\\\label{fig:w&wo_format_reward-llama-3.1-8b}\\n%     }\\\\vspace{-10pt}\\n%     \\\\caption{Comparison of accuracy and response length with and without format rewards.\\\\yh{smaller with Figure 10}}\\n%     \\\\label{fig:w&wo_format_reward}\\n% \\\\end{wrapfigure}\\n\\n\\n%  \\\\begin{wrapfigure}{r}{0.48\\\\columnwidth}\\n%     \\\\centering\\n%     \\\\includegraphics[width=\\\\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\\\\vspace{-10pt}\\n%     \\\\caption{Comparison of accuracy and response length with and without format rewards.}\\n%     \\\\label{fig:w&wo_format_reward}\\n%     \\\\vspace{-20pt}\\n% \\\\end{wrapfigure}\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/plot_figure4_v4_behavior_lines.pdf}\\n%\\\\vspace{-10pt}\\n\\\\caption{The change in reasoning behavior over the training iterations across all models. As described in \\\\S\\\\ref{sec:eval_metrics}, we use GPT-4o to extract and track shifts in reasoning behaviors on OlympiadBench. We focus on four reasoning-related behaviors: ``Backtracking\\\", ``Verification\\\", ``Subgoal Setting\\\", and ``Enumeration\\\".\\n        }\\n        \\\\label{fig4:behavior&counts}\\n    %\\\\vspace{-15pt}\\n\\\\end{figure}\\n\\n\\n\\n\\n\\n\\n% \u57fa\u672c\u4e0a\u53ef\u4ee5\u7167\u7740\u505a\u4e0dwork\u7684\u89d2\u5ea6\u8d70\u4e86\\n% First, they often struggle to accurately interpret complex instructions, such as formatting answers within specific boxes. Second, many base models tend to generate sequences continuously, failing to stop once the question has been answered. These issues lead to a high volume of irrelevant outputs in the early stages of training, contributing to model collapse, where the models get stuck in repetitive, meaningless loops.  To address this, we systematically analyze the impact of such behavior and propose straightforward yet effective solutions to mitigate these problems. \\\\wz{This content is more about tricks, it would be better to move it to the appendix or remove it}\\n\\n\\n\\n\\n\\\\subsection{Over-Reliance on Format Rewards}\\n\\nWe find that enforcing strict formatting constraints, such as requiring the final answer to be enclosed in a latex command \\\\textit{\\\\textbackslash boxed\\\\{\\\\}}, can hinder model's freely exploration and ultimately degrades performance. This is because many base models cannot follow the format constraint well in the initial stage, and imposing a format reward will penalize many correct explorations. We compare two reward functions: one without format constraints, which rewards responses solely based on answer correctness (our default design in \\\\textsection\\\\ref{sec:reward_remove}), and another that strictly enforces formatting by penalizing responses with a reward of -1 if they fail to adhere to the required format.\\n\\n\\n%  \\\\begin{wrapfigure}{r}{0.48\\\\columnwidth}\\n%     \\\\centering\\n%     \\\\includegraphics[width=\\\\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\\\\vspace{-10pt}\\n%     \\\\caption{Comparison of accuracy and response length with and without format rewards.}\\n%     \\\\label{fig:w&wo_format_reward}\\n%     \\\\vspace{-20pt}\\n% \\\\end{wrapfigure}\\n\\n\\\\label{sec:remove_format}\\n \\\\begin{wrapfigure}{r}{0.48\\\\columnwidth}\\n    \\\\centering\\n    \\\\includegraphics[width=\\\\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\\\\vspace{-10pt}\\n    \\\\caption{Accuracy and response length with and without format rewards.}\\n    \\\\label{fig:w&wo_format_reward}\\n    \\\\vspace{-5pt}\\n\\\\end{wrapfigure}\\n\\n\\nFigure~\\\\ref{fig:w&wo_format_reward} illustrates weaker models like Llama-3.1-8B struggle under strict formatting requirements, leading to a rapid increase in response length early in training without performancec improvement. The model expends excessive effort on adhering to the format but fails to learn how to answer correctly, ultimately resulting in model collapse. \\nFigure~\\\\ref{fig:w&wo_format_reward} (Left) further reveals that even stronger models, such as Qwen-2.5-7B, which initially comply with formatting constraints, suffer in later training stages. This includes both performance degradation and a significant reduction in CoT length.  \\n%  \\\\begin{wrapfigure}{r}{0.48\\\\columnwidth}\\n%     \\\\centering\\n%     \\\\includegraphics[width=\\\\linewidth]{fig/plot_figure5_v2_combined_format_reward.pdf}\\\\vspace{-10pt}\\n%     \\\\caption{Comparison of accuracy and response length with and without format rewards.}\\n%     \\\\label{fig:w&wo_format_reward}\\n%     \\\\vspace{-20pt}\\n% \\\\end{wrapfigure}\\n% \\\\wz{Here the total length with format reward is much longer, I'm wondering if we need to explain this using other metrics}\\nThese findings highlight that: in a zero RL training setting, rather than imposing rigid formatting rules, we should prioritize maintaining response verifiability while allowing sufficient flexibility for exploration.\\n\\n% \u5f71\u54cd\u6027\u80fd\u4e0a\u9650\u548creasonng behaviur\u7684\u6d8c\u73b0\\n\\\\begin{figure*}[!t]\\n    \\\\centering\\n    % \\\\subfigure[Pass@1 on GSM8K]{\\n    %     \\\\includegraphics[width=0.23\\\\textwidth]{fig/gsm8k_pass@1_math_only_v2.pdf}\\n    % }\\n    \\\\subfigure[Mistral-7b-v0.1]{\\n        \\\\includegraphics[width=0.47\\\\textwidth]{fig/plot_figure6_v3-2_mathlv-mistral-7b-v0.1.pdf}\\n        \\\\label{fig:mathlv-mistral-7b-v0.1}\\n    }\\n    \\\\subfigure[Qwen-2.5-7B]{\\n        \\\\includegraphics[width=0.47\\\\textwidth]{fig/plot_figure6_v3_mathlv-qwen-2.5-7b.pdf}\\n        \\\\label{fig:mathlv-qwen-2.5-7b}\\n    }\\n    %\\\\vspace{-10pt}\\n    \\\\caption{Comparison of accuracy and response length across different data difficulty levels. We examine three levels of data: Easy (GSM8K and MATH lv.1), Medium (MATH lv.1\u20134), and Hard (MATH lv.3\u20135), with each category containing approximately 8,000 problems.}\\n    \\\\label{fig:mathlv}\\n    %\\\\vspace{-15pt}\\n\\\\end{figure*}\\n\\\\subsection{Data Difficulty on Exploratory Behavior}\\n\\\\label{sec:data_complextiy_behaviur}\\nBase models exhibit varying performance and CoT behaviors when trained on different RL data. Figure~\\\\ref{fig:mathlv} compare the performance of Mistral-7B and Qwen-2.5-7B across Easy, Medium, and Hard datasets. We observe a clear trend: as data difficulty increases, Mistral-7B's performance progressively deteriorates. When faced with high-difficulty data (Hard: MATH levels 3-5), the model struggles to generate responses that receive positive feedback from the reward system. This failure results in a significant increase in response length without any corresponding improvement in accuracy, signaling a breakdown in the training process\u2014often referred to as training collapse. Figure~\\\\ref{fig:mathlv} Left demonstrates that Qwen-2.5-7B exhibits a pattern entirely opposite to Mistral-7B-v0.1. Specifically, as dataset difficulty decreases, both the model\u2019s average accuracy and response length decline, with the effect being most pronounced on the simplest dataset, where even response length decreases. This finding aligns with our previous analysis of Qwen-2.5-7B in \\\\textsection\\\\ref{sec:qwen_behabiur}, reinforcing the notion that Qwen inherently possesses strong reasoning capabilities. To further improve its response length, training should incorporate more challenging datasets to encourage deeper reasoning and extended thinking time.\\nThe analysis highlights that zero RL training data must align with the base model's inherent reasoning capabilities.\\n\\n% \\\\subsection{Impact of Exploration-Related Hyperparameters}\\n% \\\\jh{this entire section can go to appendix given that the paper is already long}\\n% In this section, we examine the effects of exploration-related hyperparameters on \\\"zero-training.\\\" Drawing inspiration from ~\\\\citet{zeng2024b}, we focus on two key factors: sampling size (the number of responses per query) and sampling temperature.\\n\\n% \\\\paragraph{Sampling Size\\\\jhc{}{:}} We examine how varying sampling sizes $N \\\\in \\\\{1,4,8,16,32\\\\}$ influence the training process using the Mistral 24B model; these results are presented in Figure~\\\\ref{fig:sampling_number}. Our analysis reveals a clear trend: as $N$ increases, the model's average performance notably improves, and variability in response lengths becomes significantly more stable. For example, after 100 training steps, the scenario with $N=32$ achieves an average accuracy approximately 6 points higher than that with $N=8$. Conversely, smaller sampling sizes ($N=1$ and $N=4$) cause training instability and potential collapse, indicated by rapid growth in generated length without corresponding accuracy improvements. We hypothesize that larger sample sizes enable the model to explore a broader and more diverse training space, which stabilizes advantage estimation and sustains continuous performance improvement.\\n\\n\\n\\n\\n\\n% \\\\begin{figure}[!t]\\n%     % \u5de6\u4fa7\u56fe\u7247\\n%     \\\\begin{minipage}[t]{0.48\\\\columnwidth} % \u75592%\u95f4\u9699\\n%         \\\\centering\\n%         \\\\includegraphics[width=\\\\linewidth]{fig/plot_figure8_v3_diff_n-mistral-24b}\\\\vspace{-10pt}\\n%         \\\\caption{Comparison of accuracy and response length using different sampling numbers N = 1, 4, 8, 32. The training data is the Hard part (MATH lv.3\u20135) with the same setting in main results, as described in \\\\S~\\\\ref{sec:setup}.}\\n%         \\\\label{fig:sampling_number}\\n%     \\\\end{minipage}\\n%     \\\\hfill % \u586b\u5145\u95f4\u9699\\n%     % \u53f3\u4fa7\u56fe\u7247\\n%     \\\\begin{minipage}[t]{0.48\\\\columnwidth}\\n%         \\\\centering\\n%         \\\\includegraphics[width=\\\\linewidth]{fig/plot_temperature.pdf}\\n%         \\\\caption{Impact of training and evaluation temperatures on Qwen-2.5-0.5b's average final performance (x-axis: evaluation temp, y-axis: training temp).}\\n%         \\\\label{fig:train_sample_temperature}\\n%     \\\\end{minipage}\\n% \\\\end{figure}\\n\\n% \\\\paragraph{Sampling Temperature\\\\jhc{}{:}} We conduct research on Qwen-2.5-0.5B to analyze the impact of sampling temperature during both training and evaluation on model performance. The results, presented in Figure~\\\\ref{fig:train_sample_temperature} , indicate that training with higher temperatures generally leads to better average performance. For instance, models trained with temperatures of 1.0 and 1.2 outperform those trained with 0.8 and 0.6. Additionally, we find that the optimal evaluation temperature depends on the training temperature. Specifically, models trained at higher temperatures require higher sampling temperatures during evaluation, as using greedy sampling often results in repetitive outputs. Conversely, models trained at lower temperatures perform best when evaluated with lower sampling temperatures.\\n\\n\\n\\\\section{Revisiting Traditional SFT for RL-Driven Reasoning Emergence}\\n\\n\\\\label{sec:short_cot_influence}\\n\\n\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=0.97\\\\columnwidth]{fig/plot_figure10_v1_behaviors.pdf}\\\\vspace{-10pt}\\n\\\\caption{Reasoning behavior ratio over RL training iterations after using different SFT steps as starting points. ``Base\\\" refers to the base Mistral-Small-24B model without any SFT, while ``Step 100\\\" and ``Step 500\\\" represent 100 and 500 steps of SFT, respectively. As described in \\\\S\\\\ref{sec:setup}, we use GPT-4o to track shifts in reasoning behaviors on OlympiadBench.}\\n        %\\\\vspace{-15pt}\\n         \\\\label{fig:sft_behaviur}\\n\\\\end{figure}\\n\\n\\n\\nAs base models may not follow instruction well and pose a major challenge for zero RL training, one may wonder a simple SFT stage as a cold start may be helpful to learn to follow instructions well. In this section, we revisit the impact of traditional SFT methods (where the responses are not from long CoT models) as a cold start on RL training performance and reasoning behavior--notably, this was the most commonly used post-training pipeline with RL following an SFT stage, before DeepSeek-R1. Specifically, we use a subset of the NuminaMath~\\\\citep{li2024numinamath} dataset derived from GSM8K and MATH,~\\\\footnote{We also conduct experiments using general SFT dataset beyond math-related ones, which can be found in Appendix~\\\\ref{sec:general_sft} and implies similar conclusion.} containing approximately 15K high-quality short CoT responses. We conduct SFT using Mistral 24B and select models at 100 and 500 training steps as starting points for RL training. \\n% \\\\begin{wrapfigure}{r}{0.48\\\\columnwidth}\\n%     \\\\centering\\n%     \\\\includegraphics[width=\\\\linewidth]{fig/plot_figure9_v2_diff_n-mistral-24b.pdf}\\n%     %\\\\vspace{-10pt}\\n%     \\\\caption{Accuracy and response length averaged on the six benchmarks over RL training iterations after running different SFT steps as starting points. }\\n%     \\\\label{fig:sft_accuracy_length} \\n%     %\\\\vspace{-10pt}\\n% \\\\end{wrapfigure}\\n\\nFigure~\\\\ref{fig:sft_accuracy_length} illustrates how model accuracy and output length evolve during RL training when different initial models are used. Our results indicate that starting from SFT models initially boosts performance significantly; however, these models encounter notable limitations in their maximum achievable accuracy and response length compared to starting from the base model during RL training. Crucially, we observe that these limitations become increasingly pronounced as the number of initial SFT steps grows.\\n\\n\\n\\\\begin{wrapfigure}{r}{0.48\\\\columnwidth}\\n    \\\\centering\\n    \\\\includegraphics[width=\\\\linewidth]{fig/plot_figure9_v2_diff_n-mistral-24b.pdf}\\n    %\\\\vspace{-10pt}\\n    \\\\caption{Accuracy and response length averaged on the six benchmarks over RL training iterations after running different SFT steps as starting points. }\\n    \\\\label{fig:sft_accuracy_length} \\n    %\\\\vspace{-10pt}\\n\\\\end{wrapfigure}\\n\\n%For instance, while the base model can attain a pass@1 accuracy of approximately 49.6\\\\% during RL training, models initialized with 100 and 500 SFT steps achieve maximum accuracies of only about 47.3\\\\% and 40.3\\\\%, respectively.\\n\\n%For instance, \\\\jh{need to cite the table here}while the base model can attain a pass@1 accuracy of approximately \\\\jhc{72}{72\\\\%} on MATH-500 during RL training, models initialized with 100 and 500 SFT steps achieve maximum accuracies of only about \\\\jhc{68}{68\\\\%} and \\\\jhc{59}{59\\\\%}, respectively.\\n\\nTo further investigate how initial SFT affects the emergence of reasoning behaviors, we analyze how often specific reasoning behaviors appeared during training at different starting points, as shown in Figure~\\\\ref{fig:sft_behaviur}. Our analysis reveals that initial SFT negatively impacts the development of critical reasoning behaviors. Specifically, models with 100 SFT steps exhibit reduced upper limits in essential reasoning behaviors such as \\\"enumeration,\\\" \\\"verification,\\\" and \\\"backtracking,\\\" compared to the base model. Even more notably, models with 500 SFT steps experience significant declines in \\\"enumeration\\\" and \\\"verification\\\" behaviors in later training stages, highlighting a detrimental long-term effect of extensive sft on reasoning capabilities.\\n% \\\\begin{wrapfigure}{r}{0.48\\\\columnwidth}\\n%     \\\\centering\\n%     \\\\includegraphics[width=\\\\linewidth]{fig/plot_figure9_v2_diff_n-mistral-24b.pdf}\\\\vspace{-10pt}\\n%     \\\\caption{Accuracy and response length averaged on the six benchmarks over RL training iterations after running different SFT steps as starting points. ``Base\\\" refers to the base Mistral-Small-24B model without any SFT, while ``Step 100\\\" and ``Step 500\\\" represent 100 and 500 steps of SFT on the base model, respectively.}\\n%     \\\\label{fig:sft_accuracy_length}\\n% \\\\end{wrapfigure}\\nThis prompts a reconsideration of whether traditional SFT inherently restricts model exploration, perhaps highlighting the need for future cold-start strategies to prioritize exploration capacity\u2014whether by incorporating long CoT data~\\\\citep{guo2025deepseek,yeo2025demystifying} or designing SFT techniques~\\\\citep{li2025preserving} that strike a balance between imitation and exploration\u2014to enable sustained improvements in model reasoning performance.\\n\\n\\\\section{Conclusion}\\nOur paper demonstrates the effectiveness of zero RL training across a diverse range of base models, yielding significant improvements in accuracy and response length. We provide strong evidence that zero RL training is not merely reranking, but rather a genuine enhancement. Furthermore, we identify key factors such as reward design, data difficulty, and models' inherent abilities that shape the emergence of advanced reasoning behaviors. Our findings also indicate that starting RL training from models with traditional SFT may limit the development of advanced reasoning behaviors. Overall, our work highlights key factors for effective zero RL training and offers insights for future model improvements.\\n\\n% \\\\section*{Author Contributions}\\n% If you'd like to, you may include  a section for author contributions as is done\\n% in many journals. This is optional and at the discretion of the authors.\\n\\n% \\\\section*{Acknowledgments}\\n% Use unnumbered first level headings for the acknowledgments. All\\n% acknowledgments, including those to funding agencies, go at the end of the paper.\\n\\n% \\\\section*{Ethics Statement}\\n% Authors can add an optional ethics statement to the paper. \\n% For papers that touch on ethical issues, this section will be evaluated as part of the review process. The ethics statement should come at the end of the paper. It does not count toward the page limit, but should not be more than 1 page. \\n\\n\\n\\n\\\\bibliography{colm2025_conference}\\n\\\\bibliographystyle{colm2025_conference}\\n\\\\newpage\\n\\\\input{Appendix}\\n\\\\end{document}\",\"annotations\":null}",
            "tool_call_id": "toolu_019Hvm38znSRehhAsxMe8Q8L"
        },
        {
            "role": "assistant",
            "content": "Now let me check the Appendix.tex file to see the current structure and find the TODO part:",
            "tool_calls": [
                {
                    "id": "toolu_01GsDX6YZW1XRqi1EeNwNan5",
                    "type": "function",
                    "function": {
                        "name": "filesystem-read_file",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace/simplerlcolm25/Appendix.tex\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"\\n\\\\appendix\\n\\n\\\\section{Detailed Background: ``Zero RL Training\\\"}\\n\\\\label{sec:detailed_grpo}\\n% DeepSeek-R1-Zero~\\\\citep{guo2025deepseek} demonstrates that reasoning capabilities can be effectively developed through large-scale reinforcement learning (RL), even without supervised fine-tuning (SFT) as an initial step. We follow this ``zero RL training\\\" approach, conduct our experiments on various open base models \\nIn our study, we follow the zero RL training recipe in ~\\\\citet{guo2025deepseek} using various open base models, employing the GRPO algorithm~\\\\citep{shao2024deepseekmath}. Here, zero RL training refers to reinforcement learning directly from the base model without any prior supervised fine-tuning (SFT).\\n%Additional experiments exploring alternative algorithms are included in the Appendix xxxx. \\nGRPO optimizes computational efficiency by eliminating the need for a separate value model; instead, it directly utilizes group-normalized rewards to estimate advantages. For a query \\\\( q \\\\) and a set of responses \\\\( O = \\\\{o_1, o_2, \\\\dots, o_G\\\\} \\\\) sampled from the old policy model \\\\( \\\\pi_{\\\\text{old}} \\\\), we adopt a token-level, length-rectified GRPO objective to optimize the policy model \\\\( \\\\pi \\\\):\\\\footnote{The original GRPO objective has a length normalization term that introduces length biases. We remove the length normalization term similar to concurrent works~\\\\citep{yu2025dapoopensourcellmreinforcement,liu2025understanding} -- this length-rectified objective was the default implementation of GRPO in our adapted codebase, verl~\\\\citep{sheng2024hybridflow}.}\\n% \\\\begin{align}\\n% \\\\mathcal{J}_{GRPO}(\\\\theta) &= \\\\mathbb{E}[q \\\\sim P(Q), \\\\{ o_i \\\\}_{i=1}^G \\\\sim \\\\pi_{\\\\theta_{\\\\text{old}}}(O|q)] \\\\\\\\\\n% &= \\\\frac{1}{G} \\\\sum_{i=1}^{G} \\\\frac{1}{|o_i|} \\\\sum_{t=1}^{|o_i|} \\\\left\\\\{ \\\\min \\\\left[ \\\\frac{\\\\pi_{\\\\theta}(o_{i,t} | q, o_{i,<t})}{\\\\pi_{\\\\theta_{\\\\text{old}}}(o_{i,t} | q, o_{i,<t})} \\\\hat{A}_{i}, 1 - \\\\epsilon, 1 + \\\\epsilon \\\\right] \\\\right\\\\} - \\\\beta \\\\mathbb{D}_{KL} [\\\\pi_{\\\\theta} || \\\\pi_{\\\\text{ref}}]\\n% \\\\end{align}\\n\\n\\n% \\\\begin{equation}\\n%     \\\\begin{split}\\n%         \\\\mathcal{J}_{\\\\text{GRPO}}(\\\\theta) \\n%         = & \\\\underset{\\\\text{Clipped policy update}}{\\\\underbrace{\\\\frac{1}{\\\\sum_{i=1}^{G}|o_{i}|} \\\\sum_{i=1}^G\\\\sum_{t=1}^{|o_i|} \\\\min\\\\left[ r_{i,t}(\\\\theta) \\\\hat{A}_i,\\\\, \\\\operatorname{clip}\\\\left(r_{i,t}(\\\\theta); 1-\\\\epsilon, 1+\\\\epsilon\\\\right) \\\\hat{A}_i \\\\right]}} \\\\\\\\\\n%         & - \\\\underset{\\\\text{KL penalty}}{\\\\underbrace{\\\\beta \\\\, \\\\mathbb{D}_{\\\\text{KL}}[\\\\pi_\\\\theta \\\\parallel \\\\pi_{\\\\text{ref}}]}}\\n%     \\\\end{split}\\n% \\\\end{equation}\\n\\n\\\\begin{equation}\\n    \\\\begin{aligned}\\n        \\\\mathcal{J}_{\\\\text{GRPO}}(\\\\theta) &= \\\\underbrace{\\\\frac{1}{\\\\sum_{i=1}^{G}|o_{i}|} \\\\sum_{i=1}^G\\\\sum_{t=1}^{|o_i|} \\\\min\\\\left[ r_{i,t}(\\\\theta) \\\\hat{A}_i,\\\\, \\\\operatorname{clip}\\\\left(r_{i,t}(\\\\theta); 1-\\\\epsilon, 1+\\\\epsilon\\\\right) \\\\hat{A}_i \\\\right]}_{\\\\text{Clipped policy update}} \\n        - \\\\underbrace{\\\\beta \\\\, \\\\mathbb{D}_{\\\\text{KL}}[\\\\pi_\\\\theta \\\\parallel \\\\pi_{\\\\text{ref}}]}_{\\\\text{KL penalty}} \\\\\\\\\\n        &\\\\text{where } r_{i,t}(\\\\theta)=\\\\frac{\\\\pi_\\\\theta(o_{i,t} \\\\mid q, o_{i,<t})}{\\\\pi_{\\\\theta_{\\\\text{old}}}(o_{i,t} \\\\mid q, o_{i,<t})}\\n    \\\\end{aligned}\\n\\\\end{equation}\\n\\n\\n% \\\\begin{equation}\\n%         \\\\mathcal{J}_{\\\\text{GRPO}}(\\\\theta) = \\\\underset{\\\\text{Clipped policy update}}{\\\\underbrace{\\\\frac{1}{\\\\sum_{i=1}^{G}|o_{i}|} \\\\sum_{i=1}^G\\\\sum_{t=1}^{|o_i|} \\\\min\\\\left[ \\\\frac{\\\\pi_\\\\theta(o_{i,t} \\\\mid q, o_{i,<t})}{\\\\pi_{\\\\theta_{\\\\text{old}}}(o_{i,t} \\\\mid q, o_{i,<t})} \\\\hat{A}_i,\\\\, \\\\operatorname{clip}(\\\\frac{\\\\pi_\\\\theta(o_{i,t} \\\\mid q, o_{i,<t})}{\\\\pi_{\\\\theta_{\\\\text{old}}}(o_{i,t} \\\\mid q, o_{i,<t})}; 1-\\\\epsilon, 1+\\\\epsilon) \\\\hat{A}_i \\\\right]}}  - \\\\underset{\\\\text{KL penalty}}{\\\\underbrace{\\\\beta \\\\, \\\\mathbb{D}_{\\\\text{KL}}[\\\\pi_\\\\theta \\\\parallel \\\\pi_{\\\\text{ref}}]}}\\n% \\\\end{equation}\\n\\nwhere  \\\\( \\\\pi_{\\\\text{ref}} \\\\) represents the reference model, and the term \\\\( \\\\mathbb{D}_{KL} \\\\) introduces a KL divergence constraint to limit how much the model can deviate from this reference. The advantage estimate \\\\( \\\\hat{A}_i \\\\) measures how much better the response \\\\( o_i \\\\) is compared to the average response, which is computed using a group of rewards \\\\( \\\\{r_1, r_2, \\\\dots, r_G\\\\} \\\\) for the responses in set \\\\( O \\\\):\\n\\\\begin{equation}\\n   \\\\hat{A}_i = \\\\frac{r_i - \\\\text{mean}(\\\\{r_1, r_2, \\\\dots, r_G\\\\})}{\\\\text{std}(\\\\{r_1, r_2, \\\\dots, r_G\\\\})}\\n\\\\end{equation}\\n\\n\\\\section{Detailed Experimental Setup}\\n\\\\label{appx:detailed_setup}\\n\\\\subsection{Dataset}\\n% \\\\label{sec:dataset_setting}\\nTo keep the training recipe simple, we select training data exclusively from the GSM8K~\\\\citep{cobbe2021training} and MATH~\\\\citep{hendrycks2021measuring} datasets. \\nFor the MATH dataset, following prior studies~\\\\citep{lightman2023let, wang2023math, sun2024easy}, we reserve the MATH500 subset as the test set, uniformly sample an additional 500 problems for validation, and combine the remaining 4,000 test problems with the original 7,500 training problems to form our training set. Each example in the MATH dataset is originally labeled with a difficulty level ranging from 1 to 5. In our experiments, we find that data difficulty is critical for successful zero RL (\\\\textsection\\\\ref{sec:data_complextiy_behaviur}) and it is necessary to use data that aligns with the model's capability. To investigate this phenomenon, we categorize the data into three difficulty levels: Easy (GSM8K and MATH lv.1), Medium (MATH lv.1\u20134), and Hard (MATH lv.3\u20135), with each category containing roughly 8,000 problems. \\nFor our main training runs, we use Easy for LLama-3.1-8B, Mistral- v0.1-7B, and DeepSeek-Math-7B; Medium for Qwen-2.5-0.5B; Hard for Mistral-Small-24B, Qwen-2.5-Math-7B, and Qwen-2.5-1.5B/7B/14B/32B, and we will report ablation study on data difficulty in \\\\textsection\\\\ref{sec:data_complextiy_behaviur}. \\n\\n\\n% \\\\label{sec:reward_remove}\\n\\n\\\\subsection{Reward} \\n% \\\\label{sec:reward_remove}\\nWe use a rule-based reward function that assigns rewards solely based on the correctness of the generated response: a correct final answer receives a reward of +1, while an incorrect one receives a reward of 0. Recent studies~\\\\citep{deepscaler2025,chen2025empirical} often incorporate format-based rules into reward calculation, encouraging the model to follow specific output formats. However, we find that this approach may hinder the model's exploration and ultimately harm its performance particularly for the base models which struggle with following the format in the initial stage, as detailed in \\\\textsection\\\\ref{sec:remove_format}.\\n\\n% We also discuss the impact of incorporating format-based rewards on training in \\\\textsection\\\\ref{}.\\n\\n\\\\subsection{Models} \\nWe conduct zero RL training experiments on Llama-3.1-8B~\\\\citep{dubey2024llama}, DeepSeek-Math-7B~\\\\citep{shao2024deepseekmath}, Mistral-v0.1-7B~\\\\citep{jiang2023mistral7b}, Mistral-Small-24b-Base-2501~\\\\citep{mistral2024small}, and Qwen-2.5 (0.5B, 1.5B, 7B, 14B, 32B)~\\\\citep{yang2024qwen2}.\\nAs we perform experiments for a variety of models, under extremely simple settings with small, simple datasets and only correctness reward, we refer to our obtained models as \\\\emph{SimpleRL-Zoo} to represent a simple training recipe for a zoo of open base models.\\nFor models with weaker instruction-following capabilities (Llama-3.1-8B, Mistral-v0.1-7B, and Qwen-2.5-0.5B/1.5B), we employ simpler prompts~\\\\citep{abel} requiring only step-by-step reasoning. For models with stronger instruction-following abilities, we use more complex prompts~\\\\citep{yang2024qwen2} that require the final answers to be placed in boxes. In our preliminary experiments, we observe that using complex prompts with models that have weak instruction-following capabilities often results in large amounts of irrelevant or nonsensical content being generated early in training, leading to instability.\\nThe content of simpler prompts and more complex prompts is shown in Figure~\\\\ref{fig:prompt_case} in Appendix.\\n%% todo: add figure for this prompt\\n\\n\\n% \\\\begin{figure}[!t]\\n%         \\\\centering\\n% \\\\includegraphics[width=0.95\\\\columnwidth]{fig/Prompt_Case.pdf}\\n% \\\\caption{Comparison between simple prompts and more complex prompts.  \\\\yh{This figure can be smaller, or move to appendix. }      }\\n%         \\\\label{fig:prompt_case}\\n% \\\\end{figure}\\n\\n\\\\subsection{Benchmark} \\nWe evaluate performance on standard mathematical reasoning benchmarks, including GSM8K~\\\\citep{cobbe2021training}, MATH 500~\\\\citep{hendrycks2021measuring}, Minerva Math~\\\\citep{lewkowycz2022solving}, and OlympiadBench~\\\\citep{he2024olympiadbench}, as well as on competition-level benchmarks such as AIME 2024 and AMC 2023. \\n% We also evaluate the generalization ability of zero RL training using three benchmarks: IFEVAL~\\\\citep{zhou2023instruction}, MMLU~\\\\citep{hendrycks2020measuring}, and GPQA-Diamond~\\\\citep{rein2024gpqa}. IFEVAL measures instruction-following capability, MMLU assesses the model's mastery of general knowledge, and GPQA-Diamond is a challenging benchmark that tests domain-specific expertise in chemistry, physics, and biology.\\n% \\\\jh{We also need to mention that we also test generalization, on xxx benchmarks}\\n% \\\\yh{\\n% \\\\subsection{Other Configurations:}\\n% % \\\\jh{I think we need to talk some important training/eval configs. for example, the rollout size (which is relevant to pass@k), the training max length and eval max length that we will mention again later, and mention we use verl [cite]. For some detailed configs that we want include in appendix (batch size, some GRPO hyperparams), we also need to cite that appendix here}\\n% We train our models using the verl~\\\\citep{sheng2024hybridflow} framework. We provide detailed training and evaluation details in the Appendix ~\\\\ref{sec:train_evaluate_details}.\\n% \\\\label{sec:train_evaluate_details}\\n% We typically use the same set of hyperparameters to train and evaluate all models in the SimpleRL-Zoo series in default main experiment setting.\\n\\n\\\\subsection{Training and Evaluation Details}\\n\\n\\\\label{sec:train_evaluate_details}\\nWe train our models using the verl~\\\\citep{sheng2024hybridflow} framework. And we typically use the same set of hyperparameters to train and evaluate all models in the SimpleRL-Zoo series in default main experiment setting. We use a prompt batch size of 1,024 and generate 8 rollouts per prompt, with a maximum rollout length of 8,192 tokens. Training is performed using a mini-batch size of 256. The default sampling temperature is set to 1.0, and the clip ratio is 0.2. For models ranging from 0.5B to 14B parameters, we use a KL loss coefficient of 1e-4. For models larger than 14B, the KL loss coefficient is set to 1e-3. We build our evaluation script based on ~\\\\citet{yang2024qwen2math}, using a temperature of 1.0 and a maximum generation length of 16K tokens. To ensure consistency, we adopt the same prompt template used during training. For most benchmarks, we report pass@1 results. However, for AIME 2024, which contains fewer problems, we report both pass@1 and average accuracy (avg@32), computed over 32 generated samples per problem.\\n\\n\\\\section{Detailed Evaluation Metrics}\\n\\\\label{appx:eval_detail}\\n\\\\paragraph{Reasoning Behavior Ratio:} To better understand the model's reasoning patterns throughout the training process, we adopt the cognitive behavior framework proposed by ~\\\\citet{gandhi2025cognitive} and use GPT-4o~\\\\citep{hurst2024gpt} to identify reasoning-related behaviors, including ``Backtracking\\\", ``Verification\\\", ``Subgoal Setting\\\", and ``Enumeration\\\". \\nWe report the ratio of responses that contain such cognitive behaviors.\\nWhile some recent studies suggest tracking reflection behavior using related keywords~\\\\citep{yeo2025demystifying,xie2025logic} as monitoring signals, we argue that these keywords only exhibit only a weak correlation with high-level reasoning patterns like reflection and verification. As a result, they fail to adequately capture the development of these reasoning processes.  Further details can be found in Appendix~\\\\ref{appx:bahaviour}.\\n\\n\\\\paragraph{Clip Ratio:} In the early stages of training, the base model exhibits weak instruction-following ability and often fails to stop appropriately, resulting in irrelevant or excessively long outputs. After training collapses, the model may also generate repetitive or overly extended responses. Since the model has a fixed maximum context length, such outputs may be truncated during both training and evaluation. To monitor this issue, we define the proportion of truncated outputs as the ``Clip Ratio\\\".\\n\\n\\\\paragraph{Average Stopped Length:} Generations that are truncated often result from issues such as repetitive patterns or incomplete reasoning, which typically do not contribute to effective trajectories. To account for this factor, we introduce a new metric to track the average length of responses that are stopped under normal conditions. It is a more reliable metric to consider only valid responses, thereby eliminating the interference caused by unstopped responses.\\n\\\\paragraph{Pass@k Accuracy:}\\nWe track the pass@k accuracy, which represents the percentage of questions for which at least one correct response is obtained when sampling k responses per question. Pass@k serves as an indicator of the model's exploration capabilities and is particularly relevant for RL, as it reflects the model's ability to generate responses that can achieve a positive reward. Previously, some researchers believed that RL training might merely reorder responses within the original model distribution, as evidenced by the lack of improvement in pass@k accuracy following RL training~\\\\citep{shao2024deepseekmath}.\\n\\n\\n\\n\\\\section{Detailed Result of SimpleRL}\\n\\\\label{appx:DetailedResult}\\nFollowing the setup described in Section~\\\\ref{sec:setup}, we perform ``zero training\\\" on various base models. The trained models are then evaluated on multiple benchmarks, including GSM8K, MATH 500, Minerva Math, OlympiadBench, AIME2024, and AMC2023. The average results across all these benchmarks are presented in Figures~\\\\ref{fig1:acc&len} and~\\\\ref{fig2:clip&stop}. In this section, we provide more detailed results. Figure~\\\\ref{fig:appx_acc&len} illustrates the trends in accuracy and response length, while Figure~\\\\ref{fig:appx_clip&stop} shows the trends in clip ratio and stopped length.\\n\\n\\n\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=0.8\\\\columnwidth]{fig/Prompt_Case.pdf}\\n\\\\caption{Comparison between simple prompts and more complex prompts.}\\n        \\\\label{fig:prompt_case}\\n\\\\end{figure}\\n\\n\\\\begin{figure*}[!h]\\n    \\\\centering\\n    \\\\includegraphics[width=\\\\textwidth]{fig/plot_appx_figure1_v2/Mistral-7B-v0.1.pdf} \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure1_v2/Llama-3.1-8B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure1_v2/DeepSeek-Math-7B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure1_v2/Mistral-Small-24B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure1_v2/Qwen-2.5-0.5B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure1_v2/Qwen-2.5-1.5B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure1_v2/Qwen-2.5-7B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure1_v2/Qwen-2.5-14B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure1_v2/Qwen-2.5-32B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n     \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure1_v2/Qwen-2.5-Math-7B.pdf}\\n    \\\\\\\\ \\n    \\\\vspace{-12pt}\\n    \\\\caption{A detailed evaluation of accuracy and response length throughout the training steps for various models. The x-axis represents the training steps, with the purple line showing the accuracy trend and the yellow line depicting the response length.}\\n    \\\\label{fig:appx_acc&len}\\n    \\\\vspace{-10pt}\\n\\\\end{figure*}\\n\\n\\n\\n\\n\\n\\\\begin{figure*}[!h]\\n    \\\\centering\\n    \\\\includegraphics[width=\\\\textwidth]{fig/plot_appx_figure2_v2/Mistral-7B-v0.1.pdf} \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure2_v2/Llama-3.1-8B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure2_v2/DeepSeek-Math-7B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure2_v2/Mistral-Small-24B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure2_v2/Qwen-2.5-0.5B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure2_v2/Qwen-2.5-1.5B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure2_v2/Qwen-2.5-7B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure2_v2/Qwen-2.5-14B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n    \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure2_v2/Qwen-2.5-32B.pdf}\\n    \\\\\\\\ \\\\vspace{-3pt}\\n     \\\\includegraphics[width=\\\\textwidth] {fig/plot_appx_figure2_v2/Qwen-2.5-Math-7B.pdf}\\n    \\\\\\\\ \\n    \\\\vspace{-12pt}\\n    \\\\caption{\\n    A detailed evaluation of clip ratio and stopped length throughout the training steps for various models. The x-axis represents the training steps, with the red line showing the clip ratio trend and the blue line depicting the average stopped length. }\\n    \\\\label{fig:appx_clip&stop}\\n    \\\\vspace{-10pt}\\n\\\\end{figure*}\\n% \\\\end{document}\\n\\n\\n\\n% \\\\section{Assessing Model Generalization Capability}\\n% \\\\label{sec:generalization_capability}\\n% We also evaluate the generalization ability of zero RL training using three benchmarks: IFEVAL~\\\\citep{zhou2023instruction}, MMLU~\\\\citep{hendrycks2020measuring}, and GPQA-Diamond~\\\\citep{rein2024gpqa}. IFEVAL measures instruction-following capability, MMLU assesses the model's mastery of general knowledge, and GPQA-Diamond is a challenging benchmark that tests domain-specific expertise in chemistry, physics, and biology.\\n\\n\\n\\n\\n\\n% Table~\\\\ref{table:generalization_performance} presents the changes in model performance on IFEval, MMLU, and GPQA-Diamond before and after training. Despite zero RL training being conducted on only ~8K math reasoning-related examples, the model generalizes effectively across a range of tasks. Notably, it shows significant gains in instruction-following and general knowledge on IFEval and MMLU, as well as substantial improvements on the challenging GPQA-Diamond benchmark, which spans chemistry, physics, and biology.\\n\\n\\n% \\\\begin{table*}[t]\\n% \\\\centering\\n% \\\\resizebox{0.85\\\\textwidth}{!}{\\n% \\\\begin{tabular}{lccccc}\\n% \\\\toprule\\n% \\\\multicolumn{1}{c}{\\\\textbf{Model}} & \\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}IFEVAL\\\\\\\\ strict-prompt\\\\end{tabular}} & \\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}MMLU\\\\\\\\Stem\\\\end{tabular}} & \\\\textbf{MMLU} & \\\\textbf{GPQA} & \\\\textbf{Avg.} \\\\\\\\ \\\\midrule\\n% \\\\multicolumn{6}{c}{\\\\textit{Llama, DeepSeek and Mistral Models}} \\\\\\\\\\n% Mistral-v0.1-7B & 13.5 & 26.1 & 28.0 & 23.2 & 22.7 \\\\\\\\\\n% \\\\rowcolor[rgb]{ .867, .922, .969}~~~~$\\\\hookrightarrow$~+~\\\\ourmethod & 21.8 & 28.1 & 34.6 & 30.3 & 28.7 \\\\\\\\\\n% Llama-3.1-8B & 16.1 & 27.1 & 28.7 & 22.7 & 23.6 \\\\\\\\\\n% \\\\rowcolor[rgb]{ .867, .922, .969}~~~~$\\\\hookrightarrow$~+~\\\\ourmethod & 25.1 & 40.7 & 44.5 & 20.2 & 32.6 \\\\\\\\\\n% DeepSeek-Math-7B & 11.5 & 21.6 & 22.7 & 19.2 & 18.7 \\\\\\\\\\n% \\\\rowcolor[rgb]{ .867, .922, .969}~~~~$\\\\hookrightarrow$~+~\\\\ourmethod & 16.3 & 47.4 & 45.5 & 27.3 & 34.1 \\\\\\\\\\n% Mistral-Small-24B & 17.4 & 30.9 & 31.7 & 20.2 & 25.0 \\\\\\\\\\n% \\\\rowcolor[rgb]{ .867, .922, .969}~~~~$\\\\hookrightarrow$~+~\\\\ourmethod & 23.5 & 73.9 & 78.8 & 45.0 & 55.3 \\\\\\\\ \\\\midrule\\n% \\\\multicolumn{6}{c}{\\\\textit{Qwen Series Models}} \\\\\\\\\\n% Qwen-2.5-0.5B & 9.6 & 23.2 & 24.9 & 24.8 & 20.6 \\\\\\\\\\n% \\\\rowcolor[rgb]{ .867, .922, .969}~~~~$\\\\hookrightarrow$~+~\\\\ourmethod & 14.4 & 32.1 & 34.6 & 26.3 & 26.8 \\\\\\\\\\n% Qwen-2.5-1.5B & 15.2 & 33.1 & 35.4 & 24.8 & 27.1 \\\\\\\\\\n% \\\\rowcolor[rgb]{ .867, .922, .969}~~~~$\\\\hookrightarrow$~+~\\\\ourmethod & 20.3 & 42.1 & 45.2 & 28.8 & 34.1 \\\\\\\\\\n% Qwen-2.5-7B & 21.3 & 39.8 & 38.6 & 23.7 & 30.8 \\\\\\\\\\n% \\\\rowcolor[rgb]{ .867, .922, .969}~~~~$\\\\hookrightarrow$~+~\\\\ourmethod & 25.9 & 49.6 & 47.0 & 29.8 & 38.1 \\\\\\\\\\n% Qwen-2.5-Math-7B & 14.1 & 40.6 & 38.0 & 27.8 & 30.1 \\\\\\\\\\n% \\\\rowcolor[rgb]{ .867, .922, .969}~~~~$\\\\hookrightarrow$~+~\\\\ourmethod & 17.0 & 55.6 & 56.6 & 35.4 & 41.1 \\\\\\\\\\n% Qwen-2.5-14B & 22.9 & 59.8 & 63.5 & 24.8 & 42.7 \\\\\\\\\\n% \\\\rowcolor[rgb]{ .867, .922, .969}~~~~$\\\\hookrightarrow$~+~\\\\ourmethod & 29.4 & 76.3 & 79.1 & 50.0 & 58.7 \\\\\\\\\\n% Qwen-2.5-32B & 24.6 & 60.7 & 62.7 & 38.9 & 46.7 \\\\\\\\\\n% \\\\rowcolor[rgb]{ .867, .922, .969}~~~~$\\\\hookrightarrow$~+~\\\\ourmethod & 31.2 & 79.0 & 82.5 & 49.5 & 60.6 \\\\\\\\ \\\\bottomrule\\n% \\\\end{tabular}\\n\\n% }\\n% \\\\caption{Detailed performance of various models across IFEVAL, MMLU and GPQA. The blue lines represent the models trained with our recipe.}\\n% \\\\label{table:generalization_performance}\\n% \\\\vspace{-12pt}\\n% \\\\end{table*}\\n\\n\\\\section{Quantitative Behavior Validation}\\n\\\\label{sec:human_consistency}\\nWe assess the consistency between GPT-4o labeled reasoning behaviors and human annotations by having human experts annotate 105 model outputs. Table~\\\\ref{tab:human_consistency} below presents the prediction rates and agreement rate. The prediction rate reflects how frequently each reasoning behavior is identified, while the agreement rate is the proportion of data on which the labelers (Human and GPT-4o) make the same prediction.\\n\\nOur results indicate a generally good level of agreement between GPT-4o and human annotations. However, GPT-4o tends to be more conservative when labeling certain behaviors such as Verification and Subgoal Setting. Upon closer examination, we observe that in cases with long CoT containing multiple reasoning behaviors, the model often favors labeling more obvious behaviors like Enumeration, while overlooking subtler ones.\\n\\n\\\\begin{table}[t]\\n    \\\\centering\\n\\\\resizebox{0.85\\\\textwidth}{!}{\\n\\\\begin{tabular}{llll}\\n\\\\hline\\n\\\\textbf{Behavior} & \\\\textbf{Score by GPT-4o (\\\\%)} & \\\\textbf{Score by Human (\\\\%)} & \\\\textbf{Raw Agreement (\\\\%)} \\\\\\\\ \\\\hline\\nVerification      & 78.10\\\\% (82/105)              & 85.71\\\\% (90/105)             & 90.48\\\\% (95/105)            \\\\\\\\\\nBacktracking      & 33.33\\\\% (35/105)              & 35.24\\\\% (37/105)             & 98.10\\\\% (103/105)           \\\\\\\\\\nSubgoal Setting   & 66.67\\\\% (70/105)              & 74.29\\\\% (78/105)             & 90.48\\\\% (95/105)            \\\\\\\\\\nEnumeration       & 61.90\\\\% (65/105)              & 63.81\\\\% (67/105)             & 94.29\\\\% (99/105)            \\\\\\\\ \\\\hline\\n\\\\end{tabular}}\\n\\\\caption{The consistency between GPT-4o labeled reasoning behaviors and human annotations}\\n\\\\label{tab:human_consistency}\\n\\\\end{table}\\n\\n\\n\\n\\\\section{Impact of General SFT on the Performance of Reinforcement Learning}\\\\label{sec:general_sft}\\n\\n\\\\begin{table}[t]\\n    \\\\centering\\n    \\\\small\\n    \\\\begin{tabular}{cccccccc}\\n    \\\\toprule\\n\\\\multicolumn{1}{c}{\\\\textbf{Init Model}} & \\\\textbf{GSM8K} & \\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}MATH \\\\\\\\ 500\\\\end{tabular}} & \\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Minerva \\\\\\\\ Math\\\\end{tabular}} & \\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}Olympiad\\\\\\\\ Bench\\\\end{tabular}} & \\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}AIME24 \\\\\\\\ (pass@1)\\\\end{tabular}} & \\\\textbf{AMC23} & \\\\textbf{Avg.} \\\\\\\\ \\\\midrule\\n0 Step & 92.0 & 70.6 & 36.8 & 36.6 & 16.7 & 45.0 & 49.6 \\\\\\\\\\n10 Step & 93.0 & 69.4 & 39.7 & 32.3 & 10.4 & 44.1 & 48.2 \\\\\\\\\\n20 Step & 92.6 & 65.2 & 34.2 & 30.7 & 6.7 & 38.4 & 44.6 \\\\\\\\\\n200 Step & 90.3 & 59.0 & 31.6 & 23.3 & 2.1 & 26.9 & 38.9 \\\\\\\\\\n1000 Step & 88.9 & 48.8 & 27.6 & 20.7 & 2.5 & 18.1 & 34.4 \\\\\\\\\\n2000 Step & 89.8 & 49.0 & 23.2 & 18.1 & 0.8 & 20.3 & 33.5 \\\\\\\\\\n4000 Step & 87.7 & 52.0 & 23.5 & 17.2 & 2.1 & 21.6 & 34.0 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n\\\\caption{Experimental results from multiple Mistral-Small-24B models, each fine-tuned with a different number of SFT steps on a general SFT dataset for RL. The \\\"number of steps\\\" refers to the number of SFT steps applied. The reported benchmarks reflect the performance metrics on various evaluation benchmarks, measured using the model that achieved the best average performance after 100 iterations of reinforcement learning training.}\\n\\\\label{tab:general_sft_results}\\n\\\\end{table}\\n\\nWe also investigated the general SFT setting beyond math-related datasets. In this setup, we first conducted SFT on Mistral-Small-24B using the widely adopted OpenHermes-2.5 dataset.\\\\footnote{\\\\url{https://huggingface.co/datasets/teknium/OpenHermes-2.5}} We implement with LLaMA-Factory~\\\\citep{zheng-etal-2024-llamafactory} and adopt common hyperparameters of SFT, including 512 examples per batch with a constant learning rate of 1e-5. For consistency with our other experiments, we fine-tuned the model using the Qwen chat template.\\nAfter SFT, we preserved multiple checkpoints at different training steps, and nearly 800 steps correspond to 1 epochs on the SFT dataset. We then performed reinforcement learning on these models using identical hyperparameters as in our zero-RL training experiments.\\n\\nTable~\\\\ref{tab:general_sft_results} presents our findings, with performance reported as the best results achieved during RL training up to 100 iterations. The results demonstrate an inverse relationship between SFT steps and subsequent RL performance: models with more SFT steps showed diminished performance after RL training. While the average performance after 10 SFT steps remained comparable to the base model, it still exhibited some negative effects. More significantly, models with more than 20 steps showed substantially reduced RL potential.\\nTherefore, we conclude that RL training produces the best performance gain when applied directly to the base model without any supervised fine-tuning, i.e., the zero RL training.\\n\\n\\\\section{Impact of Exploration-Related Hyperparameters}\\n\\\\label{sec:impact_explore_hyper}\\nIn this section, we examine the effects of exploration-related hyperparameters on \\\"zero-training.\\\" Drawing inspiration from ~\\\\citet{zeng2024b,liu2024diving}, we focus on two key factors: sampling size (the number of responses per query) and sampling temperature.\\n\\n\\\\paragraph{Sampling Size:} We examine how varying sampling sizes $N \\\\in \\\\{1,4,8,16,32\\\\}$ influence the training process using the Mistral 24B model; these results are presented in Figure~\\\\ref{fig:sampling_number}. Our analysis reveals a clear trend: as $N$ increases, the model's average performance notably improves, and variability in response lengths becomes significantly more stable. For example, after 100 training steps, the scenario with $N=32$ achieves an average accuracy approximately 6 points higher than that with $N=8$. Conversely, smaller sampling sizes ($N=1$ and $N=4$) cause training instability and potential collapse, indicated by rapid growth in generated length without corresponding accuracy improvements. We hypothesize that larger sample sizes enable the model to explore a broader and more diverse training space, which stabilizes advantage estimation and sustains continuous performance improvement.\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=0.6\\\\columnwidth]{fig/plot_figure8_v3_diff_n-mistral-24b}\\n\\\\caption{Comparison of accuracy and response length using different sampling numbers N = 1, 4, 8, 32. The training data is the Hard part (MATH lv.3\u20135) with the same setting in main results, as described in \\\\S~\\\\ref{sec:setup}.}\\n        \\\\label{fig:sampling_number}\\n\\\\end{figure}\\n\\n\\n% \\\\begin{figure}[!t]\\n%         \\\\centering\\n% \\\\includegraphics[width=0.7\\\\columnwidth]{fig/plot_temperature.pdf}\\n% \\\\caption{Impact of training and evaluation temperatures on Qwen-2.5-0.5b's average final performance (x-axis: evaluation temp, y-axis: training temp).}\\n%         \\\\label{fig:train_sample_temperature}\\n% \\\\end{figure}\\n\\n\\n% \\\\begin{figure}[!t]\\n%     % \u5de6\u4fa7\u56fe\u7247\\n%     \\\\begin{minipage}[t]{0.48\\\\columnwidth} % \u75592%\u95f4\u9699\\n%         \\\\centering\\n%         \\\\includegraphics[width=\\\\linewidth]{fig/plot_figure8_v3_diff_n-mistral-24b}\\\\vspace{-10pt}\\n%         \\\\caption{Comparison of accuracy and response length using different sampling numbers N = 1, 4, 8, 32. The training data is the Hard part (MATH lv.3\u20135) with the same setting in main results, as described in \\\\S~\\\\ref{sec:setup}.}\\n%         \\\\label{fig:sampling_number}\\n%     \\\\end{minipage}\\n%     \\\\hfill % \u586b\u5145\u95f4\u9699\\n%     % \u53f3\u4fa7\u56fe\u7247\\n%     \\\\begin{minipage}[t]{0.48\\\\columnwidth}\\n%         \\\\centering\\n%         \\\\includegraphics[width=\\\\linewidth]{fig/plot_temperature.pdf}\\n%         \\\\caption{Impact of training and evaluation temperatures on Qwen-2.5-0.5b's average final performance (x-axis: evaluation temp, y-axis: training temp).}\\n%         \\\\label{fig:train_sample_temperature}\\n%     \\\\end{minipage}\\n% \\\\end{figure}\\n\\n\\\\paragraph{Sampling Temperature:} We conduct research on Qwen-2.5-0.5B to analyze the impact of sampling temperature during both training and evaluation on model performance. The results, presented in Figure~\\\\ref{fig:train_sample_temperature} , indicate that training with higher temperatures generally leads to better average performance. For instance, models trained with temperatures of 1.0 and 1.2 outperform those trained with 0.8 and 0.6. Additionally, we find that the optimal evaluation temperature depends on the training temperature. Specifically, models trained at higher temperatures require higher sampling temperatures during evaluation, as using greedy sampling often results in repetitive outputs. Conversely, models trained at lower temperatures perform best when evaluated with lower sampling temperatures.\\n\\n\\\\section{SimpleRL-Zoo For Qwen2.5-Math-7B}\\nIn this section, we conduct experiments on Qwen2.5-Math-7B~\\\\citep{yang2024qwen2} using the ``hard part\\\" data, as described in \\\\S~\\\\ref{sec:setup}, which consists of only 8K examples from MATH lv3-5. We apply both the PPO and GRPO algorithms to train our base model, and the overall evaluation results across training steps are shown in Figure~\\\\ref{fig:qwen-math}. The final performance and response length for both algorithms converge to similar values, with GRPO slightly outperforming PPO. While the performance continues to improve, the response length does not exhibit a similar trend. Specifically, the stopping length for both algorithms remains relatively unchanged, and fluctuations in the average response length are primarily attributed to changes in the clip ratio. There are two main reasons for this behavior: First, the maximum context length for Qwen2.5-Math-7B is 4K, which is limited compared to other models with context lengths exceeding 8K, leading to a high clip ratio. Second, as a math-specific model, Qwen2.5-Math-7B already performs very well on MATH, the dataset we used for training, so it may not face enough challenge to further extend its response length. Therefore, we hypothesize that more challenging data might be needed to push this capable model further.\\n\\\\begin{figure*}[]\\n    \\\\centering\\n    \\\\subfigure{\\n\\\\includegraphics[width=0.45\\\\textwidth]{fig/plot_appx_figure3_v1_ppo_grpo-qwen-2.5-math.pdf}\\n    }\\n    \\\\subfigure{\\n        \\\\includegraphics[width=0.45\\\\textwidth]{fig/plot_appx_figure4_v1_ppo_grpo-clip-ratio-stop-tokens-qwen-2.5-math.pdf}\\n    } \\\\vspace{-10pt}\\n    \\\\caption{Comparison of accuracy and response length between PPO and GRPO on Qwen2.5-Math-7B. The base model is trained using 8K examples from MATH lv3-5, with the same settings described in \\\\S~\\\\ref{sec:setup}.}\\n    \\\\label{fig:qwen-math}\\n\\\\end{figure*}\\n\\n\\n\\\\section{Reasoning Behavior Analysis}\\n\\\\label{appx:bahaviour}\\nWe apply ~\\\\citet{gandhi2025cognitive}'s cognitive behavior framework to perform a detailed analysis of how model reasoning behaviors change during \\\"zero training.\\\" We first describe our analysis setup, then compare reflection keyword tracking against this framework to monitor reflective behaviors. Finally, we use case studies to illustrate how the reasoning behaviors of various models evolve during training.\\n\\n\\\\subsection{Setup}\\n\\\\label{sec:bahaviour_setup}\\nWe use GPT4-o to  identify and analyze the following key reasoning behaviors exhibited in the model's responses, with the prompt shown in Figure~\\\\ref{fig:prompt_reasoning_behaviors}:\\n\\n(1) \\\\textbf{Backtracking}: The model actively identifies errors during response generation and explicitly revises previously used methods.\\n\\n(2) \\\\textbf{Verification}: The model systematically checks intermediate results to ensure correctness.\\n\\n(3) \\\\textbf{Subgoal Setting}: The model decomposes complex problems into smaller, manageable steps.\\n\\n(4) \\\\textbf{Enumeration}: The model exhaustively considers multiple cases or possibilities to solve problems.\\n\\nNote that we replaced \\\"Backward Chaining\\\" with \\\"Enumeration,\\\" as the former was not relevant to our task.\\n\\n\\\\begin{figure*}[!t]\\n    \\\\centering\\n    \\\\begin{minipage}{0.50\\\\textwidth}\\n        \\\\centering\\n        \\\\includegraphics[width=\\\\columnwidth]{fig/plot_appx_figure5_v1_behaviors_plus_keywords_deepseek-math-7b.pdf}\\n        \\\\caption{Changes in reflection behavior identified by different methods.}\\n        \\\\label{fig:method_reasoning_behaviors}\\n    \\\\end{minipage}\\n    \\\\hfill\\n    \\\\begin{minipage}{0.40\\\\textwidth}\\n        \\\\centering\\n        \\\\includegraphics[width=\\\\columnwidth]{fig/plot_temperature.pdf}\\n        \\\\caption{Impact of training and evaluation temperatures on Qwen-2.5-0.5b's average final performance (x-axis: evaluation temp, y-axis: training temp).}\\n        \\\\label{fig:train_sample_temperature}\\n    \\\\end{minipage}\\n\\\\end{figure*}\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/Prompt_for_identifying_analyzing.pdf}\\n\\\\caption{Prompt for identifying and analyzing reasoning behaviors.\\n        }\\n        \\\\label{fig:prompt_reasoning_behaviors}\\n\\\\end{figure}\\n\\n\\\\subsection{Comparison of Different Reasoning Behavior Tracking Methods}\\n% \\\\label{appx:reason_behavior_analysis}\\n% \\\\begin{figure}[!t]\\n%     \\\\centering\\n%     \\\\includegraphics[width=0.7\\\\columnwidth]{fig/plot_appx_figure5_v1_behaviors_plus_keywords_deepseek-math-7b.pdf}\\n%     \\\\caption{Changes in reflection behavior identified by different methods.}\\n%     \\\\label{fig:method_reasoning_behaviors}\\n% \\\\end{figure}\\n\\n\\n% \\\\begin{figure*}[!t]\\n%     \\\\centering\\n%     \\\\begin{minipage}{0.50\\\\textwidth}\\n%         \\\\centering\\n%         \\\\includegraphics[width=\\\\columnwidth]{fig/plot_appx_figure5_v1_behaviors_plus_keywords_deepseek-math-7b.pdf}\\n%         \\\\caption{Changes in reflection behavior identified by different methods.}\\n%         \\\\label{fig:method_reasoning_behaviors}\\n%     \\\\end{minipage}\\n%     \\\\hfill\\n%     \\\\begin{minipage}{0.40\\\\textwidth}\\n%         \\\\centering\\n%         \\\\includegraphics[width=\\\\columnwidth]{fig/plot_temperature.pdf}\\n%         \\\\caption{Impact of training and evaluation temperatures on Qwen-2.5-0.5b's average final performance (x-axis: evaluation temp, y-axis: training temp).}\\n%         \\\\label{fig:train_sample_temperature}\\n%     \\\\end{minipage}\\n% \\\\end{figure*}\\n\\n\\nUsing DeepSeek Math's \\\"zero-training\\\" process as an example, we compare two different methods for monitoring reasoning behavior. The first method tracks the occurrence of specific keywords in the model's responses, such as \\\"recheck,\\\" \\\"rethink,\\\" \\\"try again,\\\" \\\"wait,\\\" \\\"alternatively,\\\" \\\"retry,\\\" and \\\"however.\\\" The second method employs ~\\\\citep{gandhi2025cognitive}'s cognitive framework for evaluation. Figure~\\\\ref{fig:method_reasoning_behaviors} illustrates the observed changes in reasoning behavior according to these two approaches. During the training process, we observe that the proportion of specified keywords in the DeepSeek math model's responses remains consistently low, exhibiting minimal variation. Conversely, reasoning behaviors identified by the cognitive framework demonstrate a significant upward trend.\\n\\nTo understand this intriguing discrepancy, we manually review the reasoning behaviors recorded by the cognitive framework. Our analysis reveals that many of these reasoning behaviors do not necessarily involve the predefined keywords. For instance, in Figure~\\\\ref{fig:deepseekmath_backtracking.png}, the observed reasoning behaviors include Verification and Backtracking, neither of which contains the specified keywords. This indicates that keywords alone cannot effectively distinguish or capture the nuanced differences between such behaviors. Similarly, in Figure~\\\\ref{fig:deepseekmath_base_verification}, the reasoning process involves implicit verification steps, including recalculating intermediate results such as the dot product and magnitudes before determining the cosine of the angle. Again, these subtle verification steps are not represented by the designated keywords. In Figure~\\\\ref{fig:deepseekmath_base_enumeration}, the reasoning involves considering multiple possible scenarios or outcomes. This type of exploratory reasoning is also inadequately captured by keyword-based approaches.\\nThese examples collectively illustrate that relying solely on keyword presence is insufficient for accurately identifying and differentiating complex reasoning behaviors within model responses.\\n\\n\\n\\n\\n\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/deepseek-math-backtracking.pdf}\\n\\\\caption{A comparison of DeepSeek-Math-7B\u2019s ``Backtracking\\\" behavior before and after zero RL training. Here, ``base solution\\\" represents the response of the DeepSeek-Math-7B base model,\\nwhile ``zero solution\\\" represents the response of the model after training.}\\n        \\\\label{fig:deepseekmath_backtracking.png}\\n\\\\end{figure}\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/DeepSeek-math-implict-verification.pdf}\\n\\\\caption{A comparison of DeepSeek-Math-7B\u2019s ``Verification\\\" behavior before and after zero RL training. Here, ``base solution\\\" represents the response of the DeepSeek-Math-7B base model,\\nwhile ``zero solution\\\" represents the response of the model after training. Here involves implicit verification steps, including recalculating intermediate results such as the dot product and magnitudes before determining the cosine of the angle.}\\n        \\\\label{fig:deepseekmath_base_verification}\\n\\\\end{figure}\\n\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/deepseek_math_explict_verification.pdf}\\n\\\\caption{A comparison of DeepSeek-Math-7B\u2019s ``Verification\\\" behavior before and after zero RL training. Here, ``base solution\\\" represents the response of the DeepSeek-Math-7B base model,\\nwhile ``zero solution\\\" represents the response of the model after training. This demonstrates more explicit verification, including key phrases like ``Let's check\\\".}\\n        \\\\label{fig:deepseekmath_explict_verification}\\n\\\\end{figure}\\n\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/deepseek-math-enumeration.pdf}\\n\\\\caption{A comparison of DeepSeek-Math-7B\u2019s ``Enumeration\\\" behavior before and after zero RL training. Here, ``base solution\\\" represents the response of the DeepSeek-Math-7B base model,\\nwhile ``zero solution\\\" represents the response of the model after training.}\\n        \\\\label{fig:deepseekmath_base_enumeration}\\n\\\\end{figure}\\n\\n\\n\\\\subsection{Reasoning Behavior Variations Across Different Models}\\n\\n\\\\label{sec:other_model_behaviour}\\n\\nWe present cases illustrating notable improvements in model reasoning behavior during training (Figure~\\\\ref{fig4:behavior&counts}). Specifically, these improvements are demonstrated in the following models: Mistral 24B (Figure~\\\\ref{fig7:verfication_case} and Figure~\\\\ref{fig8:enumeration_case}), Qwen 2.5-0.5B (Figure~\\\\ref{fig:qwen0.5b_base_verification}, Figure~\\\\ref{fig:qwen0.5b_base_backtracking} and Figure~\\\\ref{fig:qwen0.5b_base_enumeration}), Qwen 2.5-1.5B (Figure~\\\\ref{fig:qwen1.5b_base_verification} and Figure~\\\\ref{fig:qwen1.5b_base_enumeration}), DeepSeek-math-7B-base (Figure~\\\\ref{fig:deepseekmath_backtracking.png}, Figure~\\\\ref{fig:deepseekmath_base_verification}, Figure~\\\\ref{fig:deepseekmath_explict_verification} and Figure~\\\\ref{fig:deepseekmath_base_enumeration}), and Llama 3.1-8B (Figure~\\\\ref{fig:llama3.1-8b_base_verification} and Figure~\\\\ref{fig:llama3.1-8b_base_subgoal_setting}).\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/mistral24b_verification_appendix.pdf}\\\\vspace{-10pt}\\n\\\\caption{A comparison of Mistral-24B's \\\"verification\\\" and \\\"backtraining\\\" behavior before and after \\\"zero training.\\\" Here, \\\"base solution\\\" represents the response of the Mistral-24B base model, while \\\"zero solution\\\" represents the response of the model after training.\\n        }\\n        \\\\label{fig7:verfication_case}\\n    \\\\vspace{-10pt}\\n\\\\end{figure}\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/mistral_enumeration.pdf}\\n\\\\caption{A comparison of Mistral-Small-24B's ``Enumeration\\\" behavior before and after zero RL training. Here, ``base solution\\\" represents the response of the Mistral-Small-24B base model, while ``zero solution\\\" represents the response of the model after training.\\n        }\\n        \\\\label{fig8:enumeration_case}\\n    \\\\vspace{-10pt}\\n\\\\end{figure}\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/Qwen2.5-0.5B-Verification.pdf}\\n\\\\caption{A comparison of Qwen-2.5-0.5B\u2019s ``Verification'' behavior before and after zero RL training. Here, ``base solution'' represents the response of the Qwen-2.5-0.5B base model,\\nwhile ``zero solution'' represents the response of the model after training.}\\n        \\\\label{fig:qwen0.5b_base_verification}\\n\\\\end{figure}\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/Qwen-2.5-0.5B-backtracking.pdf}\\n\\\\caption{A comparison of Qwen-2.5-0.5B\u2019s ``Backtracking\\\" behavior before and after zero RL training. Here, ``base solution\\\" represents the response of the Qwen-2.5-0.5B base model,\\nwhile ``zero solution\\\" represents the response of the model after training.}\\n        \\\\label{fig:qwen0.5b_base_backtracking}\\n\\\\end{figure}\\n\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/Qwen-2.5-0.5B-Enumeration.pdf}\\n\\\\caption{A comparison of Qwen-2.5-0.5B\u2019s ``Enumeration\\\" behavior before and after zero RL training. Here, ``base solution\\\" represents the response of the Qwen-2.5-0.5-B base model,\\nwhile ``zero solution\\\" represents the response of the model after training.}\\n        \\\\label{fig:qwen0.5b_base_enumeration}\\n\\\\end{figure}\\n\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/Qwen-1.5B-Verification.pdf}\\n\\\\caption{A comparison of Qwen-2.5-1.5B\u2019s ``Verification\\\" behavior before and after zero RL training. Here, ``base solution\\\" represents the response of the Qwen-2.5-1.5B base model,\\nwhile ``zero solution\\\" represents the response of the model after training.}\\n        \\\\label{fig:qwen1.5b_base_verification}\\n\\\\end{figure}\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/Qwen1.5B-Enumeration.pdf}\\n\\\\caption{A comparison of Qwen-2.5-1.5B\u2019s ``Enumeration\\\" behavior before and after zero RL training. Here, ``base solution\\\" represents the response of the Qwen-2.5-1.5B base model,\\nwhile ``zero solution\\\" represents the response of the model after training.}\\n        \\\\label{fig:qwen1.5b_base_enumeration}\\n\\\\end{figure}\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/llama3.1-8b-verification.pdf}\\n\\\\caption{A comparison of Llama-3.1-8B\u2019s ``Verification\\\" behavior before and after zero RL training. Here, ``base solution\\\" represents the response of the Llama-3.1-8B base model,\\nwhile ``zero solution\\\" represents the response of the model after training.}\\n        \\\\label{fig:llama3.1-8b_base_verification}\\n\\\\end{figure}\\n\\n\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/Llama-3.1-8b-subgoal.pdf}\\n\\\\caption{A comparison of Llama-3.1-8B\u2019s ``Subgoal Setting\\\" behavior before and after zero RL training. Here, ``base solution\\\" represents the response of the Llama-3.1-8B base model,\\nwhile ``zero solution\\\" represents the response of the model after training.}\\n        \\\\label{fig:llama3.1-8b_base_subgoal_setting}\\n\\\\end{figure}\\n\\n\\\\section{Model Prompt}\\n% ==============================================\\n% TODO\\n% PLEASE ADD THE PROMPT(s) HERE\\n% DO NOT CHANGE ANY OTHER PART OF THIS FILE OR OTHER FILES\\n% ALL YOUR EDITS SHOULD BE IN THE TODO SECTION\\n% ==============================================\",\"annotations\":null}",
            "tool_call_id": "toolu_01GsDX6YZW1XRqi1EeNwNan5"
        },
        {
            "role": "assistant",
            "content": "Perfect! I found the TODO section at the very end of the Appendix.tex file. Now I need to find the prompts in the codebase. Let me first get the M-STAR paper to understand the box style they use:",
            "tool_calls": [
                {
                    "id": "toolu_017oApPCaVCkAyybuKMZX7CP",
                    "type": "function",
                    "function": {
                        "name": "arxiv-latex-get_paper_prompt",
                        "arguments": "{\"arxiv_id\": \"2412.17451\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%\\n\\n\\\\documentclass{article}\\n\\n% Recommended, but optional, packages for figures and better typesetting:\\n\\\\usepackage{tabularx}\\n\\\\usepackage{microtype}\\n\\\\usepackage{graphicx}\\n\\\\usepackage{booktabs} % for \\\\professional tables\\n\\n% hyperref makes hyperlinks in the resulting PDF.\\n% If your build breaks (sometimes temporarily if a hyperlink spans a page)\\n% please comment out the following usepackage line and replace\\n% \\\\usepackage{icml2025} with \\\\usepackage[nohyperref]{icml2025} above.\\n\\\\usepackage{hyperref}\\n\\n\\n% Attempt to make hyperref and algorithmic work together better:\\n\\\\newcommand{\\\\theHalgorithm}{\\\\arabic{algorithm}}\\n\\n% Use the following line for the initial blind version submitted for review:\\n% \\\\usepackage{icml2025}\\n\\n% If accepted, instead use the following line for the camera-ready submission:\\n\\\\usepackage[accepted]{icml2025}\\n\\n% For theorems and such\\n\\\\usepackage{hyperref}       % hyperlinks\\n\\\\usepackage{url}            % simple URL typesetting\\n\\\\usepackage{booktabs}       % professional-quality tables\\n\\\\usepackage{amsfonts}       % blackboard math symbols\\n\\\\usepackage{nicefrac}       % compact symbols for 1/2, etc.\\n\\\\usepackage{microtype}      % microtypography\\n% \\\\usepackage{xcolor}         % colors\\n\\\\usepackage{mdframed}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{graphicx}\\n\\\\usepackage{xspace}\\n\\\\usepackage{xparse}\\n\\\\usepackage{dashrule}\\n\\\\usepackage{wrapfig}\\n\\\\usepackage{subcaption}\\n\\\\usepackage{tikz}\\n\\\\usetikzlibrary{tikzmark}\\n\\\\usepackage{enumitem}\\n\\\\usepackage{multirow}\\n\\\\usepackage{dsfont}\\n\\\\usepackage{amsmath}\\n\\n\\\\usepackage{graphicx}      % \u63d2\u5165\u56fe\u7247\\n\\\\usepackage{subcaption}    % \u5904\u7406\u5b50\u56fe\\n\\\\usepackage{float}         % \u6d6e\u52a8\u4f53\\n\\\\usepackage{stfloats}      % \u589e\u5f3a\u6d6e\u52a8\\n\\\\usepackage{makecell}\\n\\\\usepackage{subcaption}\\n\\n\\\\usepackage{wrapfig}\\n\\n\\\\usepackage{amssymb}\\n\\\\usepackage{bbding}\\n\\n\\\\usepackage{tcolorbox}\\n\\\\usepackage{mathrsfs}\\n\\n\\\\usepackage{adjustbox}\\n\\\\usepackage{colortbl} % \u6dfb\u52a0\u8868\u683c\u80cc\u666f\u989c\u8272\u652f\u6301\\n% \\\\usepackage[table,xcdraw]{xcolor} % \u5141\u8bb8\u5b9a\u4e49\u66f4\u591a\u989c\u8272\\n\\\\PassOptionsToPackage{prologue,dvipsnames}{xcolor}\\n\\\\definecolor{lightblue}{rgb}{0.88, 0.92, 1} % \u5b9a\u4e49\u6de1\u84dd\u8272\\n\\n\\n\\\\usepackage{pgfmath} % \u7528\u4e8e\u6570\u5b66\u8ba1\u7b97\\n\\\\usepackage{amssymb} % \u4f7f\u7528\u7bad\u5934\u7b26\u53f7\\n\\\\usepackage{ifthen} % \u6761\u4ef6\u5224\u65ad\\n\\\\usepackage{booktabs} % \u7f8e\u5316\u8868\u683c\u7ebf\u6761\\n\\\\usepackage{adjustbox} % \u8868\u683c\u8c03\u6574\\n\\\\usepackage{array} % \u589e\u5f3a\u8868\u683c\u529f\u80fd\\n\\n% if you use cleveref..\\n\\\\usepackage[capitalize,noabbrev]{cleveref}\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n% THEOREMS\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\\\newtheorem{theorem}{Theorem}[section]\\n\\\\newtheorem{proposition}[theorem]{Proposition}\\n\\\\newtheorem{lemma}[theorem]{Lemma}\\n\\\\newtheorem{corollary}[theorem]{Corollary}\\n\\\\newtheorem{definition}[theorem]{Definition}\\n\\\\newtheorem{assumption}[theorem]{Assumption}\\n\\\\newtheorem{remark}[theorem]{Remark}\\n\\n% Todonotes is useful during development; simply uncomment the next line\\n%    and comment out the line below the next line to turn off comments\\n%\\\\usepackage[disable,textsize=tiny]{todonotes}\\n\\\\usepackage[textsize=tiny]{todonotes}\\n\\n\\\\usepackage{wrapfig}\\n\\n\\\\usepackage{amssymb}\\n\\\\usepackage{bbding}\\n\\n\\\\usepackage{tcolorbox}\\n\\\\usepackage{mathrsfs}\\n\\n\\\\usepackage{adjustbox}\\n\\\\usepackage{colortbl} % \u6dfb\u52a0\u8868\u683c\u80cc\u666f\u989c\u8272\u652f\u6301\\n% \\\\usepackage[table,xcdraw]{xcolor} % \u5141\u8bb8\u5b9a\u4e49\u66f4\u591a\u989c\u8272\\n\\\\PassOptionsToPackage{prologue,dvipsnames}{xcolor}\\n\\\\definecolor{lightblue}{rgb}{0.88, 0.92, 1} % \u5b9a\u4e49\u6de1\u84dd\u8272\\n\\n\\n\\\\usepackage{pgfmath} % \u7528\u4e8e\u6570\u5b66\u8ba1\u7b97\\n\\\\usepackage{amssymb} % \u4f7f\u7528\u7bad\u5934\u7b26\u53f7\\n\\\\usepackage{ifthen} % \u6761\u4ef6\u5224\u65ad\\n\\\\usepackage{booktabs} % \u7f8e\u5316\u8868\u683c\u7ebf\u6761\\n\\\\usepackage{adjustbox} % \u8868\u683c\u8c03\u6574\\n\\\\usepackage{array} % \u589e\u5f3a\u8868\u683c\u529f\u80fd\\n\\n% \u5b9a\u4e49\u66f4\u6df1\u7684\u7ea2\u8272\u548c\u7eff\u8272\\n\\\\definecolor{DeepGreen}{RGB}{0,100,0}\\n\\\\definecolor{DeepRed}{RGB}{139,0,0}\\n\\n% \u8bbe\u7f6e pgf \u6570\u5b57\u6253\u5370\u683c\u5f0f\uff0c\u4fdd\u7559\u4e00\u4f4d\u5c0f\u6570\\n\\\\pgfkeys{/pgf/number format/.cd, fixed, precision=1}\\n\\n% \u81ea\u5b9a\u4e49\u547d\u4ee4\uff1a\u663e\u793a\u6570\u503c + \u5dee\u503c\u7bad\u5934\uff0c\u652f\u6301\u53ef\u9009\u7684\u52a0\u7c97\\n\\\\newcommand{\\\\diffvalue}[3][normal]{%\\n  \\\\pgfmathsetmacro{\\\\diffraw}{#2 - #3}%\\n  \\\\pgfmathsetmacro{\\\\diff}{round(\\\\diffraw*10)/10} % \u56db\u820d\u4e94\u5165\u5230\u4e00\u4f4d\u5c0f\u6570\\n  % \u5224\u65ad\u662f\u5426\u9700\u8981\u52a0\u7c97\\n  \\\\ifthenelse{\\\\equal{#1}{bold}}{%\\n    \\\\def\\\\valueformat{\\\\textbf{#2}}%\\n  }{%\\n    \\\\def\\\\valueformat{#2}%\\n  }%\\n  % \u5224\u65ad\u5dee\u503c\u7684\u6b63\u8d1f\uff0c\u663e\u793a\u5bf9\u5e94\u7684\u7bad\u5934\u548c\u989c\u8272\\n  \\\\ifdim \\\\diff pt > 0pt\\n    % \u6b63\u5dee\u503c\uff0c\u663e\u793a\u6df1\u7eff\u8272\u5411\u4e0a\u7bad\u5934\\n    $\\\\valueformat_{\\\\textcolor{DeepGreen}{\\\\uparrow\\\\ \\\\pgfmathprintnumber{\\\\diff}}}$%\\n  \\\\else\\n    \\\\ifdim \\\\diff pt < 0pt\\n      % \u8d1f\u5dee\u503c\uff0c\u663e\u793a\u6df1\u7ea2\u8272\u5411\u4e0b\u7bad\u5934\uff0c\u53d6\u5dee\u503c\u7684\u7edd\u5bf9\u503c\\n      \\\\pgfmathsetmacro{\\\\absdiff}{-\\\\diff}%\\n      $\\\\valueformat_{\\\\textcolor{DeepRed}{\\\\downarrow\\\\ \\\\pgfmathprintnumber{\\\\absdiff}}}$%\\n    \\\\else\\n      % \u5dee\u503c\u4e3a\u96f6\\n      $\\\\valueformat_{0}$%\\n    \\\\fi\\n  \\\\fi\\n}\\n\\n\\n\\\\definecolor{proxGreen}{HTML}{097a54}\\n\\\\definecolor{proxDarkBlue}{HTML}{012e59}\\n\\\\definecolor{proxBlue}{HTML}{265ed4}\\n\\\\definecolor{proxLightBlue}{HTML}{012e59}\\n\\\\definecolor{proxTeal}{HTML}{00d5ff}\\n\\\\definecolor{proxYellow}{HTML}{ffbb00}\\n\\\\definecolor{proxOrange}{HTML}{e68e1a}\\n\\\\definecolor{proxPink}{HTML}{f0539b}\\n\\\\definecolor{proxYellow}{HTML}{ff9100}\\n\\\\definecolor{proxDarkYellow}{HTML}{fdac15}\\n\\\\definecolor{proxBlue}{HTML}{2E3168}\\n\\\\definecolor{proxLightBlue}{HTML}{2a88ef}\\n\\\\colorlet{lightProxPink}{proxPink!50}\\n\\\\colorlet{lightProxYellow}{proxYellow!50}\\n\\\\colorlet{lightProxLightBlue}{proxLightBlue!50}\\n\\\\colorlet{lightProxGreen}{proxGreen!50}\\n\\\\definecolor{cellHighlight}{HTML}{dbefff}\\n\\\\definecolor{removered}{HTML}{FFB3BA}\\n\\n\\\\usepackage{pgfplots}\\n\\\\pgfplotsset{compat=1.17}\\n\\n\\\\newcommand{\\\\jh}[1]{\\\\textcolor{magenta}{\\\\bf\\\\small [#1 --JH]}}\\n\\\\newcommand{\\\\jhc}[2]{\\\\textcolor{magenta}{\\\\sout{#1} #2}}\\n\\n\\\\newcommand{\\\\wl}[1]{\\\\textcolor{cyan}{\\\\bf\\\\small [#1 --WL]}}\\n\\n% \\\\newcommand{\\\\jl}[1]{{\\\\small \\\\color{purple} [JL: #1]}}\\n\\\\newcommand{\\\\jl}[1]{\\\\textcolor{purple}{\\\\bf\\\\small [#1 --JL]}}\\n\\\\newcommand{\\\\fan}[1]{{\\\\small \\\\color{blue}[#1 --FZ]}}\\n\\n\\\\definecolor{highlightedinfig1}{HTML}{b8342b}\\n\\n\\\\newcommand{\\\\mevol}{\\\\textsc{M-STaR}}\\n\\n\\n% The \\\\icmltitle you define below is probably too long as a header.\\n% Therefore, a short form for the running title is supplied here:\\n\\\\icmltitlerunning{Diving into Self-Evolving Training for Multimodal Reasoning}\\n\\n\\n\\\\newcommand{\\\\fix}{\\\\marginpar{FIX}}\\n\\\\newcommand{\\\\new}{\\\\marginpar{NEW}}\\n\\n% \\\\newcommand{\\\\wl}[1]{\\\\textcolor{cyan}{\\\\bf\\\\small [#1 --WL]}}\\n\\n\\\\begin{document}\\n\\n\\\\twocolumn[\\n\\\\icmltitle{Diving into Self-Evolving Training for Multimodal Reasoning}\\n\\n% It is OKAY to include author information, even for blind\\n% submissions: the style file will automatically remove it for you\\n% unless you've provided the [accepted] option to the icml2025\\n% package.\\n\\n% List of affiliations: The first argument should be a (short)\\n% identifier you will use later to specify author affiliations\\n% Academic affiliations should list Department, University, City, Region, Country\\n% Industry affiliations should list Company, City, Region, Country\\n\\n% You can specify symbols, otherwise they are numbered in order.\\n% Ideally, you should not use this facility. Affiliations will be numbered\\n% in order of appearance and this is the preferred way.\\n\\\\icmlsetsymbol{equal}{*}\\n\\n\\\\begin{icmlauthorlist}\\n\\\\icmlauthor{Wei Liu}{equal,hkust}\\n\\\\icmlauthor{Junlong Li}{equal,hkust,sjtu}\\n\\\\icmlauthor{Xiwen Zhang}{helixon}\\n\\\\icmlauthor{Fan Zhou}{sjtu}\\n\\\\icmlauthor{Yu Cheng}{cuhk}\\n\\\\icmlauthor{Junxian He}{hkust}\\n% \\\\icmlauthor{Firstname7 Lastname7}{comp}\\n%\\\\icmlauthor{}{sch}\\n% \\\\icmlauthor{Firstname8 Lastname8}{sch}\\n% \\\\icmlauthor{Firstname8 Lastname8}{yyy,comp}\\n%\\\\icmlauthor{}{sch}\\n%\\\\icmlauthor{}{sch}\\n\\\\end{icmlauthorlist}\\n\\n\\\\icmlaffiliation{hkust}{The Hong Kong University of Science and Technology}\\n\\\\icmlaffiliation{sjtu}{Shanghai Jiao Tong University}\\n\\\\icmlaffiliation{helixon}{Helixon Research}\\n\\\\icmlaffiliation{cuhk}{The Chinese University of Hong Kong}\\n\\n\\n\\\\icmlcorrespondingauthor{Wei Liu, Junxian He}{wliucn, junxianh@cse.ust.hk}\\n\\\\icmlcorrespondingauthor{Junlong Li}{lockonlvange@gmail.com}\\n\\n% You may provide any keywords that you\\n% find helpful for describing your paper; these are used to populate\\n% the \\\"keywords\\\" metadata in the PDF but will not be shown in the document\\n\\\\icmlkeywords{Machine Learning, ICML}\\n\\n\\\\vskip 0.3in\\n]\\n\\n% this must go after the closing bracket ] following \\\\twocolumn[ ...\\n\\n% This command actually creates the footnote in the first column\\n% listing the affiliations and the copyright notice.\\n% The command takes one argument, which is text to display at the start of the footnote.\\n% The \\\\icmlEqualContribution command is standard text for equal contribution.\\n% Remove it (just {}) if you do not need this facility.\\n\\n%\\\\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution\\n\\n\\\\printAffiliationsAndNotice{\\\\icmlEqualContribution} % otherwise use the standard text.\\n\\n% \\\\begin{figure*}[t]\\n%     \\\\begin{center}\\n%         \\\\vspace{-1.0cm}\\n%         Project Page: \\\\href{https://mstar-lmm.github.io}{\\\\texttt{%\\n%         \\\\textcolor[rgb]{0.7,0.1,0.1}{h}%\\n%         \\\\textcolor[rgb]{0.8,0.4,0.1}{t}%\\n%         \\\\textcolor[rgb]{0.7,0.7,0.0}{t}%\\n%         \\\\textcolor[rgb]{0.1,0.5,0.1}{p}%\\n%         \\\\textcolor[rgb]{0.1,0.5,0.5}{s}%\\n%         \\\\textcolor[rgb]{0.1,0.1,0.7}{:}%\\n%         \\\\textcolor[rgb]{0.5,0.1,0.5}{/}%\\n%         \\\\textcolor[rgb]{0.5,0.1,0.5}{/}%\\n%         \\\\textcolor[rgb]{0.7,0.1,0.1}{m}%\\n%         \\\\textcolor[rgb]{0.8,0.4,0.1}{s}%\\n%         \\\\textcolor[rgb]{0.7,0.7,0.0}{t}%\\n%         \\\\textcolor[rgb]{0.1,0.5,0.1}{a}%\\n%         \\\\textcolor[rgb]{0.1,0.5,0.5}{r}%\\n%         \\\\textcolor[rgb]{0.1,0.1,0.7}{-}%\\n%         \\\\textcolor[rgb]{0.5,0.1,0.5}{l}%\\n%         \\\\textcolor[rgb]{0.7,0.1,0.1}{m}%\\n%         \\\\textcolor[rgb]{0.8,0.4,0.1}{m}%\\n%         \\\\textcolor[rgb]{0.1,0.5,0.5}{.}%\\n%         \\\\textcolor[rgb]{0.1,0.1,0.7}{g}%\\n%         \\\\textcolor[rgb]{0.5,0.1,0.5}{i}%\\n%         \\\\textcolor[rgb]{0.7,0.1,0.1}{t}%\\n%         \\\\textcolor[rgb]{0.8,0.4,0.1}{h}%\\n%         \\\\textcolor[rgb]{0.7,0.7,0.0}{u}%\\n%         \\\\textcolor[rgb]{0.1,0.5,0.1}{b}%\\n%         \\\\textcolor[rgb]{0.1,0.5,0.5}{.}%\\n%         \\\\textcolor[rgb]{0.1,0.1,0.7}{i}%\\n%         \\\\textcolor[rgb]{0.5,0.1,0.5}{o}%\\n%         }}\\n%     \\\\end{center}\\n% \\\\end{figure*}\\n\\n\\\\begin{abstract}\\n% Reasoning...Self-evolving training...However, there still remains some significant questions that how to design effective self-evolving training framework and what is the \\n% Self-evolving training, where models are trained on their own outputs, has been a primary method for complex tasks like reasoning, since high-quality chain-of-thought data are hard to acquire for these tasks. However, as a more complicated and difficult task, multimodal reasoning, the effect of self-evolving training for it is hardly explored. Also, self-evolving training involves many different factors, the understanding for self-eovlving training reamins limited. Furthermore, self-evolving training is easy to meet the saturation issue, which hinders the further improvement and scalability for these methods. In this paper, we aim to explore self-evolving training in more general scenario and answer these questions that hinder the self-evolving training. \\n\\n% Self-evolving training, where models learn from their own outputs, has become a primary approach for complex tasks like reasoning, given the scarcity of high-quality chain-of-thought data. However, its effectiveness in multimodal reasoning, a more challenging and intricate task beyond text-only reasoning, remains largely unexplored. Additionally, self-evolving training involves multiple factors, yet our understanding of it remains limited. Furthermore, one critical issue for this training method is saturation, which restricts further improvement and scalability. To address these issues, we draw inspiration from reinforcement learning (RL) and investigate the intricacies of self-evolving training for multimodal reasoning through the lens of RL, pinpointing three key factors: \\\\emph{Training Method}, \\\\emph{Reward Model}, and \\\\emph{Prompt Variation}. We systematically analyze each factor and explore a set of best designs for training which sucessfully improves the ability of models for multimodal reasoning by a large margin. Furthermore, we dive deeper into the training dynamics to investigate the saturation of self-evolving training and propose a new automatic balancing mechanism to alleviate this issue. After all the investigations, we present our final framework for \\\\mevol\\\\ (\\\\textbf{M}ultimodal \\\\textbf{S}elf-evolving \\\\textbf{T}r\\\\textbf{a}ining for \\\\textbf{R}easoning), which is universally effective for models with different sizes on various benchmarks. All resources will be released to help fully understand and improve self-evolving training for multimodal reasoning.\\n\\nSelf-evolving training\u2014where models iteratively learn from their own outputs\u2014has emerged as a key approach for complex reasoning tasks, addressing the scarcity of high-quality chain-of-thought data. However, its effectiveness in multimodal reasoning, a domain more intricate than text-only reasoning, remains underexplored, and the understanding of critical factors in this training paradigm remains limited. Furthermore, a central challenge for this training method is performance saturation, which impedes further improvements and scalability. Inspired by reinforcement learning (RL), in this paper, we reframe self-evolving training for multimodal reasoning through the lens of RL, identifying three pivotal factors: \\\\emph{Training Method}, \\\\emph{Reward Model}, and \\\\emph{Prompt Variation}. Through systematic analysis, we establish relatively optimal design principles that significantly enhance multimodal reasoning capabilities. Moreover, delving deeper into training dynamics, we uncover the roots of saturation and propose a new automatic balancing mechanism to mitigate this limitation. Building on these insights, we propose \\\\mevol\\\\ (\\\\textbf{M}ultimodal \\\\textbf{S}elf-evolving \\\\textbf{T}raining for \\\\textbf{R}easoning), a framework that achieves consistent performance gains across models of varying sizes and diverse benchmarks. All resources are made publicly available at \\\\href{https://mstar-lmm.github.io}{\\\\texttt{%\\n\\\\textcolor[rgb]{0.7,0.1,0.1}{h}%\\n\\\\textcolor[rgb]{0.8,0.4,0.1}{t}%\\n\\\\textcolor[rgb]{0.7,0.7,0.0}{t}%\\n\\\\textcolor[rgb]{0.1,0.5,0.1}{p}%\\n\\\\textcolor[rgb]{0.1,0.5,0.5}{s}%\\n\\\\textcolor[rgb]{0.1,0.1,0.7}{:}%\\n\\\\textcolor[rgb]{0.5,0.1,0.5}{/}%\\n\\\\textcolor[rgb]{0.5,0.1,0.5}{/}%\\n\\\\textcolor[rgb]{0.7,0.1,0.1}{m}%\\n\\\\textcolor[rgb]{0.8,0.4,0.1}{s}%\\n\\\\textcolor[rgb]{0.7,0.7,0.0}{t}%\\n\\\\textcolor[rgb]{0.1,0.5,0.1}{a}%\\n\\\\textcolor[rgb]{0.1,0.5,0.5}{r}%\\n\\\\textcolor[rgb]{0.1,0.1,0.7}{-}%\\n\\\\textcolor[rgb]{0.5,0.1,0.5}{l}%\\n\\\\textcolor[rgb]{0.7,0.1,0.1}{m}%\\n\\\\textcolor[rgb]{0.8,0.4,0.1}{m}%\\n\\\\textcolor[rgb]{0.1,0.5,0.5}{.}%\\n\\\\textcolor[rgb]{0.1,0.1,0.7}{g}%\\n\\\\textcolor[rgb]{0.5,0.1,0.5}{i}%\\n\\\\textcolor[rgb]{0.7,0.1,0.1}{t}%\\n\\\\textcolor[rgb]{0.8,0.4,0.1}{h}%\\n\\\\textcolor[rgb]{0.7,0.7,0.0}{u}%\\n\\\\textcolor[rgb]{0.1,0.5,0.1}{b}%\\n\\\\textcolor[rgb]{0.1,0.5,0.5}{.}%\\n\\\\textcolor[rgb]{0.1,0.1,0.7}{i}%\\n\\\\textcolor[rgb]{0.5,0.1,0.5}{o}%\\n}.}\\n\\n% Self-evolving training, where models iteratively learn from their own outputs, has emerged as a key approach for complex reasoning tasks, particularly given the scarcity of high-quality chain-of-thought data. However, its effectiveness in multimodal reasoning\u2014which extends beyond text-only reasoning\u2014remains largely unexplored. Moreover, self-evolving training involves multiple interdependent factors, yet our understanding of its underlying mechanisms is still limited. A critical challenge in this paradigm is saturation, which hinders further improvement and scalability.\\n% To address these issues, we draw inspiration from reinforcement learning (RL) and systematically investigate the intricacies of self-evolving training for multimodal reasoning through three key factors: Training Method, Reward Model, and Prompt Variation. We analyze each factor in depth and propose an optimized training strategy that significantly enhances multimodal reasoning capabilities. Additionally, we delve into the training dynamics to examine the saturation issue and introduce a novel automatic balancing mechanism to mitigate it.\\n% Based on these insights, we present \\\\mevol\\\\ (\\\\textbf{M}ultimodal \\\\textbf{S}elf-evolving \\\\textbf{T}r\\\\textbf{a}ining for \\\\textbf{R}easoning)\u2014a universally effective framework that consistently improves performance across models of varying sizes and multiple benchmarks. All resources will be released to facilitate further research and advancement in self-evolving training for multimodal reasoning.\\n\\n\\n\\n% \\\\footnote{We believe this study fills a significant gap in the understanding of self-evolving training for multimodal reasoning and offers a robust framework for future research. Our policy and reward models, as well as the collected data, will be released to facilitate further investigation in multimodal reasoning.} \\n\\n% for self-evolving training in multimodal reasoning, encapsulating these design choices into a framework we call\\n% \\\\mevol\\\\ (\\\\textbf{M}ultimodal \\\\textbf{S}elf-evolving \\\\textbf{T}r\\\\textbf{a}ining for \\\\textbf{R}easoning), \\n% which is universally effective for models with different sizes on various benchmarks,\\n% e.g., surpassing the pre-evolved model significantly on 5 multimodal reasoning benchmarks without using additional human annotations, as demonstrated on MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B).\\n% We believe this study fills a significant gap in the understanding of self-evolving training for multimodal reasoning and offers a robust framework for future research. Our policy and reward models, as well as the collected data, will be released to facilitate further investigation in multimodal reasoning.\\n\\n% In this paper, we delve into the intricacies of self-evolving training for multimodal reasoning via the lens of reinforcement learning \\n\\n% we systematically investigate self-evolving training in a broader context and address key challenges that hinder its progress.\\n\\n% While multimodal reasoning has not been explored... The understanding, intricacies...Furthermore, saturation... has been emerged as a primary method... To bridge this gap, fully explore self-evolving training method, we ...\\n% Reasoning ability is essential for Large Multimodal Models (LMMs). \\n\\n% In the absence of multimodal chain-of-thought annotated data, self-evolving training, where the model learns from its own outputs, has emerged as an effective and scalable approach for enhancing reasoning abilities. \\n% Despite its growing usage, a comprehensive understanding of self-evolving training, particularly in the context of multimodal reasoning, remains limited. In this paper, we delve into the intricacies of self-evolving training for multimodal reasoning, pinpointing three key factors: \\\\emph{Training Method}, \\\\emph{Reward Model}, and \\\\emph{Prompt Variation}. We systematically examine each factor and explore how various configurations affect the training's effectiveness. Our analysis leads to a set of best practices for each factor, aimed at optimizing multimodal reasoning.\\n% Furthermore, we explore the \\\\emph{Self-Evolution Dynamics} during training and the impact of automatic balancing mechanisms in boosting performance. After all the investigations, we present a final recipe for self-evolving training in multimodal reasoning, encapsulating these design choices into a framework we call\\n% \\\\mevol\\\\ (\\\\textbf{M}ultimodal \\\\textbf{S}elf-evolving \\\\textbf{T}r\\\\textbf{a}ining for \\\\textbf{R}easoning), \\n% which is universally effective for models with different sizes on various benchmarks,\\n% e.g., surpassing the pre-evolved model significantly on 5 multimodal reasoning benchmarks without using additional human annotations, as demonstrated on MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B).\\n% We believe this study fills a significant gap in the understanding of self-evolving training for multimodal reasoning and offers a robust framework for future research. Our policy and reward models, as well as the collected data, will be released to facilitate further investigation in multimodal reasoning.\\n\\\\end{abstract}\\n\\n\\\\section{Introduction}\\n\\n% \\\\wl{New intro} \\n% With the rapid advancement of Large Language Models (LLMs), their reasoning capabilities have seen remarkable progress \\\\citep{shao2024deepseekmath, xin2024deepseekproverv15harnessingproofassistant, yang2024qwen25mathtechnicalreportmathematical}. However, the scarcity of high-quality, human-annotated chain-of-thought data poses a critical challenge for advancing complex reasoning, particularly in multimodal scenarios where textual and visual reasoning intersect. \\nMultimodal reasoning is a fundamental skill in many real-world applications, such as intelligent agents \\\\citep{liu2024visualagentbenchlargemultimodalmodels}, robotics \\\\citep{li2023manipllmembodiedmultimodallarge, liu2024selfcorrectedmultimodallargelanguage}, and autonomous driving \\\\citep{yang2023lidarllmexploringpotentiallarge}. It requires Large Multimodal Models (LMMs) to understand various modalities beyond text. For example, visual mathematical reasoning~\\\\citep{lu2023mathvista} challenges models to analyze complex figures, diagrams, and charts, leveraging the provided\\ninformation to perform reasoning tasks.\\n\\nDespite the critical role of mutlimodal reasoning, the availability of human-annotated thought processes in multimodal scenarios remains limited, hindering the learning of multimodal reasoning. \\n% To further improve the reasoning abilities for models, \\nConsequently, self-evolving training, which utilizes model's own generation ability to iteratively tune and improve itself without external annotated data, has emerged as an appealing candidate to facilitate reasoning abilities.\\nWhile research on self-evolving training has primarily focused on the text-only settings \\\\citep{hosseini2024vstar,sun2024easy2hard,shao2024deepseekmath}, its application in the multimodal domain, especially for reasoning tasks, has been limited with only a few sporadic examples \\\\citep{fang2024vila2,dubey2024llama3, deng2024enhancing}, and a unified framework has yet to be established.\\n% Self-evolving training, a paradigm where models iteratively learn from their own outputs, has emerged as a promising solution for text-only reasoning tasks \\\\citep{hosseini2024vstar,sun2024easy2hard}. Yet, its effectiveness for multimodal reasoning remains underexplored, with only sporadic efforts \\\\citep{fang2024vila2,dubey2024llama3, deng2024enhancing}, and understanding of critical components within this self-evolving training paradigm remains limited. Furthermore, such paradigms face a key challenge: performance saturation, where models stagnate despite continued training, hindering scalability and limiting further advancements.\\n\\n\\n% Self-evolving training\u2014where models iteratively learn from their own outputs\u2014has emerged as a key approach for complex reasoning tasks, addressing the scarcity of high-quality chain-of-thought data. However, its effectiveness in multimodal reasoning, a domain more intricate than text-only reasoning, remains underexplored, and the understanding of critical factors in this training paradigm remain limited. Furthermore, a central challenge for this training method is performance saturation, which impedes further improvements and scalability. Inspired by reinforcement learning (RL), in this paper, we reframe self-evolving training for multimodal reasoning through an RL lens, identifying three pivotal factors: \\\\emph{Training Method}, \\\\emph{Reward Model}, and \\\\emph{Prompt Variation}. Through systematic analysis, we establish an optimal design principles that significantly enhance multimodal reasoning capabilities. Moreover, delving deeper into training dynamics, we uncover the roots of saturation and propose a new automatic balancing mechanism to mitigate this limitation. Building on these insights, we propose \\\\mevol\\\\ (\\\\textbf{M}ultimodal \\\\textbf{S}elf-evolving \\\\textbf{T}raining for \\\\textbf{R}easoning), a framework that achieves consistent performance gains across models of varying sizes and diverse benchmarks. All resources will be made publicly available.With the rapid advancement of Large Language Models, their reasoning abilities have improved significantly \\\\citep{shao2024deepseekmath, xin2024deepseekproverv15harnessingproofassistant, yang2024qwen25mathtechnicalreportmathematical}. This progress has been accompanied by a growing demand for more realistic and general reasoning capabilities.\\n% % , including the ability to handle complex and general reasoning tasks. \\n% Multimodal reasoning, considered a fundamental skill in many real-world applications, such as intelligent agents \\\\citep{liu2024visualagentbenchlargemultimodalmodels}, robotics \\\\citep{li2023manipllmembodiedmultimodallarge, liu2024selfcorrectedmultimodallargelanguage}, and autonomous driving \\\\citep{yang2023lidarllmexploringpotentiallarge}, exemplifies this trend. \\n% % As a more practical application, \\n% Multimodal reasoning requires Large Multimodal Models (LMMs) to understand various modalities beyond text. For example, visual mathematical reasoning \\\\citep{lu2023mathvista} challenges models to analyze complex figures, diagrams, and charts, leveraging the provided information to perform reasoning tasks.\\n\\n% Despite these advances, the availability of human-annotated thought processes in multimodal scenarios remains limited, challenging the learning of multimodal reasoning. \\n% % To further improve the reasoning abilities for models, \\n% Consequently, self-evolving training, which utilizes model's own generation ability to iteratively tune and improve itself without external annotated data, has emerged as an appealing candidate to facilitate reasoning abilities.\\n% % a widely-adopted way for its effectiveness and scalability \\\\citep{huang2022llmcanselfimprove, zelikman2022star, gulcehre2023rest,singh2023restem}. \\n% While research on self-evolving training has primarily focused on the text-only settings \\\\citep{hosseini2024vstar,sun2024easy2hard,shao2024deepseekmath}, its application in the multimodal domain, especially for reasoning tasks, has been limited with only a few sporadic examples \\\\citep{fang2024vila2,dubey2024llama3, deng2024enhancing}, and a unified\\n% % \\\\fan{maybe replace it with `comprehensive', means we dive deep and perform thorough study of this direction}\\n% framework has yet to be established.\\n% % \\\\jh{Fang et al. does not apply for reasoning right? it is mainly about normal QA and caption, not sure about llama3 though.}\\n% % \\\\jh{I don't understand why we call it ``unified''}\\n\\n% % In this work, we choose to investigate Online Reject Tuning \\\\citep{yuan2023rft}, which is a more scalable and stable implementation of self-evolving methods compared with RL algorithms like PPO \\\\citep{schulman2017proximal}. \\n% % \\\\fan{some starting word: e.g.,  `Motivated by the need to fill this gap', we xxxxx and xxxx. we then identify xxx componenets: xxx} \\n% Inspired by the fact that self-evolving training can actually be formulated as a general reinforcement learning (RL) framework (\\\\S~\\\\ref{sec:overview}), we first identify the three key components of self-evolving training in multimodal reasoning via the lens of RL, namely the \\\\textbf{training method}, the use of \\\\textbf{reward model} and the \\\\textbf{prompt variation}, to build a clear and unified design space for searching the optimal strategies. Through massive controlled experiments, \\n% % \\\\jh{plz add the section references for each of the points}\\n% we (1) propose a continuous self-evolving training scheme that inherits optimizers states from the last iteration to reduce the gap towards full online learning and outperforms other iterative baselines (\\\\S \\\\ref{sec:training_methods}); (2) train the first multimodal, process-based reward model for multimodal reasoning and demonstrate its usefulness in further enhancing performance (\\\\S \\\\ref{sec:rm}); and (3) find that adding more unlabeled queries helps only when having perfect reward signals (e.g., the oracle groundtruth answers), and it hurts the performance if the reward model does not generalize well on unseen data (\\\\S \\\\ref{sec:unlabeled}).\\n\\n\\\\begin{figure*}[!t]\\n    \\\\centering\\n    \\\\includegraphics[width=0.9\\\\linewidth]{figures/main_figure_v4_color.pdf}\\n    \\\\caption{Overview of our self-evolving training framework for multimodal reasoning. We investigate the three essential design components of it, namely \\\\textcolor{highlightedinfig1}{Training method ($\\\\mathcal{T}$)}, \\\\textcolor{highlightedinfig1}{Reward model ($\\\\mathcal{R}$)}, and \\\\textcolor{highlightedinfig1}{Prompt variation ($\\\\mathcal{P}$)}. Orthogonal to the static factors, the \\\\textcolor{highlightedinfig1}{Dynamics of self-evoloution} is also monitered, and provides control signals to the training process.\\n    % \\\\fan{(1)red color in caption always reminds me of un-solved comments; maybe just black color with bold font.(2) add \\\\mevol name, not just self-evolving?}\\n    }\\n    \\\\label{fig:overview}\\n    \\\\vspace{-10pt}\\n\\\\end{figure*}\\n\\n\\nInspired by reinforcement learning (RL), in this paper, we reframe self-evolving training through the lens of RL, identifying three factors that are critical inside self-evolving training: the \\\\textbf{training method}, the use of \\\\textbf{reward model}, and \\\\textbf{prompt variation}. Through massive controlled studies, we (1) propose a continuous self-evolving training scheme \\n% that inherits optimizers states from the last iteration \\nto reduce the gap towards full online learning and outperforms other iterative baselines (\\\\S \\\\ref{sec:training_methods}); (2) train the first multimodal, process-based reward model for multimodal reasoning and demonstrate its usefulness in further enhancing performance (\\\\S \\\\ref{sec:rm}); and (3) find that adding more unlabeled queries helps only when having perfect reward signals (e.g., the oracle groundtruth answers), and it hurts the performance if the reward model does not generalize well on unseen data (\\\\S \\\\ref{sec:unlabeled}). Beyond static design principles, we investigate the training dynamics, revealing how performance saturation stems from diminishing exploration potential during training. To address this, we introduce a new metric that bridges exploration and exploitation, and propose an automatic balancing mechanism that dynamically adjusts the sampling temperature to sustain exploration-exploitation trade-offs.\\n\\nCombining all the recipes concluded through separate, controlled studies, we propose our self-evolving training algorithm named as \\\\mevol~(\\\\textbf{M}ultimodal \\\\textbf{S}elf-evolving \\\\textbf{T}r\\\\textbf{a}ining for \\\\textbf{R}easoning).\\nOur experimental results on 5 different multimodal reasoning benchmarks, including MathVista, M3CoT, MMStar, MMBench and AI2D, show that this strategy, which incorporates both optimized static design choices and dynamic adjustments, effectively mitigates exploration loss during training and enhances performance universally for models with varied sizes such as MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B).\\n\\n% Orthogonal to the static factors above, we also dive into the dynamic evolving process of the model, namely the \\\\textbf{dynamics of self-evolution} to see how model's behavior changes during the training.\\n% We find that although the model performance of greedy decoding increases, its exploration potential keeps decreasing through the training (\\\\S \\\\ref{sec:monitor}), then we introduce a mechanism to dynamically adjust the sampling temperature during training to balance exploration and exploitation (\\\\S \\\\ref{sec:final_recipe}), as shown in Figure \\\\ref{fig:overview}.\\n\\n% Combining all the recipes concluded through separate, controlled studies, we propose our self-evolving training algorithm named as \\\\mevol~(\\\\textbf{M}ultimodal \\\\textbf{S}elf-evolving \\\\textbf{T}r\\\\textbf{a}ining for \\\\textbf{R}easoning). \\n% Our experimental results on 5 different multimodal reasoning benchmarks, including MathVista, M3CoT, MMStar, MMBench and AI2D, show that this strategy, which incorporates both optimized static design choices and dynamic adjustments, effectively mitigates exploration loss during training and further enhances performance universally for models with different sizes such as MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B).\\n\\n\\n\\\\section{Overview of Self-Evolving Training for Multimodal Reasoning}\\n\\\\label{sec:overview}\\n% This section serves as Preliminary for self-evolving training, we identify the three components here\\n\\n% \\\\begin{itemize}\\n%     \\\\item It is just a draft instead of the final framework \\\\\\\\\\n%     \\\\item Self-Evolve, On-policy, SFT insufficient, cause hallucination, therefore self-evolve as a more online(on-policy) method is better \\\\wl{cite some of \\\\citep{guo2024direct,kumar2024traininglanguagemodelsselfcorrect,setlur2024rleightfold,gekhman2024doesfinetuningllmsnew}} \\\\\\\\\\n%     \\\\item Markov Decision Process \\\\\\\\\\n% \\\\end{itemize}\\n\\n% self-evolving training is a paradigm that leverages data generated by models themselves to continuously enhance their capabilities. In contrast to traditional supervised fine-tuning (SFT), which depends on human-curated data, recent studies have shown that this imitation learning approach can be less efficient \\\\citep{guo2024direct} and may heighten the risk of hallucinations \\\\citep{gekhman2024doesfinetuningllmsnew} compared to other methods like rejective fine-tuning (RFT) \\\\citep{yuan2023rft} or Proximal Policy Optimization (PPO) \\\\citep{schulman2017proximal}, which rely more on on-policy data. Particularly for complex reasoning tasks, training methods that utilize on-policy data have been shown to more effectively enhance models' reasoning abilities \\\\citep{yuan2023rft,tong2024dartmath,metallama3}.\\n\\nSelf-evolving training can be modeled as a general framework of reinforcement learning, where various algorithms can be formulated as a specific instantiation of RL, such as PPO \\\\citep{schulman2017proximal}, STaR~\\\\citep{zelikman2022star}, ReST~\\\\citep{gulcehre2023rest} and ReST$^{EM}$~\\\\citep{singh2023restem}.\\n% Many of these approaches are essentially iterative SFT training (e.g., ReST, ReST$^{EM}$, STaR) with slight variations, \\nSpecifically, given a reward function $\\\\mathcal{R}$, the objective of self-evolving training\\n% \\\\jh{I have modified several places, we should use self-evolving consistently} \\nis to train the policy model $\\\\pi_\\\\theta$ to maximize expectation of reward $\\\\mathcal{R}$:\\n\\\\begin{equation}\\n    \\\\pi_{\\\\theta} = \\\\operatorname{argmax}_{\\\\pi_{\\\\theta}}\\\\sum_i^L \\\\mathbb{E}_{x,o \\\\sim \\\\mathcal{D},\\\\hat{y}_i \\\\sim \\\\pi_{\\\\theta}[\\\\cdot|x,o]}[\\\\mathcal{R}(\\\\hat{y}_i)],\\n\\\\end{equation}\\nwhere $x$, $o$ represent the query and image in the given training data $\\\\mathcal{D}$, while $\\\\hat{y}_i$ is a response sampled from the current policy model $\\\\pi_\\\\theta$.\\nThis standard RL objective, however, can be unstable to optimize and difficult to scale up, thus a popular algorithm adopted by recent works is to decouple the response rollout $\\\\hat{y}_i \\\\sim \\\\pi_{\\\\theta}[\\\\cdot|x,o]$ and policy improvement into separate offline stages~\\\\citep{gulcehre2023rest,singh2023restem}: (1) \\\\emph{Generate}: the current policy model generates new responses $\\\\hat{y}_i \\\\sim \\\\pi_{\\\\theta}[\\\\cdot|x,o]$; and (2) \\\\emph{Improve}: using the rewards to selects certain responses from the Generate step, which are then used to train the policy model with a standard supervised fine-tuning (SFT) loss. This way, the algorithm resembles Rejection Fine-Tuning (RFT,~\\\\citet{yuan2023rft}) as it filters out negative responses in a hard manner. \\nBoth steps are performed iteratively to strike a tradeoff between offline and online training. In many tasks such as mathematical problem-solving, there exists a unique, ground-truth answer $a^*$ which is utilized in the reward function, for example,~\\\\citet{singh2023restem} directly adopts exact match to compute a binary reward by comparing $\\\\hat{y}$ and $a^*$.\\nIn such an iterative training procedure, the objective at iteration $t$ is to obtain an improved policy model $\\\\pi^{t+1}_\\\\theta$:\\n% to maximize expectation of reward $\\\\mathcal{R}$, starting from the current policy model $\\\\pi^{t}_\\\\theta$ and leveraging data generated by the model itself, \\n% using a learning algorithm $\\\\mathcal{A}$. This process can be formulated as:\\n% \\\\vspace{-1.2cm}\\n\\\\begin{equation}\\\\label{eq:self_evolve}\\n    \\\\pi_{\\\\theta}^{t+1} = \\\\operatorname{argmax}_{\\\\pi_{\\\\theta}^{t}}\\\\sum_i^L \\\\mathbb{E}_{x,o,a^* \\\\sim \\\\mathcal{D},\\\\hat{y}_i \\\\sim \\\\pi_{\\\\theta}^{t}[\\\\cdot|x,o]}[\\\\mathcal{R}(a^*,\\\\hat{y}_i)],\\n\\\\end{equation}\\nwhere the ground-truth answer $a^*$ can be empty, for example, when dealing with unlabeled inputs, the reward model must be able to score $\\\\hat{y}_i$ independently.\\n\\n\\\\textbf{The Design Spaces}\\nThrough the lens of Eq.~\\\\ref{eq:self_evolve}, we can identify three dominant factors that influence the training process, (1) \\\\emph{training method}: the training algorithms to perform this iterative process vary as well.\\nFor example, while~\\\\citet{gulcehre2023rest,xu2024chatglm} initialize the model from the last checkpoint at each iteration,~\\\\citet{zelikman2022star,singh2023restem}\\n% \\\\jh{plz confirm whether star also initializes from the beginning, I vaguely remember it is} \\nargue that initializing from the beginning checkpoint reduces overfitting and gives better performance empirically. (2) \\\\emph{reward model}: the design of reward function $\\\\mathcal{R}$. (3) \\\\emph{prompt variation}: whether to incorporate additional unlabeled inputs without $a^*$ into training. \\n% \\\\jhc{Moreover, the iteration interval may matter as well -- although the common practice is to process the entire dataset at every iteration and the performance saturates after a few iterations, it may be suboptimal and a more online fashion with frequent iteration switch could potentially lead to improvements.}{} \\\\jhc{Theoretically, a short iteration interval with inherited optimizer and learning rate scheduler from the last iteration will turn this iterative optimization into a standard online RL learning algorithm.}{}\\\\jh{I save this to later we can introduce this in a more novel way} \\nNext, we investigate these three design spaces, aiming to summarize the best practices for each factor.\\n% There are different design choices to model and implement Eq.~\\\\ref{eq:self_evolve}, for example, the design of reward function $\\\\mathcal{R}$ and whether to incorporate additional unlabeled inputs without $a^*$ into training. \\n% Additionally, the training algorithms to perform this iterative process vary as well.\\n% For example, while~\\\\citet{gulcehre2023rest,xu2024chatglm} initialize the model from the last checkpoint at each iteration,~\\\\citet{zelikman2022star,singh2023restem}\\n% % \\\\jh{plz confirm whether star also initializes from the beginning, I vaguely remember it is} \\n% argue that initializing from the beginning checkpoint reduces overfitting and gives better performance empirically.\\n% % \\\\jhc{Moreover, the iteration interval may matter as well -- although the common practice is to process the entire dataset at every iteration and the performance saturates after a few iterations, it may be suboptimal and a more online fashion with frequent iteration switch could potentially lead to improvements.}{} \\\\jhc{Theoretically, a short iteration interval with inherited optimizer and learning rate scheduler from the last iteration will turn this iterative optimization into a standard online RL learning algorithm.}{}\\\\jh{I save this to later we can introduce this in a more novel way} \\n% Next, we investigate these three design spaces, \\\\emph{training method, reward model}, and \\\\emph{prompt variation}, aiming to summarize the best practices for each factor.\\n% to faciliate multimodal reasoning learning.\\n\\n\\n% \\\\begin{equation}\\\\label{eq:self_evolve}\\n%     \\\\pi_{\\\\theta}^{t+1} = \\\\argmax_{\\\\pi_{\\\\theta}^{t}}{\\\\scriptscriptstyle \\\\mathcal{A}}\\\\sum_i^L \\\\mathbb{E}_{x,o,y^* \\\\sim \\\\mathcal{D},\\\\hat{y}_i \\\\sim \\\\pi_{\\\\theta}^{t}[\\\\cdot|x,o]}[\\\\mathcal{R}(y^*,\\\\hat{y}_i)]\\n% \\\\end{equation}\\n% \\\\begin{equation}\\\\label{eq:self_evolve}\\n%     \\\\pi_{\\\\theta}^{t+1} = \\\\texttt{optimize}(\\\\pi_{\\\\theta}^{t},\\\\sum_i^L \\\\mathrm{E}_{x,o,y^* \\\\sim \\\\mathcal{D},\\\\hat{y}_i \\\\sim \\\\pi_{\\\\theta}^{t}[\\\\cdot|x,o]}[\\\\mathcal{R}(y^*,\\\\hat{y}_i)],\\\\mathcal{A})\\n% \\\\end{equation}\\n\\n% In this context, $x$, $o$, and $y^*$ represent the prompt, image, and ground-truth response in the given training data $\\\\mathcal{D}$, while $\\\\hat{y}_i$ is a response sampled from the current policy model $\\\\pi^{t}_\\\\theta$. Equation \\\\ref{eq:self_evolve} closely resembles a reinforcement learning (RL) objective, and the learning algorithm can involve methods such as rejective sampling (i.e., Rejective Fine-Tuning, RFT) and Proximal Policy Optimization (PPO). Given the need for efficiency and scalability in training models for multimodal reasoning, i.e., the lengthened input sequence brought by images, this work primarily focuses on RFT-like training methods, whose effectiveness and scalability have been well-documented in previous studies \\\\citep{yu2023metamath, yuan2023rft, tong2024dartmath, metallama3}.\\n\\n\\n\\n% In self-evolving training \\\\wl{or standard RFT?}, the design of reward functions can be adapted based on the specific training objective. For multimodal reasoning tasks, the reward function is defined as $\\\\mathcal{R}(y^*, \\\\hat{y}_i) =\\\\mathds{1}(y^* = \\\\hat{y}_i)$. Then the goal of Equation \\\\ref{eq:self_evolve} is to find an optimal policy model that maximizes the probability of generating correct sampled responses.\\n\\n% Although self-evolving training has been widely adopted and shown to be highly effective, it remains unclear what makes it work and how to design a good recipe for self-training, especially in complex scenarios like multimodal reasoning. From the perspective of \\\\ref{eq:self_evolve}, we identify three key components inside it, including training methods (i.e. learning algorithm) $\\\\mathcal{A}$, reward function $\\\\mathcal{R}$, and policy training prompts $\\\\mathcal{D}=\\\\{(x_j, o_j, y^*_j)\\\\}_{j=1}^{M}$.\\n\\n\\n% for training methods\\n\\n\\n% for reward models\\n\\n% for training prompts\\n% Identify the three factors.\\n\\n% There are different methods to design rewards... For standard RFT, reward as binary reward xxx. ORM xxx. PRM xxx.\\n\\n\\n\\n\\n\\n\\\\section{Diving into Self-Evolving Design Components}\\n\\\\label{sec:design}\\n\\nIn this section, we explore the three key components of self-evolving training, examining various strategies within each. We begin by outlining the general setup (\\\\S \\\\ref{sec:setup}), followed by a comprehensive analysis of each component to identify the best practices for multimodal self-evolution (\\\\S \\\\ref{sec:training_methods}-\\\\S \\\\ref{sec:unlabeled}).\\n\\n\\\\subsection{General Setup}\\n\\\\label{sec:setup}\\n\\n\\\\textbf{Models}\\nWe base our main exploration on MiniCPM-V-2.5 (8B) \\\\citep{yao2024minicpmv}, and we also validate the final design choice for each component on two extra models with different sizes: Phi-3.5-Vision (4B) \\\\citep{abdin2024phi} and InternVL-2 (2B) \\\\citep{chen2024internvl}. The details of these models can be found in Appendix \\\\ref{app:model_details}. To make the analysis process easier to understand, we mainly present the results of MiniCPM-V-2.5 in this section, while we include the results of the other models in \\\\S \\\\ref{sec:final_recipe}. \\n% For ease of space, we mainly report the MiniCPM-V-2.5 results in the following analysis with full details,  of Phi-3.5-Vision and InternVL2-2B later in \\\\S \\\\ref{sec:final_recipe}.\\n\\n% \\\\jh{I have checked Table 5, actually the results of the other two models align with miniCPM-v-2.5, right? Then we can just say our exploration is on 3 models, it is just we only report minicpm in this section, but we include the full analysis results of the other two models later in section xxx. This way, I think we can introduce all three models somewhere in the appendix. Basically we don't need to make minicpm very special in the paper}MiniCPM-V-2.5 \\\\citep{yao2024minicpmv}, a powerful, openly released LMM. MiniCPM-V-2.5 leverages LLaMA-3-8B \\\\citep{metallama3} for its language model and SigLIP \\\\citep{siglip} as its vision encoder, resulting in strong multimodal capabilities. Its performance on a wide range of multimodal benchmarks significantly surpasses previous openly released LMMs such as LLaVA \\\\citep{liu2023llava,liu2024improvedllava} and Qwen-VL \\\\citep{Qwen-VL}. This superior performance makes MiniCPM-V-2.5 an ideal model for investigating self-evolving training in multimodal reasoning, with fewer risks of being constrained by the model's inherent capacities.\\\\jh{in this version, here we need to describe that although our controlled experiments are performed only using one model due to limited computing resources, in section xxx we demonstrate that our final recipe works well for multiple different vLMs on diverse benchmarks}\\n% \\\\jh{How minicpm-v was trained is not that important to write here, instead what matters to be written here is (1) the size of this model; (2) use 1 or 2 sentences to express minicpm-v-2.5 has strong multimodal abilities, for example, outperform qwen-vl, llava, xxx (which are basically the main reason we chose it)}\\n% \\\\jhc{Note that toconduct controlled experiments without the confounder of model,}\\n\\n\\n\\n\\\\textbf{Datasets}\\n% \\\\jh{this section 3.1 is generally too long,  I think the Datasets and the Training and Evaluation paragraphs can be merged to be shorter. Some details can be moved to appendix}\\nWe utilize MathV360K \\\\citep{shi2024mathllava}, a high-quality and diverse multimodal reasoning dataset as our seed training dataset.\\n% that includes 40K human-curated examples and 320K synthetic samples generated by GPT-4V as our seed training dataset. The images and queries in this dataset span various subjects in multimodal reasoning including algebra, arithmetic, geometry, logic, numeric commonsense and science.\\nSpecifically, we downsample half of the examples (180K) from it to serve as our labeled training set, while setting aside the remaining half as a unlabeled training set by not using the answers in it. \\n% For the labeled set, each data sample is composed of $(x, o, a)$ without the intermediate thought process annotation, while for the unlabeled set we only have access to $(x, o)$. \\n% This is a realistic setting as there are many multimodal SFT datasets with the final answer labels, but annotated thought processes are scarce. \\nFor evaluation, we split 750 samples from the unlabeled part of MathV360K as the in-domain (ID) testset.\\n% , using regular expression to extract and match the answers. \\nFor our out-of-domain (OOD) testset we use the \\\\texttt{testmini} split of MathVista \\\\citep{lu2023mathvista}, a comprehensive benchmark encompassing a wide range of multimodal reasoning tasks, including visual question answering, figure-based question answering, science question answering, and more. \\n% We follow its original setting and use GPT-4 to extract and match the answers. \\nWe also keep an non-overlapping 250 samples from MathV360K as the global validation set in training.\\n% This is to mimic the setting where we have \\n% The use of the remaining data will be further discussed in \\n% Section \\\\ref{sec:unlabeled}.\\n\\n% \\\\paragraph{Warm-Up Phase to Unlock the Chain-of-Thought (CoT) Capability of LMMs}\\n\\n% For the prompt during the warm-up phase, please refer to Appendix \\\\ref{app:prompt} for more details.\\n\\n% While diverse responses with various intermedia steps are crucial for self-evolving training, we \\n\\n\\n\\\\textbf{Main Training Settings}\\nBefore self-evolving training, a self-warmup stage (Appendix \\\\ref{app:prompt}) is applied to enhance the CoT generation ability of all models as many LLMs tend to output final answers directly without CoT. We adopt most of the training settings from \\\\citet{yao2024minicpmv} (see Appendix~\\\\ref{app:hyperparam}), using a constant learning rate of $1e-6$ and training for 10K steps across all experiments. During all rollout phases in training, we sample 16 responses per query and set the sampling temperature to 1.0. Unless explicitly stated otherwise, we follow existing practices~\\\\citep{singh2023restem,zelikman2022star} and only use the labeled training data.\\n\\n% then we will study the impact of the unlabeled training data in \\\\textsection\\\\ref{sec:unlabeled}.\\n% More details can be referred to Appendix~\\\\ref{app:hyperparam}.\\n% \\\\jh{if we do not validate for mathvista and always use the same checkpoint for both mathv360k and mathvista, it is worth emphasizing that here which could make our ood results more impressive}\\n% To balance effectiveness with training efficiency, \\n% We set the maximum number of training steps as 10K for all experiments. \\n% \\\\jh{I think we should mention some key hyperparams here, like k, and temperature?}For additional details, please refer to Appendix \\\\ref{app:hyperparam}. \\n% \\\\wl{Should we mention constant scheduler here? Or we mention it on other 3.2?}\\n\\n\\n% \\\\jh{Note: this is a separate paragraph not belonging to training and evaluation, it should be at the end of 3.1}\\n% {For ease of space,} we mainly \\\\jhc{use}{report the} MiniCPM-V-2.5 \\\\jhc{}{results} in \\\\jhc{analyzing and finalizing different design choices.}{the following analysis with full details, while we include the results of Phi-3.5-Vision and InternVL2-2B later in \\\\S \\\\ref{sec:final_recipe}.} \\n% However, we also include more models, Phi-3.5-Vision and InternVL2-2B to demonstrate the effectiveness of our final recipe in \\\\S \\\\ref{sec:final_recipe}. \\n\\n\\\\subsection{Training Methods}\\n\\\\label{sec:training_methods}\\n% We start from a unified form \\\\citep{shao2024deepseekmath} for the learning algorithm $\\\\mathcal{A}$ of self-evolving training.\\n% \\\\begin{equation}\\n% \\\\nabla_{\\\\theta} \\\\mathcal{J}_{\\\\mathcal{A}}(\\\\theta) = \\\\mathbb{E}_{(q,o) \\\\sim \\\\mathcal{D}} \\\\left[ \\\\frac{1}{|o|} \\\\sum_{t=1}^{|o|} \\\\textcolor{red}{GC_{\\\\mathcal{A}}(q, o, t, \\\\pi_{rf})} \\\\nabla_{\\\\theta} \\\\log \\\\pi_{\\\\theta}(o_t | q, o_{<t}) \\\\right].\\n% \\\\end{equation}\\nAs described in \\\\textsection\\\\ref{sec:overview}, there are multiple variants on how we would train to update the policy model. \\n% \\\\jhc{We decouple the variation dimensions by thinking of the gap between iterative training and online RL -- when the iteration interval is small, the checkpoint at each iteration is initialized from one from the last iteration, and the optimizer as well as the learning rate scheduler is inherited between iterations, then iterative training becomes an online RL algorithm. Therefore, we cluster the variations by three factors:}\\nPrevious works mainly vary the model initialization factor, where at the ``Improve'' step,\\n% \\\\jhc{(1) {\\\\bf Model initialization}: when training is performed at the ``Improve'' step,}{} \\nthe model can be initialized from either the last checkpoint~\\\\citep{xu2024chatglm,pang2024iterative} or the beginning checkpoint before the first iteration~\\\\citep{zelikman2022star,singh2023restem}.\\n% \\\\jhc{;}{\\nBesides model initialization, in this work, we introduce new variants of iterative self-evolving through delving into the gap between iterative training and online RL -- concretely, when the iteration interval is small, the checkpoint at each iteration is initialized from one from the last iteration, and the optimizer as well as the learning rate scheduler is inherited between iterations, then iterative training becomes an online RL algorithm. \\nTherefore, we propose {\\\\bf Continuous Self-Evolving}, a new iterative self-evolving training variant that represents a smoother interpolation between iterative training and online training. \\nIn continuous self-evolving training, we inherit the optimizers as well as the learning rate schedulers from the last iteration besides inheriting the model checkpoint, so that the optimization is continuous and closer to purely online learning algorithms.   \\n% }\\n% \\\\jhc{\\n% (2) {\\\\bf Iteration Interval}: while the common practice is to adopt a long iteration interval to process all the data queries for one iteration, we study the effect of having a shorter iteration interval, bringing it closer to online learning; and\\n% (3) {\\\\bf Continuous Optimization}: we propose to inherit the optimizers as well as the learning rate schedulers from the last iteration besides inheriting the model checkpoint, so that the optimization is continuous and closer to purely online learning algorithms. }{} \\nThis way, we only have a global optimizer and learning rate scheduler essentially across the entire iterative training process.\\n% \\\\jhc{}{\\nWe also analyze the \\\\emph{iteration interval} effect in continuous self-evolving, which is defined as the training queries passed for one iteration -- we specifically study the effect of having a shorter iteration interval, which stands in contrast to the common practice that adopts a long iteration interval to process all the data queries for one iteration.\\n% }\\n% \\\\jhc{We note that while the variation on model initialization has been studied before, the other two factors, iteration interval and continuous optimization, have been rarely discussed in previous implementations of iterative self-evolving training, and they turn out to be important empirically as we will show next. }{}\\n% \\\\fan{small advice: maybe mention show in which section.}\\n\\n\\n% the training method determines how data is processed and utilized to update the current policy model $\\\\pi^t_\\\\theta$ during self-evolving training. Unlike traditional supervised fine-tuning, which relies on static, \\\\textbf{offline} data curated by humans or models like GPT-4 \\\\citep{openai2023gpt4v}, self-evolving training emphasizes dynamic, \\\\textbf{online} data generation and exploitation. This online nature operates on two critical dimensions: data and learning. On the data dimension, the model samples from its own evolving policy, ensuring alignment with the current policy's behavior. On the learning side, the model continually trains from its current state\u2014preserving ongoing training dynamics such as optimizer states\u2014without resetting or reinitializing, mitigating distributional shifts and maintaining coherence with the evolving policy.\\n\\n% In this section, we explore the impact of these two dimensions\u2014data sampling and learning dynamics\u2014on the performance of self-evolving training. We begin by contrasting two paradigms: \\\\textbf{iterative} and \\\\textbf{online} learning. The iterative approach decouples data sampling from training, where the model samples data and then trains on it, either starting from an SFT model $\\\\pi_{\\\\theta_{sft}}$ or the current policy, without carrying over optimizer states or learning schedules. In contrast, the online paradigm integrates data sampling and training into a continuous process, maintaining the model's training state and ensuring ongoing alignment between learning and the evolving data distribution.\\n\\n\\n\\\\textbf{Setup}\\n% \\\\jh{across the paper, sometimes we have ``:'' after the paragraph title, sometimes we don't, please be consistent}\\nWe perform controlled experiments to study the effect of different training methods, thus in this experiment we use the labeled dataset only and simply adopt the binary exact-match reward between ground-truth answer $a^*$ and the generated answer. \\nWe compare with the most common iterative self-evolving algorithms ReST$^{EM}$~\\\\citep{singh2023restem} and iterative RFT, which are specific instantiations of our training methods design space. \\n% \\\\jhc{To distinguish from the baselines, the variants with continuous optimization are named as \\\\emph{Continuous Self-Evolving}.}\\n% {} \\nTo study the effect of iteration interval in the proposed continuous self-evolving, we experiment with different percentage of all the queries per iteration, varying from [6.25\\\\%, 12.5\\\\%, 25\\\\%, 50\\\\%, 100\\\\%].\\n% For iterative learning, we examine methods like ReST-EM \\\\citep{singh2023restem} and iterative RFT. In ReST-EM, training restarts from the SFT model at each iteration, while iterative RFT begins from the current policy model but reinitializes training dynamics at each iteration. On the other hand, the online paradigm, represented by online RFT, provides more efficient and scalable learning by continuously updating the model without resetting training states, making it particularly suitable for multimodal reasoning tasks.\\n% From a data perspective, we analyze how varying the degree of online data sampling affects performance.\\n% Specifically, we experiment with different rollout batch sizes for online RFT\u2014[6.25\\\\%, 12.5\\\\%, 50\\\\%, 100\\\\%] of the dataset\u2014to assess how the degree of online sampling influences the effectiveness of training in this evolving context.\\n\\n\\\\begin{table}[t!]\\n  \\\\centering\\n  \\\\caption{\\n  % \\\\jh{the full results of mathvista will be useful on this table -- if we can still find it, plz record it. For here, it is better to have the table with full mathvista results in appendix} \\\\jl{we already have them in Table \\\\ref{tab:full_mathvista}, I think adding one hyperlink is enough} \\n  Accuracy results (\\\\%) of self-evolving training using various training methods and iteration intervals.\\n  % \\\\jh{explain what the two numbers in iteration interval column mean}\\n  % \\\\jh{not consistent, we call it iteration interval in all other places even in the first sentence of this caption. Iteration interval means interval between iterations which I think is better}\\n  Interval (\\\\#) stands for iteration interval, the ratio of data we traverse in one iteration, and we also record the number of corresponding queries.\\n  $\\\\mathcal{M}$ represents the policy model from which training is initialized in each iteration. $\\\\mathcal{O}$ denotes whether the optimization process is continuous, i.e., the optimizer states and lr scheduler are inherited from the last checkpoint. Please refer to Table \\\\ref{tab:full_mathvista} to check the full results on all sub-tasks of MathVista.\\n  }\\n  % \\\\jh{briefly explain in the caption what it means when it is continuous or not}\\n  % \\\\jh{why did we remove the SFT baselines in the table? I think we should have them in the first table to show our RFT stuff is successful and at least improves baselines}}\\n\\\\setlength{\\\\tabcolsep}{2pt}\\n  \\\\resizebox{0.48\\\\textwidth}{!}{\\n  \\\\begin{tabular}{lllrrr}\\n    \\\\toprule\\n    Method   & $\\\\mathcal{M}$  & $\\\\mathcal{O}$ &  Interval (\\\\%) & MathV360K & MathVista \\\\\\\\\\n    \\\\midrule\\n     MiniCPM-V-2.5 & - & -&- & 13.6 & 52.4 \\\\\\\\\\n     \\\\ \\\\ \\\\ \\\\ +warmup &- &- &- & 38.8 & 52.6 \\\\\\\\\\n     SFT& - & -&- & 44.3 & 54.8 \\\\\\\\\\n     \\\\midrule\\n     Iterative RFT   & $\\\\pi_{\\\\theta}^t$  &  $\\\\times$  & 100\\\\% & 42.3 & 55.7 \\\\\\\\\\n     Rest$^{EM}$ & $\\\\pi_{\\\\theta}^0$ & $\\\\times$   & 100\\\\% & 42.3 & 55.1 \\\\\\\\\\n    \\\\midrule\\n     \\\\multirow{5}{*}{\\\\makecell{Continous\\\\\\\\ Self-Evolving}} \\n     & \\\\multirow{5}{*}{$\\\\pi_{\\\\theta}^t$} \\n     & \\\\multirow{5}{*}{$\\\\checkmark$} \\n     & 100\\\\% & 42.2 & 56.7 \\\\\\\\\\n     & & & 50\\\\% & \\\\textbf{43.1} & 56.2  \\\\\\\\\\n     & & & 25\\\\% & \\\\textbf{43.1} & \\\\textbf{57.2}  \\\\\\\\\\n     & & & 12.5\\\\% & 42.3 & 56.1  \\\\\\\\\\n     & & & 6.25\\\\% & 41.0 & 56.8  \\\\\\\\\\n    \\\\bottomrule\\n\\\\end{tabular}}\\n  \\\\label{tab:online_rft}%\\n    \\\\vspace{-10pt}\\n\\\\end{table}\\n\\n\\\\textbf{Results}\\n% \\\\jh{We don't need to make miniCPM special since we have the full results of other two models. Here when cite the results, plz cite Table 5 as well to draw conclusions based on results of 3 models, not just one}\\nTable \\\\ref{tab:online_rft} presents the experimental results of various training methods. Overall, initializing training from the last policy model checkpoint $\\\\pi_{\\\\theta}^t$ and maintaining a continuous optimization process contribute most significantly to the effectiveness of self-evolving training, particularly on MathVista. Continuous self-evolving achieves the best performance both on the in-domain MathV360K test set, with 43.1\\\\%, and on the OOD test set, MathVista, with 57.2\\\\%. We also see the importance of maintaining a proper interval to traverse the data queries. With a large interval, the training method becomes closer to an offline one, and the model cannot get timely updates on data matching its current output distribution. On the other hand, switching over the \\\\textit{Improve} and \\\\textit{Generate} steps too frequently makes the learning process unstable, leading to a lower score, especially on the in-domain test set. The strategy of continuous self-evolving with proper intervals also works for other smaller models, as shown in Table \\\\ref{tab:full_mathvista} compared with representative baselines, indicating its effectiveness and generalizability across different model sizes.\\n\\n% \\\\jl{Hope we can also add avg rollout utils here to indicate 45k is good for exploitation}\\n\\n% \\\\begin{table}[t!]\\n%   \\\\centering\\n%   \\\\caption{Experiments for Section 3.1 - Training Algorithm\\\\jh{Are the RestEM results from the same training steps or longer?}\\\\jh{maybe we should report on both benchmarks}\\\\jh{we should add the baseline results to the table (the initial performance before rft)}}\\n%     \\\\begin{tabular}{llllrrrrr}\\n%     \\\\toprule\\n%     status &   Method   & ckpt  & optimizer & data per interval & perf & perf id & perf avg & Roll utils \\\\\\\\\\n%     \\\\midrule\\n%     finished (mbzui)  & IRT   & last  & new   & 100\\\\% (180K) & 55.7 & 42.6 & xx.x & \\\\\\\\\\n%     finished (mbzui)  & RestEM & first & new   & 100\\\\% (180K) & xx.xx & xx.xx & xx.xx \\\\\\\\\\n%     idle  & IRT global eval   & last  & new   & 100\\\\% (180K) & & & & \\\\\\\\\\n%     idle  & RestEM global eval & first & new   & 100\\\\% (180K) & & & & \\\\\\\\\\n%     \\\\midrule\\n%     finished (mbzui) & \\\\multirow{5}[2]{*}{online RFT} & \\\\multirow{5}[2]{*}{last} & \\\\multirow{5}[2]{*}{last} & 100\\\\% (180K) & 56.7 & 42.2 & 49.45 & \\\\\\\\\\n%     finished (mbzui)  &       &       &       & 50\\\\% (90K) & 56.2 & 43.1 & 49.65 &\\\\\\\\\\n%     finished (mbzui) &       &       &       & 25\\\\%  (45K) & 57.2 & 43.1 & 50.15 & \\\\\\\\\\n%     finished (mbzui) &       &       &       & 12.5\\\\% (22K) & 56.1 & 42.3 & 49.2 &\\\\\\\\\\n%     finished (mbzui) &       &       &       & 6.25\\\\% (11K) & 56.8 & 41.0 & 48.9 &\\\\\\\\\\n%     \\\\bottomrule\\n%     \\\\end{tabular}%\\n%   \\\\label{tab:addlabel}%\\n% \\\\end{table}%\\n\\n% For online RFT, we experiment with different sampling intervals, using [6.25\\\\%, 12.5\\\\%, 25\\\\%, 50\\\\%, 100\\\\%] of the dataset in each iteration.\\n\\n% From the perspective of data dimension, we investigate how varying online degrees of data sampling in different \\n\\n% \\\\begin{table}[h!]\\n%     \\\\centering\\n%     \\\\begin{tabular}{llll|}\\n%         \\\\hline\\n%         \\\\textbf{Methods} & \\\\textbf{Data Source} & \\\\textbf{Reward Function} & \\\\textbf{Gradient Coefficient} \\\\\\\\\\n%         \\\\hline\\n%         SFT & $q, o \\\\sim P_{sft}(Q, O)$ & - & 1 \\\\\\\\\\n%         \\\\hline\\n%         \\\\multirow{2}{*}{RFT} & $q \\\\sim P_{sft}(Q), o \\\\sim \\\\pi_{sft}(O|q)$ & \\\\multirow{2}{*}{Rule} & Equation \\\\textcolor{red}{10} \\\\\\\\\\n%         & & & \\\\\\\\\\n%         \\\\hline\\n%         \\\\multirow{2}{*}{DPO} & $q \\\\sim P_{sft}(Q), o^{+}, o^{-} \\\\sim \\\\pi_{sft}(O|q)$ & \\\\multirow{2}{*}{Rule} & Equation \\\\textcolor{red}{14} \\\\\\\\\\n%         & & & \\\\\\\\\\n%         \\\\hline\\n%         \\\\multirow{2}{*}{Online RFT} & $q \\\\sim P_{sft}(Q), o \\\\sim \\\\pi_{\\\\theta}(O|q)$ & \\\\multirow{2}{*}{Rule} & Equation \\\\textcolor{red}{10} \\\\\\\\\\n%         & & & \\\\\\\\\\n%         \\\\hline\\n%         \\\\multirow{2}{*}{PPO} & $q \\\\sim P_{sft}(Q), o \\\\sim \\\\pi_{\\\\theta}(O|q)$ & \\\\multirow{2}{*}{Model} & Equation \\\\textcolor{red}{18} \\\\\\\\\\n%         & & & \\\\\\\\\\n%         \\\\hline\\n%         \\\\multirow{2}{*}{GRPO} & $q \\\\sim P_{sft}(Q), \\\\{ o_i \\\\}_{i=1}^{G} \\\\sim \\\\pi_{\\\\theta}(O|q)$ & \\\\multirow{2}{*}{Model} & Equation \\\\textcolor{red}{21} \\\\\\\\\\n%         & & & \\\\\\\\\\n%         \\\\hline\\n%     \\\\end{tabular}\\n%     \\\\caption{Methods comparison table}\\n%     \\\\label{tab:methods}\\n% \\\\end{table}\\n\\n% We split self-evolving training methods into two manners, iterative manners and online manners. For iterative manners, \\n\\n% baseline: (1) iterative rejection tuning (continual from last checkpoint, new optimizer, invertal is at least one epoch)\\n\\n% (2) Rest-EM -- starting from beginning\\n\\n% (3) Online RFT -- continual from last checkpoint, interval is much shorter) \\n% optionally, compare with different lr schedule \\n\\n% We can study the effect of different intervals. k=16. We vary the interval. No RM. \\n\\n% % Table generated by Excel2LaTeX from sheet 'Sheet1'\\n% \\\\begin{table}[htbp]\\n%   \\\\centering\\n%   \\\\caption{Experiments for Section 3.1 - Training Algorithm}\\n%     \\\\begin{tabular}{llllr}\\n%     \\\\toprule\\n%     status &       & ckpt  & optimizer & data per interval \\\\\\\\\\n%     \\\\midrule\\n%     idle  & IRT   & last  & new   & 100\\\\% (180K) \\\\\\\\\\n%     idle  & IRT-plus & last  & last  & 100\\\\%+ (180K+) \\\\\\\\\\n%     idle  & RestEM & first & new   & 100\\\\% (180K) \\\\\\\\\\n%     \\\\midrule\\n%     idle  & \\\\multirow{5}[2]{*}{online RFT} & \\\\multirow{5}[2]{*}{last} & \\\\multirow{5}[2]{*}{last} & 100\\\\% (180K) \\\\\\\\\\n%     idle  &       &       &       & 50\\\\% (90K) \\\\\\\\\\n%     idle  &       &       &       & 25\\\\%  (45K) \\\\\\\\\\n%     idle  &       &       &       & 12.5\\\\% (22K) \\\\\\\\\\n%     idle  &       &       &       & 6.25\\\\% (11K) \\\\\\\\\\n%     \\\\bottomrule\\n%     \\\\end{tabular}%\\n%   \\\\label{tab:addlabel}%\\n% \\\\end{table}%\\n\\n% Table generated by Excel2LaTeX from sheet 'Sheet1'\\n\\n% \\\\begin{table}[htbp]\\n%   \\\\centering\\n%   \\\\caption{Experiments for Section 3.1 - Training Algorithm\\\\jh{Are the RestEM results from the same training steps or longer?}\\\\jh{maybe we should report on both benchmarks}\\\\jh{we should add the baseline results to the table (the initial performance before rft)}}\\n%     \\\\begin{tabular}{llllrrrrr}\\n%     \\\\toprule\\n%     status &   Method   & ckpt  & optimizer & data per interval & perf & perf id & perf avg & Roll utils \\\\\\\\\\n%     \\\\midrule\\n%     finished (mbzui)  & IRT   & last  & new   & 100\\\\% (180K) & 55.7 & 43.4 & 49.55 & \\\\\\\\\\n%     finished (mbzui)  & RestEM & first & new   & 100\\\\% (180K) & xx.xx & xx.xx & xx.xx \\\\\\\\\\n%     idle  & IRT global eval   & last  & new   & 100\\\\% (180K) & & & & \\\\\\\\\\n%     idle  & RestEM global eval & first & new   & 100\\\\% (180K) & & & & \\\\\\\\\\n%     \\\\midrule\\n%     finished (mbzui) & \\\\multirow{5}[2]{*}{online RFT} & \\\\multirow{5}[2]{*}{last} & \\\\multirow{5}[2]{*}{last} & 100\\\\% (180K) & 56.5 & 44.1 & 50.3 & \\\\\\\\\\n%     finished (mbzui)  &       &       &       & 50\\\\% (90K) & 56.2 & 43.3 & 49.75 &\\\\\\\\\\n%     finished (mbzui) &       &       &       & 25\\\\%  (45K) & 57.2 & 44.4 & 50.8 & \\\\\\\\\\n%     finished (mbzui) &       &       &       & 12.5\\\\% (22K) & 56.1 & 45.1 & 50.6 &\\\\\\\\\\n%     finished (mbzui) &       &       &       & 6.25\\\\% (11K) & 56.8 & 44.1 & 50.45 &\\\\\\\\\\n%     \\\\bottomrule\\n%     \\\\end{tabular}%\\n%   \\\\label{tab:addlabel}%\\n% \\\\end{table}%\\n\\n% With decay.\\n\\n% \\\\paragraph{Other Settings}\\n% \\\\begin{itemize}\\n%     \\\\item K (number of rollout per query) = 16\\n%     \\\\item Temperature = 1.0\\n%     \\\\item Max. global steps = 10K\\n% \\\\end{itemize}\\n\\n% \\\\begin{table}[h]\\n%     \\\\centering\\n%     \\\\begin{tabular}{c|c|c}\\n%     \\\\toprule\\n%         Name & State & Priority \\\\\\\\\\n%     \\\\midrule\\n%     \\\\multicolumn{3}{c}{Iterative Rejection Fine Tuning (IRFT)} \\\\\\\\\\n%     \\\\midrule\\n%         IRFT from last ckpt but new optimizer, only new data & TO RUN & P0 \\\\\\\\\\n%         IRFT from initial ckpt but new data mixed with old data & TO RUN & Optional \\\\\\\\\\n%     \\\\midrule\\n%     \\\\multicolumn{3}{c}{Rest-EM} \\\\\\\\\\n%     \\\\midrule\\n%         IRT from initial ckpt and new optimizer (Sampling from the 180K per iteration) & TO RUN & P0\\\\\\\\\\n%     \\\\midrule\\n%     \\\\multicolumn{3}{c}{Online RFT} \\\\\\\\\\n%     \\\\midrule\\n%         with 180K & TO RUN & P0\\\\\\\\\\n%         with 90K & TO RUN & P0\\\\\\\\\\n%         with 45K & TO RUN & P0\\\\\\\\\\n%         with 22.5K & TO RUN & P0\\\\\\\\\\n%         with 11.25K & TO RUN & P0\\\\\\\\\\n%     \\\\bottomrule\\n%     % \\\\midrule\\n%     % \\\\multicolumn{3}{c}{Reference Baseline} \\\\\\\\\\n%     % \\\\midrule\\n%     %      &  \\\\\\\\\\n%     %      &  \\\\\\\\\\n%     % \\\\bottomrule\\n%     \\\\end{tabular}\\n%     \\\\caption{TODO}\\n%     \\\\label{tab:my_label}\\n% \\\\end{table}\\n\\n% After this section, we fix the training algorithm and the interval.\\n\\n\\\\subsection{Reward Models}\\n\\\\label{sec:rm}\\nIn self-evolving training, the most common approach to reward function design uses a binary reward $\\\\mathcal{R}(\\\\hat{y}_i) = \\\\mathds{1}(\\\\hat{a}_i = a^*)$, where $\\\\hat{a}_i$ is the predicted answer inside $\\\\hat{y}_i$ and incorrect responses are filtered out to maximize rewards. While effective, this sparse binary reward has limitations. It overlooks the quality of the intermediate reasoning steps within a response. Additionally, reward models trained from equal or higher capacity models than the policy model \\\\citep{fried2022incoder,wang2024mathshepherd,sun2024easy2hard} can provide richer signals to improve the policy model\u2019s learning. \\n\\nIn this section, we introduce a Process Reward Model (PRM) \\\\citep{lightman2023let, wang2024mathshepherd}\\n% \\\\jh{cite some PRM papers} \\nfor multimodal reasoning\u2014the first of its kind, to our knowledge\u2014and explore how integrating PRM can enhance reward design and whether it can improve policy model learning in self-evolving training for multimodal reasoning. To incorporate the reward scores into the objective of self-evolving training, the reward function is reformulated as:\\n% \\\\fan{equation shoule be numbered.}\\n\\\\begin{align}\\n&\\\\mathcal{R}(\\\\hat{y}_i) = \\\\mathcal{H}(\\\\mathds{1}(a^* = \\\\hat{a}_i)\\\\times \\\\mathcal{R}_p(\\\\hat{y}_i))\\\\\\\\\\n&\\\\mathcal{R}_p(\\\\hat{y}_i) = \\\\min(f(s_{i}^0), f(s_{i}^1),..., f(s_{i}^m))\\n\\\\end{align}\\nHere, $\\\\mathcal{H}$ is an operation that processes responses based on the final reward scores, where we ensure all responses are correct by matching the ground truths,\\n% \\\\jh{we need to explain here this definition means we have pre-filtering and all the responses must be correct on the labeled data.}\\n and $\\\\mathcal{R}_p(\\\\hat{y}_i)$ represents the process reward score for each sampled response. The function $f(s_i^k)$ denotes the reward score at each intermediate step. Following \\\\citet{lightman2023let}, we use the $\\\\min$ operation to aggregate stepwise rewards.\\n% ensuring that the overall reward reflects the lowest-scoring step in the reasoning process.\\n\\n% \\\\paragraph{Training Reward Models:}\\n% \\\\jh{Please just use two or three sentences to describe how we train the reward models and move many details in this paragraph to appendix (even in appendix we don't need to describe math-shepherd in detail). We can just say we follow math-shepherd xxxx [two sentences on the high-level description of the approach].\\n% We are not inventing this PRM training approach, thus we don't need to write this long to describe it. \\n% If this is just two or three sentences, it could be merged into Setup paragraph.\\n% }\\n\\n% To train our PRM, we first train another checkpoint \\n% (denoted as $\\\\hat{\\\\pi}_{\\\\theta}^0$) on our CoT-augmented training data for a much longer period to make sure it fully converges. \\n\\n% Based on this model, we leverage Monte Carlo Rollut method~\\\\citep{wang2024mathshepherd} to collect the training data for PRM. Specially, we randomly pick 50K questions from the full training set, and sample 16 responses for each of them with $\\\\hat{\\\\pi}_{\\\\theta}^0$. We de-duplicate these responses, and only keep at most 4 responses for each question. After that we randomly sample 50K question-response pairs from all the pairs, where we control the ratio of correct and wrong responses as 1:1, and the ratio of multi-choice and free-form question as 1:1 as well, to keep a balanced distribution. \\n\\n% To construct the labels of each step, we use $\\\\hat{\\\\pi}_{\\\\theta}^0$ as the completer to complete the solution from the end of each step in one response. For the $k^{\\\\text{th}}$ step, the step label is annotated as $\\\\frac{1}{N}\\\\sum_{j=1}^{N}{\\\\mathds{1}(C_j(s^{\\\\leq k})=a^*)}$, where $N (=16)$ is the number of completion, $C_j$ is the $j$-th completion.\\n\\n% Based on the stepwise annotations, we train our PRM from $\\\\hat{\\\\pi}_{\\\\theta}^0$. We initialize the linear reward model head as the average of the embeddings, and train with MSE loss on all tokens, where the label of each token is identical to the step end token. In experiments we freeze the visual encoder as we find it brings a slight improvement. \\n% \\\\wl{It include too many training details consider to move sth to appendix}\\n\\n\\\\textbf{Setup}\\nWe conduct controlled experiments to assess the impact of incorporating the Process Reward Model (PRM) into self-evolving training and explore how best to utilize it. Notably, before applying PRM, responses are pre-filtered based on their final answers to ensure consistency and quality during training. To train our PRM, we use Monte Carlo rollouts starting from prefixes with partial reasoning steps \\\\citep{wang2024mathshepherd} to generate the training data. Specifically, we sample 16 responses per question and complete each step 8 times to obtain step-level annotations and more details can be found in Appendix \\\\ref{app:prm}. We evaluate two different $\\\\mathcal{H}$ operations: (1) Top-K: Pick the top-K correct responses according to their reward scores, and (2) Filtering by a Threshold $\\\\alpha$: Filtering out sampled responses with lower aggregated rewards than $\\\\alpha$. The optimal value of $\\\\alpha$ is set $0.2$ by grid searching on the validation set. Additionally, we investigate how varying the value of $\\\\operatorname{K}$ in $\\\\operatorname{Top-K}$ affects training, as it represents a trade-off between the quality and diversity of the samples. According to \\\\textsection\\\\ref{sec:training_methods}, we fix training methods as continuous self-evolving with 45k interval and set continuous self-evolving, with or without randomly selected correct responses as our baselines.\\n\\n\\\\textbf{Results}\\n% See Table \\\\ref{tab:sec33_main}\\n% \\\\jh{We don't need to make miniCPM special since we have the full results of other two models. Here when cite the results, plz cite Table 5 as well to draw conclusions based on results of 3 models, not just one}\\nTable \\\\ref{tab:sec33_main} presents the results of integrating the PRM into self-evolving training, along with the impact of different $\\\\mathcal{H}$ choices. Continuous Self-Evolving with PRM using $\\\\operatorname{Top-2}$ achieves the best performance in both the ID and OOD tests, with scores of 45.3\\\\% and 59.2\\\\%, respectively. Compared to training without PRM, most instances of self-evolving training with PRM show improved performance, especially in the OOD test. Interestingly, randomly selecting a subset of correct responses actually leads to worse performance than continuous self-evolving, suggesting that even correct answers can be noisy. Random selection may increase the proportion of these noisy samples, undermining the effectiveness of self-evolving training.\\n\\n% In terms of leveraging PRM, we found that using $\\\\operatorname{Top-K}$ with a moderate number of responses\u2014a re-ranking operation to select $\\\\operatorname{K}$ correct responses with the highest-quality intermediate steps\u2014outperforms filtering by a threshold.\\nIn terms of leveraging PRM, we found that using $\\\\operatorname{Top-K}$ to select the a fixed number of best $\\\\operatorname{K}$ responses with high-quality intermediate steps outperforms threshold-based filtering.\\nThe results also highlight the importance of balancing the quality and diversity of sampled responses. Selecting $\\\\operatorname{K}=2$ strikes this balance well, ensuring both response diversity and high-quality reasoning steps for each question. Similar to the results in \\\\S \\\\ref{sec:training_methods}, we also see improvement when involving PRM on smaller models in Table \\\\ref{tab:full_mathvista}, Appendix \\\\ref{ap:full_mathvista_analysis}.\\n\\n\\\\begin{table}[!t]\\n  \\\\centering\\n  \\\\caption{The results of self-evolving training with PRM and different strategies to leverage reward scores. $\\\\mathcal{H}$ is the method to further pick out high-quality responses from the correct rollouts: (1) $\\\\operatorname{Top-k}$ is to select K correct responses with highest rewards, and (2) $>\\\\alpha$ is to pick out the correct responses with rewards larger than $\\\\alpha$.\\n  % \\\\jh{what is Standard? Plz always try to make the table/figures self-contained, like readers can read the table without reading the text in the sections. if Standard is Continuous Self-Evolving, just write Continuous Self-Evolving} \\n  % \\\\jh{we don't call it online RFT}\\n  % \\\\wl{Consider to draw a figure if we have time to do that}\\n  % \\\\fan{first column is redundant?}\\n  Please refer to Table \\\\ref{tab:full_mathvista} to check the full results on all sub-tasks of MathVista.\\n  } \\n  \\\\setlength{\\\\tabcolsep}{2.2pt}\\n    \\\\resizebox{0.48\\\\textwidth}{!}{\\n    \\\\begin{tabular}{lllrrrr}\\n    \\\\toprule\\n    Method & $\\\\mathcal{H}$ & PRM & MathV360K & MathVista \\\\\\\\\\n    \\\\midrule\\n    \\\\makecell{Cont.\\\\\\\\ Self-Evolving} & - & $\\\\times$ &  43.1 & 57.2 \\\\\\\\\\n    \\\\ \\\\ \\\\ \\\\ + Random & $\\\\operatorname{Random-2}$  & $\\\\times$ &  41.0 & 55.5 \\\\\\\\\\n    \\\\midrule\\n    \\\\multirow{4}[0]{*}{\\\\ \\\\ \\\\ \\\\ \\\\makecell{+PRM-based\\\\\\\\ Selection}}  & $> \\\\alpha$  & \\\\multirow{4}[0]{*}{$\\\\checkmark$}& 43.8 & 57.5 \\\\\\\\ \\n     & $\\\\operatorname{Top-1}$ &  & 43.0 & 59.0  \\\\\\\\\\n     &$\\\\operatorname{Top-2}$ &  & \\\\textbf{45.3} & \\\\textbf{59.2} \\\\\\\\\\n     &$\\\\operatorname{Top-4}$ &  & 44.0 & 58.4 \\\\\\\\\\n    \\\\bottomrule\\n    \\\\end{tabular}%\\n    }\\n  \\\\label{tab:sec33_main}%\\n    \\\\vspace{-10pt}\\n\\\\end{table}%\\n\\n\\\\textbf{What makes PRM work for self-evolving training?}\\nTo pursue deeper insights into the role of PRM in self-evolving training, we conduct an analysis presented in Figure~\\\\ref{fig:sec33rm_quality}. Based on the results from \\\\textsection\\\\ref{sec:rm}, we explore PRM\u2019s impact from two key perspectives: (1) Can PRM help the model to select out correct responses among multiple rollouts? (2) How different are the Top 2 and the rest correct solutions re-ranked by reward scores? We use the first checkpoint after warmup $\\\\pi_\\\\theta^0$ as policy model to sample 16 responses for each question in the validation set with temperature=$1.0$ and reveal the behaviors of PRM in these samples. \\n\\nWe evaluate the verification ability of our PRM using two metrics, Best-of-N (BoN) and weighted voting \\\\citep{sun2024easy2hard}, which are commonly employed to assess the performance of reward models. Surprisingly, as shown in Figure~\\\\ref{fig:sec33rm_1}, our PRM underperforms in both metrics. Notably, BoN and weighted voting yield worse results than vanilla majority voting when $N<16$. We speculate that this is due to the lack of high-quality step-level annotations compared to text-only reasoning tasks. These findings suggest that our PRM is not an effective \\\\textbf{verifier}.\\n\\nTo understand why our PRM can still significantly contributes to self-evolving training despite its weaker verification abilities, we analyzed the distribution of other metrics for the top-2 selected responses compared to other correct responses. We approached this from two perspectives: the average number of reasoning steps, and how much a response is directly relevant to the question (see Appendix \\\\ref{app:gpt4orela}), since we do not find incorrect steps but find some irrelevant steps after randomly checking some examples.\\nWe found the responses re-ranked by our PRM generally have \\\\textbf{fewer reasoning steps} (Figure \\\\ref{fig:rm_steps} in Appendix~\\\\ref{ap:dist_steps}) and \\\\textbf{more relevant to the query} (Figure~\\\\ref{fig:sec33rm_3}). This highlights the \\\\textbf{precision} of our PRM in recognizing genuinely high-quality responses. \\n% While it may not rank all correct responses, it consistently selects those that are truly high quality, avoiding false positives. \\nTherefore, our PRM acts as an effective \\\\textbf{reranker} to identify top-quality responses. This is especially critical when responses are already filtered by ground-truth answers, and the ability to accurately assess the quality of reasoning steps beyond accuracy becomes vital.\\n\\n\\\\begin{figure}[!t]  % \u52a0\u5165[htbp]\uff0c\u4f7f\u56fe\u50cf\u66f4\u5bb9\u6613\u6392\u5e03\\n    \\\\centering\\n    \\\\begin{subfigure}[b]{0.23\\\\textwidth}\\n        \\\\centering\\n        \\\\includegraphics[width=1\\\\linewidth]\\n        {figures/sec3.3/MATHV360KEVAL250_temperature-1.0_acc.pdf}\\n        \\\\caption{}\\n        \\\\label{fig:sec33rm_1}\\n    \\\\end{subfigure}\\n    % \\\\begin{subfigure}[b]{0.3\\\\textwidth}  % \u8bbe\u7f6e\u6bcf\u4e2a\u5b50\u56fe\u7684\u5bbd\u5ea6\u4e3a0.32\\\\textwidth\\n    %     \\\\centering\\n    %     \\\\includegraphics[width=0.85\\\\linewidth]{figures/sec3.3/rm_steps.pdf}\\n    %     \\\\caption{}\\n    %     \\\\label{fig:sec33rm_2}\\n    % \\\\end{subfigure}\\n    \\\\begin{subfigure}[b]{0.23\\\\textwidth}  % \u8bbe\u7f6e\u6bcf\u4e2a\u5b50\u56fe\u7684\u5bbd\u5ea6\u4e3a0.32\\\\textwidth\\n        \\\\centering\\n        \\\\includegraphics[width=1\\\\linewidth]{figures/sec3.3/rm_relscore.pdf}\\n        \\\\caption{}\\n        \\\\label{fig:sec33rm_3}\\n    \\\\end{subfigure}\\n    \\\\caption{\\n        \\\\textbf{(a)}: Accuracy on the val. set of greedy decoding and three selection strategy across different numbers of rollouts;\\n        \\\\textbf{(b)}: Average relativity score annotated by GPT4-o of Top 2 and the rest responses re-ranked by rewards, we only calculate on correct ones.\\n        % For distribution of average \\\\# of steps, please refer to Appendix~\\\\ref{ap:dist_steps} \\\\wl{move steps figure to appendix. fixed content in text parts}\\n    }\\n    \\\\label{fig:sec33rm_quality}\\n    \\\\vspace{-15pt}\\n\\\\end{figure}\\n\\n% As shown in Figures~\\\\ref{fig:sec33rm_2} and \\\\ref{fig:sec33rm_3},\\n\\n% \\\\jh{these are cool results! I wonder why we are thinking of scoring relativity rather than correctness like using gpt4-o as PRM (reviewers may wonder too)}\\n\\nIn addition to the aforementioned analysis, we also investigate why leveraging $\\\\alpha$ to filter responses with lower reward scores performs worse than $\\\\operatorname{Top-K}$. The results indicate that, even with the optimal threshold value determined from the validation set, it tends to either retain or filter out all responses for each query, which reduces diversity and makes the learning process more challenging. This further supports the conclusion that \\\\textbf{our PRM performs better as a Reranker than as a Verifier}.\\n\\n% we hypothesize ... According to ..., we find...  here for self-evolving training for multimodal reasoning.\\n\\n% In Figure \\\\ref{fig:sec33rm_quality} we find that our RM is unsuitable to be a verifier, but if we have already known several answers for a questions are correct, we can use the RM as a re-ranker to distinguish them.  \\n\\n% On the other hand, setting a threshold $\\\\theta$ is bad, since in most cases (130/181), the $\\\\theta$ we pick as the optimal one one on the dev set can not distinguish responses that are correct.\\n\\n% The goal of analysis in this section is to provide basic intuition why using RM is better than not, and why reranking is better than theta based.\\n\\n\\n\\n% Table generated by Excel2LaTeX from sheet 'Sheet1'\\n% \\\\begin{table}\\n%   \\\\centering\\n%   \\\\caption{Experiments for Section 3.2 - Why and How to use RM, $\\\\theta^\\\\star$ is selected on the validation set: $\\\\theta^\\\\star=\\\\argmax_{\\\\theta} f_1(\\\\theta, M|D)$}\\n%     \\\\begin{tabular}{llrrrr}\\n%     \\\\toprule\\n%     status & selection & \\\\multicolumn{1}{l}{num per query} & interval & perf & perf id\\\\\\\\\\n%     \\\\midrule\\n%     idle (from 3.1) & - & - & 45k & 57.2 & 43.1 \\\\\\\\\\n%     finished (xiwen)  & random & 2 & & 55.5 & 41.0 \\\\\\\\\\n%     running (mbzui)  & \\\\multirow{4}[0]{*}{rerank} & 1 & & 59.0 & 43.0  \\\\\\\\\\n%     running (mbzui xiwen)  &       & 2 & $45k$ & 59.2 & 45.3 \\\\\\\\\\n%     running (mbzui)  &       & 4 & & 58.4 & 44.0 \\\\\\\\\\n%     idle  & threshold & $>\\\\theta^\\\\star$ & & 57.5 & 43.8\\\\\\\\\\n%     \\\\bottomrule\\n%     \\\\end{tabular}%\\n%   \\\\label{tab:sec33_main}%\\n% \\\\end{table}%\\n% \\\\begin{table}\\n%   \\\\centering\\n%   \\\\caption{Experiments for Section 3.2 - Why and How to use RM, $\\\\theta^\\\\star$ is selected on the validation set: $\\\\theta^\\\\star=\\\\argmax_{\\\\theta} f_1(\\\\theta, M|D)$}\\n%     \\\\begin{tabular}{llrrrr}\\n%     \\\\toprule\\n%     status & selection & \\\\multicolumn{1}{l}{num per query} & interval & perf & perf id\\\\\\\\\\n%     \\\\midrule\\n%     idle (from 3.1) & - & - & 45k & 57.2 & 43.4 \\\\\\\\\\n%     finished (xiwen)  & random & 2 & & 55.5 & 41.0 \\\\\\\\\\n%     running (mbzui)  & \\\\multirow{4}[0]{*}{rerank} & 1 & & 59.0 & 43.0  \\\\\\\\\\n%     running (mbzui xiwen)  &       & 2 & $\\\\boldsymbol{45k}$ & 59.2 & 46.0 \\\\\\\\\\n%     running (mbzui)  &       & 4 & & 58.4 & 45.9 \\\\\\\\\\n%     idle  & threshold & $>\\\\theta^\\\\star$ & & 57.5 & 43.8\\\\\\\\\\n%     \\\\bottomrule\\n%     \\\\end{tabular}%\\n%   \\\\label{tab:addlabel}%\\n% \\\\end{table}%\\n\\n\\n% \\\\begin{table}[htbp]\\n%   \\\\centering\\n%   \\\\caption{Experiments for Section 3.2}\\n%     \\\\begin{tabular}{llllr}\\n%     \\\\toprule\\n%     status &       & ckpt  & optimizer & data per interval \\\\\\\\\\n%     \\\\midrule\\n%     idle  & IRT   & last  & new   & 100\\\\% (180K) \\\\\\\\\\n%     idle  & IRT-plus & last  & last  & 100\\\\%+ (180K+) \\\\\\\\\\n%     idle  & RestEM & first & new   & 100\\\\% (180K) \\\\\\\\\\n%     \\\\midrule\\n%     idle  & \\\\multirow{5}[2]{*}{online RFT} & \\\\multirow{5}[2]{*}{last} & \\\\multirow{5}[2]{*}{last} & 100\\\\% (180K) \\\\\\\\\\n%     idle  &       &       &       & 50\\\\% (90K) \\\\\\\\\\n%     idle  &       &       &       & 25\\\\%  (45K) \\\\\\\\\\n%     idle  &       &       &       & 12.5\\\\% (22K) \\\\\\\\\\n%     idle  &       &       &       & 6.25\\\\% (11K) \\\\\\\\\\n%     \\\\bottomrule\\n%     \\\\end{tabular}%\\n%   \\\\label{tab:addlabel}%\\n% \\\\end{table}%\\n\\n\\n\\n\\n\\n\\\\subsection{Prompt Variation}\\n\\\\label{sec:unlabeled}\\n% \\\\jh{as we discussed on slack, we study threshold and when to add the prompts?}\\nIn this section, we explore how prompt variation affects self-evolving training. There are two primary types of prompts: labeled prompts and unlabeled prompts. Labeled prompts come with annotated ground truth answers, which can be used to filter out incorrect responses during training. In contrast, utilizing unlabeled prompts in self-evolving training is more challenging due to the absence of ground truth annotations. To maintain the quality of unlabeled prompts in training, surrogates like reward scores or pseudo labels must be employed. Meanwhile, unlike labeled prompts, unlabeled prompts are not be trained in SFT period, which increases the difficulty of learning for policy models. \\n\\n\\\\textbf{Skylines: Unlabeled Prompts with Oracle Reward Signals} The coupling of these additional factors introduces complexity, making the effective use of unlabeled prompts less predictable. Therefore we start by establishing a baseline with ``skyline\\\" experiments, where both the unlabeled prompts and their ground truth answers are available but not used during the SFT phase. These unlabeled prompts with oracle reward signals serve as an intermediate difficulty between fully unlabeled and labeled prompts, providing insight into the challenges of training with unlabeled data.\\n\\n\\\\textbf{Unlabeled Prompts} We incorporate unlabeled prompts into self-evolving training. To ensure the quality of sampled responses for these prompts, we use weighted voting to ensemble the predictions from different responses, treating the ensembled prediction as a pseudo label $\\\\Tilde{a}$. This pseudo label is then used to filter out responses with conflicting predictions, ensuring consistency. Following the best practices outlined in \\\\textsection\\\\ref{sec:rm}, we apply PRM as a reranker to select the top-2 responses with the predicted answer $\\\\Tilde{a}$. These unlabeled prompts are then mixed with labeled ones for self-evolving training. Additionally, since learning from unlabeled prompts is more challenging for policy models, we investigate the optimal timing to introduce them into training as well.\\n% to better understand their impact on model performance. \\nWe maintain an interval of 45k prompts and adjust when unlabeled prompts are introduced into the training process. Specifically, we introduce unlabeled prompts after [0\\\\%, 25\\\\%, 50\\\\%, 75\\\\%] of the total training process.\\n% \\\\wl{Its clarity could be improved}\\n\\n\\n\\n\\n\\\\textbf{A Glimpse at Unlabeled Prompts: Potential Efforts to Make Them Effective} Table~\\\\ref{tab:unlabeled} presents the results of incorporating unlabeled prompts with and without oracle reward signals.\\nWhen training relies solely on oracle reward signals without the PRM, continuous self-evolving with unlabeled prompts outperforms standard continuous self-evolving trained only on labeled prompts in the out-of-domain test but underperforms in the in-domain test. \\nThis indicates that additional prompts help the model generalize better to underrepresented questions but also increase the risk of forgetting previously learned information. \\nHowever, after combining with our PRM, all policy models perform worse than our best model trained exclusively on labeled prompts in both benchmarks, even when oracle reward signals are provided. \\nBased on the analysis in \\\\textsection\\\\ref{sec:rm}, this occurs since our PRM is unable to verify responses without ground-truth answers, and its generalization remains a concern.\\n% \\\\begin{wraptable}[16]{r}{6.0cm}\\n\\n% \\\\end{wraptable}%\\n\\n\\\\begin{table}[]\\n  \\\\centering\\n  % \\\\small\\n  % \\\\vspace{-.5mm}\\n  % \\\\setlength{\\\\tabcolsep}{8pt}\\n  \\\\caption{Results of involving unlabeled data. $T_{\\\\texttt{mixin}}$ denotes when to mixin the unlabeled data. The use of PRM follows \\\\S \\\\ref{sec:rm}, except we first get a pesudo ``ground truth\\\" through weighted voting on unlabeled prompts. \\n  % \\\\fan{consider to shorten caption}\\n  % \\\\wl{Just reformat it for now we can change it to a table at right side}\\n  }\\n  % \\\\vspace{-2.3mm}\\n    \\\\resizebox{0.45\\\\textwidth}{!}{\\n    \\\\begin{tabular}{llrrr}\\n    \\\\toprule\\n    Oracle & PRM   & $T_{\\\\texttt{mixin}}$ & MathV360K & MathVista\\\\\\\\\\n    \\\\midrule\\n    -     & $\\\\times$     & - & 43.1 & 57.2 \\\\\\\\\\n    -     & $\\\\checkmark$     & - & 45.3 & 59.2 \\\\\\\\\\n    $\\\\checkmark$     & $\\\\times$     & 0\\\\%   & 42.5 & 58.2 \\\\\\\\\\n    $\\\\checkmark$     & $\\\\checkmark$     & 0\\\\%   & 42.9 & 59.1 \\\\\\\\\\n    \\\\midrule\\n    $\\\\times$     & $\\\\checkmark$     & 0\\\\%   & $43.3$ & $58.2$ \\\\\\\\\\n    $\\\\times$     & $\\\\checkmark$     & 25\\\\% & 42.4 & 57.6 \\\\\\\\\\n    $\\\\times$     & $\\\\checkmark$     & 50\\\\%  & 42.9 & 58.2 \\\\\\\\\\n    $\\\\times$     & $\\\\checkmark$     & 75\\\\% & 45.0  & 58.8 \\\\\\\\\\n    \\\\bottomrule\\n    \\\\end{tabular}\\n    }\\n    \\\\label{tab:unlabeled}%\\n    % \\\\vspace{-20pt}\\n\\\\end{table}%\\n\\n\\\\begin{figure}\\n    \\\\centering % \u56fe\u7247\u5c45\u4e2d\\n    \\\\includegraphics[width=1\\\\linewidth]{figures//sec4/progress_or_regress_icml.pdf} % \u63d2\u5165\u56fe\u7247\uff0c\u5bbd\u5ea65cm\uff0c\u66ff\u6362\u4e3a\u5b9e\u9645\u56fe\u7247\u8def\u5f84\\n    \\\\caption{Opposite trend of Greedy Accuracy and Pass@K.} % \u56fe\u7247\u6807\u9898\\n    \\\\label{fig:sec4dynamics_1}\\n    \\\\vspace{-15pt}\\n\\\\end{figure}\\n\\nWhen examining the timing for introducing unlabeled prompts, we find that adding them from the beginning helps mitigate the negative impact on model performance, compared to introducing them in midway. However, when unlabeled prompts are introduced later in the training process, they participate less in the overall training, leading to better results simply due to their limited involvement. This suggests that, without sufficient surrogate supervision (e.g., precise reward signals), introducing unlabeled prompts into self-evolving training can harm the process, potentially causing a deviation in the policy model's distribution.\\n\\n% skyline: we use unlabeled queries as labeled queries (w/ RM and w/o RM) (1:1, 2:1)\\n\\n% by default, we mix with an intuitive ratio in the beginning (optionally we can try add since the half)\\n\\n% We don't change the interval. \\n\\n% After this section, we have a temporary recipe for both w/ and w/o unlabeled data. \\n\\n% \\\\begin{itemize}\\n%     \\\\item Other datasets: Oracle, PRM\\n%     \\\\item Synthetic datasets\\n% \\\\end{itemize}\\n% Table generated by Excel2LaTeX from sheet 'Sheet1'\\n% \\\\begin{wraptable}{t}{8cm}\\n% \\\\begin{table}[t!]\\n%   \\\\centering\\n%   \\\\caption{Results on the involvement of unlabeled data. $T_{mixin}$ denotes the starting point to mixin the unlabeled data into the labeled data. The use PRM is identical to \\\\S \\\\ref{sec:rm}, except we first get a pesudo ``groundtruth\\\" answer through weighted voting on unlabeled prompts. \\\\wl{Just reformat it for now we can change it to a table at right side}}\\n%     \\\\begin{tabular}{llrrr}\\n%     \\\\toprule\\n%     Oracle & PRM   & $T_{mixin}$ & MathVista & MathV360K\\\\\\\\\\n%     \\\\midrule\\n%     -     & $\\\\times$     & - & 57.2 & 43.1 \\\\\\\\\\n%      -     & $\\\\checkmark$     & - & 59.2 & 45.3 \\\\\\\\\\n%     $\\\\checkmark$     & $\\\\times$     & 0\\\\%   & 58.2 & 42.5 \\\\\\\\\\n%     $\\\\checkmark$     & $\\\\checkmark$     & 0\\\\%   & 59.1 & 42.9 \\\\\\\\\\n%     \\\\midrule\\n%     $\\\\times$     & $\\\\checkmark$     & 0\\\\%   & $58.2$ & $43.3$ \\\\\\\\\\n%     $\\\\times$     & $\\\\checkmark$     & 25\\\\% & 57.6 & 42.4 \\\\\\\\\\n%     $\\\\times$     & $\\\\checkmark$     & 50\\\\%  & 58.2 & 42.9 \\\\\\\\\\n%     $\\\\times$     & $\\\\checkmark$     & 75\\\\% & 58.8  & 45.0 \\\\\\\\\\n%     % \\\\midrule\\n%     % (placeholder for synthetic labeled data) & & & & \\\\\\\\\\n%     \\\\bottomrule\\n%     \\\\end{tabular}%\\n%   \\\\label{tab:unlabeled}%\\n% \\\\end{table}\\n\\n% \\\\begin{table}[htbp]\\n%   \\\\centering\\n%   \\\\caption{Experiments for Section 3.3 - The involvement of unlabeled data}\\n%     \\\\begin{tabular}{lllrr}\\n%     \\\\toprule\\n%     status & oracle & prm   & involve unlabeled at & perf \\\\\\\\\\n%     \\\\midrule\\n%     idle (from 3.1) & -     & $\\\\times$     & - & 57.2 \\\\\\\\\\n%     idle (from 3.2) & -     & $\\\\checkmark$     & - & 59.4 \\\\\\\\\\n%     running (mbzui)  & $\\\\checkmark$     & $\\\\times$     & 0\\\\%   & 58.0 \\\\\\\\\\n%     running (mbzui)  & $\\\\checkmark$     & $\\\\checkmark$     & 0\\\\%   & $\\\\geq 59.1$ \\\\\\\\\\n%     \\\\midrule\\n%     running (mbzui)  & $\\\\times$     & $\\\\checkmark$     & 0\\\\%   &  \\\\\\\\\\n%     running (mbzui)  & $\\\\times$     & $\\\\checkmark$     & 25\\\\% &  \\\\\\\\\\n%     running (mbzui)  & $\\\\times$     & $\\\\checkmark$     & 50\\\\%  &  \\\\\\\\\\n%     idle  & $\\\\times$     & $\\\\checkmark$     & 75\\\\%  &  \\\\\\\\\\n%     \\\\midrule\\n%     (placeholder for synthetic labeled data) & & & & \\\\\\\\\\n%     \\\\bottomrule\\n%     \\\\end{tabular}%\\n%   \\\\label{tab:addlabel}%\\n% \\\\end{table}%\\n\\n\\n\\\\section{Dynamics of Self-Evolution \\\\& Final Recipe}\\n\\\\label{sec:dynamics}\\nSo far, we have explored the impact of three pivotal factors within our design space, leading to established best practices for learning multimodal reasoning -- we adopt continuous self-evolving training coupled with a reward model to help data selection as described in \\\\textsection\\\\ref{sec:rm}, and we perform the training process on SFT datasets with final answer annotations. \\nIn this section, we delve even deeper into the current self-evolution strategy to better understand the bottlenecks.\\nInstead of analyzing from a design space perspective as previously, we now fix the design parameters and focus exclusively on the training dynamics during the model\u2019s self-evolution. This shift in focus allows us to examine the process from an orthogonal angle, providing further insights into the underlying mechanisms that drive or impede progress in multimodal reasoning capabilities.\\n\\n\\n\\n\\n\\\\subsection{Monitoring the Training Dynamics}\\n\\\\label{sec:monitor}\\nIntuitively, two critical conditions must be met for the success of self-evolving training: (1) the presence of high-quality candidate responses generated by the model, otherwise self-evolving will not work no matter how strong the reward is; \\n% For example, the model must be able to sample correct responses for the queries that the model cannot answer correctly with greedy decoding; \\nand (2) the reward function's ability to effectively distinguish and prioritize these high-quality responses. These conditions align with the traditional reinforcement learning concepts of \\\\emph{exploration} and \\\\emph{exploitation}.\\nApparently, both exploration and exploitation capabilities are dynamic targets in self-evolving training, as the policy model evolves and the distribution of rollout responses changes with each iteration. To better understand these training dynamics, we propose tracking and visualizing three metrics, where we introduce a novel metric, Reward-Pass@2, to monitor the exploration-expolitation trade-offs:\\n% \\\\wl{passk - greedy has been removed}\\n% \\\\fan{consider make the left indent smaller to save some line space}\\n\\\\begin{itemize}[leftmargin=*]\\n\\n\\\\item \\\\emph{Greedy Accuracy}: the model's accuracy with greedy decoding. We track this metric for reference to compare with other metrics.\\n\\n\\\\item \\\\emph{Pass@K Accuracy}:  the percentage of samples for which the model produces at least one correct response when sampling $K$ candidates. This metric measures the model's exploration ability.\\n\\n    % \\\\item \\\\emph{(Pass@K - Greedy) Accuracy}: the difference between Pass@K and Greedy accuracy. Typically, Pass@K is an upper bound of Greedy Accuracy, and the gap roughly reflects the percentage of samples where the model, while failing in greedy decoding, can generate a correct response when sampling more candidates. This gap is crucial for the success of self-evolving training\u2014a zero gap indicates that the model fails to explore correct responses for the current failure cases, suggesting that further training is unlikely to yield significant improvement.\\n    \\n\\\\item \\\\emph{Reward-Pass@2}: the ratio (\\\\%) of samples for which there exist correct responses among the top 2 responses ranked by the reward model. This metric directly reflects the exploitation efficacy of the reward model for the current policy. We choose Pass@2 since our training strategy involves selecting the top 2 responses using the reward model (\\\\textsection\\\\ref{sec:rm}).\\n\\\\end{itemize}\\n\\n% \\\\footnote{We note that there is a slight mismatch between this metric and our training strategy as we use the ground-truth answer to pre-filter responses before the reward model reranks, and a more aligned metric to evaluate reward model should measure the reasoning quality of the top 2 responses both of which have correct answers. \\n%     Given that there is no reliably way to score the quality of the thought processes, we regard Reward-Pass@2 as a reasonable approximation.\\n%     }\\n\\n% In the self-evolution of (M)LLMs, both previous works and the discussions so far in this paper have focused on the \\\\textbf{exploitation} side, such as the use of RM as a Re-Ranker in Section \\\\ref{sec:rm}. However, we believe that encouraging \\\\textbf{exploration} is equally important, as exemplified by strategies in reinforcement learning like the $\\\\epsilon$-greedy method \\\\citep{sutton1999reinforcement}, Boltzmann distribution \\\\citep{cesa2006prediction}, or Upper Confidence Bound (UCB) \\\\citep{auer2002finite}, which aim to try out new outputs and gather new feedback. Therefore, in this section, we investigate \\\\textbf{the dynamics of self-evolution}. For simplicity, we will limit our analysis to a single training run under the current optimal strategy, i.e., the online RFT, 25\\\\% query interval on labeled prompts, and with RM as re-ranker (Exp xx in Section \\\\ref ...\n\n(The output of the tool call (shortuuid identifier: QrfFejKEMkcGCKkZ4MrZQY) is too long! Only the first 100000 characters are shown here. The original output length is 170849 characters. The full output has been saved to the file /workspace/dumps/workspace/.overlong_tool_outputs/QrfFejKEMkcGCKkZ4MrZQY.json. Please check this file carefully, as it may be very long!)",
            "tool_call_id": "toolu_017oApPCaVCkAyybuKMZX7CP"
        },
        {
            "role": "assistant",
            "content": "Now I can see the box style used in M-STAR. Let me search for the specific prompt box definitions in the M-STAR paper:",
            "tool_calls": [
                {
                    "id": "toolu_015MW3F46eq1UjaB4WKRfPtH",
                    "type": "function",
                    "function": {
                        "name": "local-search_overlong_tooloutput",
                        "arguments": "{\"shortuuid\": \"QrfFejKEMkcGCKkZ4MrZQY\", \"pattern\": \"\\\\\\\\\\\\\\\\begin\\\\\\\\{tcolorbox\\\\\\\\}|tcolorbox|\\\\\\\\\\\\\\\\colorbox|\\\\\\\\\\\\\\\\fbox\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "Search Results in QrfFejKEMkcGCKkZ4MrZQY (Page 1/1)\nPattern: '\\\\\\\\begin\\\\{tcolorbox\\\\}|tcolorbox|\\\\\\\\colorbox|\\\\\\\\fbox' | Total matches: 6 | File size: 170849 chars\nSearch Session ID: 159f32c6\n================================================================================\n\nMatch 1 (Line ~1, Pos 1958-1967):\n------------------------------------------------------------\ne}\\n\\\\usepackage{dashrule}\\n\\\\usepackage{wrapfig}\\n\\\\usepackage{subcaption}\\n\\\\usepackage{tikz}\\n\\\\usetikzlibrary{tikzmark}\\n\\\\usepackage{enumitem}\\n\\\\usepackage{multirow}\\n\\\\usepackage{dsfont}\\n\\\\usepackage{amsmath}\\n\\n\\\\usepackage{graphicx}      % \u63d2\u5165\u56fe\u7247\\n\\\\usepackage{subcaption}    % \u5904\u7406\u5b50\u56fe\\n\\\\usepackage{float}         % \u6d6e\u52a8\u4f53\\n\\\\usepackage{stfloats}      % \u589e\u5f3a\u6d6e\u52a8\\n\\\\usepackage{makecell}\\n\\\\usepackage{subcaption}\\n\\n\\\\usepackage{wrapfig}\\n\\n\\\\usepackage{amssymb}\\n\\\\usepackage{bbding}\\n\\n\\\\usepackage{>>>tcolorbox<<<}\\n\\\\usepackage{mathrsfs}\\n\\n\\\\usepackage{adjustbox}\\n\\\\usepackage{colortbl} % \u6dfb\u52a0\u8868\u683c\u80cc\u666f\u989c\u8272\u652f\u6301\\n% \\\\usepackage[table,xcdraw]{xcolor} % \u5141\u8bb8\u5b9a\u4e49\u66f4\u591a\u989c\u8272\\n\\\\PassOptionsToPackage{prologue,dvipsnames}{xcolor}\\n\\\\definecolor{lightblue}{rgb}{0.88, 0.92, 1} % \u5b9a\u4e49\u6de1\u84dd\u8272\\n\\n\\n\\\\usepackage{pgfmath} % \u7528\u4e8e\u6570\u5b66\u8ba1\u7b97\\n\\\\usepackage{amssymb} % \u4f7f\u7528\u7bad\u5934\u7b26\u53f7\\n\\\\usepackage{ifthen} % \u6761\u4ef6\u5224\u65ad\\n\\\\usepackage{booktabs} % \u7f8e\u5316\u8868\u683c\u7ebf\u6761\\n\\\\usepackage{adjustbox} % \u8868\u683c\u8c03\u6574\\n\\\\usepackage{array} % \u589e\u5f3a\u8868\u683c\u529f\u80fd\\n\\n% if you use cleveref..\\n\\\\usepackage[capitalize,noabbrev]{\n\nMatch 2 (Line ~1, Pos 3189-3198):\n------------------------------------------------------------\nnewtheorem{corollary}[theorem]{Corollary}\\n\\\\newtheorem{definition}[theorem]{Definition}\\n\\\\newtheorem{assumption}[theorem]{Assumption}\\n\\\\newtheorem{remark}[theorem]{Remark}\\n\\n% Todonotes is useful during development; simply uncomment the next line\\n%    and comment out the line below the next line to turn off comments\\n%\\\\usepackage[disable,textsize=tiny]{todonotes}\\n\\\\usepackage[textsize=tiny]{todonotes}\\n\\n\\\\usepackage{wrapfig}\\n\\n\\\\usepackage{amssymb}\\n\\\\usepackage{bbding}\\n\\n\\\\usepackage{>>>tcolorbox<<<}\\n\\\\usepackage{mathrsfs}\\n\\n\\\\usepackage{adjustbox}\\n\\\\usepackage{colortbl} % \u6dfb\u52a0\u8868\u683c\u80cc\u666f\u989c\u8272\u652f\u6301\\n% \\\\usepackage[table,xcdraw]{xcolor} % \u5141\u8bb8\u5b9a\u4e49\u66f4\u591a\u989c\u8272\\n\\\\PassOptionsToPackage{prologue,dvipsnames}{xcolor}\\n\\\\definecolor{lightblue}{rgb}{0.88, 0.92, 1} % \u5b9a\u4e49\u6de1\u84dd\u8272\\n\\n\\n\\\\usepackage{pgfmath} % \u7528\u4e8e\u6570\u5b66\u8ba1\u7b97\\n\\\\usepackage{amssymb} % \u4f7f\u7528\u7bad\u5934\u7b26\u53f7\\n\\\\usepackage{ifthen} % \u6761\u4ef6\u5224\u65ad\\n\\\\usepackage{booktabs} % \u7f8e\u5316\u8868\u683c\u7ebf\u6761\\n\\\\usepackage{adjustbox} % \u8868\u683c\u8c03\u6574\\n\\\\usepackage{array} % \u589e\u5f3a\u8868\u683c\u529f\u80fd\\n\\n% \u5b9a\u4e49\u66f4\u6df1\u7684\u7ea2\u8272\u548c\u7eff\u8272\\n\\\\definecolor{DeepGreen}{RGB}{0,100,0}\\n\\\\def\n\nMatch 3 (Line ~1, Pos 142799-142808):\n------------------------------------------------------------\nompting the model to answer questions directly, we prompt it to generate intermediate reasoning steps for a given triplet (question, image, and answer) using the following instruction:\\n\\n% Since our base model typically outputs the answer directly when responding to multimodal reasoning questions, during the warmup phase, we added additional instructions along with the input question, requiring the model to output the rationale. The instructions used in this process are as follows: \\n\\n\\\\begin{>>>tcolorbox<<<}[\\ncolback=lightProxYellow!10,\\ncolframe=lightProxYellow,\\nleft=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Extra instruction to guide CoT}}]\\n\\n\\\\begin{small}\\nOffer a comprehensive breakdown of your analytical process, detailing each step, the reasoning behind your decisions, and how you integrated various pieces of information, and put your answer at the end.\\n\\\\end{small}\\n\\n\\\\end{tcolorbox}\\n\\nFor each triplet, we ask models to rollout 16 samples with $\\\\text{temperature}=1.0$. We the\n\nMatch 4 (Line ~1, Pos 143202-143211):\n------------------------------------------------------------\nmodel to output the rationale. The instructions used in this process are as follows: \\n\\n\\\\begin{tcolorbox}[\\ncolback=lightProxYellow!10,\\ncolframe=lightProxYellow,\\nleft=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Extra instruction to guide CoT}}]\\n\\n\\\\begin{small}\\nOffer a comprehensive breakdown of your analytical process, detailing each step, the reasoning behind your decisions, and how you integrated various pieces of information, and put your answer at the end.\\n\\\\end{small}\\n\\n\\\\end{>>>tcolorbox<<<}\\n\\nFor each triplet, we ask models to rollout 16 samples with $\\\\text{temperature}=1.0$. We then filter out results where the final answers do not match the ground truth and sample 100K from the generated dataset to \\n% avoid overfitting, thus \\ncreate a warm-up CoT dataset $\\\\mathcal{D}_w$ with correct answers. \\nFinally, we fine-tune our models on this dataset, treating it as a standard RFT process. \\nOur iterative self-evolving training process will then start from this model checkpoint aft\n\nMatch 5 (Line ~1, Pos 146276-146285):\n------------------------------------------------------------\nal to the step end token. In experiments we freeze the visual encoder as we find it brings a slight improvement. \\n\\n\\n\\n\\n\\n\\\\section{Measuring Response Relativity}\\n\\\\label{app:gpt4orela}\\nTo get a comprehensive understanding of how our PRM works as a re-ranker, we conduct a quantitative analysis using GPT4-o (\\\\texttt{gpt-4o-2024-08-06}) to see how much a correct response is directly related to the query, e.g., does not contain irrelvant steps. The prompt we use is as follows:\\n\\n\\n\\n\\\\begin{>>>tcolorbox<<<}[\\ncolback=lightProxYellow!10,\\ncolframe=lightProxYellow,\\nleft=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Prompt for GPT4-o to annotate the relativity score}}]\\n\\n\\\\begin{small}\\nGiven the image and a related question, you need to judge how a candidate solution is directly related to the question.\\nYou need to consider all its steps, and return a final value bewteen 1-10 as a overall score.\\n\\nConclude your judgement at the end as \\\"So the relativity score is X\\\" where X is the score you\n\nMatch 6 (Line ~1, Pos 146892-146901):\n------------------------------------------------------------\ntextbf{Prompt for GPT4-o to annotate the relativity score}}]\\n\\n\\\\begin{small}\\nGiven the image and a related question, you need to judge how a candidate solution is directly related to the question.\\nYou need to consider all its steps, and return a final value bewteen 1-10 as a overall score.\\n\\nConclude your judgement at the end as \\\"So the relativity score is X\\\" where X is the score you give.\\n\\n\\\\\\n\\n[Question]\\n\\n\\\\{question\\\\}\\n\\n\\\\ \\n\\n[Solution]\\n\\n\\\\{solution\\\\}\\n\\\\end{small}\\n\\n\\\\end{>>>tcolorbox<<<}\\n\\n\\\\begin{wrapfigure}[14]{r}{5cm}\\n    \\\\vspace{-5.0mm}\\n    \\\\centering % \u56fe\u7247\u5c45\u4e2d\\n    \\\\includegraphics[width=1\\\\linewidth]{figures/sec3.3/rm_steps.pdf} % \u63d2\u5165\u56fe\u7247\uff0c\u5bbd\u5ea65cm\uff0c\u66ff\u6362\u4e3a\u5b9e\u9645\u56fe\u7247\u8def\u5f84\\n    \\\\vspace{-5.5mm}\\n    \\\\caption{Number of reasoning steps for top-2 solutions selected by our reward model and the rest solutions.\\n    } % \u56fe\u7247\u6807\u9898\\n    % \\\\vspace{mm}\\n    \\\\label{fig:rm_steps}\\n\\\\end{wrapfigure}\\n    % \\\\vspace{-5.5mm}\\n\\n\\\\section{Extra Comparison between RM-selected Solutions and Others}\\n\\\\label{a\n\nUse search_session_id '159f32c6' with search_navigate tool for pagination\nAvailable commands: next_page, prev_page, jump_to_page, first_page, last_page",
            "tool_call_id": "toolu_015MW3F46eq1UjaB4WKRfPtH"
        },
        {
            "role": "assistant",
            "content": "Great! I can see the box style used in M-STAR. It uses `tcolorbox` with specific styling. Now let me find the prompts in the codebase. Let me search for prompts in the simpleRL-reason directory:",
            "tool_calls": [
                {
                    "id": "toolu_01D2F9JvPCTdjiFN72rZL618",
                    "type": "function",
                    "function": {
                        "name": "filesystem-search_files",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace/simpleRL-reason\", \"pattern\": \"prompt\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"No matches found\",\"annotations\":null}",
            "tool_call_id": "toolu_01D2F9JvPCTdjiFN72rZL618"
        },
        {
            "role": "assistant",
            "content": "Let me search more broadly for files containing prompt information:",
            "tool_calls": [
                {
                    "id": "toolu_012KKxbtsm3LN9K1GpNBzcQr",
                    "type": "function",
                    "function": {
                        "name": "terminal-run_command",
                        "arguments": "{\"command\": \"find /workspace/dumps/workspace/simpleRL-reason -type f -name \\\"*.py\\\" | head -10\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "[{\"type\": \"text\", \"text\": \"/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/trajectory.py\\n/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/grader.py\\n/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/sh/collect_results.py\\n/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/visualizer.py\\n/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/utils.py\\n/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/math_utils.py\\n/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/latex2sympy/gen/__init__.py\\n/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/latex2sympy/gen/PSParser.py\\n/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/latex2sympy/gen/PSLexer.py\\n/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/latex2sympy/gen/PSListener.py\\n\", \"annotations\": null}, {\"type\": \"text\", \"text\": \"\\nCommand completed with return code: 0\", \"annotations\": null}]",
            "tool_call_id": "toolu_012KKxbtsm3LN9K1GpNBzcQr"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01JHY9YruGxc4d9Zc9qucZZD",
                    "type": "function",
                    "function": {
                        "name": "terminal-run_command",
                        "arguments": "{\"command\": \"grep -r \\\"simple.*prompt\\\\|complex.*prompt\\\" /workspace/dumps/workspace/simpleRL-reason --include=\\\"*.py\\\" | head -5\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Security violation: Invalid command part 'grep -r \\\"simple.*prompt\\\\': Invalid command format: No escaped character\",\"annotations\":null,\"error\":true}",
            "tool_call_id": "toolu_01JHY9YruGxc4d9Zc9qucZZD"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01XNBmAgEHVLboCyVNFWq7w1",
                    "type": "function",
                    "function": {
                        "name": "terminal-run_command",
                        "arguments": "{\"command\": \"grep -r \\\"simple.*prompt\\\" /workspace/dumps/workspace/simpleRL-reason --include=\\\"*.py\\\"\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "[{\"type\": \"text\", \"text\": \"/workspace/dumps/workspace/simpleRL-reason/tests/e2e/arithmetic_sequence/data/create_dataset.py:    all_prompts = simple_task.get_all_prompts()\\n\", \"annotations\": null}, {\"type\": \"text\", \"text\": \"\\nCommand completed with return code: 0\", \"annotations\": null}]",
            "tool_call_id": "toolu_01XNBmAgEHVLboCyVNFWq7w1"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01TcJeqVm62TgesTNFTk1Cds",
                    "type": "function",
                    "function": {
                        "name": "terminal-run_command",
                        "arguments": "{\"command\": \"grep -r \\\"complex.*prompt\\\" /workspace/dumps/workspace/simpleRL-reason --include=\\\"*.py\\\"\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"\\nCommand completed with return code: 1\",\"annotations\":null}",
            "tool_call_id": "toolu_01TcJeqVm62TgesTNFTk1Cds"
        },
        {
            "role": "assistant",
            "content": "Let me search for prompt-related files more broadly:",
            "tool_calls": [
                {
                    "id": "toolu_01Xxxv7wNXtDuV9iuj4CEfX6",
                    "type": "function",
                    "function": {
                        "name": "terminal-run_command",
                        "arguments": "{\"command\": \"find /workspace/dumps/workspace/simpleRL-reason -type f -name \\\"*prompt*\\\"\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"\\nCommand completed with return code: 0\",\"annotations\":null}",
            "tool_call_id": "toolu_01Xxxv7wNXtDuV9iuj4CEfX6"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_011qN846RjYB2D2pogLMZpuD",
                    "type": "function",
                    "function": {
                        "name": "terminal-run_command",
                        "arguments": "{\"command\": \"grep -r \\\"step.*by.*step\\\" /workspace/dumps/workspace/simpleRL-reason --include=\\\"*.py\\\" | head -5\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "[{\"type\": \"text\", \"text\": \"/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/utils.py:        \\\"### Instruction:\\\\n{input}\\\\n\\\\n### Response: Let's think step by step.\\\",\\n/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/utils.py:        \\\"User: {input}\\\\nPlease reason step by step, \\\"\\n/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/utils.py:        \\\"User: Please reason step by step and put your final answer at the end \\\"\\n/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/utils.py:        \\\"Question:\\\\n{input}\\\\nAnswer:\\\\nLet's think step by step.\\\\n\\\",\\n/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/utils.py:        \\\"<|im_start|>user\\\\n{input}\\\\nPlease reason step by step, and put your final answer within \\\\\\\\boxed{{}}.<|im_end|>\\\\n\\\"\\n\", \"annotations\": null}, {\"type\": \"text\", \"text\": \"\\nCommand completed with return code: 0\", \"annotations\": null}]",
            "tool_call_id": "toolu_011qN846RjYB2D2pogLMZpuD"
        },
        {
            "role": "assistant",
            "content": "Perfect! Let me examine the utils.py file which seems to contain the prompts:",
            "tool_calls": [
                {
                    "id": "toolu_016euwRY9LgNxJPWfTPFMhY2",
                    "type": "function",
                    "function": {
                        "name": "filesystem-read_file",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/utils.py\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"import os\\nimport json\\nimport random\\nimport json\\nimport os\\nimport numpy as np\\nfrom pathlib import Path\\nfrom typing import Iterable, Union, Any\\n\\nfrom examples import get_examples\\n\\n\\ndef set_seed(seed: int = 42) -> None:\\n    np.random.seed(seed)\\n    random.seed(seed)\\n    os.environ[\\\"PYTHONHASHSEED\\\"] = str(seed)\\n    print(f\\\"Random seed set as {seed}\\\")\\n\\n\\ndef load_jsonl(file: Union[str, Path]) -> Iterable[Any]:\\n    with open(file, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n        for line in f:\\n            try:\\n                yield json.loads(line)\\n            except:\\n                print(\\\"Error in loading:\\\", line)\\n                exit()\\n\\n\\ndef save_jsonl(samples, save_path):\\n    # ensure path\\n    folder = os.path.dirname(save_path)\\n    os.makedirs(folder, exist_ok=True)\\n\\n    with open(save_path, \\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n        for sample in samples:\\n            f.write(json.dumps(sample, ensure_ascii=False) + \\\"\\\\n\\\")\\n    print(\\\"Saved to\\\", save_path)\\n\\n\\ndef lower_keys(example):\\n    new_example = {}\\n    for key, value in example.items():\\n        if key != key.lower():\\n            new_key = key.lower()\\n            new_example[new_key] = value\\n        else:\\n            new_example[key] = value\\n    return new_example\\n\\n\\nEXAMPLES = get_examples()\\n\\n\\ndef load_prompt(data_name, prompt_type, num_shots):\\n    if not num_shots:\\n        return []\\n\\n    if data_name in [\\\"gsm_hard\\\", \\\"svamp\\\", \\\"tabmwp\\\", \\\"asdiv\\\", \\\"mawps\\\"]:\\n        data_name = \\\"gsm8k\\\"\\n    if data_name in [\\\"math_oai\\\", \\\"hungarian_exam\\\", \\\"math-oai\\\", \\\"aime24\\\", \\\"amc23\\\"]:\\n        data_name = \\\"math\\\"\\n    if data_name in [\\\"sat_math\\\"]:\\n        data_name = \\\"mmlu_stem\\\"\\n    if data_name in [\\n        \\\"gaokao2024_I\\\",\\n        \\\"gaokao2024_II\\\",\\n        \\\"gaokao_math_qa\\\",\\n        \\\"gaokao2024_mix\\\",\\n        \\\"cn_middle_school\\\",\\n    ]:\\n        data_name = \\\"gaokao\\\"\\n\\n    if prompt_type in [\\\"tool-integrated\\\"]:\\n        prompt_type = \\\"tora\\\"\\n\\n    return EXAMPLES[data_name][:num_shots]\\n\\n\\nPROMPT_TEMPLATES = {\\n    \\\"direct\\\": (\\\"Question: {input}\\\\nAnswer: \\\", \\\"{output}\\\", \\\"\\\\n\\\\n\\\"),\\n    \\\"cot\\\": (\\\"Question: {input}\\\\nAnswer: \\\", \\\"{output}\\\", \\\"\\\\n\\\\n\\\\n\\\"),\\n    \\\"pal\\\": (\\\"Question: {input}\\\\n\\\\n\\\", \\\"{output}\\\", \\\"\\\\n---\\\\n\\\"),\\n    \\\"tool-integrated\\\": (\\\"Question: {input}\\\\n\\\\nSolution:\\\\n\\\", \\\"{output}\\\", \\\"\\\\n---\\\\n\\\"),\\n    \\\"self-instruct\\\": (\\\"<|user|>\\\\n{input}\\\\n<|assistant|>\\\\n\\\", \\\"{output}\\\", \\\"\\\\n\\\"),\\n    \\\"tora\\\": (\\\"<|user|>\\\\n{input}\\\\n<|assistant|>\\\\n\\\", \\\"{output}\\\", \\\"\\\\n\\\"),\\n    \\\"wizard_zs\\\": (\\n        \\\"### Instruction:\\\\n{input}\\\\n\\\\n### Response: Let's think step by step.\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\\n\\\",\\n    ),\\n    \\\"platypus_fs\\\": (\\n        \\\"### Instruction:\\\\n{input}\\\\n\\\\n### Response:\\\\n\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\\n\\\",\\n    ),\\n    \\\"deepseek-math\\\": (\\n        \\\"User: {input}\\\\nPlease reason step by step, \\\"\\n        \\\"and put your final answer within \\\\\\\\boxed{{}}.\\\\n\\\\nAssistant:\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\\n\\\",\\n    ),\\n    \\\"kpmath\\\": (\\n        \\\"User: Please reason step by step and put your final answer at the end \\\"\\n        'with \\\"The answer is: \\\".\\\\n\\\\n{input}\\\\n\\\\nAssistant:',\\n        \\\"{output}\\\",\\n    ),\\n    \\\"jiuzhang\\\": (\\n        \\\"## Question\\\\n{input}\\\\n\\\\n## Solution\\\\n\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\\n\\\",\\n    ),\\n    \\\"jiuzhang_tora\\\": (\\n        \\\"## Question\\\\n{input}\\\\n\\\\n## Code Solution\\\\n\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\\n\\\",\\n    ),\\n    \\\"jiuzhang_nl\\\": (\\n        \\\"## Question\\\\n{input}\\\\n\\\\n## Natural Language Solution\\\\n\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\\n\\\",\\n    ),\\n    \\\"mmiqc\\\": (\\n        'Please solve the following problem and put your answer at the end with \\\"The answer is: \\\".\\\\n\\\\n{input}\\\\n\\\\n',\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\\n\\\",\\n    ),\\n    \\\"abel\\\": (\\n        \\\"Question:\\\\n{input}\\\\nAnswer:\\\\nLet's think step by step.\\\\n\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\",\\n    ),\\n    \\\"shepherd\\\": (\\\"{input}\\\\n\\\", \\\"{output}\\\", \\\"\\\\n\\\\n\\\\n\\\"),\\n    \\\"qwen-boxed\\\": (\\n        \\\"<|im_start|>system\\\\nYou are a helpful assistant.<|im_end|>\\\\n\\\"\\n        \\\"<|im_start|>user\\\\n{input}\\\\nPlease reason step by step, and put your final answer within \\\\\\\\boxed{{}}.<|im_end|>\\\\n\\\"\\n        \\\"<|im_start|>assistant\\\\n\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\",\\n    ),\\n    \\\"qwen25-math-cot\\\": (\\n        \\\"<|im_start|>system\\\\nPlease reason step by step, and put your final answer within \\\\\\\\boxed{{}}.<|im_end|>\\\\n\\\"\\n        \\\"<|im_start|>user\\\\n{input}<|im_end|>\\\\n\\\"\\n        \\\"<|im_start|>assistant\\\\n\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\",\\n    ),\\n    \\\"mathstral\\\": (\\n        \\\"{input}\\\\nPlease reason step by step, and put your final answer within \\\\\\\\boxed{{}}.\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\",\\n    ),\\n    \\\"internlm-math-fs\\\": (\\\"Question:{input}\\\\nAnswer:\\\", \\\"{output}\\\", \\\"\\\\n\\\"),\\n    \\\"internlm-math-chat\\\": (\\n        \\\"<|im_start|>user\\\\n{input}<|im_end|>\\\\n\\\" \\\"<|im_start|>assistant\\\\n\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\",\\n    ),\\n    \\\"mistral\\\": (\\n        \\\"[INST] {input}[/INST]\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\",\\n    ),\\n    \\\"numina\\\": (\\\"### Problem: {input}\\\\n### Solution:\\\", \\\" {output}\\\", \\\"\\\\n\\\\n\\\"),\\n    \\\"o1_cot\\\": (\\n        '[Round 0] USER:\\\\n{input}\\\\nPlease reason step by step, and put your final answer within \\\\\\\\boxed{{}}. ASSISTANT:\\\\n',\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\"\\n    ),\\n    \\\"deepseek-r1\\\": (\\n        'A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: {input}. Assistant:',\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\"\\n    ),\\n}\\n\\n\\ndef construct_prompt(example, data_name, args):\\n    if args.adapt_few_shot and data_name in [\\n        \\\"gaokao2024_I\\\",\\n        \\\"gaokao2024_II\\\",\\n        \\\"gaokao_math_qa\\\",\\n        \\\"gaokao2024_mix\\\",\\n        \\\"cn_middle_school\\\",\\n    ]:\\n        demos = load_prompt(data_name, args.prompt_type, 5)\\n    else:\\n        demos = load_prompt(data_name, args.prompt_type, args.num_shots)\\n    prompt_type = args.prompt_type\\n    if prompt_type == \\\"platypus_fs\\\":\\n        prompt_type = \\\"cot\\\"\\n    if prompt_type == \\\"tool-integrated\\\":\\n        prompt_type = \\\"tora\\\"\\n\\n    prompt_temp = PROMPT_TEMPLATES[args.prompt_type]\\n\\n    splitter = prompt_temp[2]\\n    input_template, output_template, splitter = (\\n        prompt_temp[0],\\n        prompt_temp[1],\\n        prompt_temp[2],\\n    )\\n    if args.prompt_type == \\\"qwen25-math-cot\\\":\\n        # Hotfix to support putting all demos into a single turn\\n        demo_prompt = splitter.join([q + \\\"\\\\n\\\" + a for q, a in demos])\\n    else:\\n        demo_prompt = splitter.join(\\n            [\\n                input_template.format(input=q) + output_template.format(output=a)\\n                for q, a in demos\\n            ]\\n        )\\n    context = input_template.format(input=example[\\\"question\\\"])\\n    if len(demo_prompt) == 0 or (\\n        args.adapt_few_shot and example[\\\"gt_ans\\\"] not in [\\\"A\\\", \\\"B\\\", \\\"C\\\", \\\"D\\\", \\\"E\\\"]\\n    ):\\n        full_prompt = context\\n    else:\\n        if args.prompt_type == \\\"qwen25-math-cot\\\":\\n            # Hotfix to supportting put all demos into a single turn\\n            full_prompt = demo_prompt + splitter + example[\\\"question\\\"]\\n            full_prompt = input_template.format(input=full_prompt)\\n        else:\\n            full_prompt = demo_prompt + splitter + context\\n\\n    if args.prompt_type == \\\"platypus_fs\\\":\\n        full_prompt_temp = (\\n            \\\"Below is an instruction that describes a task. \\\"\\n            \\\"Write a response that appropriately completes the request.\\\\n\\\\n\\\"\\n            \\\"### Instruction:\\\\n{instruction}\\\\n\\\\n### Response:\\\\n\\\"\\n        )\\n        full_prompt = full_prompt_temp.format(instruction=full_prompt)\\n\\n    if prompt_type == \\\"tora\\\":\\n        full_prompt = (\\n            \\\"\\\"\\\"Integrate step-by-step reasoning and Python code to solve math problems using the following guidelines:\\n\\n- Analyze the question and write functions to solve the problem; the function should not take any arguments.\\n- Present the final result in LaTeX using a `\\\\boxed{}` without any units.\\n- Utilize the `pi` symbol and `Rational`` from Sympy for $\\\\pi$ and fractions, and simplify all fractions and square roots without converting them to decimal values.\\n\\nHere are some examples you may refer to:\\n\\n---\\n\\n\\\"\\\"\\\"\\n            + full_prompt\\n        )\\n\\n    return full_prompt.strip(\\\" \\\")  # important!\\n\\n\\nkey_map = {\\n    \\\"gt\\\": \\\"Ground Truth\\\",\\n    \\\"pred\\\": \\\"Prediction\\\",\\n    \\\"gt_cot\\\": \\\"Reference CoT\\\",\\n    \\\"score\\\": \\\"Score\\\",\\n}\\n\\n\\ndef show_sample(sample, print_all_preds=False):\\n    print(\\\"==\\\" * 20)\\n    for key in [\\\"idx\\\", \\\"type\\\", \\\"level\\\", \\\"dataset\\\"]:\\n        if key in sample:\\n            # capitalize\\n            print(\\\"{}: {}\\\".format(key[0].upper() + key[1:], sample[key]))\\n    print(\\\"Question:\\\", repr(sample[\\\"question\\\"]))\\n    if \\\"code\\\" in sample:\\n        if print_all_preds:\\n            for code in sample[\\\"code\\\"]:\\n                print(\\\"-\\\" * 20)\\n                print(\\\"code:\\\", code)\\n            print(\\\"Execution:\\\", sample[\\\"report\\\"])\\n        else:\\n            print(\\\"Solution:\\\\n\\\", sample[\\\"code\\\"][0])\\n            print(\\\"Execution:\\\", sample[\\\"report\\\"][0])\\n    if \\\"pred\\\" in sample:\\n        print(\\\"Prediction:\\\", repr(sample[\\\"pred\\\"][0]))\\n    for key in [\\\"gt\\\", \\\"score\\\", \\\"unit\\\", \\\"gt_cot\\\"]:\\n        if key in sample:\\n            _key = key_map.get(key, key)\\n            print(\\\"{}: {}\\\".format(_key, repr(sample[key])))\\n    print()\\n\",\"annotations\":null}",
            "tool_call_id": "toolu_016euwRY9LgNxJPWfTPFMhY2"
        },
        {
            "role": "assistant",
            "content": "Perfect! I can see the prompts in the `PROMPT_TEMPLATES` dictionary. I can see there are two main types that match what the paper mentioned:\n\n1. **Simple Prompt** (like \"abel\"): `\"Question:\\\\n{input}\\\\nAnswer:\\\\nLet's think step by step.\\\\n\"`\n2. **Complex Prompt** (like \"qwen-boxed\"): `\"<|im_start|>user\\\\n{input}\\\\nPlease reason step by step, and put your final answer within \\\\\\\\boxed{{}}.<|im_end|>\\\\n\"`\n\nNow let me check the current LaTeX file to see if the `tcolorbox` package is already imported:",
            "tool_calls": [
                {
                    "id": "toolu_019AJrtjGzhmDrd4XXpNyR1s",
                    "type": "function",
                    "function": {
                        "name": "filesystem-read_file",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace/simplerlcolm25/main.tex\", \"head\": 50}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"\\n\\\\documentclass{article} % For LaTeX2e\\n\\\\PassOptionsToPackage{table,xcdraw,dvipsnames}{xcolor}\\n\\\\usepackage[preprint]{colm2025_conference}\\n\\n\\\\usepackage{microtype}\\n\\\\usepackage{hyperref}\\n\\\\usepackage{url}\\n\\\\usepackage{booktabs}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{lineno}\\n\\\\usepackage{graphicx}\\n% \\\\usepackage{color, colortbl}\\n\\\\usepackage{subfigure}\\n\\\\usepackage{wrapfig}\\n\\\\usepackage{xspace}\\n\\\\usepackage[normalem]{ulem}\\n\\\\usepackage{enumitem}\\n\\n% \\\\usepackage[dvipsnames]{xcolor}    % colors\\n\\n\\\\definecolor{darkblue}{rgb}{0,0,0.5}\\n\\\\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}\\n\\\\definecolor{lightblue}{RGB}{221,235,247}\\n\\\\newcommand{\\\\ourmethod}{SimpleRL-Zoo\\\\xspace}\\n\\\\usepackage[most]{tcolorbox}\\n\\n\\\\title{SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild}\\n% \\\\title{SimpleRL: On Emerging Reasoning with Reinforcement Learning for Diverse Open Models -- Insights and Practices}\\n\\n% Authors must not appear in the submitted version. They should be hidden\\n% as long as the \\\\colmfinalcopy macro remains commented out below.\\n% Non-anonymous submissions will be rejected without review.\\n\\n\\\\author{Weihao Zeng\\\\thanks{Equal Contribution. Correspondence to Weihao Zeng (wzengak@connect.ust.hk), Yuzhen Huang (yhuanghj@cse.ust.hk), and Junxian He (junxianh@cse.ust.hk).}\\\\hspace{4pt}$^1$ \\\\quad Yuzhen Huang$^{*1}$ \\\\quad Qian Liu$^{*2}$ \\\\quad\\nWei Liu$^1$ \\\\quad\\nKeqing He$^3$ \\\\\\\\ \\\\textbf{Zejun Ma$^2$ \\\\quad Junxian He$^1$}\\\\\\\\\\n$^1$HKUST \\\\quad $^2$TikTok \\\\quad $^3$Meituan\\\\\\\\\\n\\\\href{https://github.com/hkust-nlp/simpleRL-reason}{https://github.com/hkust-nlp/simpleRL-reason}\\n% Pittsburgh, PA 15213, USA \\\\\\\\\\n% \\\\texttt{\\\\{hippo,brain,jen\\\\}@cs.cranberry-lemon.edu} \\\\\\\\\\n% \\\\And\\n% Ji Q. Ren \\\\& Yevgeny LeNet \\\\\\\\\\n% Department of Computational Neuroscience \\\\\\\\\\n% University of the Witwatersrand \\\\\\\\\\n% Joburg, South Africa \\\\\\\\\\n% \\\\texttt{\\\\{robot,net\\\\}@wits.ac.za} \\\\\\\\\\n% \\\\AND\\n% Coauthor \\\\\\\\\\n% Affiliation \\\\\\\\\",\"annotations\":null}",
            "tool_call_id": "toolu_019AJrtjGzhmDrd4XXpNyR1s"
        },
        {
            "role": "assistant",
            "content": "Great! I can see that `tcolorbox` is already imported with `\\usepackage[most]{tcolorbox}` on line 25. Now I need to define the colors used in M-STAR. Let me check what colors are already defined:",
            "tool_calls": [
                {
                    "id": "toolu_01GGLUoeETp35HhMaogXYV6b",
                    "type": "function",
                    "function": {
                        "name": "filesystem-read_file",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace/simplerlcolm25/main.tex\", \"head\": 30, \"tail\": 30}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Error: Cannot specify both head and tail parameters simultaneously\",\"annotations\":null}",
            "tool_call_id": "toolu_01GGLUoeETp35HhMaogXYV6b"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01YXWeAFHZWWJjZZThpFcCr4",
                    "type": "function",
                    "function": {
                        "name": "terminal-run_command",
                        "arguments": "{\"command\": \"grep -n \\\"definecolor\\\" /workspace/dumps/workspace/simplerlcolm25/main.tex\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "[{\"type\": \"text\", \"text\": \"22:\\\\definecolor{darkblue}{rgb}{0,0,0.5}\\n24:\\\\definecolor{lightblue}{RGB}{221,235,247}\\n\", \"annotations\": null}, {\"type\": \"text\", \"text\": \"\\nCommand completed with return code: 0\", \"annotations\": null}]",
            "tool_call_id": "toolu_01YXWeAFHZWWJjZZThpFcCr4"
        },
        {
            "role": "assistant",
            "content": "Now I need to add the color definition used in M-STAR and create the prompt boxes. Based on the M-STAR paper, I need to add the `lightProxYellow` color. Let me create the complete TODO section:",
            "tool_calls": [
                {
                    "id": "toolu_01QPp9WLj3AvgcAvJGcBpbZW",
                    "type": "function",
                    "function": {
                        "name": "filesystem-edit_file",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace/simplerlcolm25/main.tex\", \"edits\": [{\"oldText\":\"\\\\definecolor{lightblue}{RGB}{221,235,247}\\n\\\\newcommand{\\\\ourmethod}{SimpleRL-Zoo\\\\xspace}\\n\\\\usepackage[most]{tcolorbox}\",\"newText\":\"\\\\definecolor{lightblue}{RGB}{221,235,247}\\n\\\\definecolor{lightProxYellow}{HTML}{ffbb00}\\n\\\\newcommand{\\\\ourmethod}{SimpleRL-Zoo\\\\xspace}\\n\\\\usepackage[most]{tcolorbox}\"}]}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"```diff\\nIndex: /workspace/dumps/workspace/simplerlcolm25/main.tex\\n===================================================================\\n--- /workspace/dumps/workspace/simplerlcolm25/main.tex\\toriginal\\n+++ /workspace/dumps/workspace/simplerlcolm25/main.tex\\tmodified\\n@@ -21,8 +21,9 @@\\n \\n \\\\definecolor{darkblue}{rgb}{0,0,0.5}\\n \\\\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}\\n \\\\definecolor{lightblue}{RGB}{221,235,247}\\n+\\\\definecolor{lightProxYellow}{HTML}{ffbb00}\\n \\\\newcommand{\\\\ourmethod}{SimpleRL-Zoo\\\\xspace}\\n \\\\usepackage[most]{tcolorbox}\\n \\n \\\\title{SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild}\\n```\\n\\n\",\"annotations\":null}",
            "tool_call_id": "toolu_01QPp9WLj3AvgcAvJGcBpbZW"
        },
        {
            "role": "assistant",
            "content": "Now let me replace the TODO section in the Appendix.tex file with the prompt boxes:",
            "tool_calls": [
                {
                    "id": "toolu_01R6bBKheR1Hz9Ea9ZAda3e5",
                    "type": "function",
                    "function": {
                        "name": "filesystem-edit_file",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace/simplerlcolm25/Appendix.tex\", \"edits\": [{\"oldText\":\"\\\\section{Model Prompt}\\n% ==============================================\\n% TODO\\n% PLEASE ADD THE PROMPT(s) HERE\\n% DO NOT CHANGE ANY OTHER PART OF THIS FILE OR OTHER FILES\\n% ALL YOUR EDITS SHOULD BE IN THE TODO SECTION\\n% ==============================================\",\"newText\":\"\\\\section{Model Prompt}\\n\\nIn our experiments, we use two types of prompts depending on the model's instruction-following capabilities. For models with weaker instruction-following abilities (Llama-3.1-8B, Mistral-v0.1-7B, and Qwen-2.5-0.5B/1.5B), we employ simple prompts that require only step-by-step reasoning. For models with stronger instruction-following abilities, we use more complex prompts that require the final answers to be placed in boxes.\\n\\n\\\\begin{tcolorbox}[\\ncolback=lightProxYellow!10,\\ncolframe=lightProxYellow,\\nleft=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Simple Prompt}}]\\n\\n\\\\begin{small}\\nQuestion:\\\\\\\\{input}\\\\\\\\Answer:\\\\\\\\Let's think step by step.\\n\\\\end{small}\\n\\n\\\\end{tcolorbox}\\n\\n\\\\begin{tcolorbox}[\\ncolback=lightProxYellow!10,\\ncolframe=lightProxYellow,\\nleft=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Complex Prompt}}]\\n\\n\\\\begin{small}\\n<|im\\\\_start|>user\\\\\\\\{input}\\\\\\\\Please reason step by step, and put your final answer within \\\\\\\\boxed\\\\{\\\\}.<|im\\\\_end|>\\\\\\\\<|im\\\\_start|>assistant\\n\\\\end{small}\\n\\n\\\\end{tcolorbox}\"}]}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"```diff\\nIndex: /workspace/dumps/workspace/simplerlcolm25/Appendix.tex\\n===================================================================\\n--- /workspace/dumps/workspace/simplerlcolm25/Appendix.tex\\toriginal\\n+++ /workspace/dumps/workspace/simplerlcolm25/Appendix.tex\\tmodified\\n@@ -543,10 +543,28 @@\\n         \\\\label{fig:llama3.1-8b_base_subgoal_setting}\\n \\\\end{figure}\\n \\n \\\\section{Model Prompt}\\n-% ==============================================\\n-% TODO\\n-% PLEASE ADD THE PROMPT(s) HERE\\n-% DO NOT CHANGE ANY OTHER PART OF THIS FILE OR OTHER FILES\\n-% ALL YOUR EDITS SHOULD BE IN THE TODO SECTION\\n-% ==============================================\\n\\\\ No newline at end of file\\n+\\n+In our experiments, we use two types of prompts depending on the model's instruction-following capabilities. For models with weaker instruction-following abilities (Llama-3.1-8B, Mistral-v0.1-7B, and Qwen-2.5-0.5B/1.5B), we employ simple prompts that require only step-by-step reasoning. For models with stronger instruction-following abilities, we use more complex prompts that require the final answers to be placed in boxes.\\n+\\n+\\\\begin{tcolorbox}[\\n+colback=lightProxYellow!10,\\n+colframe=lightProxYellow,\\n+left=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Simple Prompt}}]\\n+\\n+\\\\begin{small}\\n+Question:\\\\\\\\{input}\\\\\\\\Answer:\\\\\\\\Let's think step by step.\\n+\\\\end{small}\\n+\\n+\\\\end{tcolorbox}\\n+\\n+\\\\begin{tcolorbox}[\\n+colback=lightProxYellow!10,\\n+colframe=lightProxYellow,\\n+left=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Complex Prompt}}]\\n+\\n+\\\\begin{small}\\n+<|im\\\\_start|>user\\\\\\\\{input}\\\\\\\\Please reason step by step, and put your final answer within \\\\\\\\boxed\\\\{\\\\}.<|im\\\\_end|>\\\\\\\\<|im\\\\_start|>assistant\\n+\\\\end{small}\\n+\\n+\\\\end{tcolorbox}\\n\\\\ No newline at end of file\\n```\\n\\n\",\"annotations\":null}",
            "tool_call_id": "toolu_01R6bBKheR1Hz9Ea9ZAda3e5"
        },
        {
            "role": "assistant",
            "content": "Perfect! I have successfully added the Simple Prompt and Complex Prompt sections to the appendix. Let me verify that the changes look correct by reading the end of the Appendix.tex file:",
            "tool_calls": [
                {
                    "id": "toolu_01QNupXcguA94CFfirwKb7TC",
                    "type": "function",
                    "function": {
                        "name": "filesystem-read_file",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace/simplerlcolm25/Appendix.tex\", \"tail\": 35}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"\\n\\n\\\\begin{figure}[!t]\\n        \\\\centering\\n\\\\includegraphics[width=\\\\columnwidth]{fig/Llama-3.1-8b-subgoal.pdf}\\n\\\\caption{A comparison of Llama-3.1-8B\u2019s ``Subgoal Setting\\\" behavior before and after zero RL training. Here, ``base solution\\\" represents the response of the Llama-3.1-8B base model,\\nwhile ``zero solution\\\" represents the response of the model after training.}\\n        \\\\label{fig:llama3.1-8b_base_subgoal_setting}\\n\\\\end{figure}\\n\\n\\\\section{Model Prompt}\\n\\nIn our experiments, we use two types of prompts depending on the model's instruction-following capabilities. For models with weaker instruction-following abilities (Llama-3.1-8B, Mistral-v0.1-7B, and Qwen-2.5-0.5B/1.5B), we employ simple prompts that require only step-by-step reasoning. For models with stronger instruction-following abilities, we use more complex prompts that require the final answers to be placed in boxes.\\n\\n\\\\begin{tcolorbox}[\\ncolback=lightProxYellow!10,\\ncolframe=lightProxYellow,\\nleft=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Simple Prompt}}]\\n\\n\\\\begin{small}\\nQuestion:\\\\\\\\{input}\\\\\\\\Answer:\\\\\\\\Let's think step by step.\\n\\\\end{small}\\n\\n\\\\end{tcolorbox}\\n\\n\\\\begin{tcolorbox}[\\ncolback=lightProxYellow!10,\\ncolframe=lightProxYellow,\\nleft=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Complex Prompt}}]\\n\\n\\\\begin{small}\\n<|im\\\\_start|>user\\\\\\\\{input}\\\\\\\\Please reason step by step, and put your final answer within \\\\\\\\boxed\\\\{\\\\}.<|im\\\\_end|>\\\\\\\\<|im\\\\_start|>assistant\\n\\\\end{small}\\n\\n\\\\end{tcolorbox}\",\"annotations\":null}",
            "tool_call_id": "toolu_01QNupXcguA94CFfirwKb7TC"
        },
        {
            "role": "assistant",
            "content": "Excellent! Let me also verify that the color definition was added correctly to main.tex:",
            "tool_calls": [
                {
                    "id": "toolu_01Fr18ecDwGVfNQ5WZDJ9EoK",
                    "type": "function",
                    "function": {
                        "name": "filesystem-read_file",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace/simplerlcolm25/main.tex\", \"head\": 30}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"\\n\\\\documentclass{article} % For LaTeX2e\\n\\\\PassOptionsToPackage{table,xcdraw,dvipsnames}{xcolor}\\n\\\\usepackage[preprint]{colm2025_conference}\\n\\n\\\\usepackage{microtype}\\n\\\\usepackage{hyperref}\\n\\\\usepackage{url}\\n\\\\usepackage{booktabs}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{lineno}\\n\\\\usepackage{graphicx}\\n% \\\\usepackage{color, colortbl}\\n\\\\usepackage{subfigure}\\n\\\\usepackage{wrapfig}\\n\\\\usepackage{xspace}\\n\\\\usepackage[normalem]{ulem}\\n\\\\usepackage{enumitem}\\n\\n% \\\\usepackage[dvipsnames]{xcolor}    % colors\\n\\n\\\\definecolor{darkblue}{rgb}{0,0,0.5}\\n\\\\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}\\n\\\\definecolor{lightblue}{RGB}{221,235,247}\\n\\\\definecolor{lightProxYellow}{HTML}{ffbb00}\\n\\\\newcommand{\\\\ourmethod}{SimpleRL-Zoo\\\\xspace}\\n\\\\usepackage[most]{tcolorbox}\\n\\n\\\\title{SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild}\\n% \\\\title{SimpleRL: On Emerging Reasoning with Reinforcement Learning for Diverse Open Models -- Insights and Practices}\",\"annotations\":null}",
            "tool_call_id": "toolu_01Fr18ecDwGVfNQ5WZDJ9EoK"
        },
        {
            "role": "assistant",
            "content": "Perfect! Let me now test if the LaTeX compiles correctly by attempting a quick compilation test:",
            "tool_calls": [
                {
                    "id": "toolu_013iAzYaLbJBhoxuaZHdEYm3",
                    "type": "function",
                    "function": {
                        "name": "terminal-run_command",
                        "arguments": "{\"command\": \"cd /workspace/dumps/workspace/simplerlcolm25 && pdflatex -interaction=nonstopmode main.tex > /dev/null 2>&1 && echo \\\"LaTeX compilation successful\\\" || echo \\\"LaTeX compilation failed\\\"\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Security violation: Invalid command part 'pdflatex -interaction=nonstopmode main.tex': Command 'pdflatex' is not allowed\",\"annotations\":null,\"error\":true}",
            "tool_call_id": "toolu_013iAzYaLbJBhoxuaZHdEYm3"
        },
        {
            "role": "assistant",
            "content": "Since I can't test the LaTeX compilation directly, let me verify that the syntax looks correct by checking the structure. Let me also double-check the prompts I extracted from the code to make sure they match what was described in the paper:",
            "tool_calls": [
                {
                    "id": "toolu_01Coo1qvegoFc9dWnV2Da6AU",
                    "type": "function",
                    "function": {
                        "name": "terminal-run_command",
                        "arguments": "{\"command\": \"grep -A 5 -B 5 \\\"abel\\\\|qwen-boxed\\\" /workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/utils.py\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"Security violation: Invalid command part 'grep -A 5 -B 5 \\\"abel\\\\': Invalid command format: No escaped character\",\"annotations\":null,\"error\":true}",
            "tool_call_id": "toolu_01Coo1qvegoFc9dWnV2Da6AU"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01TwKkL4GPnGGQad9JwyBe76",
                    "type": "function",
                    "function": {
                        "name": "terminal-run_command",
                        "arguments": "{\"command\": \"grep -A 5 -B 5 \\\"abel\\\" /workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/utils.py\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "[{\"type\": \"text\", \"text\": \"    \\\"mmiqc\\\": (\\n        'Please solve the following problem and put your answer at the end with \\\"The answer is: \\\".\\\\n\\\\n{input}\\\\n\\\\n',\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\\n\\\",\\n    ),\\n    \\\"abel\\\": (\\n        \\\"Question:\\\\n{input}\\\\nAnswer:\\\\nLet's think step by step.\\\\n\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\",\\n    ),\\n    \\\"shepherd\\\": (\\\"{input}\\\\n\\\", \\\"{output}\\\", \\\"\\\\n\\\\n\\\\n\\\"),\\n\", \"annotations\": null}, {\"type\": \"text\", \"text\": \"\\nCommand completed with return code: 0\", \"annotations\": null}]",
            "tool_call_id": "toolu_01TwKkL4GPnGGQad9JwyBe76"
        },
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [
                {
                    "id": "toolu_01FBM2WHjxV3rSRg13APfPhj",
                    "type": "function",
                    "function": {
                        "name": "terminal-run_command",
                        "arguments": "{\"command\": \"grep -A 5 -B 5 \\\"qwen-boxed\\\" /workspace/dumps/workspace/simpleRL-reason/examples/simplelr_math_eval/utils.py\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "[{\"type\": \"text\", \"text\": \"        \\\"Question:\\\\n{input}\\\\nAnswer:\\\\nLet's think step by step.\\\\n\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\",\\n    ),\\n    \\\"shepherd\\\": (\\\"{input}\\\\n\\\", \\\"{output}\\\", \\\"\\\\n\\\\n\\\\n\\\"),\\n    \\\"qwen-boxed\\\": (\\n        \\\"<|im_start|>system\\\\nYou are a helpful assistant.<|im_end|>\\\\n\\\"\\n        \\\"<|im_start|>user\\\\n{input}\\\\nPlease reason step by step, and put your final answer within \\\\\\\\boxed{{}}.<|im_end|>\\\\n\\\"\\n        \\\"<|im_start|>assistant\\\\n\\\",\\n        \\\"{output}\\\",\\n        \\\"\\\\n\\\\n\\\",\\n\", \"annotations\": null}, {\"type\": \"text\", \"text\": \"\\nCommand completed with return code: 0\", \"annotations\": null}]",
            "tool_call_id": "toolu_01FBM2WHjxV3rSRg13APfPhj"
        },
        {
            "role": "assistant",
            "content": "I see that the \"qwen-boxed\" prompt is more complex than what I used. Let me update the Complex Prompt to match exactly what's in the codebase:",
            "tool_calls": [
                {
                    "id": "toolu_01EgorDnh3zG9f36fruc6ThF",
                    "type": "function",
                    "function": {
                        "name": "filesystem-edit_file",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace/simplerlcolm25/Appendix.tex\", \"edits\": [{\"oldText\":\"\\\\begin{tcolorbox}[\\ncolback=lightProxYellow!10,\\ncolframe=lightProxYellow,\\nleft=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Complex Prompt}}]\\n\\n\\\\begin{small}\\n<|im\\\\_start|>user\\\\\\\\{input}\\\\\\\\Please reason step by step, and put your final answer within \\\\\\\\boxed\\\\{\\\\}.<|im\\\\_end|>\\\\\\\\<|im\\\\_start|>assistant\\n\\\\end{small}\\n\\n\\\\end{tcolorbox}\",\"newText\":\"\\\\begin{tcolorbox}[\\ncolback=lightProxYellow!10,\\ncolframe=lightProxYellow,\\nleft=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Complex Prompt}}]\\n\\n\\\\begin{small}\\n<|im\\\\_start|>system\\\\\\\\You are a helpful assistant.<|im\\\\_end|>\\\\\\\\<|im\\\\_start|>user\\\\\\\\{input}\\\\\\\\Please reason step by step, and put your final answer within \\\\\\\\boxed\\\\{\\\\}.<|im\\\\_end|>\\\\\\\\<|im\\\\_start|>assistant\\n\\\\end{small}\\n\\n\\\\end{tcolorbox}\"}]}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"```diff\\nIndex: /workspace/dumps/workspace/simplerlcolm25/Appendix.tex\\n===================================================================\\n--- /workspace/dumps/workspace/simplerlcolm25/Appendix.tex\\toriginal\\n+++ /workspace/dumps/workspace/simplerlcolm25/Appendix.tex\\tmodified\\n@@ -563,8 +563,8 @@\\n colframe=lightProxYellow,\\n left=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Complex Prompt}}]\\n \\n \\\\begin{small}\\n-<|im\\\\_start|>user\\\\\\\\{input}\\\\\\\\Please reason step by step, and put your final answer within \\\\\\\\boxed\\\\{\\\\}.<|im\\\\_end|>\\\\\\\\<|im\\\\_start|>assistant\\n+<|im\\\\_start|>system\\\\\\\\You are a helpful assistant.<|im\\\\_end|>\\\\\\\\<|im\\\\_start|>user\\\\\\\\{input}\\\\\\\\Please reason step by step, and put your final answer within \\\\\\\\boxed\\\\{\\\\}.<|im\\\\_end|>\\\\\\\\<|im\\\\_start|>assistant\\n \\\\end{small}\\n \\n \\\\end{tcolorbox}\\n\\\\ No newline at end of file\\n```\\n\\n\",\"annotations\":null}",
            "tool_call_id": "toolu_01EgorDnh3zG9f36fruc6ThF"
        },
        {
            "role": "assistant",
            "content": "Perfect! Now let me verify the final result by reading the complete Model Prompt section:",
            "tool_calls": [
                {
                    "id": "toolu_01DH9YjivrCBfZLVya18G8ep",
                    "type": "function",
                    "function": {
                        "name": "filesystem-read_file",
                        "arguments": "{\"path\": \"/workspace/dumps/workspace/simplerlcolm25/Appendix.tex\", \"tail\": 25}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "{\"type\":\"text\",\"text\":\"\\\\section{Model Prompt}\\n\\nIn our experiments, we use two types of prompts depending on the model's instruction-following capabilities. For models with weaker instruction-following abilities (Llama-3.1-8B, Mistral-v0.1-7B, and Qwen-2.5-0.5B/1.5B), we employ simple prompts that require only step-by-step reasoning. For models with stronger instruction-following abilities, we use more complex prompts that require the final answers to be placed in boxes.\\n\\n\\\\begin{tcolorbox}[\\ncolback=lightProxYellow!10,\\ncolframe=lightProxYellow,\\nleft=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Simple Prompt}}]\\n\\n\\\\begin{small}\\nQuestion:\\\\\\\\{input}\\\\\\\\Answer:\\\\\\\\Let's think step by step.\\n\\\\end{small}\\n\\n\\\\end{tcolorbox}\\n\\n\\\\begin{tcolorbox}[\\ncolback=lightProxYellow!10,\\ncolframe=lightProxYellow,\\nleft=2mm, right=2mm,title=\\\\textcolor{black}{\\\\textbf{Complex Prompt}}]\\n\\n\\\\begin{small}\\n<|im\\\\_start|>system\\\\\\\\You are a helpful assistant.<|im\\\\_end|>\\\\\\\\<|im\\\\_start|>user\\\\\\\\{input}\\\\\\\\Please reason step by step, and put your final answer within \\\\\\\\boxed\\\\{\\\\}.<|im\\\\_end|>\\\\\\\\<|im\\\\_start|>assistant\\n\\\\end{small}\\n\\n\\\\end{tcolorbox}\",\"annotations\":null}",
            "tool_call_id": "toolu_01DH9YjivrCBfZLVya18G8ep"
        },
        {
            "role": "assistant",
            "content": "Excellent! Let me also verify that the color definition is properly added to the main.tex file:",
            "tool_calls": [
                {
                    "id": "toolu_019xDo31HTNWtyCcEhnD9mAu",
                    "type": "function",
                    "function": {
                        "name": "terminal-run_command",
                        "arguments": "{\"command\": \"grep -n \\\"lightProxYellow\\\" /workspace/dumps/workspace/simplerlcolm25/main.tex\"}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "[{\"type\": \"text\", \"text\": \"25:\\\\definecolor{lightProxYellow}{HTML}{ffbb00}\\n\", \"annotations\": null}, {\"type\": \"text\", \"text\": \"\\nCommand completed with return code: 0\", \"annotations\": null}]",
            "tool_call_id": "toolu_019xDo31HTNWtyCcEhnD9mAu"
        },
        {
            "role": "assistant",
            "content": "Perfect! Now let me create a summary of what I've accomplished:\n\n## Summary\n\nI have successfully completed the task by adding the Simple Prompt and Complex Prompt sections to the appendix of your LaTeX paper. Here's what I did:\n\n### 1. **Added the required color definition**\n- Added `\\definecolor{lightProxYellow}{HTML}{ffbb00}` to `main.tex` (line 25)\n- This color matches the one used in M-STAR paper for consistent styling\n\n### 2. **Replaced the TODO section in Appendix.tex**\n- Located the TODO section at the end of the appendix\n- Replaced it with a complete \"Model Prompt\" section that includes:\n  - Explanatory text describing the two types of prompts used\n  - Two `tcolorbox` environments styled exactly like M-STAR\n\n### 3. **Created the prompt boxes**\n- **Simple Prompt box**: Contains the \"abel\" prompt template from your codebase:\n  ```\n  Question:\\{input}\\Answer:\\Let's think step by step.\n  ```\n- **Complex Prompt box**: Contains the \"qwen-boxed\" prompt template from your codebase:\n  ```\n  <|im_start|>system\\You are a helpful assistant.<|im_end|>\\<|im_start|>user\\{input}\\Please reason step by step, and put your final answer within \\boxed{}.<|im_end|>\\<|im_start|>assistant\n  ```\n\n### 4. **Maintained exact styling from M-STAR**\n- Used the same `tcolorbox` styling with `lightProxYellow` color scheme\n- Same margins, frame style, and title formatting\n- Used `\\\\` for line breaks as requested\n- Preserved all existing formatting and colors\n\n### 5. **Key features of the implementation**:\n- \u2705 Only edited the TODO section as requested\n- \u2705 No other parts of the paper were modified\n- \u2705 Used exact box style from M-STAR paper\n- \u2705 Proper package import (`tcolorbox` was already available)\n- \u2705 Consistent color definition added only where necessary\n- \u2705 Used actual prompts from your codebase\n- \u2705 Proper LaTeX formatting with `\\\\` for line breaks\n- \u2705 No file names were changed\n\nThe prompts are now displayed in professional-looking boxes that match the M-STAR paper's style, and readers can clearly see the difference between the simple and complex prompts used in your experiments.",
            "tool_calls": [
                {
                    "id": "toolu_012ztQjaDYNK7FPDyJhkrxN3",
                    "type": "function",
                    "function": {
                        "name": "local-claim_done",
                        "arguments": ""
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "you have claimed the task is done!",
            "tool_call_id": "toolu_012ztQjaDYNK7FPDyJhkrxN3"
        }
    ]
}